<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd"> 
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="data-paper" dtd-version="1.3"><?properties open_access?><?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?><?DTDIdentifier.IdentifierType public?><?SourceDTD.DTDName A++V2.4.dtd?><?SourceDTD.Version 2.4?><?ConverterInfo.XSLTName springer2nlmx2.xsl?><?ConverterInfo.Version 1?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sci Data</journal-id><journal-id journal-id-type="iso-abbrev">Sci Data</journal-id><journal-title-group><journal-title>Scientific Data</journal-title></journal-title-group><issn pub-type="epub">2052-4463</issn><publisher><publisher-name>Nature Publishing Group UK</publisher-name><publisher-loc>London</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmcid">11014927</article-id><article-id pub-id-type="publisher-id">3193</article-id><article-id pub-id-type="doi">10.1038/s41597-024-03193-4</article-id><article-categories><subj-group subj-group-type="heading"><subject>Data Descriptor</subject></subj-group></article-categories><title-group><article-title>Cataract-1K Dataset for Deep-Learning-Assisted Analysis of Cataract Surgery Videos</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Ghamsarian</surname><given-names>Negin</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author"><name><surname>El-Shabrawi</surname><given-names>Yosuf</given-names></name><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author"><name><surname>Nasirihaghighi</surname><given-names>Sahar</given-names></name><xref ref-type="aff" rid="Aff3">3</xref></contrib><contrib contrib-type="author"><name><surname>Putzgruber-Adamitsch</surname><given-names>Doris</given-names></name><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-3447-2359</contrib-id><name><surname>Zinkernagel</surname><given-names>Martin</given-names></name><xref ref-type="aff" rid="Aff4">4</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-7467-7028</contrib-id><name><surname>Wolf</surname><given-names>Sebastian</given-names></name><xref ref-type="aff" rid="Aff4">4</xref></contrib><contrib contrib-type="author" corresp="yes"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-9218-1704</contrib-id><name><surname>Schoeffmann</surname><given-names>Klaus</given-names></name><address><email>ks@itec.aau.at</email></address><xref ref-type="aff" rid="Aff3">3</xref></contrib><contrib contrib-type="author"><name><surname>Sznitman</surname><given-names>Raphael</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/02k7v4d05</institution-id><institution-id institution-id-type="GRID">grid.5734.5</institution-id><institution-id institution-id-type="ISNI">0000 0001 0726 5157</institution-id><institution>Center for Artificial Intelligence in Medicine (CAIM), Department of Medicine, </institution><institution>University of Bern, </institution></institution-wrap>Bern, Switzerland </aff><aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/007xcwj53</institution-id><institution-id institution-id-type="GRID">grid.415431.6</institution-id><institution-id institution-id-type="ISNI">0000 0000 9124 9231</institution-id><institution>Department of Ophthalmology, </institution><institution>Klinikum Klagenfurt, </institution></institution-wrap>Klagenfurt, Austria </aff><aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/05q9m0937</institution-id><institution-id institution-id-type="GRID">grid.7520.0</institution-id><institution-id institution-id-type="ISNI">0000 0001 2196 3349</institution-id><institution>Department of Information Technology, </institution><institution>University of Klagenfurt, </institution></institution-wrap>Klagenfurt, Austria </aff><aff id="Aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/01q9sj412</institution-id><institution-id institution-id-type="GRID">grid.411656.1</institution-id><institution-id institution-id-type="ISNI">0000 0004 0479 0855</institution-id><institution>Department of Ophthalmology, </institution><institution>Inselspital, </institution></institution-wrap>Bern, Switzerland </aff></contrib-group><pub-date pub-type="epub"><day>12</day><month>4</month><year>2024</year></pub-date><pub-date pub-type="pmc-release"><day>12</day><month>4</month><year>2024</year></pub-date><pub-date pub-type="collection"><year>2024</year></pub-date><volume>11</volume><elocation-id>373</elocation-id><history><date date-type="received"><day>8</day><month>11</month><year>2023</year></date><date date-type="accepted"><day>28</day><month>3</month><year>2024</year></date></history><permissions><copyright-statement>&#x000a9; The Author(s) 2024</copyright-statement><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article&#x02019;s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article&#x02019;s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p></license></permissions><abstract id="Abs1"><p id="Par1">In recent years, the landscape of computer-assisted interventions and post-operative surgical video analysis has been dramatically reshaped by deep-learning techniques, resulting in significant advancements in surgeons&#x02019; skills, operation room management, and overall surgical outcomes. However, the progression of deep-learning-powered surgical technologies is profoundly reliant on large-scale datasets and annotations. In particular, surgical scene understanding and phase recognition stand as pivotal pillars within the realm of computer-assisted surgery and post-operative assessment of cataract surgery videos. In this context, we present the largest cataract surgery video dataset that addresses diverse requisites for constructing computerized surgical workflow analysis and detecting post-operative irregularities in cataract surgery. We validate the quality of annotations by benchmarking the performance of several state-of-the-art neural network architectures for phase recognition and surgical scene segmentation. Besides, we initiate the research on domain adaptation for instrument segmentation in cataract surgery by evaluating cross-domain instrument segmentation performance in cataract surgery videos. The dataset and annotations are publicly available in Synapse.</p></abstract><kwd-group kwd-group-type="npg-subject"><title>Subject terms</title><kwd>Medical imaging</kwd><kwd>Vision disorders</kwd></kwd-group><funding-group><award-group><funding-source><institution>Haag-Streit Foundation Switzerland</institution></funding-source></award-group></funding-group><funding-group><award-group><funding-source><institution>FWF Austrian Science Fund under grant P 32010-N38.</institution></funding-source></award-group></funding-group><custom-meta-group><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#x000a9; Springer Nature Limited 2024</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1"><title>Background &#x00026; Summary</title><p id="Par2">Following the technological advancements in surgery, operation rooms are evolving into intelligent environments. Context-aware systems (CAS) are emerging as pivotal components of this evolution, empowered to advance pre-operative surgical planning<sup><xref ref-type="bibr" rid="CR1">1</xref>&#x02013;<xref ref-type="bibr" rid="CR3">3</xref></sup>, automate skill assessment<sup><xref ref-type="bibr" rid="CR4">4</xref>&#x02013;<xref ref-type="bibr" rid="CR8">8</xref></sup>, support operation room planning<sup><xref ref-type="bibr" rid="CR9">9</xref>&#x02013;<xref ref-type="bibr" rid="CR11">11</xref></sup>, and interpret the surgical context comprehensively<sup><xref ref-type="bibr" rid="CR12">12</xref></sup>. By enabling real-time alerts and offering decision-making support, these systems prove invaluable, especially but not only for less-experienced surgeons. Their capabilities extend to the automatic analysis of surgical videos, encompassing functions like indexing, documentation, and generating post-operative reports<sup><xref ref-type="bibr" rid="CR13">13</xref></sup>. The ever-increasing demand for such automatic systems has sparked machine-learning-based approaches to surgical video analysis.</p><p id="Par3">Cataract Surgery, renowned as the most commonly conducted ophthalmic surgical procedure and one of the most demanding surgeries worldwide, is a major operation where deep learning can be of great benefit. Cataract, characterized by the opacification of the eye&#x02019;s natural lens, is often attributed to aging and leads to impaired visual acuity, reduced brightness, visual distortion, double vision, and color perception degradation. Globally, cataracts stand as the primary cause of blindness<sup><xref ref-type="bibr" rid="CR14">14</xref></sup>. Owing to the aging demographic and increased lifespans, the World Health Organization forecasts a surge in cataract-related blindness cases, estimating the number to reach 40 million by the year 2025<sup><xref ref-type="bibr" rid="CR14">14</xref></sup>. This prevalent disease can be remedied through cataract surgery involving the substitution of the eye&#x02019;s natural lens with a synthetic counterpart known as an intraocular lens (IOL). Advancements in technology have driven the evolution of cataract surgery techniques. This evolution spans from intracapsular cataract extraction (ICCE) in the 1960s and 1970s to extracapsular cataract extraction (ECCE) in the 1980s and 1990s. Today, the primary method involves sutureless small-incision phacoemulsification surgery with an injectable intraocular lens (IOL) implantation. Throughout this paper, the term &#x0201c;Cataract Surgery&#x0201d; is synonymous with &#x0201c;Phacoemulsification Cataract Surgery&#x0201d;. Due to the widespread occurrence of cataract surgery and its substantial influence on patients&#x02019; quality of life, a significant focus has been directed towards the analysis of cataract surgery content using deep learning methodologies over the past decade. In particular, Surgical phase recognition and scene segmentation are joint building blocks in various applications related to cataract surgery video analysis<sup><xref ref-type="bibr" rid="CR13">13</xref></sup>. These applications include but are not limited to relevance detection<sup><xref ref-type="bibr" rid="CR15">15</xref></sup>, relevance-based compression<sup><xref ref-type="bibr" rid="CR16">16</xref></sup>, irregularity detection<sup><xref ref-type="bibr" rid="CR17">17</xref>,<xref ref-type="bibr" rid="CR18">18</xref></sup>, and surgical outcome prediction. The current public datasets for cataract surgery either provide annotations for a particular sub-task such as instrument recognition<sup><xref ref-type="bibr" rid="CR19">19</xref></sup>, scene and relevant anatomical structure segmentation<sup><xref ref-type="bibr" rid="CR15">15</xref>,<xref ref-type="bibr" rid="CR20">20</xref>&#x02013;<xref ref-type="bibr" rid="CR23">23</xref></sup>, or offer small multi-task datasets targeting specific problems such as intraocular lens (IOL) irregularity detection<sup><xref ref-type="bibr" rid="CR17">17</xref></sup>. As a result of the lack of a comprehensive dataset, there exists a considerable gap in exploring deep-learning-based approaches and frameworks to enhance cataract surgery outcomes. To facilitate the development of such systems and models, there is a compelling need for large-scale datasets that encompass multi-task annotations.</p><p id="Par4">This paper introduces the largest cataract surgery video dataset, including 1000 videos of cataract surgery recorded in Klinikum Klagenfurt, Austria, between 2021 and 2023. We provide large-scale ground-truth annotations for the semantic segmentation of different instruments and relevant anatomical structures, as well as surgical phases. Besides, the dataset features two subsets for major irregularities in cataract surgery, which affect surgical workflow, including intraocular lens (IOL) rotation, and pupil contraction in cataract surgery. Together, these 1000 videos, annotated datasets, and irregularity subsets form a complete dataset to empower computer-assisted interventions (CAI) in cataract surgery.</p></sec><sec id="Sec2"><title>Methods</title><p id="Par5">This work is performed under ethics committee approval (EK 28/17) from Ethikkommission K&#x000e4;rnten<sup><xref ref-type="bibr" rid="CR24">24</xref></sup>. All patients have given written consent to the video recording and open publication.</p><sec id="Sec3"><title>Dataset acquisition</title><p id="Par6">Cataract surgery is performed utilizing a binocular microscope, which offers a three-dimensional magnified and illuminated view of the eye, ensuring precise observation of the patient&#x02019;s eye. The surgeon manually adjusts the microscope&#x02019;s focus to optimize visual clarity during the procedure. Additionally, a mounted camera within the microscope captures and archives the entire surgical process, facilitating subsequent analysis for various post-operative purposes.</p></sec><sec id="Sec4"><title>Cataract-1K dataset description</title><p id="Par7">The Cataract-1K dataset consists of 1000 videos of cataract surgeries conducted in the eye clinic of Klinikum Klagenfurt from 2021 to 2023. The videos are recorded using a MediLive Trio Eye device mounted on a ZEISS OPMI Vario microscope. The Cataract-1K dataset comprises videos conducted by surgeons with a cumulative count of completed surgeries ranging from 1,000 to over 40,000 procedures. On average, the videos have a duration of 7.12&#x02009;minutes, with a standard duration of 200&#x02009;seconds. In addition to this large-scale dataset, we provide surgical phase annotations for 56 regular videos and relevant anatomical plus instrument pixel-level annotations for 2256 frames out of 30 cataract surgery videos. Furthermore, we provide a small subset of surgeries with two major irregularities, including &#x0201c;pupil reaction&#x0201d; and &#x0201c;IOL rotation,&#x0201d; to support further research on irregularity detection in cataract surgery. Except for the annotated videos and images, the remaining videos in the Cataract-1K dataset are encoded with a temporal resolution of 25 fps and a spatial resolution of 512&#x02009;&#x000d7;&#x02009;324. Table&#x000a0;<xref rid="Tab1" ref-type="table">1</xref> provides a comparison between the annotated subsets in the Cataract-1K dataset and currently existing datasets for semantic segmentation and phase recognition in cataract surgery. We delineate the challenges and annotation procedures for each subset in the following paragraphs.<table-wrap id="Tab1"><label>Table 1</label><caption><p>Comparison of annotated subsets in the Cataract-1K dataset with existing datasets for semantic segmentation and phase recognition in cataract surgery.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Dataset</th><th>CaDIS<sup><xref ref-type="bibr" rid="CR20">20</xref></sup></th><th>CatRel<sup><xref ref-type="bibr" rid="CR15">15</xref></sup></th><th colspan="2">Cataract-1K</th></tr></thead><tbody><tr><td>Acquisition Time</td><td>2015</td><td>2017&#x02013;2018</td><td colspan="2">2021&#x02013;2023</td></tr><tr><td>Task</td><td>Semantic Segmentation</td><td>Phase Recognition</td><td>Semantic Segmentation</td><td>Phase Recognition</td></tr><tr><td>Videos</td><td>25</td><td>22</td><td>30</td><td>56</td></tr><tr><td>Annotations</td><td>4670</td><td>Frame-wise</td><td>2256 frames</td><td>Frame-wise</td></tr><tr><td>Labels</td><td>Anatomy, Instruments, Other</td><td>Four Relevant Phases vs. Rest</td><td>Anatomy, Instruments</td><td>Thirteen Phases</td></tr><tr><td>Resolution</td><td>960&#x02009;&#x000d7;&#x02009;540</td><td>224&#x02009;&#x000d7;&#x02009;224</td><td>1024&#x02009;&#x000d7;&#x02009;768</td><td>1024&#x02009;&#x000d7;&#x02009;768</td></tr><tr><td>Frame-Rate</td><td>N/A</td><td>25</td><td>N/A</td><td>30</td></tr></tbody></table></table-wrap></p><sec id="Sec5"><title>Phase recognition dataset</title><p id="Par8">Crafting an approach to detect and classify significant phases within these videos, considering frame-by-frame temporal details, presents considerable challenges due to several factors:<list list-type="bullet"><list-item><p id="Par9">As shown in Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>, phase recognition datasets for cataract surgery are extremely imbalanced, as the longest phase (phacoemulsification) and the shortest phase (incision) cover 28.72% and 2.1% of the annotations, respectively.<fig id="Fig1"><label>Fig. 1</label><caption><p>Total duration of the annotated phases in the 56 annotated cataract surgery videos (in seconds).</p></caption><graphic xlink:href="41597_2024_3193_Fig1_HTML" id="d33e439"/></fig></p></list-item><list-item><p id="Par10">Videos may exhibit defocus blur stemming from manual camera focus adjustments<sup><xref ref-type="bibr" rid="CR25">25</xref></sup>.</p></list-item><list-item><p id="Par11">Unintentional eye movements and rapid instrument motions close to the camera result in motion blur, impairing distinctive spatial details.</p></list-item><list-item><p id="Par12">As illustrated in Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref>, instruments, which play a fundamental role in distinguishing between relevant phases, share a substantial resemblance in certain phases, leading to a narrow variation between different classes in a trained classification model.<fig id="Fig2"><label>Fig. 2</label><caption><p>Sample frames from different phases in a regular cataract surgery.</p></caption><graphic xlink:href="41597_2024_3193_Fig2_HTML" id="d33e462"/></fig></p></list-item><list-item><p id="Par13">Lack of metadata in stored videos precludes additional contextual information.</p></list-item><list-item><p id="Par14">Variances in patients&#x02019; eye visuals generate substantial inter-video distribution disparities, demanding ample training data to build networks with generalizable performance.</p></list-item></list></p><p id="Par15">As shown in Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref>, regular cataract surgery can include twelve action phases, including incision, viscoelastic, capsulorhexis, hydrodissection, phacoemulsification, irrigation-aspiration, capsule polishing, lens implantation, lens positioning, viscoelastic-suction, anterior-chamber flushing, and tonifying/antibiotics. Besides, the idle phases refer to the time spans in the middle of a phase or between two phases when the surgeons mainly change the instruments and no instrument is visible inside the frames. We provide a large annotated dataset to enable comprehensive studies on deep-learning-based phase recognition in cataract surgery videos. Table&#x000a0;<xref rid="Tab2" ref-type="table">2</xref> visualizes the phase annotations corresponding to 56 regular cataract surgery videos, with a spatial resolution of 1024&#x02009;&#x000d7;&#x02009;768, a temporal resolution of 30 fps, and an average duration of 6.45&#x02009;minutes with a standard deviation of 2.04&#x02009;minutes. This dataset comprises patients with an average age of 75 years, ranging from 51 to 93 years, and a standard deviation of 8.69 years. The videos present in the phase recognition dataset correspond to surgeries executed by surgeons with an average experience of 8929 surgeries and a standard deviation of 6350 surgeries.<table-wrap id="Tab2"><label>Table 2</label><caption><p>Visualizations of phase annotations for 56 normal cataract surgeries. The durations of the videos are different and normalized for better visualization.</p></caption><table frame="hsides" rules="groups"><tbody><tr><td colspan="2"><inline-graphic xlink:href="41597_2024_3193_Tab1_HTML.gif" id="d33e490"/></td></tr></tbody></table></table-wrap></p></sec><sec id="Sec6"><title>Semantic segmentation dataset</title><p id="Par16">Figure&#x000a0;<xref rid="Fig3" ref-type="fig">3</xref> visualizes pixel-level annotations for relevant anatomical objects and instruments. As illustrated in Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3</xref>, semantic segmentation in cataract surgery videos poses the following challenges:<list list-type="bullet"><list-item><p id="Par17">Variations in color, shape, size, and texture in pupil.</p></list-item><list-item><p id="Par18">Transparency and deformations in the artificial lens,</p></list-item><list-item><p id="Par19">Smooth edges and color variations in iris,</p></list-item><list-item><p id="Par20">Occlusion, motion blur, reflection, and partly visibility in instruments,</p></list-item><list-item><p id="Par21">Visual similarities between different instruments in case of multi-class instrument segmentation,</p></list-item></list><fig id="Fig3"><label>Fig. 3</label><caption><p>Visualization of pixel-based annotations corresponding to relevant anatomical structures and instruments in cataract surgery and the challenges associated with different objects.</p></caption><graphic xlink:href="41597_2024_3193_Fig3_HTML" id="d33e523"/></fig></p><p id="Par22">The semantic segmentation dataset includes frames from 30 regular cataract surgery videos with a spatial resolution of 1024&#x02009;&#x000d7;&#x02009;768 and an average duration of 6.52&#x02009;minutes with a standard deviation of two minutes. Frame extraction is performed at the rate of one frame per five seconds. Subsequently, the frames featuring very harsh motion blur or out-of-scene iris are excluded from the dataset. We provide pixel-level annotations for three relevant anatomical structures, including the iris, pupil, and intraocular lens, as well as nine instruments used in regular cataract surgeries, including slit/incision knife, gauge, spatula, capsulorhexis cystome, phacoemulsifier tip, irrigation-aspiration, lens injector, capsulorhexis forceps, and katana forceps. All annotations are performed using polygons in the <ext-link ext-link-type="uri" xlink:href="https://supervisely.com/">Supervisely platform</ext-link>, and exported as JSON files. Within this dataset, the included individuals possess an average age of 74.5 years, spanning from 51 to 90 years, with a standard deviation of 8.43 years. Additionally, the videos contained in the semantic segmentation dataset depict surgeries conducted by surgeons whose collective experience averages 8033 surgeries, with a standard deviation of 3894 surgeries. The provided dataset enables a reliable study of segmentation performance for relevant anatomical structures, binary instruments, and multi-class instruments.</p></sec><sec id="Sec7"><title>Irregularity detection dataset</title><p id="Par23">This dataset contains two small subsets of major intra-operative irregularities in cataract surgery, including pupil reaction<sup><xref ref-type="bibr" rid="CR18">18</xref></sup> and lens rotation<sup><xref ref-type="bibr" rid="CR17">17</xref></sup>.<list list-type="bullet"><list-item><p id="Par24"><italic>Pupil Contraction:</italic> During the phacoemulsification phase, where the occluded natural lens is fragmented and suctioned, there exists a heightened risk of causing damage to the delicate iris. Even very subtle trauma to the tissue can lead to undesirable pupil constriction<sup><xref ref-type="bibr" rid="CR26">26</xref></sup>. These sudden reactions in pupil size can lead to serious intra-operative implications. Especially during the phacoemulsification phase, where the instrument is deeply inserted inside the eye, sudden changes in pupil size may lead to injuries to the eye&#x000e2;&#x020ac;&#x02122;s tender tissues. Besides, achieving precise IOL alignment or centration becomes challenging in cases where intraoperative pupil contraction (miosis) occurs. Particularly in multifocal IOLs, minor displacements or tilts, which might be negligible for conventional mono-focal IOLs, can significantly compromise visual performance. In the case of toric IOLs, precise alignment of the torus is crucial, as any deviation diminishes the IOL&#x02019;s effectiveness. Detection of unusual pupil reactions and severe pupil contractions during the surgery can highly contribute to the overall outcomes of cataract surgery and provide important insight for further post-operative investigations. Figure&#x000a0;<xref rid="Fig4" ref-type="fig">4</xref>-top demonstrates an example of severe pupil contraction during cataract surgery. Pupil contraction can be automatically detected via accurate segmentation of the pupil and cornea, and tracking the relative area of the pupil over time<sup><xref ref-type="bibr" rid="CR18">18</xref></sup>.<fig id="Fig4"><label>Fig. 4</label><caption><p>Intra-operative irregularities in cataract surgery.</p></caption><graphic xlink:href="41597_2024_3193_Fig4_HTML" id="d33e565"/></fig></p></list-item><list-item><p id="Par25"><italic>IOL Rotation:</italic> Although aligned and centered upon surgery&#x02019;s conclusion, the IOL may rotate or dislocate following the surgery. Even slight deviations, such as minor misalignments of the torus in toric IOLs or the slight displacement and tilting of multifocal IOLs, can result in significant distortions in vision and leave patients dissatisfied. The sole way to address this postoperative complication is follow-up surgery, which entails added costs, heightened surgical risks, and patient discomfort. Identification of intra-operative indicators for predicting and preventing post-surgical IOL dislocation is an unmet clinical need. It is argued that intra-operative rotation of IOLs during cataract surgery is the leading cause of post-operative misalignments<sup><xref ref-type="bibr" rid="CR27">27</xref></sup>. Hence, automatic detection and measurement of intra-operative lens rotations can effectively contribute to preventing post-operative IOL dislocation. Figure&#x000a0;<xref rid="Fig4" ref-type="fig">4</xref>-bottom represents fast clockwise rotations of IOL during unfolding, which occur in less than seven seconds. While intra-operative IOL rotation is a serious irregularity, its occurrence within cataract surgery videos is relatively infrequent. Consequently, conventional classification techniques designed to discriminate videos exhibiting IOL rotation struggle due to the considerable class imbalance present in the training data. Indeed, lens rotation computation entails more complicated frameworks and accurate computation of lens rotation necessitates more intricate methodologies. In our extensive investigation into predicting post-operative IOL dislocation, we have introduced, implemented, and assessed a robust framework for precisely calculating IOL rotation. This framework incorporates advanced techniques such as phase recognition, semantic segmentation, and object localization networks to precisely measure the sum of absolute rotations of the IOL after lens unfolding<sup><xref ref-type="bibr" rid="CR28">28</xref></sup>.</p></list-item></list></p></sec></sec><sec id="Sec8"><title>Experimental settings for phase recognition</title><sec id="Sec9"><title>Network architectures</title><p id="Par26">We adopt a combined CNN-RNN framework for phase recognition. The CNN component, serving as the backbone model, is responsible for the extraction of distinctive features from individual frames within the video sequence. To achieve this, two different pre-trained CNN architectures, VGG16 and ResNet50, are employed. The output feature map of the CNN is fed into a recurrent neural network (RNN). The RNN component focuses on capturing temporal features from the input video clip. We compare the performance of four different RNN architectures, including LSTM, GRU, BiLSTM, and BiGRU.</p></sec><sec id="Sec10"><title>Training settings</title><p id="Par27">We adopt a one-versus-rest strategy to evaluate phase recognition performance<sup><xref ref-type="bibr" rid="CR15">15</xref>,<xref ref-type="bibr" rid="CR29">29</xref></sup>. Accordingly, we segment all videos corresponding to each phase into three-second clips with an overlap of one second. Afterward, the entire dataset is split into two categories: the designated target phase and the remaining phases (the &#x0201c;rest&#x0201d; class). We apply offline augmentations to the videos across all categories. Typically, the number of clips in the target category is significantly lower than in the rest category. To rectify this imbalance problem, we employ a random selection process from the &#x0201c;rest&#x0201d; category, aligning it with the clip count in the target category. This strategy ensures an equivalent number of clips in both classes. The employed augmentations include gamma and contrast adjustments with a factor of 0.5, Gaussian blur with a sigma of 10, random rotation up to 20 degrees, brightness within a range of [&#x02212;0.3, 0.3], and saturation within a range of [0.5, 1.5]. To maximize diversity within our training set, we employ a random sampling strategy during training. Specifically, we configure the network&#x02019;s input sequence to comprise 10 frames randomly selected from 90 frames within each three-second clip. In all settings, the backbone network employed for feature extraction is pre-trained on the ImageNet dataset. The RNN component is constructed with a single recurrent layer comprising 64 units. This is followed by a dense layer with 64 units, and finally, a two-unit layer with a Softmax activation function. To mitigate the risk of overfitting, the last four layers of the CNN component are kept frozen during training, and dropout regularization with a rate of 0.5 is applied to the output feature map of the recurrent layer. All models are trained on 32 videos and tested on non-overlapping clips from the remaining videos. We use a binary cross-entropy loss function and Adam optimizer, a learning rate equal to 0.001, and a batch size of 16. The network&#x02019;s input dimensions are set to 224&#x02009;&#x000d7;&#x02009;224. We compare the performance of the trained models using accuracy and F1 score.</p></sec></sec><sec id="Sec11"><title>Experimental settings for semantic segmentation</title><sec id="Sec12"><title>Network architectures</title><p id="Par28">We perform experiments to validate the robustness of our pixel-level annotations using several state-of-the-art baselines targetting general images, medical images, and surgical videos. The specifications of the baselines are listed in Table&#x000a0;<xref rid="Tab3" ref-type="table">3</xref>.<table-wrap id="Tab3"><label>Table 3</label><caption><p>Specifications of the proposed and alternative approaches.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Model</th><th>Backbone</th><th>Params.</th><th>Upsampling</th><th>Target</th><th>Reference</th></tr></thead><tbody><tr><td>DeepPyramid</td><td>VGG16</td><td>33.57&#x02009;M</td><td>Bilinear</td><td>Medical Images</td><td><sup><xref ref-type="bibr" rid="CR22">22</xref></sup></td></tr><tr><td>Adapt-Net</td><td>VGG16</td><td>24.69&#x02009;M</td><td>Bilinear</td><td>Medical Images</td><td><sup><xref ref-type="bibr" rid="CR17">17</xref></sup></td></tr><tr><td>UNet++</td><td>VGG16</td><td>24.24&#x02009;M</td><td>Bilinear</td><td>Medical Images</td><td><sup><xref ref-type="bibr" rid="CR33">33</xref></sup></td></tr><tr><td>ReCal-Net</td><td>VGG16</td><td>22.93&#x02009;M</td><td>Bilinear</td><td>Medical Images</td><td><sup><xref ref-type="bibr" rid="CR21">21</xref></sup></td></tr><tr><td>CPFNet</td><td>VGG16 | ResNet34</td><td>39.17&#x02009;M | 34.66&#x02009;M</td><td>Bilinear</td><td>Medical Images</td><td><sup><xref ref-type="bibr" rid="CR34">34</xref></sup></td></tr><tr><td>CE-Net</td><td>VGG16 | ResNet34</td><td>33.50&#x02009;M | 29.90&#x02009;M</td><td>Trans Conv</td><td>Medical Images</td><td><sup><xref ref-type="bibr" rid="CR35">35</xref></sup></td></tr><tr><td>FED-Net</td><td>ResNet50</td><td>59.52&#x02009;M</td><td>Trans Conv &#x00026; PixelShuffle</td><td>Liver Lesion</td><td><sup><xref ref-type="bibr" rid="CR36">36</xref></sup></td></tr><tr><td>scSENet</td><td>VGG16 | ResNet34</td><td>22.90&#x02009;M | 25.25&#x02009;M</td><td>Bilinear</td><td>Medical Images</td><td><sup><xref ref-type="bibr" rid="CR37">37</xref></sup></td></tr><tr><td>DeepLabV3+</td><td>ResNet50</td><td>26.68&#x02009;M</td><td>Bilinear</td><td>Scene</td><td><sup><xref ref-type="bibr" rid="CR38">38</xref></sup></td></tr><tr><td>UPerNet</td><td>ResNet50</td><td>51.26&#x02009;M</td><td>Bilinear</td><td>Scene</td><td><sup><xref ref-type="bibr" rid="CR39">39</xref></sup></td></tr><tr><td>U-Net+<sup>1</sup></td><td>VGG16</td><td>22.55&#x02009;M</td><td>Bilinear</td><td>Medical Images</td><td><sup><xref ref-type="bibr" rid="CR40">40</xref></sup></td></tr></tbody></table><table-wrap-foot><p>In &#x0201c;Upsampling&#x0201d; column, &#x0201c;Trans Conv&#x0201d; stands for <italic>Transposed Convolution</italic>.</p><p><sup>1</sup>Note that UNet&#x02009;+&#x02009;is an improved version of UNet, where we use VGG16 as the backbone network and double convolutional blocks (two consecutive convolutions followed by batch normalization and ReLU layers) as decoder modules.</p></table-wrap-foot></table-wrap></p></sec><sec id="Sec13"><title>Training settings</title><p id="Par29">For all neural networks, the backbones are initialized with ImageNet&#x02019;s pre-trained parameters<sup><xref ref-type="bibr" rid="CR30">30</xref></sup>. We train all networks with a batch size of eight and set the initial learning rate to 0.001, which decreases during training using polynomial decay <inline-formula id="IEq1"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$lr=l{r}_{init}\times {\left(1-\frac{iter}{total-iter}\right)}^{0.9}$$\end{document}</tex-math><mml:math id="M2"><mml:mi>l</mml:mi><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mi>l</mml:mi><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>&#x000d7;</mml:mo><mml:msup><mml:mrow><mml:mfenced close=")" open="("><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mfrac><mml:mrow><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>0.9</mml:mn></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="41597_2024_3193_Article_IEq1.gif"/></alternatives></inline-formula>. The input size of the networks is set to 512&#x02009;&#x000d7;&#x02009;512. We apply cropping and random rotation (up to 30 degrees), color jittering (brightness&#x02009;=&#x02009;0.7, contrast&#x02009;=&#x02009;0.7, saturation&#x02009;=&#x02009;0.7), Gaussian blurring, and random sharpening as augmentations during training, and use the <italic>cross entropy log dice</italic> loss during training as in Eq. (<xref rid="Equ1" ref-type="disp-formula">1</xref>),<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathscr{L}}=(\lambda )\times BCE({{\mathscr{X}}}_{true}(i,j),{{\mathscr{X}}}_{pred}(i,j))-(1-\lambda )\times \left(\mathrm{log}\frac{2\sum {{\mathscr{X}}}_{true}\odot {{\mathscr{X}}}_{pred}+\sigma }{\sum {{\mathscr{X}}}_{true}+\sum {{\mathscr{X}}}_{pred}+\sigma }\right),$$\end{document}</tex-math><mml:math id="M4" display="block"><mml:mi mathvariant="script">L</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>&#x003bb;</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x000d7;</mml:mo><mml:mi>B</mml:mi><mml:mi>C</mml:mi><mml:mi>E</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">X</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">X</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mi>&#x003bb;</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x000d7;</mml:mo><mml:mrow><mml:mfenced close=")" open="("><mml:mrow><mml:mn>log</mml:mn><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo mathsize="big">&#x02211;</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">X</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02299;</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">X</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>&#x003c3;</mml:mi></mml:mrow><mml:mrow><mml:mo mathsize="big">&#x02211;</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">X</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mo mathsize="big">&#x02211;</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">X</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>&#x003c3;</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced></mml:mrow><mml:mo>,</mml:mo></mml:math><graphic xlink:href="41597_2024_3193_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq2"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{\mathscr{X}}}_{true}$$\end{document}</tex-math><mml:math id="M6"><mml:msub><mml:mrow><mml:mi mathvariant="script">X</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41597_2024_3193_Article_IEq2.gif"/></alternatives></inline-formula> denotes the ground truth binary mask, and <inline-formula id="IEq3"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{\mathscr{X}}}_{pred}$$\end{document}</tex-math><mml:math id="M8"><mml:msub><mml:mrow><mml:mi mathvariant="script">X</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41597_2024_3193_Article_IEq3.gif"/></alternatives></inline-formula> denotes the predicted mask <inline-formula id="IEq4"><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\left(0\le {{\mathscr{X}}}_{pred}(i,j)\le 1\right)$$\end{document}</tex-math><mml:math id="M10"><mml:mfenced close=")" open="("><mml:mrow><mml:mn>0</mml:mn><mml:mo>&#x02264;</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">X</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x02264;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:math><inline-graphic xlink:href="41597_2024_3193_Article_IEq4.gif"/></alternatives></inline-formula>. The parameter <italic>&#x003bb;</italic>&#x02009;&#x02208;&#x02009;[0, 1] is set to 0.8 in our experiments, and <inline-formula id="IEq5"><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\odot $$\end{document}</tex-math><mml:math id="M12"><mml:mo>&#x02299;</mml:mo></mml:math><inline-graphic xlink:href="41597_2024_3193_Article_IEq5.gif"/></alternatives></inline-formula> refers to the Hadamard product (element-wise multiplication). Besides, the parameter <italic>&#x003c3;</italic> is the Laplacian smoothing factor, which is added to (i) prevent division by zero and (ii) avoid overfitting (in experiments, <italic>&#x003c3;</italic>&#x02009;=&#x02009;1). We compare the performance of baselines using average dice and average intersection over union (IoU).</p></sec></sec></sec><sec id="Sec14"><title>Data Records</title><p id="Par30">All datasets and annotations including the 1000 raw videos, phase recognition set, semantic segmentation set, and irregularity detection set are publicly released in Synapse<sup><xref ref-type="bibr" rid="CR31">31</xref></sup>.</p><p id="Par31">Frame-level annotations for phase recognition are provided in CSV files, determining the first and the last frames for all action phases per video. The preprocessing codes to extract all action and idle phases from a video using the CSV files are provided in the GitHub repository of the paper. Table&#x000a0;<xref rid="Tab2" ref-type="table">2</xref> visualizes our phase annotations for 56 cataract surgery videos. Furthermore, Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref> demonstrates the total duration of the annotations corresponding to each phase from 56 videos.</p><p id="Par32">Pixel-level annotations are provided in two formats: (1) Supervisely format, for which we provide Python codes for mask creation from JSON files, and (2) COCO format, which also provides bounding box annotations for all pixel-level annotated objects. The latter annotations can be used for object localization problems. The preprocessing codes to create training masks for &#x0201c;anatomy plus instrument segmentation&#x0201d;, &#x0201c;binary instrument segmentation&#x0201d;, and &#x0201c;multi-class instrument segmentation&#x0201d; are provided in the GitHub repository of the paper. We have formed five folds with patient-wise separation, meaning every fold consists of the frames corresponding to six distinct videos. Table&#x000a0;<xref rid="Tab4" ref-type="table">4</xref> compares the number of instances and their appearance percentage in the frames. Besides, Table&#x000a0;<xref rid="Tab5" ref-type="table">5</xref> lists the average number of pixels per frame corresponding to each label.<table-wrap id="Tab4"><label>Table 4</label><caption><p>Number of instances and presence in the frames (% of total number of frames in each fold).</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Category</th><th>Class Name</th><th>All Videos</th><th>Fold1</th><th>Fold2</th><th>Fold3</th><th>Fold4</th><th>Fold5</th></tr></thead><tbody><tr><td rowspan="3">Anatomy</td><td>Iris</td><td>2256 (100.0%)</td><td>561 (100.0%)</td><td>459 (100.0%)</td><td>420 (100.0%)</td><td>385 (100.0%)</td><td>431 (100.0%)</td></tr><tr><td>Pupil</td><td>2256 (100.0%)</td><td>561 (100.0%)</td><td>459 (100.0%)</td><td>420 (100.0%)</td><td>385 (100.0%)</td><td>431 (100.0%)</td></tr><tr><td>Intraocular Lens</td><td>537 (23.8%)</td><td>107 (19.07%)</td><td>119 (25.93%)</td><td>102 (24.29%)</td><td>106 (27.53%)</td><td>103 (23.9%)</td></tr><tr><td rowspan="10">Instruments</td><td>Slit/Incision Knife</td><td>50 (2.22%)</td><td>12 (2.14%)</td><td>10 (2.18%)</td><td>12 (2.86%)</td><td>4 (1.04%)</td><td>12 (2.78%)</td></tr><tr><td>Gauge</td><td>426 (18.88%)</td><td>103 (18.36%)</td><td>90 (19.61%)</td><td>79 (18.81%)</td><td>76 (19.74%)</td><td>78 (18.1%)</td></tr><tr><td>Spatula</td><td>728 (32.27%)</td><td>214 (38.15%)</td><td>132 (28.76%)</td><td>148 (35.24%)</td><td>105 (27.27%)</td><td>129 (29.93%)</td></tr><tr><td>Capsulorhexis Cystotome</td><td>85 (3.77%)</td><td>20 (3.57%)</td><td>18 (3.92%)</td><td>12 (2.86%)</td><td>11 (2.86%)</td><td>24 (5.57%)</td></tr><tr><td>Phacoemulsifier Tip</td><td>547 (24.25%)</td><td>148 (26.38%)</td><td>91 (19.83%)</td><td>101 (24.05%)</td><td>95 (24.68%)</td><td>112 (25.99%)</td></tr><tr><td>Irrigation-Aspiration</td><td>456 (20.21%)</td><td>122 (21.75%)</td><td>91 (19.83%)</td><td>98 (23.33%)</td><td>71 (18.44%)</td><td>74 (17.17%)</td></tr><tr><td>Lens Injector</td><td>66 (2.93%)</td><td>14 (2.5%)</td><td>11 (2.4%)</td><td>14 (3.33%)</td><td>13 (3.38%)</td><td>14 (3.25%)</td></tr><tr><td>Capsulorhexis Forceps</td><td>108 (4.79%)</td><td>33 (5.88%)</td><td>21 (4.58%)</td><td>22 (5.24%)</td><td>21 (5.45%)</td><td>11 (2.55%)</td></tr><tr><td>Katena Forceps</td><td>29 (1.29%)</td><td>8 (1.43%)</td><td>3 (0.65%)</td><td>8 (1.9%)</td><td>3 (0.78%)</td><td>7 (1.62%)</td></tr><tr><td>All</td><td>1778 (78.81%)</td><td>462 (82.35%)</td><td>345 (75.16%)</td><td>344 (81.9%)</td><td>296 (76.88%)</td><td>331 (76.8%)</td></tr></tbody></table></table-wrap><table-wrap id="Tab5"><label>Table 5</label><caption><p>Average pixels corresponding to different labels per frame.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Category</th><th>Class Name</th><th>All Videos</th><th>Fold1</th><th>Fold2</th><th>Fold3</th><th>Fold4</th><th>Fold5</th></tr></thead><tbody><tr><td rowspan="3">Anatomy</td><td>Iris</td><td>45939</td><td>41874</td><td>47792</td><td>44867</td><td>47963</td><td>48494</td></tr><tr><td>Pupil</td><td>36013</td><td>38594</td><td>33578</td><td>35900</td><td>35291</td><td>35999</td></tr><tr><td>Intraocular Lens</td><td>9135</td><td>7056</td><td>10017</td><td>9153</td><td>10405</td><td>9748</td></tr><tr><td rowspan="10">Instruments</td><td>Slit/Incision Knife</td><td>1140</td><td>1088</td><td>1179</td><td>1163</td><td>1206</td><td>1086</td></tr><tr><td>Gauge</td><td>299</td><td>222</td><td>337</td><td>454</td><td>168</td><td>326</td></tr><tr><td>Spatula</td><td>2613</td><td>3163</td><td>2078</td><td>2893</td><td>2309</td><td>2466</td></tr><tr><td>Capsulorhexis Cystotome</td><td>5523</td><td>4760</td><td>4773</td><td>6551</td><td>6345</td><td>5580</td></tr><tr><td>Phacoemulsifier Tip</td><td>5230</td><td>4388</td><td>5526</td><td>7646</td><td>4451</td><td>4354</td></tr><tr><td>Irrigation-Aspiration</td><td>1083</td><td>790</td><td>1138</td><td>1311</td><td>1153</td><td>1123</td></tr><tr><td>Lens Injector</td><td>512</td><td>465</td><td>543</td><td>673</td><td>318</td><td>556</td></tr><tr><td>Capsulorhexis Forceps</td><td>172</td><td>288</td><td>104</td><td>225</td><td>23</td><td>176</td></tr><tr><td>Katena Forceps</td><td>823</td><td>906</td><td>678</td><td>1065</td><td>1133</td><td>357</td></tr><tr><td>All</td><td>17397</td><td>16069</td><td>16357</td><td>21981</td><td>17105</td><td>16025</td></tr></tbody></table></table-wrap></p></sec><sec id="Sec15"><title>Technical Validation</title><p id="Par33">In this section, we bolster the quality control of our multi-task annotations by rigorously training several state-of-the-art neural network architectures for each task. We meticulously evaluate the performance of the trained models using relevant metrics to ensure the accuracy and reliability of our annotations.</p><p id="Par34">Table&#x000a0;<xref rid="Tab6" ref-type="table">6</xref> showcases the phase recognition performance of several CNN-RNN architectures. In our evaluations, we have combined the phases of viscoelastic and anterior-chamber flushing due to their shared visual features. The collective findings reveal commendable and satisfactory phase recognition performance across diverse backbones and recurrent network setups. Notably, the incorporation of bidirectional recurrent layers has consistently amplified detection accuracy and F1-Score across all configurations. Furthermore, networks leveraging the ResNet50 backbone display marginally superior performance compared to those utilizing VGG16. This outcome can be attributed to the deeper architecture of ResNet50, facilitating the extraction of intricate features essential for accurate recognition. The results also reveal the distinguishability of different phases in cataract surgery. Precisely, the phacoemulsification phase consistently attains the highest accuracy and F1 score, attributed to the distinctive phacoemulsification instrument and the unique texture of the pupil during this phase. Conversely, the least robust detection performance aligns with the viscoelastic/AC flushing phases, accentuating the visual resemblances shared between these phases and other phases within cataract surgery videos.<table-wrap id="Tab6"><label>Table 6</label><caption><p>Phase recognition performance of several CNN-RNN architectures.</p></caption><table frame="hsides" rules="groups"><tbody><tr><td colspan="2"><inline-graphic xlink:href="41597_2024_3193_Tab2_HTML.gif" id="d33e1603"/></td></tr></tbody></table></table-wrap></p><p id="Par35">Table&#x000a0;<xref rid="Tab7" ref-type="table">7</xref> provides a quantitative analysis of &#x0201c;anatomy plus instrument&#x0201d; segmentation performance for various neural network architectures. The results notably highlight that segmenting the relevant anatomical structures emerges as a comparatively less challenging task than instrument segmentation for all networks. Specifically, the best performance corresponds to pupil segmentation, attributable to its distinct features and sharp boundaries. In contrast, lens segmentation demonstrates relatively lower performance due to its transparent nature and an inherent imbalance issue (outlined in Table&#x000a0;<xref rid="Tab4" ref-type="table">4</xref>). The segments involving instruments, however, confront significant challenges. This class is marked by major distortions, encompassing motion blur, reflections, and occlusions, collectively contributing to the relatively low performance of the networks. The best performance corresponds to the DeepPyramid network with a VGG16 backbone, consistently yielding optimal results across all classes.<table-wrap id="Tab7"><label>Table 7</label><caption><p>Quantitative evaluations of &#x0201c;anatomy plus instrument&#x0201d; segmentation performance for neural network architectures listed in Table&#x000a0;<xref rid="Tab3" ref-type="table">3</xref>.</p></caption><table frame="hsides" rules="groups"><tbody><tr><td colspan="2"><inline-graphic xlink:href="41597_2024_3193_Tab3_HTML.gif" id="d33e1629"/></td></tr></tbody></table></table-wrap></p><p id="Par36">Figure&#x000a0;<xref rid="Fig5" ref-type="fig">5</xref> visually compares the Dice and IoU metrics&#x02019; averages and standard deviations across five folds for the evaluated neural networks. According to the results, DeepPyramid, AdaptNet, and ReCal-Net are the three best-performing networks for anatomy and instrument segmentation in cataract surgery videos.<fig id="Fig5"><label>Fig. 5</label><caption><p>Average and standard deviation of &#x0201c;anatomy plus instrument&#x0201d; segmentation results for neural network architectures listed in Table&#x000a0;<xref rid="Tab3" ref-type="table">3</xref>.</p></caption><graphic xlink:href="41597_2024_3193_Fig5_HTML" id="d33e1645"/></fig></p><p id="Par37">Within Table&#x000a0;<xref rid="Tab8" ref-type="table">8</xref>, a thorough comparison is made between the performance of various neural network architectures concerning intra-domain and cross-domain scenarios. These architectures are trained using our binary instrument annotations. The results clearly indicate statistical differences between Cataract-1k and CaDIS datasets. Concretely, the average dice coefficient for binary instrument segmentation equals 77% within the Cataract-1k dataset. However, this performance metric markedly contracts, remaining confined to around 67% (with AdaptNet illustrating 66.23%) when extended to the CaDIS dataset. This considerable variance starkly underscores the substantial domain shift inherently present between these two datasets. These results demonstrate the necessity of strategically exploring semi-supervised and domain adaptation techniques to elevate instrument segmentation performance in cataract surgery videos with cross-dataset domain shifts<sup><xref ref-type="bibr" rid="CR32">32</xref></sup>.<table-wrap id="Tab8"><label>Table 8</label><caption><p>Single domain and cross-domain binary instrument segmentation performance for neural network architectures listed in Table&#x000a0;<xref rid="Tab3" ref-type="table">3</xref>.</p></caption><table frame="hsides" rules="groups"><tbody><tr><td colspan="2"><inline-graphic xlink:href="41597_2024_3193_Tab4_HTML.gif" id="d33e1672"/></td></tr></tbody></table></table-wrap></p></sec><sec id="Sec16"><title>Usage Notes</title><p id="Par38">The datasets are licensed under CC BY. For further legal details, we kindly request the readers to refer to the complete <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">license terms</ext-link>. Besides, anyone can view the sample videos and images from the dataset and access the GitHub repository for dataset preparation codes.</p></sec></body><back><fn-group><fn><p><bold>Publisher&#x02019;s note</bold> Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></fn></fn-group><ack><title>Acknowledgements</title><p>We would like to thank Daniela Stefanics for helping us in annotating the datasets to the highest quality. This work was funded by Haag-Streit Foundation Switzerland and the FWF Austrian Science Fund under grant P 32010-N38.</p></ack><notes notes-type="author-contribution"><title>Author contributions</title><p>N.G. wrote the original draft. R.S., M.Z., S.W., K.S., Y.E. and D.P. acquired the projects&#x02019; funding. R.S. and K.S. were responsible for the projects&#x02019; supervision. N.G. and K.S. organized the annotation process. Y.E. and D.P. provided expert information on the cataract surgery phases, relevant anatomical structures, and instruments used in regular cataract surgery videos. N.G. and D.P. partly annotated the datasets and reviewed and corrected the remaining annotations. N.G. designed, implemented, and evaluated semantic segmentation experiments of the technical validation. N.G. and S.N. designed phase recognition experiments of technical validation. S.N. implemented and evaluated phase recognition experiments of technical validation. All authors reviewed the manuscript.</p></notes><notes notes-type="data-availability"><title>Code availability</title><p>We provide all code for mask creation using JSON annotations and phase extraction using CSV files, as well as the training IDs for four-fold validation and usage instructions in the GitHub repository of the paper (<ext-link ext-link-type="uri" xlink:href="https://github.com/Negin-Ghamsarian/Cataract-1K">https://github.com/Negin-Ghamsarian/Cataract-1K</ext-link>).</p></notes><notes id="FPar1" notes-type="COI-statement"><title>Competing interests</title><p id="Par39">The authors declare no competing interests.</p></notes><ref-list id="Bib1"><title>References</title><ref id="CR1"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ma</surname><given-names>L</given-names></name><etal/></person-group><article-title>Simulation of postoperative facial appearances via geometric deep learning for efficient orthognathic surgical planning</article-title><source>IEEE Transactions on Medical Imaging</source><year>2023</year><volume>42</volume><fpage>336</fpage><lpage>345</lpage><pub-id pub-id-type="doi">10.1109/TMI.2022.3180078</pub-id><?supplied-pmid 35657829?><pub-id pub-id-type="pmid">35657829</pub-id>
</element-citation></ref><ref id="CR2"><label>2.</label><mixed-citation publication-type="other">Quon, J. <italic>et al</italic>. Deep learning for automated delineation of pediatric cerebral arteries on pre-operative brain magnetic resonance imaging. front surg 2020; <bold>7</bold> (2020).</mixed-citation></ref><ref id="CR3"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xiao</surname><given-names>D</given-names></name><etal/></person-group><article-title>Estimating reference bony shape models for orthognathic surgical planning using 3d point-cloud deep learning</article-title><source>IEEE Journal of Biomedical and Health Informatics</source><year>2021</year><volume>25</volume><fpage>2958</fpage><lpage>2966</lpage><pub-id pub-id-type="doi">10.1109/JBHI.2021.3054494</pub-id><?supplied-pmid 33497345?><pub-id pub-id-type="pmid">33497345</pub-id>
</element-citation></ref><ref id="CR4"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yanik</surname><given-names>E</given-names></name><etal/></person-group><article-title>Deep neural networks for the assessment of surgical skills: A systematic review</article-title><source>The Journal of Defense Modeling and Simulation</source><year>2022</year><volume>19</volume><fpage>159</fpage><lpage>171</lpage><pub-id pub-id-type="doi">10.1177/15485129211034586</pub-id></element-citation></ref><ref id="CR5"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lam</surname><given-names>K</given-names></name><etal/></person-group><article-title>Machine learning for technical skill assessment in surgery: a systematic review</article-title><source>NPJ digital medicine</source><year>2022</year><volume>5</volume><fpage>24</fpage><pub-id pub-id-type="doi">10.1038/s41746-022-00566-0</pub-id><?supplied-pmid 35241760?><pub-id pub-id-type="pmid">35241760</pub-id>
</element-citation></ref><ref id="CR6"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Z</given-names></name><name><surname>Majewicz Fey</surname><given-names>A</given-names></name></person-group><article-title>Deep learning with convolutional neural network for objective skill evaluation in robot-assisted surgery</article-title><source>International journal of computer assisted radiology and surgery</source><year>2018</year><volume>13</volume><fpage>1959</fpage><lpage>1970</lpage><pub-id pub-id-type="doi">10.1007/s11548-018-1860-1</pub-id><?supplied-pmid 30255463?><pub-id pub-id-type="pmid">30255463</pub-id>
</element-citation></ref><ref id="CR7"><label>7.</label><mixed-citation publication-type="other">Wang, Z. &#x00026; Fey, A. M. Satr-dl: improving surgical skill assessment and task recognition in robot-assisted surgery with deep neural networks. In <italic>2018 40th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)</italic>, 1793&#x02013;1796 (IEEE, 2018).</mixed-citation></ref><ref id="CR8"><label>8.</label><mixed-citation publication-type="other">Soleymani, A. <italic>et al</italic>. Surgical skill evaluation from robot-assisted surgery recordings. In <italic>2021 International Symposium on Medical Robotics (ISMR)</italic>, 1&#x02013;6 (IEEE, 2021).</mixed-citation></ref><ref id="CR9"><label>9.</label><mixed-citation publication-type="other">Aksamentov, I., Twinanda, A. P., Mutter, D., Marescaux, J. &#x00026; Padoy, N. Deep neural networks predict remaining surgery duration from cholecystectomy videos. In <italic>Medical Image Computing and Computer-Assisted Intervention- MICCAI 2017: 20th International Conference, Quebec City, QC, Canada, September 11-13, 2017, Proceedings, Part II 20</italic>, 586&#x02013;593 (Springer, 2017).</mixed-citation></ref><ref id="CR10"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Twinanda</surname><given-names>AP</given-names></name><name><surname>Yengera</surname><given-names>G</given-names></name><name><surname>Mutter</surname><given-names>D</given-names></name><name><surname>Marescaux</surname><given-names>J</given-names></name><name><surname>Padoy</surname><given-names>N</given-names></name></person-group><article-title>Rsdnet: Learning to predict remaining surgery duration from laparoscopic videos without manual annotations</article-title><source>IEEE transactions on medical imaging</source><year>2018</year><volume>38</volume><fpage>1069</fpage><lpage>1078</lpage><pub-id pub-id-type="doi">10.1109/TMI.2018.2878055</pub-id><?supplied-pmid 30371356?><pub-id pub-id-type="pmid">30371356</pub-id>
</element-citation></ref><ref id="CR11"><label>11.</label><mixed-citation publication-type="other">Marafioti, A. <italic>et al</italic>. Catanet: predicting remaining cataract surgery duration. In <italic>Medical Image Computing and Computer Assisted Intervention</italic>&#x02013;<italic>MICCAI 2021: 24th International Conference, Strasbourg, France, September 27</italic>&#x02013;<italic>October 1, 2021, Proceedings, Part IV 24</italic>, 426&#x02013;435 (Springer, 2021).</mixed-citation></ref><ref id="CR12"><label>12.</label><mixed-citation publication-type="other">Ghamsarian, N. <italic>Deep-learning-assisted analysis of cataract surgery videos</italic>, (2021).</mixed-citation></ref><ref id="CR13"><label>13.</label><mixed-citation publication-type="other">Ghamsarian, N. Enabling relevance-based exploration of cataract videos. In <italic>Proceedings of the 2020 International Conference on Multimedia Retrieval</italic>, ICMR&#x02019;20, 378&#x02013;382, 10.1145/3372278.3391937 (2020).</mixed-citation></ref><ref id="CR14"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Burton</surname><given-names>MJ</given-names></name><etal/></person-group><article-title>The lancet global health commission on global eye health: vision beyond 2020</article-title><source>The Lancet Global Health</source><year>2021</year><volume>9</volume><fpage>e489</fpage><lpage>e551</lpage><pub-id pub-id-type="doi">10.1016/S2214-109X(20)30488-5</pub-id><?supplied-pmid 33607016?><pub-id pub-id-type="pmid">33607016</pub-id>
</element-citation></ref><ref id="CR15"><label>15.</label><mixed-citation publication-type="other">Ghamsarian, N., Taschwer, M., Putzgruber-Adamitsch, D., Sarny, S. &#x00026; Schoeffmann, K. Relevance detection in cataract surgery videos by spatio- temporal action localization. In <italic>2020 25th International Conference on Pattern Recognition (ICPR)</italic>, 10720&#x02013;10727 (2021).</mixed-citation></ref><ref id="CR16"><label>16.</label><mixed-citation publication-type="other">Ghamsarian, N., Amirpourazarian, H., Timmerer, C., Taschwer, M. &#x00026; Sch&#x000f6;ffmann, K. Relevance-based compression of cataract surgery videos using convolutional neural networks. In <italic>Proceedings of the 28th ACM International Conference on Multimedia</italic>, 3577&#x02013;3585 (2020).</mixed-citation></ref><ref id="CR17"><label>17.</label><mixed-citation publication-type="other">Ghamsarian, N. <italic>et al</italic>. Lensid: A cnn-rnn-based framework towards lens irregularity detection in cataract surgery videos. In de Bruijne, M. <italic>et al</italic>. (eds.) <italic>Medical Image Computing and Computer Assisted Intervention</italic> &#x02013; <italic>MICCAI 2021</italic>, 76&#x02013;86 (Springer International Publishing, Cham, 2021).</mixed-citation></ref><ref id="CR18"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sokolova</surname><given-names>N</given-names></name><etal/></person-group><article-title>Automatic detection of pupil reactions in cataract surgery videos</article-title><source>Plos one</source><year>2021</year><volume>16</volume><fpage>e0258390</fpage><pub-id pub-id-type="doi">10.1371/journal.pone.0258390</pub-id><?supplied-pmid 34673784?><pub-id pub-id-type="pmid">34673784</pub-id>
</element-citation></ref><ref id="CR19"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Al Hajj</surname><given-names>H</given-names></name><etal/></person-group><article-title>Cataracts: Challenge on automatic tool annotation for cataract surgery</article-title><source>Medical image analysis</source><year>2019</year><volume>52</volume><fpage>24</fpage><lpage>41</lpage><pub-id pub-id-type="doi">10.1016/j.media.2018.11.008</pub-id><?supplied-pmid 30468970?><pub-id pub-id-type="pmid">30468970</pub-id>
</element-citation></ref><ref id="CR20"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grammatikopoulou</surname><given-names>M</given-names></name><etal/></person-group><article-title>Cadis: Cataract dataset for surgical rgb-image segmentation</article-title><source>Medical Image Analysis</source><year>2021</year><volume>71</volume><fpage>102053</fpage><pub-id pub-id-type="doi">10.1016/j.media.2021.102053</pub-id><?supplied-pmid 33864969?><pub-id pub-id-type="pmid">33864969</pub-id>
</element-citation></ref><ref id="CR21"><label>21.</label><mixed-citation publication-type="other">Ghamsarian, N. <italic>et al</italic>. Recal-net: Joint region-channel-wise calibrated network for semantic segmentation in cataract surgery videos. In Mantoro, T., Lee, M., Ayu, M. A., Wong, K. W. &#x00026; Hidayanto, A. N. (eds.) <italic>Neural Information Processing</italic>, 391&#x02013;402 (Springer International Publishing, Cham, 2021).</mixed-citation></ref><ref id="CR22"><label>22.</label><mixed-citation publication-type="other">Ghamsarian, N., Taschwer, M., Sznitman, R. &#x00026; Schoeffmann, K. Deeppyramid: Enabling pyramid view and deformable pyramid reception for semantic segmentation in cataract surgery videos. In <italic>International Conference on Medical Image Computing and Computer-Assisted Intervention</italic>, 276&#x02013;286 (Springer, 2022).</mixed-citation></ref><ref id="CR23"><label>23.</label><mixed-citation publication-type="other">Ghamsarian, N., Wolf, S., Zinkernagel, M., Schoeffmann, K. &#x00026; Sznitman, R. Deeppyramid+: medical image segmentation using pyramid view fusion and deformable pyramid reception. <italic>International journal of computer assisted radiology and surgery</italic> 1&#x02013;9 (2024).</mixed-citation></ref><ref id="CR24"><label>24.</label><mixed-citation publication-type="other">Ethikkommission k&#x000e4;rnten. <ext-link ext-link-type="uri" xlink:href="https://www.ethikkommission-kaernten.at/ueber-uns/kommission">https://www.ethikkommission-kaernten.at/ueber-uns/kommission</ext-link></mixed-citation></ref><ref id="CR25"><label>25.</label><mixed-citation publication-type="other">Ghamsarian, N., Taschwer, M. &#x00026; Schoeffmann, K. Deblurring cataract surgery videos using a multi-scale deconvolutional neural network. In <italic>2020 IEEE 17th International Symposium on Biomedical Imaging (ISBI)</italic>, 872&#x02013;876 (2020).</mixed-citation></ref><ref id="CR26"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mirza</surname><given-names>SA</given-names></name><name><surname>Alexandridou</surname><given-names>A</given-names></name><name><surname>Marshall</surname><given-names>T</given-names></name><name><surname>Stavrou</surname><given-names>P</given-names></name></person-group><article-title>Surgically induced miosis during phacoemulsification in patients with diabetes mellitus</article-title><source>Eye</source><year>2003</year><volume>17</volume><fpage>194</fpage><lpage>199</lpage><pub-id pub-id-type="doi">10.1038/sj.eye.6700268</pub-id><?supplied-pmid 12640406?><pub-id pub-id-type="pmid">12640406</pub-id>
</element-citation></ref><ref id="CR27"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oshika</surname><given-names>T</given-names></name><etal/></person-group><article-title>Prospective assessment of plate-haptic rotationally asymmetric multifocal toric intraocular lens with near addition of +1.5 diopters</article-title><source>BMC Ophthalmology</source><year>2020</year><volume>20</volume><fpage>454</fpage><pub-id pub-id-type="doi">10.1186/s12886-020-01731-3</pub-id><?supplied-pmid 33208137?><pub-id pub-id-type="pmid">33208137</pub-id>
</element-citation></ref><ref id="CR28"><label>28.</label><mixed-citation publication-type="other">Ghamsarian, N. <italic>et al</italic>. Predicting postoperative intraocular lens dislocation in cataract surgery via deep learning. <italic>IEEE Access</italic> 1&#x02013;1, 10.1109/ACCESS.2024.3361042 (2024).</mixed-citation></ref><ref id="CR29"><label>29.</label><mixed-citation publication-type="other">Nasirihaghighi, S., Ghamsarian, N., Stefanics, D., Schoeffmann, K. &#x00026; Husslein, H. Action recognition in video recordings from gynecologic laparoscopy. In <italic>2023 IEEE 36th International Symposium on Computer-Based Medical Systems (CBMS)</italic>, 29&#x02013;34, 10.1109/CBMS58004.2023.00187 (2023).</mixed-citation></ref><ref id="CR30"><label>30.</label><mixed-citation publication-type="other">Deng, J. <italic>et al</italic>. Imagenet: A large-scale hierarchical image database. In <italic>2009 IEEE conference on computer vision and pattern recognition</italic>, 248&#x02013;255 (Ieee, 2009).</mixed-citation></ref><ref id="CR31"><label>31.</label><element-citation publication-type="data"><name><surname>Ghamsarian</surname><given-names>N</given-names></name><etal/><year>2024</year><data-title>Cataract-1k</data-title><source>Synapse</source><pub-id pub-id-type="doi">10.7303/syn52540135</pub-id></element-citation></ref><ref id="CR32"><label>32.</label><mixed-citation publication-type="other">Ghamsarian, N. <italic>et al</italic>. Domain adaptation for medical image segmentation using transformation-invariant self-training. In <italic>International Conference on Medical Image Computing and Computer-Assisted Intervention</italic>, 331&#x02013;341 (Springer, 2023).</mixed-citation></ref><ref id="CR33"><label>33.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhou</surname><given-names>Z</given-names></name><name><surname>Siddiquee</surname><given-names>MMR</given-names></name><name><surname>Tajbakhsh</surname><given-names>N</given-names></name><name><surname>Liang</surname><given-names>J</given-names></name></person-group><article-title>Unet++: Redesigning skip connections to exploit multiscale features in image segmentation</article-title><source>IEEE Transactions on Medical Imaging</source><year>2020</year><volume>39</volume><fpage>1856</fpage><lpage>1867</lpage><pub-id pub-id-type="doi">10.1109/TMI.2019.2959609</pub-id><?supplied-pmid 31841402?><pub-id pub-id-type="pmid">31841402</pub-id>
</element-citation></ref><ref id="CR34"><label>34.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Feng</surname><given-names>S</given-names></name><etal/></person-group><article-title>Cpfnet: Context pyramid fusion network for medical image segmentation</article-title><source>IEEE Transactions on Medical Imaging</source><year>2020</year><volume>39</volume><fpage>3008</fpage><lpage>3018</lpage><pub-id pub-id-type="doi">10.1109/TMI.2020.2983721</pub-id><?supplied-pmid 32224453?><pub-id pub-id-type="pmid">32224453</pub-id>
</element-citation></ref><ref id="CR35"><label>35.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gu</surname><given-names>Z</given-names></name><etal/></person-group><article-title>Ce-net: Context encoder network for 2d medical image segmentation</article-title><source>IEEE Transactions on Medical Imaging</source><year>2019</year><volume>38</volume><fpage>2281</fpage><lpage>2292</lpage><pub-id pub-id-type="doi">10.1109/TMI.2019.2903562</pub-id><?supplied-pmid 30843824?><pub-id pub-id-type="pmid">30843824</pub-id>
</element-citation></ref><ref id="CR36"><label>36.</label><mixed-citation publication-type="other">Chen, X., Zhang, R. &#x00026; Yan, P. Feature fusion encoder decoder network for automatic liver lesion segmentation. In <italic>2019 IEEE 16th International Symposium on Biomedical Imaging (ISBI 2019)</italic>, 430&#x02013;433, 10.1109/ISBI.2019.8759555 (2019).</mixed-citation></ref><ref id="CR37"><label>37.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roy</surname><given-names>AG</given-names></name><name><surname>Navab</surname><given-names>N</given-names></name><name><surname>Wachinger</surname><given-names>C</given-names></name></person-group><article-title>Recalibrating fully convolutional networks with spatial and channel &#x0201c;squeeze and excitation&#x0201d; blocks</article-title><source>IEEE Transactions on Medical Imaging</source><year>2019</year><volume>38</volume><fpage>540</fpage><lpage>549</lpage><pub-id pub-id-type="doi">10.1109/TMI.2018.2867261</pub-id><?supplied-pmid 30716024?><pub-id pub-id-type="pmid">30716024</pub-id>
</element-citation></ref><ref id="CR38"><label>38.</label><mixed-citation publication-type="other">Chen, L.-C., Zhu, Y., Papandreou, G., Schroff, F. &#x00026; Adam, H. Encoder-decoder with atrous separable convolution for semantic image segmentation. In <italic>Proceedings of the European conference on computer vision (ECCV)</italic>, 801&#x02013;818 (2018).</mixed-citation></ref><ref id="CR39"><label>39.</label><mixed-citation publication-type="other">Xiao, T., Liu, Y., Zhou, B., Jiang, Y. &#x00026; Sun, J. Unified perceptual parsing for scene understanding. In <italic>Proceedings of the European conference on computer vision (ECCV)</italic>, 418&#x02013;434 (2018).</mixed-citation></ref><ref id="CR40"><label>40.</label><mixed-citation publication-type="other">Ronneberger, O., Fischer, P. &#x00026; Brox, T. U-net: Convolutional networks for biomedical image segmentation. In <italic>Medical Image Computing and Computer-Assisted Intervention</italic> &#x02013; <italic>MICCAI 2015</italic>, 234&#x02013;241 (2015).</mixed-citation></ref></ref-list></back></article>