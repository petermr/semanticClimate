<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd"> 
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><?properties open_access?><?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?><?DTDIdentifier.IdentifierType public?><?SourceDTD.DTDName A++V2.4.dtd?><?SourceDTD.Version 2.4?><?ConverterInfo.XSLTName springer2nlmx2.xsl?><?ConverterInfo.Version 1?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sci Rep</journal-id><journal-id journal-id-type="iso-abbrev">Sci Rep</journal-id><journal-title-group><journal-title>Scientific Reports</journal-title></journal-title-group><issn pub-type="epub">2045-2322</issn><publisher><publisher-name>Nature Publishing Group UK</publisher-name><publisher-loc>London</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmcid">11106303</article-id><article-id pub-id-type="pmid">38769129</article-id>
<article-id pub-id-type="publisher-id">62342</article-id><article-id pub-id-type="doi">10.1038/s41598-024-62342-2</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Time division multiplexing based multi-spectral semantic camera for LiDAR applications</article-title></title-group><contrib-group><contrib contrib-type="author" equal-contrib="yes"><contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-6765-2471</contrib-id><name><surname>Kim</surname><given-names>Sehyeon</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author" equal-contrib="yes"><name><surname>Jeong</surname><given-names>Tae-In</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author"><name><surname>Kim</surname><given-names>San</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author"><name><surname>Choi</surname><given-names>Eunji</given-names></name><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author"><name><surname>Yang</surname><given-names>Eunju</given-names></name><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author"><name><surname>Song</surname><given-names>Munki</given-names></name><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author"><name><surname>Eom</surname><given-names>Tae Joong</given-names></name><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Kim</surname><given-names>Chang-Seok</given-names></name><address><email>ckim@pusan.ac.kr</email></address><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Gliserin</surname><given-names>Alexander</given-names></name><address><email>alex@mpk.or.kr</email></address><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Kim</surname><given-names>Seungchul</given-names></name><address><email>s.kim@pusan.ac.kr</email></address><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff2">2</xref></contrib><aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/01an57a31</institution-id><institution-id institution-id-type="GRID">grid.262229.f</institution-id><institution-id institution-id-type="ISNI">0000 0001 0719 8572</institution-id><institution>Department of Cogno-Mechatronics Engineering, College of Nanoscience and Nanotechnology, </institution><institution>Pusan National University, </institution></institution-wrap>Busan, 46241 Republic of Korea </aff><aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/01an57a31</institution-id><institution-id institution-id-type="GRID">grid.262229.f</institution-id><institution-id institution-id-type="ISNI">0000 0001 0719 8572</institution-id><institution>Department of Optics and Mechatronics Engineering, College of Nanoscience and Nanotechnology, </institution><institution>Pusan National University, </institution></institution-wrap>Busan, 46241 Republic of Korea </aff></contrib-group><pub-date pub-type="epub"><day>20</day><month>5</month><year>2024</year></pub-date><pub-date pub-type="pmc-release"><day>20</day><month>5</month><year>2024</year></pub-date><pub-date pub-type="collection"><year>2024</year></pub-date><volume>14</volume><elocation-id>11445</elocation-id><history><date date-type="received"><day>5</day><month>12</month><year>2023</year></date><date date-type="accepted"><day>15</day><month>5</month><year>2024</year></date></history><permissions><copyright-statement>&#x000a9; The Author(s) 2024</copyright-statement><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p></license></permissions><abstract id="Abs1"><p id="Par1">The recent progress in the development of measurement systems for autonomous recognition had a substantial impact on emerging technology in numerous fields, especially robotics and automotive applications. In particular, time-of-flight (TOF) based light detection and ranging (LiDAR) systems enable to map the surrounding environmental information over long distances and with high accuracy. The combination of advanced LiDAR with an artificial intelligence platform allows enhanced object recognition and classification, which however still suffers from limitations of inaccuracy and misidentification. Recently, multi-spectral LiDAR systems have been employed to increase the object recognition performance by additionally providing material information in the short-wave infrared (SWIR) range where the reflection spectrum characteristics are typically very sensitive to material properties. However, previous multi-spectral LiDAR systems utilized band-pass filters or complex dispersive optical systems and even required multiple photodetectors, adding complexity and cost. In this work, we propose a time-division-multiplexing (TDM) based multi-spectral LiDAR system for semantic object inference by the simultaneous acquisition of spatial and spectral information. By utilizing the TDM method, we enable the simultaneous acquisition of spatial and spectral information as well as a TOF based distance map with minimized optical loss using only a single photodetector. Our LiDAR system utilizes nanosecond pulses of five different wavelengths in the SWIR range to acquire sufficient material information in addition to 3D spatial information. To demonstrate the recognition performance, we map the multi-spectral image from a human hand, a mannequin hand, a fabric gloved hand, a nitrile gloved hand, and a printed human hand onto an RGB-color encoded image, which clearly visualizes spectral differences as RGB color depending on the material while having a similar shape. Additionally, the classification performance of the multi-spectral image is demonstrated with a convolution neural network (CNN) model using the full multi-spectral data set. Our work presents a compact novel spectroscopic LiDAR system, which provides increased recognition performance and thus a great potential to improve safety and reliability in autonomous driving.</p></abstract><kwd-group xml:lang="en"><title>Keyword</title><kwd>Time division multiplexing</kwd><kwd>LiDAR</kwd><kwd>Multi-spectral camera</kwd><kwd>Time of flight</kwd></kwd-group><kwd-group kwd-group-type="npg-subject"><title>Subject terms</title><kwd>Imaging and sensing</kwd><kwd>Near-infrared spectroscopy</kwd></kwd-group><custom-meta-group><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#x000a9; Springer Nature Limited 2024</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1"><title>Introduction</title><p id="Par2">Autonomous recognition systems have made remarkable progress in recent years and have various applications including robotics<sup><xref ref-type="bibr" rid="CR1">1</xref>&#x02013;<xref ref-type="bibr" rid="CR4">4</xref></sup> and driving systems<sup><xref ref-type="bibr" rid="CR5">5</xref>&#x02013;<xref ref-type="bibr" rid="CR8">8</xref></sup>. Autonomous driving solutions have employed camera<sup><xref ref-type="bibr" rid="CR9">9</xref>&#x02013;<xref ref-type="bibr" rid="CR14">14</xref></sup> or light detection and ranging (LiDAR) systems<sup><xref ref-type="bibr" rid="CR15">15</xref>&#x02013;<xref ref-type="bibr" rid="CR19">19</xref></sup> to acquire the surrounding environment information during driving, which provides crucial feedback for navigation and safety strategies such as routing and collision avoidance. State-of-the-art time-of-flight (TOF) based LiDAR systems allow measuring object distances over hundreds of meters with centimeter accuracy<sup><xref ref-type="bibr" rid="CR20">20</xref>,<xref ref-type="bibr" rid="CR21">21</xref></sup>. The utilization of artificial intelligence technology with LiDAR systems remarkably improves the safety and reliability of autonomous driving<sup><xref ref-type="bibr" rid="CR19">19</xref>,<xref ref-type="bibr" rid="CR22">22</xref>&#x02013;<xref ref-type="bibr" rid="CR24">24</xref></sup>; however, the object recognition and classification through shape information alone still suffers from the problem of inaccuracy and misidentification, such as ice on the road<sup><xref ref-type="bibr" rid="CR25">25</xref>,<xref ref-type="bibr" rid="CR26">26</xref></sup> or distinguishing between real people and human shaped objects such as mannequins or 2D pictures.</p><p id="Par3">Recently, multi-spectral LiDAR systems have been introduced to overcome this limitation in object recognition by providing additional material information based on spectroscopic imaging<sup><xref ref-type="bibr" rid="CR27">27</xref>&#x02013;<xref ref-type="bibr" rid="CR32">32</xref></sup>. Especially the reflection spectrum in the short-wave infrared (SWIR) range (900&#x02013;2500&#x000a0;nm wavelength) provides more comprehensive information of the material properties compared to visible-range spectroscopy or simple color imaging of the object<sup><xref ref-type="bibr" rid="CR32">32</xref>&#x02013;<xref ref-type="bibr" rid="CR35">35</xref></sup>. SWIR range multi-spectral LiDAR systems have demonstrated enhanced identification and recognition capabilities by simultaneous acquisition of spatial and spectral information. However, most of the previous multi-spectral LiDAR systems have employed spectrally resolved detection methods by using band-pass filters or complex dispersive optical systems<sup><xref ref-type="bibr" rid="CR31">31</xref>,<xref ref-type="bibr" rid="CR36">36</xref>,<xref ref-type="bibr" rid="CR37">37</xref></sup> which have inherent limitations. As the number of sampled wavelengths increases, not only does the inevitable optical loss increase but also the required multiple photodetectors and spectral separation of the reflected light make the system complex and expensive.</p><p id="Par4">Here, we demonstrate a novel time-division-multiplexing (TDM) based multi-spectral LiDAR system for semantic object inference by the simultaneous acquisition of spatial and spectral information. The employed TDM method, which implements spectroscopy by sampling pulses of different wavelengths in the time domain, not only eliminates the optical loss in dispersive spectroscopy but also provides a simple, compact and cost-effective system. By minimizing the time delay between the pulses of different wavelengths within a TDM burst, all pulses arrive at the same location during a scan, thereby collecting spectral information from the same spot on the object, which simplifies data processing for the object classification and allows maintaining a sufficient scan rate of the LiDAR system. Our TDM based multi-spectral LiDAR system utilizes nanosecond pulse lasers with five different wavelengths (980&#x000a0;nm, 1060&#x000a0;nm, 1310&#x000a0;nm, 1550&#x000a0;nm, and 1650&#x000a0;nm) in the SWIR range covering a bandwidth of 670&#x000a0;nm to acquire sufficient material-dependent differences in reflectance.</p><p id="Par5">For the simultaneous spatial and spectral visualization of the object classification, we assigned individual colors to the spatial intensity maps for each wavelength based on an RGB color model with priority. This approach maps the SWIR spectral information onto RGB colors, allowing for straightforward visual distinction of material properties with the human eye based on color differences. For the proof-of-concept demonstration, we compared such RGB-color encoded images of a human hand, a mannequin hand, a fabric gloved hand, a nitrile gloved hand, and a printed human hand, having similar visible-light colors, but showing clear differences depending on the material. Additionally, a convolution neural network (CNN) framework was trained on the full five-wavelength multi-spectral data set to demonstrate the material classification performance. The validation result of the trained model shows that the multi-spectral image is clearly classified with high accuracy according to the material. Additionally, we demonstrate the distance mapping with our TDM based multi-spectral LiDAR system which shows about 10&#x000a0;cm of ranging accuracy. These results signify a high application potential for advanced autonomous vehicle systems.</p></sec><sec id="Sec2"><title>Results and discussion</title><sec id="Sec3"><title>Principle of the time division multiplexing (TDM) based multi-spectral LiDAR system</title><p id="Par6">TDM is a ubiquitously used method in the field of data communication where multiple data streams are composed into a single signal by allocating different time slots within the signal relative to some data clock to carry short segments of the different data streams<sup><xref ref-type="bibr" rid="CR38">38</xref></sup>. Through synchronization between the transmitter and receiver, TDM ensures accurate transmission and sampling of the individual data streams within the allocated time slots while using a single physical channel. A TDM based multi-spectral LiDAR system enables spectroscopy with a single spectrally integrating detector in the time domain unlike conventional spectroscopic LiDAR systems which rely on complicated spectrally resolved detectors<sup><xref ref-type="bibr" rid="CR31">31</xref>,<xref ref-type="bibr" rid="CR39">39</xref></sup>. The principle of our TDM based multi-spectral LiDAR system is illustrated in Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>a. A collinear burst of five nanosecond laser pulses at different wavelengths in the SWIR range (Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>a inset) with fixed time delays between the pulses is directed onto a galvano mirror to scan an object. The reflected burst of pulses is collected and focused onto a single avalanche photodetector to measure the distance via the time of flight as well as the amplitude ratio between the different pulses as a spectral fingerprint of the object, since the reflectivity in the SWIR range is usually highly sensitive to material properties. To demonstrate the potential of enhanced identification and recognition performance by using a multi-spectral LiDAR system, we chose a human as the object recognition target, since pedestrians constitute one of the most frequent hazards during driving. The SWIR reflection spectrum of the identification target (human skin) and relevant error objects (mannequin, fabric, printed human image, nitrile) were measured using a broadband tungsten-halogen light source (HL-2000, Ocean Optics) and a SWIR spectrometer (Flame-NIR, Ocean Optics) (Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>b). The reflection spectrum shows clearly distinguishable differences depending on the material. In particular, the human skin is not only clearly distinguished from other objects but also exhibits a similar spectral response among numerous different test subjects, which is consistent with previous research<sup><xref ref-type="bibr" rid="CR34">34</xref></sup>. This result shows that multi-spectral measurements in the SWIR range can provide sufficient information to classify the reflecting material.<fig id="Fig1"><label>Figure 1</label><caption><p>Principle of the TDM based multi-spectral semantic camera and object classification based on SWIR spectral fingerprinting. (<bold>a</bold>) Schematic illustration of the TDM based multi-spectral LiDAR system for target classification. The inset shows the spectrum of the five nanosecond pulsed laser sources in the SWIR range. GM: galvano mirror. (<bold>b</bold>) Reflection spectra of human skin, mannequin, nitrile glove, fabric, and printed material. The circles on the black solid lines represent the discrete wavelengths of 980&#x000a0;nm, 1060&#x000a0;nm, 1310&#x000a0;nm, 1550&#x000a0;nm, and 1650&#x000a0;nm of the five pulsed laser sources used in this work.</p></caption><graphic xlink:href="41598_2024_62342_Fig1_HTML" id="MO1"/></fig></p></sec><sec id="Sec4"><title>Multi-wavelength TDM synchronization, detection and image reconstruction</title><p id="Par7">Five different commercial nanosecond pulsed diode laser sources (Aerodiode) at wavelengths of 980 nm, 1060 nm, 1310 nm, 1550 nm and 1650 nm, each with a bandwidth of approximately 40 nm (see Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>a inset), are employed in the TDM based multi-spectral LiDAR camera system (Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref>a). The beams are collinearly combined in a home-built table-top setup using off-the-shelf optical components, such as individual dichroic mirrors for the three short-wavelength laser beams (DMLP series, Thorlabs) as well as a polarizing beam splitter (PBS, PBS254, Thorlabs) for the 1550 nm and 1650 nm lasers due to their higher efficiency, and the two resulting beams are combined with a long-pass dichroic mirror (LPDM, DMLP1500R, Thorlabs) to form a single beam in free space. The TDM multi-spectral measurement is controlled by a single clock source creating the required trigger signals to synchronize the pulsed laser sources, scanning, and data acquisition system. Here, we utilize a general-purpose data acquisition device (PCIe-6321, National Instruments), which provides programmable digital trigger capabilities as well as analog outputs for the beam scanning. The trigger pulses for the five lasers are derived from the pulse clock with an individual fixed nanosecond delay applied for each laser source (&#x00394;t<sub>1</sub>&#x000a0;&#x02212;&#x000a0;&#x00394;t<sub>5</sub>, Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref>b) resulting in a five-pulse burst for every clock pulse (Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref>c,d). The delays are chosen to bunch the individual pulses closely together in the time domain in order to maximize the available range for the time-of-flight measurement while avoiding temporal overlap between pulses. The bursts pass through a broad-band beam splitter (BS, BSW29R, Thorlabs) onto a gold mirror driven by a galvo scanner (SG2203, Sino-galvo) illuminating a spot on the object. The galvo scanner is driven by analog voltage signals for the X and Y axes controlling the deflection, which are provided by the built-in digital-to-analog converter (DAC) module of the data acquisition device used for trigger generation. The X axis of the galvo mirror oscillates rapidly back and forth with a triangular waveform synchronized with multiples of the clock pulses (defining the lateral resolution) while the Y axis moves slowly and continuously in one direction, creating a meandering scan over the object. Lastly, the back-scattered light is collected through the beam splitter and focused onto an avalanche photodetector (APD430C, Thorlabs) where the signal is recorded by a high-speed digitizer (ATS9371, Alazartech) at a rate of up to 1 GS/s, providing a 1-ns sampling resolution. Each pulse from the clock source triggers a continuous analog-to-digital conversion (ADC) of the photodetector signal over a fixed time window (shorter than the time between two trigger pulses) with the ADC data being buffered and continuously transferred to the computer such that no pulse is missed. The temporal position of the scattered burst within the ADC window constitutes a TOF measurement of the laser pulses to and from the object in addition to a fixed instrumental time offset (Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref>d).<fig id="Fig2"><label>Figure 2</label><caption><p>Operation principle of the TDM based multi-spectral LiDAR system. (<bold>a</bold>) Schematic illustration of the TDM based multi-spectral camera system. M: regular/dichroic mirrors; PBS: polarizing beam splitter; LPDM: long-pass dichroic mirror; BS: beam splitter; CL: collecting lens; APD: avalanche photodetector; ADC: analog&#x02013;digital converter; DAC: digital-analog converter. (<bold>b</bold>) Delayed timing of the five individual nanosecond pulses constituting a burst. (<bold>c</bold>) Repetitive bursts of pulses with a 20 &#x000b5;s time interval generated by the pulse clock trigger. (<bold>d</bold>) Single burst of pulses with five individual integration windows yielding the five spectral intensities for this LiDAR image pixel. The TOF is determined by threshold or peak detection of the first laser pulse within the burst.</p></caption><graphic xlink:href="41598_2024_62342_Fig2_HTML" id="MO2"/></fig></p><p id="Par8">The pulse clock corresponds to the pixel rate of the LiDAR imaging system and a trade-off has to be chosen between image resolution, image refresh rate, and maximum LiDAR range, while the temporal steepness of the laser pulse edge and the ADC sampling rate define the temporal and therefore distance resolution of the LiDAR system. In addition, the opening angle of the LiDAR camera is limited by the maximum mechanical deflection speed of the galvo scanner, which affects all other performance figures and limits our pulse clock to 50 kHz for reasonable measurement parameters, in particular image resolution (~&#x02009;40,000 pixels), refresh rate (~&#x02009;1 Hz) and opening angle (~&#x02009;50&#x000b0;). Apart from this mechanical limitation, the pulse clock speed is only limited by the shortest burst length achievable with the laser sources, which is around 300&#x000a0;ns for our five-pulse burst; shorter laser pulses are possible at the expense of a significantly reduced signal amplitude. Hence, the trigger system was designed for a maximum pulse clock of about 1&#x000a0;MHz, allowing for&#x02009;~&#x02009;700&#x000a0;ns (or about 100&#x000a0;m) of usable LiDAR range. Even faster pulse clock speeds (and thus pixel rates) are in principle achievable at a reduced LiDAR range, since the nanosecond pulsed laser diode driver (CCS, Aerodiode) supports a repetition rate of up to 250&#x000a0;MHz. Since both the pixel clock and the image geometry are freely adjustable in our system, the ideal parameters can be chosen with priority on LiDAR range, pixel rate, or image refresh rate, depending on the particular application.</p><p id="Par9">For best performance, the ADC data for an entire image scan, consisting of fixed-sized ADC records (one for each trigger pulse), is buffered on the computer and processed into a multi-spectral image stack while the next image is acquired in the background. Each ADC record is processed by first determining the temporal position of the burst, which provides the LiDAR TOF information. This can be achieved by a simple threshold detection of the first laser pulse&#x02019;s edge within the burst, or by a more accurate peak detection algorithm at the expense of higher computation effort. Next, integration windows are applied to the five laser pulses within the burst using a fixed offset and length for each window relative to the measured temporal position of the burst (Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref>d). Each integration window is uniquely related to one particular laser source via the TDM method, yielding the five spectral intensities for this LiDAR image pixel. Since the sequence of the ADC records within the data set is related to the galvo mirror position during the image scan, the five integrated spectral intensity values in each ADC record can be easily composed into a multi-spectral image stack, i.e., a separate image for each laser wavelength, with additional depth information from the LiDAR TOF measurement. The processing of each ADC record can be done independently and in parallel for optimum performance.</p></sec><sec id="Sec5"><title>Visualization and semantic inference image processing of the multi-spectral data</title><p id="Par10">After obtaining the individual images for each wavelength from the optimized parallel ADC record processing as gray-scale intensity maps, the material dependence of the SWIR reflection spectrum was demonstrated by using various hand-shaped target materials (human, mannequin, print, fabric glove, and nitrile glove) (Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3</xref>a). For a quantitative analysis, the normalized spectral intensities at the individual wavelengths within the red dotted region of interest in Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3</xref>a are shown as circles in Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3</xref>b. The continuous SWIR reflection spectra from Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>b obtained by a commercial spectrometer are plotted as black dotted lines as a guide and show a reasonable agreement, which demonstrates successful extraction of the relevant information for spectroscopic fingerprinting in the SWIR range by our method. The deviations are due to inconsistencies in measuring the spectral information at the exact same sample position as well as laser power drift. To distinguish the reflectance characteristic of different materials in the SWIR range with the human eye, we assigned an individual color to the intensity map for each of the five wavelengths based on the RGB color model. In this model, any color is represented by additive mixing of the three primary colors of light (red, green, and blue) expressed as R, G, and B values, which can be mixed to create secondary colors (yellow, cyan, and magenta). For a clear visualization, the three primary colors were assigned to the wavelengths with the strongest material-dependent variations (980 nm, 1310 nm and 1650 nm) and secondary colors to the auxiliary wavelengths (1060 nm, 1550 nm). The five colored intensity maps were then combined into an RGB-color encoded image, which allows distinguishing the material properties with the human eye (Fig.&#x000a0;<xref rid="Fig4" ref-type="fig">4</xref>a). The visible-color encoded multi-spectral images for hand-shaped objects of various materials (human, mannequin, print, fabric glove, and nitrile glove) are shown in Fig.&#x000a0;<xref rid="Fig4" ref-type="fig">4</xref>b along with the numerical RGB values (between 0 and 255) within the white dotted regions of interest, revealing significant material-dependent spectral differences encoded as RGB colors in a single image. Additionally, we studied differently colored cotton samples, which only differ by their color in the visible range but are made of the same material (Fig. <xref rid="MOESM1" ref-type="media">S1</xref>). In this case, the image encoded in RGB color reveals no distinction between the objects, demonstrating that our method is capable of identifying different types of materials irrespective of their colors within the visible spectrum.<fig id="Fig3"><label>Figure 3</label><caption><p>TDM multi-spectral imaging and spectroscopic analysis. (<bold>a</bold>) Gray-scale multi-spectral image stack for a human hand, a mannequin hand, a printed hand, a nitrile gloved hand, and a fabric gloved hand, as well as a color photograph for reference. (<bold>b</bold>) Normalized multi-spectral intensities in the red dotted region of interest in (<bold>a</bold>) for each object with the corresponding continuous reflection spectra of the target materials from Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>b as a guide to the eye (black dotted lines).</p></caption><graphic xlink:href="41598_2024_62342_Fig3_HTML" id="MO3"/></fig><fig id="Fig4"><label>Figure 4</label><caption><p>Multi-spectral visualization for various materials by RGB color-coding. (<bold>a</bold>) Principle of multi-spectral image processing for color coding. Primary and secondary colors of the RGB model are assigned to each wavelength with the primary colors given to wavelengths with the highest spectral variation between objects. R: red; G: green; B: blue; Y: yellow; C: cyan; M: magenta; W: white. (<bold>b</bold>) Multi-spectral RGB-color encoded images of various hand-shaped objects. The numerical RGB values refer to the white dotted region of interest.</p></caption><graphic xlink:href="41598_2024_62342_Fig4_HTML" id="MO4"/></fig></p></sec><sec id="Sec6"><title>Multi-spectral material species classification using a CNN model</title><p id="Par11">The CNN architecture is most frequently employed as an artificial intelligence model for image classification because of its outstanding abilities to capture spatial patterns and hierarchical features in the visual data<sup><xref ref-type="bibr" rid="CR24">24</xref>,<xref ref-type="bibr" rid="CR40">40</xref></sup>. The 2D convolutional filter shares weights in the x and y dimensions to preserve spatial information and features while maintaining associations between neighboring pixels<sup><xref ref-type="bibr" rid="CR41">41</xref>,<xref ref-type="bibr" rid="CR42">42</xref></sup>. The excellent object recognition performance of CNNs makes them highly suitable to provide object recognition capabilities for autonomous vehicles, which are crucial for detecting potential obstacles such as pedestrians, other vehicles, and bad road conditions using a camera sensor<sup><xref ref-type="bibr" rid="CR1">1</xref>,<xref ref-type="bibr" rid="CR2">2</xref></sup>. However, a conventional camera image lacks the distinction between objects of different materials but of similar shape and color that frequently appear while driving. For example, it is difficult to distinguish between a pedestrian and a signboard depicting a human or between a wet and icy road surface<sup><xref ref-type="bibr" rid="CR25">25</xref>,<xref ref-type="bibr" rid="CR26">26</xref></sup> using only a visible-light camera image. Here, we extend the superior shape recognition capabilities of a CNN with a multi-spectral signature in the SWIR wavelength range for enhanced autonomous object recognition using spectroscopic material information as an additional inference channel.</p><p id="Par12">In order to apply our TDM multi-spectral imaging method to material species classification and evaluate its performance, we created a CNN model with multi-spectral images of a human hand, a fabric gloved hand, and a mannequin hand. The human hand and fabric gloved hand multi-spectral images were obtained from 40 different persons&#x02019; bare hands and wearing the same fabric glove, respectively, while 40 different multi-spectral images of the mannequin hand were acquired by repositioning the same mannequin hand. For the CNN model training and classification, the five-wavelength multi-spectral image stack was flattened by compositing the individual intensity maps into a single image (Fig.&#x000a0;<xref rid="Fig5" ref-type="fig">5</xref>a) with fixed relative positions, thus retaining the full spatial and spectral information. The CNN model was designed with three filtering layers followed by a flattening layer with each building block comprising a series of 2D convolutional layers followed by a maximum pooling layer, and a rectified linear unit (ReLu) as an activation function. In particular, a softmax unit was employed as an activation function at the end of the fully connected layer to classify the material species (Fig.&#x000a0;<xref rid="Fig5" ref-type="fig">5</xref>a). The adaptive moment estimation (Adam) method was employed as an optimizer, which is a stochastic gradient descent method based on adaptive estimation of the first and second moments. The loss and accuracy curves represent the learning process of the model, which is composed of a training part (orange lines) and a validation part (blue lines) (Fig.&#x000a0;<xref rid="Fig5" ref-type="fig">5</xref>b,c). The loss function is the difference between the true value and the value predicted by the model, which is utilized to optimize the weight parameters in the neural network. The accuracy value indicates the performance of the model for the entire data set as the fraction of correct predictions, i.e., the ratio of predictions where the predicted value is equal to the true value. The loss and accuracy converge to 0 and 1, respectively, with increasing epoch number, and the training part shows a similar trend as the validation part, which indicates that the CNN model is optimized without overfitting issues.<fig id="Fig5"><label>Figure 5</label><caption><p>Material classification of multi-spectral images via the CNN platform. (<bold>a</bold>) Schematic of the CNN model training process with multi-spectral images from human, fabric, and mannequin hands. FC layer: fully connected layer. (<bold>b</bold>, <bold>c</bold>) Loss and accuracy values for each epoch for the training (orange lines) and validation (blue lines) data. (<bold>d</bold>) Confusion matrix values of the labeled and predicted material species. (<bold>e)</bold> ROC curves and their AUC values.</p></caption><graphic xlink:href="41598_2024_62342_Fig5_HTML" id="MO5"/></fig></p><p id="Par13">A confusion matrix (Fig.&#x000a0;<xref rid="Fig5" ref-type="fig">5</xref>d) represents the prediction distribution of the model for each actual label in the classification data set as a percentage. Here, the principal diagonal percentage indicates the accuracy of the model to correctly classify the data, and the result shows that the optimized CNN model can successfully classify the multi-spectral image based on material species. Furthermore, the classifier performance is summarized by plotting a receiver operating characteristic (ROC) curve (Fig.&#x000a0;<xref rid="Fig5" ref-type="fig">5</xref>e), which compares the rate of true positives with the rate of false positives for our CNN classifier using the results of the confusion matrix (the dashed diagonal line represents a random-guess classifier for reference). The area under this curve (AUC) provides a normalized statistical metric for the performance of a CNN classifier, also referred to as an index of accuracy, where a perfect classifier has an AUC value of 1 (no false positives or negatives) and a random-guess classifier has an AUC value of 0.5. Our optimized CNN model classified all samples without error and thus achieved a perfect AUC value of 1. Although the data set was somewhat limited, these results demonstrate that our TDM based multi-spectral LiDAR system and data processing method are well suited for accurate semantic inference of the material species.</p></sec><sec id="Sec7"><title>Demonstration of light detection and ranging</title><p id="Par14">The TDM based multi-spectral LiDAR system permits not only multi-spectral imaging but also distance range mapping simultaneously by measuring the reflected TOF as the delay between a clock trigger pulse and the arrival time of the five-pulse burst. To demonstrate the ranging performance, &#x02018;P&#x02019;, &#x02018;N&#x02019;, and &#x02018;U&#x02019; letter shaped objects were positioned with the &#x02018;N&#x02019; 1&#x000a0;m apart from the other letters in the direction of the LiDAR camera (Fig.&#x000a0;<xref rid="Fig6" ref-type="fig">6</xref>a) and each object having dimensions of 10&#x000a0;cm height and 5&#x000a0;cm width. The temporal position of the burst is measured by finding the peak of the first pulse in the burst (Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref>d), which is more accurate than a simple threshold detector where the relative temporal position of a threshold value can shift with the pulse amplitude. Here, the peak is determined via a rudimentary search for a turning point with linear interpolation for sub-sample numerical accuracy. The search is performed above a certain threshold level in order to exclude background noise from the peak detection and reduce the search range. The (fractional) sample index of the peak within an acquisition record is directly related to the TOF, since each record is hardware timed with the pulse clock trigger.<fig id="Fig6"><label>Figure 6</label><caption><p>Ranging performance characterization of the multi-spectral LiDAR system. (<bold>a</bold>) Photograph of the target object arrangement showing the front and top view. (<bold>b</bold>) Distance map obtained by the multi-spectral LiDAR system, shown as top view and tilted view. The distance values shown by white arrows are averaged over the respective regions of interest denoted by the white rectangles.</p></caption><graphic xlink:href="41598_2024_62342_Fig6_HTML" id="MO6"/></fig></p><p id="Par15">The processed distance mapping results are shown in Fig.&#x000a0;<xref rid="Fig6" ref-type="fig">6</xref>b as a front view and as a tilted view. The averaged distance values within the white rectangular regions of interest yield a measured distance between the &#x02018;N&#x02019; and the other letters of 1.12 m, which is in good agreement with the actual distance of 1 m. The measured distance error corresponds to a TOF error of about 0.8 ns which is less than one sample at our 1 GS/s sampling rate. The timing and therefore ranging resolution is ultimately limited by the overall electronic timing jitter of our system (clock generation and laser trigger) as well as amplitude noise of the laser sources and the measurement system, which limits the temporal accuracy of the peak detection to about one sample (1 ns).</p></sec></sec><sec id="Sec8"><title>Conclusions</title><p id="Par16">In this work, we demonstrated a TDM based multi-spectral LiDAR system, which can provide spatial and spectral information for material identification with both the human eye and machine learning. Our time-domain spectroscopic method not only minimizes optical loss of the system but also enables simultaneous ranging of the target objects. The individual different-wavelength pulses are closely packed in the time domain within a burst, which allows measuring the time-of-flight information over a large distance for ranging the object. Five different nanosecond pulsed lasers in the SWIR range (980 nm, 1060 nm, 1310 nm, 1550 nm, and 1650 nm) were employed to utilize the material-dependent spectral characteristics in the SWIR range for semantic inference classification of different object materials in addition to the object&#x02019;s shape. Our multi-spectral images show a clear distinction between different materials when mapped into an RGB-color encoded image and are particularly well-suited for systematic classification via the CNN architecture with high accuracy, which makes use of the full spectral information. The proposed technology offers a great potential for the development of compact multi-spectral LiDAR systems to enhance the safety and reliability of autonomous driving.</p></sec><sec sec-type="supplementary-material"><sec id="Sec9"><title>Supplementary Information</title><p>
<supplementary-material content-type="local-data" id="MOESM1"><media xlink:href="41598_2024_62342_MOESM1_ESM.docx"><caption><p>Supplementary Figure S1.</p></caption></media></supplementary-material></p></sec></sec></body><back><fn-group><fn><p><bold>Publisher's note</bold></p><p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></fn><fn><p>These authors contributed equally: Sehyeon Kim and Tae-In Jeong.</p></fn></fn-group><sec><title>Supplementary Information</title><p>The online version contains supplementary material available at 10.1038/s41598-024-62342-2.</p></sec><notes notes-type="author-contribution"><title>Author contributions</title><p>T.E., C.K., A.G., and S.C.K. conceived and designed the study. S.H.K., T.J., and A.G. designed the optical systems and software. S.K., T.J, S.H.K., E.C., E.Y., and M.S. performed the optical measurements. S.H.K., T.J., A.G., and S.C.K. conducted the data analysis. All authors contributed to the preparation of the manuscript.</p></notes><notes notes-type="funding-information"><title>Funding</title><p>This research was supported by the Korea Evaluation Institute of Industrial Technology (KEIT) grant funded by the Korea government (MOTIE) (No. 1415181754). It was additionally supported by the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (No. NRF-2021R1A5A1032937) and by the BrainLink program funded by the Ministry of Science and ICT through the National Research Foundation of Korea (RS-2023-00236798).</p></notes><notes notes-type="data-availability"><title>Data availability</title><p>The datasets generated and/or analyzed during the current study are available from the corresponding author on reasonable request.</p></notes><notes id="FPar1" notes-type="COI-statement"><title>Competing interests</title><p id="Par17">The authors declare no competing interests.</p></notes><ref-list id="Bib1"><title>References</title><ref id="CR1"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lindner</surname><given-names>T</given-names></name><name><surname>Wyrwa&#x00142;</surname><given-names>D</given-names></name><name><surname>Milecki</surname><given-names>A</given-names></name></person-group><article-title>An autonomous humanoid robot designed to assist a human with a gesture recognition system</article-title><source>Electronics</source><year>2023</year><volume>12</volume><fpage>2652</fpage><pub-id pub-id-type="doi">10.3390/electronics12122652</pub-id></element-citation></ref><ref id="CR2"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Podgorelec</surname><given-names>D</given-names></name><etal/></person-group><article-title>LiDAR-based maintenance of a safe distance between a human and a robot arm</article-title><source>Sensors</source><year>2023</year><volume>23</volume><fpage>4305</fpage><pub-id pub-id-type="doi">10.3390/s23094305</pub-id><?supplied-pmid 37177509?><pub-id pub-id-type="pmid">37177509</pub-id>
</element-citation></ref><ref id="CR3"><label>3.</label><mixed-citation publication-type="other">Eitel, A., Springenberg, J. T., Spinello, L., Riedmiller, M. &#x00026; Burgard, W. in <italic>2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS).</italic> 681&#x02013;687 (IEEE).</mixed-citation></ref><ref id="CR4"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>P</given-names></name><name><surname>Chen</surname><given-names>J</given-names></name><name><surname>Cho</surname><given-names>YK</given-names></name></person-group><article-title>SLAM-driven robotic mapping and registration of 3D point clouds</article-title><source>Autom. Construct.</source><year>2018</year><volume>89</volume><fpage>38</fpage><lpage>48</lpage><pub-id pub-id-type="doi">10.1016/j.autcon.2018.01.009</pub-id></element-citation></ref><ref id="CR5"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>Y</given-names></name><etal/></person-group><article-title>A deep learning-based hybrid framework for object detection and recognition in autonomous driving</article-title><source>IEEE Access</source><year>2020</year><volume>8</volume><fpage>194228</fpage><lpage>194239</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2020.3033289</pub-id></element-citation></ref><ref id="CR6"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grigorescu</surname><given-names>S</given-names></name><name><surname>Trasnea</surname><given-names>B</given-names></name><name><surname>Cocias</surname><given-names>T</given-names></name><name><surname>Macesanu</surname><given-names>G</given-names></name></person-group><article-title>A survey of deep learning techniques for autonomous driving</article-title><source>J. Field Robot.</source><year>2020</year><volume>37</volume><fpage>362</fpage><lpage>386</lpage><pub-id pub-id-type="doi">10.1002/rob.21918</pub-id></element-citation></ref><ref id="CR7"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fujiyoshi</surname><given-names>H</given-names></name><name><surname>Hirakawa</surname><given-names>T</given-names></name><name><surname>Yamashita</surname><given-names>T</given-names></name></person-group><article-title>Deep learning-based image recognition for autonomous driving</article-title><source>IATSS Res.</source><year>2019</year><volume>43</volume><fpage>244</fpage><lpage>252</lpage><pub-id pub-id-type="doi">10.1016/j.iatssr.2019.11.008</pub-id></element-citation></ref><ref id="CR8"><label>8.</label><mixed-citation publication-type="other">Teichman, A. &#x00026; Thrun, S. in <italic>Advanced Robotics and its Social Impacts.</italic> 35&#x02013;38 (IEEE).</mixed-citation></ref><ref id="CR9"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sotelo</surname><given-names>MA</given-names></name><name><surname>Rodriguez</surname><given-names>FJ</given-names></name><name><surname>Magdalena</surname><given-names>L</given-names></name><name><surname>Bergasa</surname><given-names>LM</given-names></name><name><surname>Boquete</surname><given-names>L</given-names></name></person-group><article-title>A color vision-based lane tracking system for autonomous driving on unmarked roads</article-title><source>Autonom. Robot.</source><year>2004</year><volume>16</volume><fpage>95</fpage><lpage>116</lpage><pub-id pub-id-type="doi">10.1023/B:AURO.0000008673.96984.28</pub-id></element-citation></ref><ref id="CR10"><label>10.</label><mixed-citation publication-type="other">Teichmann, M., Weber, M., Zoellner, M., Cipolla, R. &#x00026; Urtasun, R. in <italic>2018 IEEE Intelligent Vehicles Symposium (IV).</italic> 1013&#x02013;1020 (IEEE).</mixed-citation></ref><ref id="CR11"><label>11.</label><mixed-citation publication-type="other">Deepika, N. &#x00026; Variyar, V. S. in <italic>2017 International Conference on Advances in Computing, Communications and Informatics (ICACCI).</italic> 2092&#x02013;2097 (IEEE).</mixed-citation></ref><ref id="CR12"><label>12.</label><mixed-citation publication-type="other">Heng, L. <italic>et al.</italic> in <italic>2019 International Conference on Robotics and Automation (ICRA).</italic> 4695&#x02013;4702 (IEEE).</mixed-citation></ref><ref id="CR13"><label>13.</label><mixed-citation publication-type="other">Premebida, C., Melotti, G. &#x00026; Asvadi, A. RGB-D object classification for autonomous driving perception. <italic>RGB-D Image Anal. Proc.</italic> 377&#x02013;395 (2019).</mixed-citation></ref><ref id="CR14"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>L</given-names></name><name><surname>Li</surname><given-names>H</given-names></name><name><surname>Dai</surname><given-names>Y</given-names></name><name><surname>Pan</surname><given-names>Q</given-names></name></person-group><article-title>Robust and efficient relative pose with a multi-camera system for autonomous driving in highly dynamic environments</article-title><source>IEEE Trans. Intell. Transp. Syst.</source><year>2017</year><volume>19</volume><fpage>2432</fpage><lpage>2444</lpage><pub-id pub-id-type="doi">10.1109/TITS.2017.2749409</pub-id></element-citation></ref><ref id="CR15"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Royo</surname><given-names>S</given-names></name><name><surname>Ballesta-Garcia</surname><given-names>M</given-names></name></person-group><article-title>An overview of lidar imaging systems for autonomous vehicles</article-title><source>Appl. Sci.</source><year>2019</year><volume>9</volume><fpage>4093</fpage><pub-id pub-id-type="doi">10.3390/app9194093</pub-id></element-citation></ref><ref id="CR16"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>R</given-names></name><etal/></person-group><article-title>Breaking the temporal and frequency congestion of LiDAR by parallel chaos</article-title><source>Nat. Photon.</source><year>2023</year><volume>17</volume><fpage>306</fpage><lpage>314</lpage><pub-id pub-id-type="doi">10.1038/s41566-023-01158-4</pub-id></element-citation></ref><ref id="CR17"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>Y</given-names></name><name><surname>Ibanez-Guzman</surname><given-names>J</given-names></name></person-group><article-title>Lidar for autonomous driving: The principles, challenges, and trends for automotive lidar and perception systems</article-title><source>IEEE Signal Proc. Magaz.</source><year>2020</year><volume>37</volume><fpage>50</fpage><lpage>61</lpage></element-citation></ref><ref id="CR18"><label>18.</label><mixed-citation publication-type="other">Himmelsbach, M., Mueller, A., L&#x000fc;ttel, T. &#x00026; W&#x000fc;nsche, H.-J. in <italic>Proceedings of 1st international workshop on cognition for technical systems.</italic></mixed-citation></ref><ref id="CR19"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>H</given-names></name><name><surname>Wang</surname><given-names>B</given-names></name><name><surname>Liu</surname><given-names>B</given-names></name><name><surname>Meng</surname><given-names>X</given-names></name><name><surname>Yang</surname><given-names>G</given-names></name></person-group><article-title>Pedestrian recognition and tracking using 3D LiDAR for autonomous vehicle</article-title><source>Robot. Autonom. Syst.</source><year>2017</year><volume>88</volume><fpage>71</fpage><lpage>78</lpage><pub-id pub-id-type="doi">10.1016/j.robot.2016.11.014</pub-id></element-citation></ref><ref id="CR20"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wu</surname><given-names>D</given-names></name><etal/></person-group><article-title>Multi-beam single-photon LiDAR with hybrid multiplexing in wavelength and time</article-title><source>Opt. Laser Technol.</source><year>2022</year><volume>145</volume><fpage>107477</fpage><pub-id pub-id-type="doi">10.1016/j.optlastec.2021.107477</pub-id></element-citation></ref><ref id="CR21"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jiang</surname><given-names>Y</given-names></name><name><surname>Karpf</surname><given-names>S</given-names></name><name><surname>Jalali</surname><given-names>B</given-names></name></person-group><article-title>Time-stretch LiDAR as a spectrally scanned time-of-flight ranging camera</article-title><source>Nat. Photon.</source><year>2020</year><volume>14</volume><fpage>14</fpage><lpage>18</lpage><pub-id pub-id-type="doi">10.1038/s41566-019-0548-6</pub-id></element-citation></ref><ref id="CR22"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zamanakos</surname><given-names>G</given-names></name><name><surname>Tsochatzidis</surname><given-names>L</given-names></name><name><surname>Amanatiadis</surname><given-names>A</given-names></name><name><surname>Pratikakis</surname><given-names>I</given-names></name></person-group><article-title>A comprehensive survey of LIDAR-based 3D object detection methods with deep learning for autonomous driving</article-title><source>Comput. Graph.</source><year>2021</year><volume>99</volume><fpage>153</fpage><lpage>181</lpage><pub-id pub-id-type="doi">10.1016/j.cag.2021.07.003</pub-id></element-citation></ref><ref id="CR23"><label>23.</label><mixed-citation publication-type="other">Du, X., Ang, M. H. &#x00026; Rus, D. in <italic>2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS).</italic> 749&#x02013;754 (IEEE).</mixed-citation></ref><ref id="CR24"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gao</surname><given-names>H</given-names></name><etal/></person-group><article-title>Object classification using CNN-based fusion of vision and LIDAR in autonomous vehicle environment</article-title><source>IEEE Trans. Ind. Inf.</source><year>2018</year><volume>14</volume><fpage>4224</fpage><lpage>4231</lpage><pub-id pub-id-type="doi">10.1109/TII.2018.2822828</pub-id></element-citation></ref><ref id="CR25"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>J</given-names></name><name><surname>Kim</surname><given-names>E</given-names></name><name><surname>Kim</surname><given-names>D</given-names></name></person-group><article-title>A black ice detection method based on 1-dimensional CNN using mmWave sensor backscattering</article-title><source>Remote Sensing</source><year>2022</year><volume>14</volume><fpage>5252</fpage><pub-id pub-id-type="doi">10.3390/rs14205252</pub-id></element-citation></ref><ref id="CR26"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ma</surname><given-names>X</given-names></name><name><surname>Ruan</surname><given-names>C</given-names></name></person-group><article-title>Method for black ice detection on roads using tri-wavelength backscattering measurements</article-title><source>Appl. Opt.</source><year>2020</year><volume>59</volume><fpage>7242</fpage><lpage>7246</lpage><pub-id pub-id-type="doi">10.1364/AO.398772</pub-id><?supplied-pmid 32902498?><pub-id pub-id-type="pmid">32902498</pub-id>
</element-citation></ref><ref id="CR27"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pan</surname><given-names>S</given-names></name><etal/></person-group><article-title>Land-cover classification of multispectral LiDAR data using CNN with optimized hyper-parameters</article-title><source>ISPRS J. Photogram. Remote Sensing</source><year>2020</year><volume>166</volume><fpage>241</fpage><lpage>254</lpage><pub-id pub-id-type="doi">10.1016/j.isprsjprs.2020.05.022</pub-id></element-citation></ref><ref id="CR28"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hopkinson</surname><given-names>C</given-names></name><name><surname>Chasmer</surname><given-names>L</given-names></name><name><surname>Gynan</surname><given-names>C</given-names></name><name><surname>Mahoney</surname><given-names>C</given-names></name><name><surname>Sitar</surname><given-names>M</given-names></name></person-group><article-title>Multisensor and multispectral lidar characterization and classification of a forest environment</article-title><source>Can. J. Remote Sensing</source><year>2016</year><volume>42</volume><fpage>501</fpage><lpage>520</lpage><pub-id pub-id-type="doi">10.1080/07038992.2016.1196584</pub-id></element-citation></ref><ref id="CR29"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morsy</surname><given-names>S</given-names></name><name><surname>Shaker</surname><given-names>A</given-names></name><name><surname>El-Rabbany</surname><given-names>A</given-names></name></person-group><article-title>Multispectral LiDAR data for land cover classification of urban areas</article-title><source>Sensors</source><year>2017</year><volume>17</volume><fpage>958</fpage><pub-id pub-id-type="doi">10.3390/s17050958</pub-id><?supplied-pmid 28445432?><pub-id pub-id-type="pmid">28445432</pub-id>
</element-citation></ref><ref id="CR30"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jurado</surname><given-names>JM</given-names></name><name><surname>Ortega</surname><given-names>L</given-names></name><name><surname>Cubillas</surname><given-names>JJ</given-names></name><name><surname>Feito</surname><given-names>F</given-names></name></person-group><article-title>Multispectral mapping on 3D models and multi-temporal monitoring for individual characterization of olive trees</article-title><source>Remote Sensing</source><year>2020</year><volume>12</volume><fpage>1106</fpage><pub-id pub-id-type="doi">10.3390/rs12071106</pub-id></element-citation></ref><ref id="CR31"><label>31.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sivaprakasam</surname><given-names>V</given-names></name><etal/></person-group><article-title>Multi-spectral SWIR lidar for imaging and spectral discrimination through partial obscurations</article-title><source>Opt. Express</source><year>2023</year><volume>31</volume><fpage>5443</fpage><lpage>5457</lpage><pub-id pub-id-type="doi">10.1364/OE.477499</pub-id><?supplied-pmid 36823824?><pub-id pub-id-type="pmid">36823824</pub-id>
</element-citation></ref><ref id="CR32"><label>32.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>N</given-names></name><etal/></person-group><article-title>Spectral imaging and spectral LIDAR systems: Moving toward compact nanophotonics-based sensing</article-title><source>Nanophotonics</source><year>2021</year><volume>10</volume><fpage>1437</fpage><lpage>1467</lpage><pub-id pub-id-type="doi">10.1515/nanoph-2020-0625</pub-id></element-citation></ref><ref id="CR33"><label>33.</label><mixed-citation publication-type="other">Hansen, M. P. &#x00026; Malchow, D. S. in <italic>Thermosense.</italic> 94&#x02013;104 (SPIE).</mixed-citation></ref><ref id="CR34"><label>34.</label><mixed-citation publication-type="other">Steiner, H., Sporrer, S., Kolb, A. &#x00026; Jung, N. Design of an active multispectral SWIR camera system for skin detection and face verification. <italic>J. Sensors</italic><bold>2016</bold> (2016).</mixed-citation></ref><ref id="CR35"><label>35.</label><mixed-citation publication-type="other">Hussein, M. E., Spinoulas, L., Xiong, F. &#x00026; Abd-Almageed, W. in <italic>2018 IEEE International Workshop on Information Forensics and Security (WIFS).</italic> 1&#x02013;8 (IEEE).</mixed-citation></ref><ref id="CR36"><label>36.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>Y</given-names></name><etal/></person-group><article-title>Two-channel hyperspectral LiDAR with a supercontinuum laser source</article-title><source>Sensors</source><year>2010</year><volume>10</volume><fpage>7057</fpage><lpage>7066</lpage><pub-id pub-id-type="doi">10.3390/s100707057</pub-id><?supplied-pmid 22163589?><pub-id pub-id-type="pmid">22163589</pub-id>
</element-citation></ref><ref id="CR37"><label>37.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Song</surname><given-names>S</given-names></name><etal/></person-group><article-title>A new waveform decomposition method for multispectral LiDAR</article-title><source>ISPRS J. Photogram. Remote Sensing</source><year>2019</year><volume>149</volume><fpage>40</fpage><lpage>49</lpage><pub-id pub-id-type="doi">10.1016/j.isprsjprs.2019.01.014</pub-id></element-citation></ref><ref id="CR38"><label>38.</label><mixed-citation publication-type="other">TechTarget. time-division multiplexing (TDM) <ext-link ext-link-type="uri" xlink:href="https://www.techtarget.com/whatis/definition/time-division-multiplexing-TDM">https://www.techtarget.com/whatis/definition/time-division-multiplexing-TDM</ext-link> (2021).</mixed-citation></ref><ref id="CR39"><label>39.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhao</surname><given-names>X</given-names></name><etal/></person-group><article-title>Active 3D imaging of vegetation based on multi-wavelength fluorescence LiDAR</article-title><source>Sensors</source><year>2020</year><volume>20</volume><fpage>935</fpage><pub-id pub-id-type="doi">10.3390/s20030935</pub-id><?supplied-pmid 32050619?><pub-id pub-id-type="pmid">32050619</pub-id>
</element-citation></ref><ref id="CR40"><label>40.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yu</surname><given-names>C</given-names></name><name><surname>Han</surname><given-names>R</given-names></name><name><surname>Song</surname><given-names>M</given-names></name><name><surname>Liu</surname><given-names>C</given-names></name><name><surname>Chang</surname><given-names>C-I</given-names></name></person-group><article-title>A simplified 2D&#x02013;3D CNN architecture for hyperspectral image classification based on spatial&#x02013;spectral fusion</article-title><source>IEEE J. Select. Top. Appl. Earth Observ. Remote Sensing</source><year>2020</year><volume>13</volume><fpage>2485</fpage><lpage>2501</lpage><pub-id pub-id-type="doi">10.1109/JSTARS.2020.2983224</pub-id></element-citation></ref><ref id="CR41"><label>41.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>Y</given-names></name><etal/></person-group><article-title>Deep learning for lidar point clouds in autonomous driving: A review</article-title><source>IEEE Trans. Neural Netw. Learn. Syst.</source><year>2020</year><volume>32</volume><fpage>3412</fpage><lpage>3432</lpage><pub-id pub-id-type="doi">10.1109/TNNLS.2020.3015992</pub-id></element-citation></ref><ref id="CR42"><label>42.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Paoletti</surname><given-names>M</given-names></name><name><surname>Haut</surname><given-names>J</given-names></name><name><surname>Plaza</surname><given-names>J</given-names></name><name><surname>Plaza</surname><given-names>A</given-names></name></person-group><article-title>Deep learning classifiers for hyperspectral imaging: A review</article-title><source>ISPRS J. Photogram. Remote Sensing</source><year>2019</year><volume>158</volume><fpage>279</fpage><lpage>317</lpage><pub-id pub-id-type="doi">10.1016/j.isprsjprs.2019.09.006</pub-id></element-citation></ref></ref-list></back></article>