,pmcid,doi,title,authorString,pubYear,abstractText,isOpenAccess,license
0,PMC10280262,10.7717/peerj-cs.1159,Nanopublication-based semantic publishing and reviewing: a field study with formalization papers.,"Bucur CI, Kuhn T, Ceolin D, van Ossenbruggen J.",2023,"With the rapidly increasing amount of scientific literature, it is getting continuously more difficult for researchers in different disciplines to keep up-to-date with the recent findings in their field of study. Processing scientific articles in an automated fashion has been proposed as a solution to this problem, but the accuracy of such processing remains very poor for extraction tasks beyond the most basic ones (like locating and identifying entities and simple classification based on predefined categories). Few approaches have tried to change how we publish scientific results in the first place, such as by making articles machine-interpretable by expressing them with formal semantics from the start. In the work presented here, we propose a first step in this direction by setting out to demonstrate that we can formally publish high-level scientific claims in formal logic, and publish the results in a special issue of an existing journal. We use the concept and technology of nanopublications for this endeavor, and represent not just the submissions and final papers in this RDF-based format, but also the whole process in between, including reviews, responses, and decisions. We do this by performing a field study with what we call formalization papers, which contribute a novel formalization of a previously published claim. We received 15 submissions from 18 authors, who then went through the whole publication process leading to the publication of their contributions in the special issue. Our evaluation shows the technical and practical feasibility of our approach. The participating authors mostly showed high levels of interest and confidence, and mostly experienced the process as not very difficult, despite the technical nature of the current user interfaces. We believe that these results indicate that it is possible to publish scientific results from different fields with machine-interpretable semantics from the start, which in turn opens countless possibilities to radically improve in the future the effectiveness and efficiency of the scientific endeavor as a whole.",Y,cc by
1,PMC11192941,10.1038/s41598-024-64858-z,Author Correction: Research on underwater robot ranging technology based on semantic segmentation and binocular vision.,"Hu Q, Wang K, Ren F, Wang Z.",2024,,Y,cc by
2,PMC2663789,10.1371/journal.pcbi.1000361,Adventures in semantic publishing: exemplar semantic enhancements of a research article.,"Shotton D, Portwin K, Klyne G, Miles A.",2009,"Scientific innovation depends on finding, integrating, and re-using the products of previous research. Here we explore how recent developments in Web technology, particularly those related to the publication of data and metadata, might assist that process by providing semantic enhancements to journal articles within the mainstream process of scholarly journal publishing. We exemplify this by describing semantic enhancements we have made to a recent biomedical research article taken from PLoS Neglected Tropical Diseases, providing enrichment to its content and increased access to datasets within it. These semantic enhancements include provision of live DOIs and hyperlinks; semantic markup of textual terms, with links to relevant third-party information resources; interactive figures; a re-orderable reference list; a document summary containing a study summary, a tag cloud, and a citation analysis; and two novel types of semantic enrichment: the first, a Supporting Claims Tooltip to permit ""Citations in Context"", and the second, Tag Trees that bring together semantically related terms. In addition, we have published downloadable spreadsheets containing data from within tables and figures, have enriched these with provenance information, and have demonstrated various types of data fusion (mashups) with results from other research articles and with Google Maps. We have also published machine-readable RDF metadata both about the article and about the references it cites, for which we developed a Citation Typing Ontology, CiTO (http://purl.org/net/cito/). The enhanced article, which is available at http://dx.doi.org/10.1371/journal.pntd.0000228.x001, presents a compelling existence proof of the possibilities of semantic publication. We hope the showcase of examples and ideas it contains, described in this paper, will excite the imaginations of researchers and publishers, stimulating them to explore the possibilities of semantic publishing for their own research articles, and thereby break down present barriers to the discovery and re-use of information within traditional modes of scholarly communication.",Y,cc by
3,PMC10876301,10.1155/2024/9804280,Retracted: Dynamic Data Infrastructure Security for Interoperable e-Healthcare Systems: A Semantic Feature-Driven NoSQL Intrusion Attack Detection Model.,International BR.,2024,[This retracts the article DOI: 10.1155/2022/4080199.].,Y,cc by
4,PMC11087458,10.1038/s41597-024-03185-4,A maturity model for catalogues of semantic artefacts.,"Corcho O, Ekaputra FJ, Heibi I, Jonquet C, Micsik A, Peroni S, Storti E.",2024,"This work presents a maturity model for assessing catalogues of semantic artefacts, one of the keystones that permit semantic interoperability of systems. We defined the dimensions and related features to include in the maturity model by analysing the current literature and existing catalogues of semantic artefacts provided by experts. In addition, we assessed 26 different catalogues to demonstrate the effectiveness of the maturity model, which includes 12 different dimensions (Metadata, Openness, Quality, Availability, Statistics, PID, Governance, Community, Sustainability, Technology, Transparency, and Assessment) and 43 related features (or sub-criteria) associated with these dimensions. Such a maturity model is one of the first attempts to provide recommendations for governance and processes for preserving and maintaining semantic artefacts and helps assess/address interoperability challenges.",Y,cc by
5,PMC11061179,10.1038/s41597-024-03169-4,Semantic integration of diverse data in materials science: Assessing Orowan strengthening.,"Bayerlein B, Schilling M, von Hartrott P, Waitelonis J.",2024,"This study applies Semantic Web technologies to advance Materials Science and Engineering (MSE) through the integration of diverse datasets. Focusing on a 2000 series age-hardenable aluminum alloy, we correlate mechanical and microstructural properties derived from tensile tests and dark-field transmission electron microscopy across varied aging times. An expandable knowledge graph, constructed using the Tensile Test and Precipitate Geometry Ontologies aligned with the PMD Core Ontology, facilitates this integration. This approach adheres to FAIR principles and enables sophisticated analysis via SPARQL queries, revealing correlations consistent with the Orowan mechanism. The study highlights the potential of semantic data integration in MSE, offering a new approach for data-centric research and enhanced analytical capabilities.",Y,cc by
6,PMC11153642,10.1038/s41598-024-63279-2,A novel model for relation prediction in knowledge graphs exploiting semantic and structural feature integration.,"Yang J, Lu G, He S, Cao Q, Liu Y.",2024,"Relation prediction is a critical task in knowledge graph completion and associated downstream tasks that rely on knowledge representation. Previous studies indicate that both structural features and semantic information are meaningful for predicting missing relations in knowledge graphs. This has led to the development of two types of methods: structure-based methods and semantics-based methods. Since these two approaches represent two distinct learning paradigms, it is difficult to fully utilize both sets of features within a single learning model, especially deep features. As a result, existing studies usually focus on only one type of feature. This leads to an insufficient representation of knowledge in current methods and makes them prone to overlooking certain patterns when predicting missing relations. In this study, we introduce a novel model, RP-ISS, which combines deep semantic and structural features for relation prediction. The RP-ISS model utilizes a two-part architecture, with the first component being a RoBERTa module that is responsible for extracting semantic features from entity nodes. The second part of the system employs an edge-based relational message-passing network designed to capture and interpret structural information within the data. To alleviate the computational burden of the message-passing network on the RoBERTa module during the sampling process, RP-ISS introduces a node embedding memory bank, which updates asynchronously to circumvent excessive computation. The model was assessed on three publicly accessible datasets (WN18RR, WN18, and FB15k-237), and the results revealed that RP-ISS surpasses all baseline methods across all evaluation metrics. Moreover, RP-ISS showcases robust performance in graph inductive learning.",Y,cc by
7,PMC11079068,10.1038/s41467-024-48115-5,Semantic regularization of electromagnetic inverse problems.,"Zhang H, Chen Y, Wang Z, Cui TJ, Del Hougne P, Li L.",2024,"Solving ill-posed inverse problems typically requires regularization based on prior knowledge. To date, only prior knowledge that is formulated mathematically (e.g., sparsity of the unknown) or implicitly learned from quantitative data can be used for regularization. Thereby, semantically formulated prior knowledge derived from human reasoning and recognition is excluded. Here, we introduce and demonstrate the concept of semantic regularization based on a pre-trained large language model to overcome this vexing limitation. We study the approach, first, numerically in a prototypical 2D inverse scattering problem, and, second, experimentally in 3D and 4D compressive microwave imaging problems based on programmable metasurfaces. We highlight that semantic regularization enables new forms of highly-sought privacy protection for applications like smart homes, touchless human-machine interaction and security screening: selected subjects in the scene can be concealed, or their actions and postures can be altered in the reconstruction by manipulating the semantic prior with suitable language-based control commands.",Y,cc by
8,PMC11045089,10.1371/journal.pdig.0000501,PLOS-LLM: Can and should AI enable a new paradigm of scientific knowledge sharing?,"Hughes RC, van Heerden A.",2024,,Y,cc by
9,PMC11186750,10.1038/s41586-024-07421-0,Detecting hallucinations in large language models using semantic entropy.,"Farquhar S, Kossen J, Kuhn L, Gal Y.",2024,"Large language model (LLM) systems, such as ChatGPT<sup>1</sup> or Gemini<sup>2</sup>, can show impressive reasoning and question-answering capabilities but often 'hallucinate' false outputs and unsubstantiated answers<sup>3,4</sup>. Answering unreliably or without the necessary information prevents adoption in diverse fields, with problems including fabrication of legal precedents<sup>5</sup> or untrue facts in news articles<sup>6</sup> and even posing a risk to human life in medical domains such as radiology<sup>7</sup>. Encouraging truthfulness through supervision or reinforcement has been only partially successful<sup>8</sup>. Researchers need a general method for detecting hallucinations in LLMs that works even with new and unseen questions to which humans might not know the answer. Here we develop new methods grounded in statistics, proposing entropy-based uncertainty estimators for LLMs to detect a subset of hallucinations-confabulations-which are arbitrary and incorrect generations. Our method addresses the fact that one idea can be expressed in many ways by computing uncertainty at the level of meaning rather than specific sequences of words. Our method works across datasets and tasks without a priori knowledge of the task, requires no task-specific data and robustly generalizes to new tasks not seen before. By detecting when a prompt is likely to produce a confabulation, our method helps users understand when they must take extra care with LLMs and opens up new possibilities for using LLMs that are otherwise prevented by their unreliability.",Y,cc by
10,PMC10701029,10.1155/2023/9784387,Retracted: Location-Dependent Query Processing: Semantic Cache for Real-Time Smart City Analytics.,And Biomechanics AB.,2023,[This retracts the article DOI: 10.1155/2021/9958647.].,Y,cc by
11,PMC10747896,10.3390/s23249652,A Privacy-Preserving Trajectory Publishing Method Based on Multi-Dimensional Sub-Trajectory Similarities.,"Shen H, Wang Y, Zhang M.",2023,"With the popularity of location services and the widespread use of trajectory data, trajectory privacy protection has become a popular research area. <i>k</i>-anonymity technology is a common method for achieving privacy-preserved trajectory publishing. When constructing virtual trajectories, most existing trajectory <i>k</i>-anonymity methods just consider point similarity, which results in a large dummy trajectory space. Suppose there are <i>n</i> similar point sets, each consisting of <i>m</i> points. The size of the space is then mn. Furthermore, to choose suitable <i>k</i>- 1 dummy trajectories for a given real trajectory, these methods need to evaluate the similarity between each trajectory in the space and the real trajectory, leading to a large performance overhead. To address these challenges, this paper proposes a <i>k</i>-anonymity trajectory privacy protection method based on the similarity of sub-trajectories. This method not only considers the multidimensional similarity of points, but also synthetically considers the area between the historic sub-trajectories and the real sub-trajectories to more fully describe the similarity between sub-trajectories. By quantifying the area enclosed by sub-trajectories, we can more accurately capture the spatial relationship between trajectories. Finally, our approach generates k-1 dummy trajectories that are indistinguishable from real trajectories, effectively achieving <i>k</i>-anonymity for a given trajectory. Furthermore, our proposed method utilizes real historic sub-trajectories to generate dummy trajectories, making them more authentic and providing better privacy protection for real trajectories. In comparison to other frequently employed trajectory privacy protection methods, our method has a better privacy protection effect, higher data quality, and better performance.",Y,cc by
12,PMC11198863,10.1371/journal.pone.0289384,Registered report: Age-preserved semantic memory and the CRUNCH effect manifested as differential semantic control networks: An fMRI study.,"Haitas N, Dubuc J, Massé-Leblanc C, Chamberland V, Amiri M, Glatard T, Wilson M, Joanette Y, Steffener J.",2024,"Semantic memory representations are generally well maintained in aging, whereas semantic control is thought to be more affected. To explain this phenomenon, this study tested the predictions of the Compensation-Related Utilization of Neural Circuits Hypothesis (CRUNCH), focusing on task demands in aging as a possible framework. The CRUNCH effect would manifest itself in semantic tasks through a compensatory increase in neural activation in semantic control network regions but only up to a certain threshold of task demands. This study compares 39 younger (20-35 years old) with 39 older participants (60-75 years old) in a triad-based semantic judgment task performed in an fMRI scanner while manipulating task demand levels (low versus high) through semantic distance. In line with the CRUNCH predictions, differences in neurofunctional activation and behavioral performance (accuracy and response times) were expected in younger versus older participants in the low- versus high-demand conditions, which should be manifested in semantic control Regions of Interest (ROIs). Our older participants had intact behavioral performance, as proposed in the literature for semantic memory tasks (maintained accuracy and slower response times (RTs)). Age-invariant behavioral performance in the older group compared to the younger one is necessary to test the CRUNCH predictions. The older adults were also characterized by high cognitive reserve, as our neuropsychological tests showed. Our behavioral results confirmed that our task successfully manipulated task demands: error rates, RTs and perceived difficulty increased with increasing task demands in both age groups. We did not find an interaction between age group and task demand, or a statistically significant difference in activation between the low- and high-demand conditions for either RTs or accuracy. As for brain activation, we did not find the expected age group by task demand interaction, or a significant main effect of task demand. Overall, our results are compatible with some neural activation in the semantic network and the semantic control network, largely in frontotemporoparietal regions. ROI analyses demonstrated significant effects (but no interactions) of task demand in the left and right inferior frontal gyrus, the left posterior middle temporal gyrus, the posterior inferior temporal gyrus and the prefrontal gyrus. Overall, our test did not confirm the CRUNCH predictions.",Y,cc by
13,PMC11086904,10.1371/journal.pone.0302333,A novel code representation for detecting Java code clones using high-level and abstract compiled code representations.,"Quradaa FH, Shahzad S, Saeed R, Sufyan MM.",2024,"In software development, it's common to reuse existing source code by copying and pasting, resulting in the proliferation of numerous code clones-similar or identical code fragments-that detrimentally affect software quality and maintainability. Although several techniques for code clone detection exist, many encounter challenges in effectively identifying semantic clones due to their inability to extract syntax and semantics information. Fewer techniques leverage low-level source code representations like bytecode or assembly for clone detection. This work introduces a novel code representation for identifying syntactic and semantic clones in Java source code. It integrates high-level features extracted from the Abstract Syntax Tree with low-level features derived from intermediate representations generated by static analysis tools, like the Soot framework. Leveraging this combined representation, fifteen machine-learning models are trained to effectively detect code clones. Evaluation on a large dataset demonstrates the models' efficacy in accurately identifying semantic clones. Among these classifiers, ensemble classifiers, such as the LightGBM classifier, exhibit exceptional accuracy. Linearly combining features enhances the effectiveness of the models compared to multiplication and distance combination techniques. The experimental findings indicate that the proposed method can outperform the current clone detection techniques in detecting semantic clones.",Y,cc by
14,PMC11139993,10.1038/s41598-024-63195-5,Multiscale knowledge distillation with attention based fusion for robust human activity recognition.,"Yuan Z, Yang Z, Ning H, Tang X.",2024,"Knowledge distillation is an effective approach for training robust multi-modal machine learning models when synchronous multimodal data are unavailable. However, traditional knowledge distillation techniques have limitations in comprehensively transferring knowledge across modalities and models. This paper proposes a multiscale knowledge distillation framework to address these limitations. Specifically, we introduce a multiscale semantic graph mapping (SGM) loss function to enable more comprehensive knowledge transfer between teacher and student networks at multiple feature scales. We also design a fusion and tuning (FT) module to fully utilize correlations within and between different data types of the same modality when training teacher networks. Furthermore, we adopt transformer-based backbones to improve feature learning compared to traditional convolutional neural networks. We apply the proposed techniques to multimodal human activity recognition and compared with the baseline method, it improved by 2.31% and 0.29% on the MMAct and UTD-MHAD datasets. Ablation studies validate the necessity of each component.",Y,cc by
15,PMC11053063,10.1038/s41598-024-58422-y,Rate splitting with semantics as a generalized multi-access framework for intelligent reflecting surfaces.,"Jagatheesaperumal SK, Yang Z, Hassan MR, Hassan MM, Fortino G.",2024,"The rapid advancement of modern communication technologies necessitates the development of generalized multi-access frameworks and the continuous implementation of rate splitting, augmented with semantic awareness. This trend, coupled with the mounting pressure on wireless services, underscores the need for intelligent approaches to radio signal propagation. In response to these challenges, intelligent reflecting surfaces (IRS) have garnered significant attention for their ability to control data transmission systems in a goal-oriented and dynamic manner. This innovation is largely attributed to equitable resource allocation and the dynamic enhancement of network performance. However, the integration of rate-splitting multi-access (RSMA) architecture with semantic considerations imposes stringent requirements on IRS platforms to ensure seamless connectivity and broad coverage for a diverse user base without interference. Semantic communications hinge on a knowledge base-a centralized repository of integrated information related to the transmitted data-which becomes critically important in multi-antenna scenarios. This article proposes a novel set of design strategies for RSMA-IRS systems, enabled by reconfigurable intelligent surface synergizing with semantic communication principles. An experimental analysis is presented, demonstrating the effectiveness of these design guidelines in the context of Beyond 5G/6G communication systems. The RSMA-IRS model, infused with semantic communication, offers a promising solution for future wireless networks. Performance evaluations of the proposed approach reveal that, despite an increase in the number of users, the delay in the RSMA-IRS framework incorporating semantics is 2.94% less than that of a RSMA-IRS system without semantic integration.",Y,cc by
16,PMC11189566,10.1038/s41467-024-49528-y,Simple autonomous agents can enhance creative semantic discovery by human groups.,"Ueshima A, Jones MI, Christakis NA.",2024,"Innovation is challenging, and theory and experiments indicate that groups may be better able to identify and preserve innovations than individuals. But innovation within groups faces its own challenges, including groupthink and truncated diffusion. We performed experiments involving a game in which people search for ideas in various conditions: alone, in networked social groups, or in networked groups featuring autonomous agents (bots). The objective was to search a semantic space of 20,000 nouns with defined similarities for an arbitrary noun with the highest point value. Participants (N = 1875) were embedded in networks (n = 125) of 15 nodes to which we sometimes added 2 bots. The bots had 3 possible strategies: they shared a random noun generated by their immediate neighbors, or a noun most similar from among those identified, or a noun least similar. We first confirm that groups are better able to explore a semantic space than isolated individuals. Then we show that when bots that share the most similar noun operate in groups facing a semantic space that is relatively easy to navigate, group performance is superior. Simple autonomous agents with interpretable behavior can affect the capacity for creative discovery of human groups.",Y,cc by
17,PMC11035687,10.1038/s42003-024-06162-0,Revealing the mechanisms of semantic satiation with deep learning models.,"Zhang X, Lian J, Yu Z, Tang H, Liang D, Liu J, Liu JK.",2024,"The phenomenon of semantic satiation, which refers to the loss of meaning of a word or phrase after being repeated many times, is a well-known psychological phenomenon. However, the microscopic neural computational principles responsible for these mechanisms remain unknown. In this study, we use a deep learning model of continuous coupled neural networks to investigate the mechanism underlying semantic satiation and precisely describe this process with neuronal components. Our results suggest that, from a mesoscopic perspective, semantic satiation may be a bottom-up process. Unlike existing macroscopic psychological studies that suggest that semantic satiation is a top-down process, our simulations use a similar experimental paradigm as classical psychology experiments and observe similar results. Satiation of semantic objectives, similar to the learning process of our network model used for object recognition, relies on continuous learning and switching between objects. The underlying neural coupling strengthens or weakens satiation. Taken together, both neural and network mechanisms play a role in controlling semantic satiation.",Y,cc by
18,PMC11151246,,Prediction of the stage shift growth of early-stage lung adenocarcinomas by volume-doubling time,"Tang E, Wu Y, Chen C, Wu F.",2024,,Y,cc by-nc-nd
19,PMC11139306,10.1371/journal.pbio.3002622,Negation mitigates rather than inverts the neural representations of adjectives.,"Zuanazzi A, Ripollés P, Lin WM, Gwilliams L, King JR, Poeppel D.",2024,"Combinatoric linguistic operations underpin human language processes, but how meaning is composed and refined in the mind of the reader is not well understood. We address this puzzle by exploiting the ubiquitous function of negation. We track the online effects of negation (""not"") and intensifiers (""really"") on the representation of scalar adjectives (e.g., ""good"") in parametrically designed behavioral and neurophysiological (MEG) experiments. The behavioral data show that participants first interpret negated adjectives as affirmative and later modify their interpretation towards, but never exactly as, the opposite meaning. Decoding analyses of neural activity further reveal significant above chance decoding accuracy for negated adjectives within 600 ms from adjective onset, suggesting that negation does not invert the representation of adjectives (i.e., ""not bad"" represented as ""good""); furthermore, decoding accuracy for negated adjectives is found to be significantly lower than that for affirmative adjectives. Overall, these results suggest that negation mitigates rather than inverts the neural representations of adjectives. This putative suppression mechanism of negation is supported by increased synchronization of beta-band neural activity in sensorimotor areas. The analysis of negation provides a steppingstone to understand how the human brain represents changes of meaning over time.",Y,cc by
20,PMC10586433,10.1155/2023/9790234,Retracted: A Lightweight Semantic Segmentation Algorithm Based on Deep Convolutional Neural Networks.,Intelligence And Neuroscience C.,2023,[This retracts the article DOI: 10.1155/2022/5339664.].,Y,cc by
21,PMC10973368,10.1038/s41598-024-57408-0,A deep inverse convolutional neural network-based semantic classification method for land cover remote sensing images.,"Wang M, She A, Chang H, Cheng F, Yang H.",2024,"The imbalance of land cover categories is a common problem. Some categories appear less frequently in the image, while others may occupy the vast majority of the proportion. This imbalance can lead the classifier to tend to predict categories with higher frequency of occurrence, while the recognition effect on minority categories is poor. In view of the difficulty of land cover remote sensing image multi-target semantic classification, a semantic classification method of land cover remote sensing image based on depth deconvolution neural network is proposed. In this method, the land cover remote sensing image semantic segmentation algorithm based on depth deconvolution neural network is used to segment the land cover remote sensing image with multi-target semantic segmentation; Four semantic features of color, texture, shape and size in land cover remote sensing image are extracted by using the semantic feature extraction method of remote sensing image based on improved sequential clustering algorithm; The classification and recognition method of remote sensing image semantic features based on random forest algorithm is adopted to classify and identify four semantic feature types of land cover remote sensing image, and realize the semantic classification of land cover remote sensing image. The experimental results show that after this method classifies the multi-target semantic types of land cover remote sensing images, the average values of Dice similarity coefficient and Hausdorff distance are 0.9877 and 0.9911 respectively, which can accurately classify the multi-target semantic types of land cover remote sensing images.",Y,cc by
22,PMC11150377,10.1038/s41598-024-63623-6,RA-Net: reverse attention for generalizing residual learning.,"Wang Z, Xie X, Yang J, Song X.",2024,"Since residual learning was proposed, identity mapping has been widely utilized in various neural networks. The method enables information transfer without any attenuation, which plays a significant role in training deeper networks. However, interference with unhindered transmission also affects the network's performance. Accordingly, we propose a generalized residual learning architecture called reverse attention (RA), which applies high-level semantic features to supervise low-level information in the identity mapping branch. It means that higher semantic features selectively transmit low-level information to deeper layers. In addition, we propose a Modified Global Response Normalization(M-GRN) to implement reverse attention. RA-Net is derived by embedding M-GRN in the residual learning framework. The experiments show that the RA-Net brings significant improvements over residual networks on typical computer vision tasks. For classification on ImageNet-1K, compared with resnet101, RA-Net improves the Top-1 accuracy by 1.7% with comparable parameters and computational cost. For COCO detection, on Faster R-CNN, reverse attention improves box AP by 1.9%. Meanwhile, reverse attention improves UpperNet's mIoU by 0.7% on ADE20K segmentation.",Y,cc by
23,PMC10567481,10.1155/2023/9837126,Retracted: Emotional Intervention and Education System Construction for Rural Children Based on Semantic Analysis.,International OT.,2023,[This retracts the article DOI: 10.1155/2022/1073717.].,Y,cc by
24,PMC10821911,10.1038/s41598-024-52873-z,Effects of workbook training using editorials and newspaper articles in adults with preclinical stage of dementia.,"Jin S, Yoon JH, Na DL.",2024,"Early detection and intervention in individuals in the pre-clinical stage of dementia are crucial. This study aimed to examine whether there are significant differences in (1) word retrieval, (2) subjective communication ability, (3) intervention satisfaction through the 'Fill-in-the-blanks in editorial and newspaper articles' training in patients with subjective cognitive decline and mild cognitive impairment corresponding to the pre-clinical stage of dementia. Ninety-nine patients (50 in the intervention group and 49 in the control group) aged 50-84 years were administered pre- and post-test after 6 weeks of intervention (30 sessions). Regarding word retrieval, there were significant intervention effects on confrontation naming, semantic fluency, and phonemic fluency. The majority of participants in the intervention group were highly satisfied with the training. In terms of intervention satisfaction, the majority of the participants in the intervention group showed high satisfaction with all the questions. This result confirmed the improvement of word retrieval ability through mass communication content-based 'Fill-in-the-blanks' training, and ultimately helps to provide a clinical basis for applying this intervention to prevent dementia.",Y,cc by
25,PMC11178556,10.1007/s11548-024-03091-5,EndoViT: pretraining vision transformers on a large collection of endoscopic images.,"Batić D, Holm F, Özsoy E, Czempiel T, Navab N.",2024,"<h4>Purpose</h4>Automated endoscopy video analysis is essential for assisting surgeons during medical procedures, but it faces challenges due to complex surgical scenes and limited annotated data. Large-scale pretraining has shown great success in natural language processing and computer vision communities in recent years. These approaches reduce the need for annotated data, which is of great interest in the medical domain. In this work, we investigate endoscopy domain-specific self-supervised pretraining on large collections of data.<h4>Methods</h4>To this end, we first collect Endo700k, the largest publicly available corpus of endoscopic images, extracted from nine public Minimally Invasive Surgery (MIS) datasets. Endo700k comprises more than 700,000 images. Next, we introduce EndoViT, an endoscopy-pretrained Vision Transformer (ViT), and evaluate it on a diverse set of surgical downstream tasks.<h4>Results</h4>Our findings indicate that domain-specific pretraining with EndoViT yields notable advantages in complex downstream tasks. In the case of action triplet recognition, our approach outperforms ImageNet pretraining. In semantic segmentation, we surpass the state-of-the-art (SOTA) performance. These results demonstrate the effectiveness of our domain-specific pretraining approach in addressing the challenges of automated endoscopy video analysis.<h4>Conclusion</h4>Our study contributes to the field of medical computer vision by showcasing the benefits of domain-specific large-scale self-supervised pretraining for vision transformers. We release both our code and pretrained models to facilitate further research in this direction: https://github.com/DominikBatic/EndoViT .",Y,cc by
26,PMC10991757,10.3389/fpsyg.2024.1308098,"Measuring the menu, not the food: ""psychometric"" data may instead measure ""lingometrics"" (and miss its greatest potential).","Arnulf JK, Olsson UH, Nimon K.",2024,"This is a review of a range of empirical studies that use digital text algorithms to predict and model response patterns from humans to Likert-scale items, using texts only as inputs. The studies show that statistics used in construct validation is predictable on sample and individual levels, that this happens across languages and cultures, and that the relationship between variables are often semantic instead of empirical. That is, the relationships among variables are given a priori and evidently computable as such. We explain this by replacing the idea of ""nomological networks"" with ""semantic networks"" to designate computable relationships between abstract concepts. Understanding constructs as nodes in semantic networks makes it clear why psychological research has produced constant average explained variance at 42% since 1956. Together, these findings shed new light on the formidable capability of human minds to operate with fast and intersubjectively similar semantic processing. Our review identifies a categorical error present in much psychological research, measuring representations instead of the purportedly represented. We discuss how this has grave consequences for the empirical truth in research using traditional psychometric methods.",Y,cc by
27,PMC11076349,10.1007/s00701-024-06062-6,An update on tests used for intraoperative monitoring of cognition during awake craniotomy.,"de Zwart B, Ruis C.",2024,"<h4>Purpose</h4>Mapping higher-order cognitive functions during awake brain surgery is important for cognitive preservation which is related to postoperative quality of life. A systematic review from 2018 about neuropsychological tests used during awake craniotomy made clear that until 2017 language was most often monitored and that the other cognitive domains were underexposed (Ruis, J Clin Exp Neuropsychol 40(10):1081-1104, 218). The field of awake craniotomy and cognitive monitoring is however developing rapidly. The aim of the current review is therefore, to investigate whether there is a change in the field towards incorporation of new tests and more complete mapping of (higher-order) cognitive functions.<h4>Methods</h4>We replicated the systematic search of the study from 2018 in PubMed and Embase from February 2017 to November 2023, yielding 5130 potentially relevant articles. We used the artificial machine learning tool ASReview for screening and included 272 papers that gave a detailed description of the neuropsychological tests used during awake craniotomy.<h4>Results</h4>Comparable to the previous study of 2018, the majority of studies (90.4%) reported tests for assessing language functions (Ruis, J Clin Exp Neuropsychol 40(10):1081-1104, 218). Nevertheless, an increasing number of studies now also describe tests for monitoring visuospatial functions, social cognition, and executive functions.<h4>Conclusions</h4>Language remains the most extensively tested cognitive domain. However, a broader range of tests are now implemented during awake craniotomy and there are (new developed) tests which received more attention. The rapid development in the field is reflected in the included studies in this review. Nevertheless, for some cognitive domains (e.g., executive functions and memory), there is still a need for developing tests that can be used during awake surgery.",Y,cc by
28,PMC10874957,10.1038/s41598-024-54640-6,Transductive meta-learning with enhanced feature ensemble for few-shot semantic segmentation.,"Karimi A, Poullis C.",2024,"This paper addresses few-shot semantic segmentation and proposes a novel transductive end-to-end method that overcomes three key problems affecting performance. First, we present a novel ensemble of visual features learned from pretrained classification and semantic segmentation networks with the same architecture. Our approach leverages the varying discriminative power of these networks, resulting in rich and diverse visual features that are more informative than a pretrained classification backbone that is not optimized for dense pixel-wise classification tasks used in most state-of-the-art methods. Secondly, the pretrained semantic segmentation network serves as a base class extractor, which effectively mitigates false positives that occur during inference time and are caused by base objects other than the object of interest. Thirdly, a two-step segmentation approach using transductive meta-learning is presented to address the episodes with poor similarity between the support and query images. The proposed transductive meta-learning method addresses the prediction by first learning the relationship between labeled and unlabeled data points with matching support foreground to query features (intra-class similarity) and then applying this knowledge to predict on the unlabeled query image (intra-object similarity), which simultaneously learns propagation and false positive suppression. To evaluate our method, we performed experiments on benchmark datasets, and the results demonstrate significant improvement with minimal trainable parameters of 2.98M. Specifically, using Resnet-101, we achieve state-of-the-art performance for both 1-shot and 5-shot Pascal-[Formula: see text], as well as for 1-shot and 5-shot COCO-[Formula: see text].",Y,cc by
29,PMC10912646,10.1038/s41598-024-51793-2,The interplay of semantic and syntactic processing across hemispheres.,"Kim S, Nam K, Lee EH.",2024,"The current study investigated the hemispheric dynamics underlying semantic and syntactic priming in lexical decision tasks. Utilizing primed-lateralized paradigms, we observed a distinct pattern of semantic priming contingent on the priming hemisphere. The right hemisphere (RH) exhibited robust semantic priming irrespective of syntactic congruency between prime and target, underscoring its proclivity for semantic processing. Conversely, the left hemisphere (LH) demonstrated slower response times for semantically congruent yet syntactically incongruent word pairs, highlighting its syntactic processing specialization. Additionally, nonword data revealed a hemispheric divergence in syntactic processing, with the LH showing significant intrahemispheric syntactic priming. These findings illuminate the intrinsic hemispheric specializations for semantic and syntactic processing, offering empirical support for serial processing models. The study advances our understanding of the complex interplay between semantic and syntactic factors in hemispheric interactions.",Y,cc by
30,PMC10771426,10.1038/s41598-023-50242-w,Semantic embedding based online cross-modal hashing method.,"Zhang M, Li J, Zheng X.",2024,"Hashing has been extensively utilized in cross-modal retrieval due to its high efficiency in handling large-scale, high-dimensional data. However, most existing cross-modal hashing methods operate as offline learning models, which learn hash codes in a batch-based manner and prove to be inefficient for streaming data. Recently, several online cross-modal hashing methods have been proposed to address the streaming data scenario. Nevertheless, these methods fail to fully leverage the semantic information and accurately optimize hashing in a discrete fashion. As a result, both the accuracy and efficiency of online cross-modal hashing methods are not ideal. To address these issues, this paper introduces the Semantic Embedding-based Online Cross-modal Hashing (SEOCH) method, which integrates semantic information exploitation and online learning into a unified framework. To exploit the semantic information, we map the semantic labels to a latent semantic space and construct a semantic similarity matrix to preserve the similarity between new data and existing data in the Hamming space. Moreover, we employ a discrete optimization strategy to enhance the efficiency of cross-modal retrieval for online hashing. Through extensive experiments on two publicly available multi-label datasets, we demonstrate the superiority of the SEOCH method.",Y,cc by
31,PMC11019695,10.1364/boe.510908,Improved dual-aggregation polyp segmentation network combining a pyramid vision transformer with a fully convolutional network.,"Li F, Huang Z, Zhou L, Chen Y, Tang S, Ding P, Peng H, Chu Y.",2024,"Automatic and precise polyp segmentation in colonoscopy images is highly valuable for diagnosis at an early stage and surgery of colorectal cancer. Nevertheless, it still posed a major challenge due to variations in the size and intricate morphological characteristics of polyps coupled with the indistinct demarcation between polyps and mucosas. To alleviate these challenges, we proposed an improved dual-aggregation polyp segmentation network, dubbed Dua-PSNet, for automatic and accurate full-size polyp prediction by combining both the transformer branch and a fully convolutional network (FCN) branch in a parallel style. Concretely, in the transformer branch, we adopted the B3 variant of pyramid vision transformer v2 (PVTv2-B3) as an image encoder for capturing multi-scale global features and modeling long-distant interdependencies between them whilst designing an innovative multi-stage feature aggregation decoder (MFAD) to highlight critical local feature details and effectively integrate them into global features. In the decoder, the adaptive feature aggregation (AFA) block was constructed for fusing high-level feature representations of different scales generated by the PVTv2-B3 encoder in a stepwise adaptive manner for refining global semantic information, while the ResidualBlock module was devised to mine detailed boundary cues disguised in low-level features. With the assistance of the selective global-to-local fusion head (SGLFH) module, the resulting boundary details were aggregated selectively with these global semantic features, strengthening these hierarchical features to cope with scale variations of polyps. The FCN branch embedded in the designed ResidualBlock module was used to encourage extraction of highly merged fine features to match the outputs of the Transformer branch into full-size segmentation maps. In this way, both branches were reciprocally influenced and complemented to enhance the discrimination capability of polyp features and enable a more accurate prediction of a full-size segmentation map. Extensive experiments on five challenging polyp segmentation benchmarks demonstrated that the proposed Dua-PSNet owned powerful learning and generalization ability and advanced the state-of-the-art segmentation performance among existing cutting-edge methods. These excellent results showed our Dua-PSNet had great potential to be a promising solution for practical polyp segmentation tasks in which wide variations of data typically occurred.",Y,
32,PMC10984971,10.1038/s41598-024-58190-9,Spatial-temporal graph neural ODE networks for skeleton-based action recognition.,"Pan L, Lu J, Tang X.",2024,"In the field of skeleton-based action recognition, accurately recognizing human actions is crucial for applications such as virtual reality and motion analysis. However, this task faces challenges such intraindividual action differences and long-term temporal dependencies. To address these challenges, we propose an innovative model called spatial-temporal graph neural ordinary differential equations (STG-NODE). First, in the data preprocessing stage, the dynamic time warping (DTW) algorithm is used to normalize and calculate 3D skeleton data to facilitate the derivation of customized adjacency matrices for improving the influence of intraindividual action differences. Secondly, a custom ordinary differential equation (ODE) integrator is applied based on the initial conditions of the temporal features, producing a solution function that simulates the dynamic evolution trend of the events of interest. Finally, the outstanding ODE solver is used to numerically solve the time features based on the solution function to increase the influence of long-term dependencies on the recognition accuracy of the model and provide it with a more powerful temporal modeling ability. Through extensive experiments conducted on the NTU RGB+D 60 and Kinetics Skeleton 400 benchmark datasets, we demonstrate the superior performance of STG-NODE in the action recognition domain. The success of the STG-NODE model also provides new ideas and methods for the future development of the action recognition field.",Y,cc by
33,PMC11026521,10.1038/s41598-024-59735-8,Computed tomography-based automated measurement of abdominal aortic aneurysm using semantic segmentation with active learning.,"Kim T, On S, Gwon JG, Kim N.",2024,"Accurate measurement of abdominal aortic aneurysm is essential for selecting suitable stent-grafts to avoid complications of endovascular aneurysm repair. However, the conventional image-based measurements are inaccurate and time-consuming. We introduce the automated workflow including semantic segmentation with active learning (AL) and measurement using an application programming interface of computer-aided design. 300 patients underwent CT scans, and semantic segmentation for aorta, thrombus, calcification, and vessels was performed in 60-300 cases with AL across five stages using UNETR, SwinUNETR, and nnU-Net consisted of 2D, 3D U-Net, 2D-3D U-Net ensemble, and cascaded 3D U-Net. 7 clinical landmarks were automatically measured for 96 patients. In AL stage 5, 3D U-Net achieved the highest dice similarity coefficient (DSC) with statistically significant differences (p < 0.01) except from the 2D-3D U-Net ensemble and cascade 3D U-Net. SwinUNETR excelled in 95% Hausdorff distance (HD95) with significant differences (p < 0.01) except from UNETR and 3D U-Net. DSC of aorta and calcification were saturated at stage 1 and 4, whereas thrombus and vessels were continuously improved at stage 5. The segmentation time between the manual and AL-corrected segmentation using the best model (3D U-Net) was reduced to 9.51 ± 1.02, 2.09 ± 1.06, 1.07 ± 1.10, and 1.07 ± 0.97 min for the aorta, thrombus, calcification, and vessels, respectively (p < 0.001). All measurement and tortuosity ratio measured - 1.71 ± 6.53 mm and - 0.15 ± 0.25. We developed an automated workflow with semantic segmentation and measurement, demonstrating its efficiency compared to conventional methods.",Y,cc by
34,PMC11007792,10.1002/dad2.12577,Examining the propensity and nature of criminal risk behaviours in frontotemporal dementia syndromes and Alzheimer's disease.,"Kumfor F, Wei G, Ries N, Bennett H, D'Mello M, Kaizik C, Piguet O, Hodges JR.",2024,"<h4>Introduction</h4>Some people with dementia develop changes in behaviour and cognition that may lead to interactions with police or the legal system. However, large, prospective case-control studies examining these behaviours are lacking.<h4>Methods</h4>One hundred and forty-four people with dementia and 53 controls completed the Misdemeanours and Transgressions Screener.<h4>Results</h4>Criminal risk behaviours were reported in: 65.6% of behavioural-variant frontotemporal dementia, 46.2% of right-lateralised semantic dementia, and 27.0% of Alzheimer's disease patients. In 19.1% of patients these behaviours led to contact with police or authority figures. Compared to controls, people with dementia showed higher rates of physical assault (<i>p </i>= 0.024), financial/professional recklessness (<i>p </i>= 0.009), and inappropriate behaviours (<i>p</i> <i> </i>= 0.052).<h4>Discussion</h4>Criminal risk behaviours are common across dementia subtypes and may be one of the first clinical signs of frontotemporal dementia. Further research to understand how to balance risk minimisation with an individual's liberties as well as the inappropriate criminalisation of people with dementia is needed.<h4>Highlights</h4>The Misdemeanours and Transgressions Screener is a new tool to assess criminal risk behaviours.Forty-seven percent of patients with dementia show criminal risk behaviour after dementia onset.Behaviours included verbal abuse, traffic violations, physical assault.New onset of criminal risk behaviours >50 years is a clinical sign for frontotemporal dementia.",Y,cc by-nc-nd
35,PMC10923834,10.1038/s41598-024-56211-1,MFCA-Net: a deep learning method for semantic segmentation of remote sensing images.,"Li X, Li J.",2024,"Semantic segmentation of remote sensing images (RSI) is an important research direction in remote sensing technology. This paper proposes a multi-feature fusion and channel attention network, MFCA-Net, aiming to improve the segmentation accuracy of remote sensing images and the recognition performance of small target objects. The architecture is built on an encoding-decoding structure. The encoding structure includes the improved MobileNet V2 (IMV2) and multi-feature dense fusion (MFDF). In IMV2, the attention mechanism is introduced twice to enhance the feature extraction capability, and the design of MFDF can obtain more dense feature sampling points and larger receptive fields. In the decoding section, three branches of shallow features of the backbone network are fused with deep features, and upsampling is performed to achieve the pixel-level classification. Comparative experimental results of the six most advanced methods effectively prove that the segmentation accuracy of the proposed network has been significantly improved. Furthermore, the recognition degree of small target objects is higher. For example, the proposed MFCA-Net achieves about 3.65-23.55% MIoU improvement on the dataset Vaihingen.",Y,cc by
36,PMC11002062,10.1016/j.heliyon.2024.e28701,A Bayesian probabilistic analysis of the use of English modal verbs in L2 writing: Focusing on L1 influence and topic effects.,"Bozdağ FÜ, Morris G, Mo J.",2024,"Modal verbs, with their multifaceted semantic nuances and varied grammatical configurations, present notable challenges for L2 learners and regularly intrigue L2 researchers. This study attempts to investigate and compare how English modal verbs are used by L2 learners from different L1 backgrounds. By exploring the Turkish and Chinese learners' subcorpora of the International Corpus of Learner English (ICLE), this work scrutizes the overall frequencies of nine core English modal verbs as grouped into three major semantic classes along with the influential lexico-syntatic variables that are semantic classes of collocated verbs, grammatical patterns and subject pronominality. The results of a Bayesian probabilistic analysis show that both the Turkish and Chinese learners primed similar modal verbs and constructional preferences without topic as the normalizing factor. While the broader analysis reveals no statistically significant divergences between these two learner groups in English modal verb preferences, a pronounced contextual influence is evident when the dataset narrows to essays on a unified theme. This nuanced shift underscores the intricate relationship between essay topics and linguistic structures, thus emphasizing the pivotal role of context in modal verb usage.",Y,cc by
37,PMC10965085,10.1371/journal.pone.0298650,Anomalous diffusion analysis of semantic evolution in major Indo-European languages.,"Asztalos B, Palla G, Czégel D.",2024,"How do words change their meaning? Although semantic evolution is driven by a variety of distinct factors, including linguistic, societal, and technological ones, we find that there is one law that holds universally across five major Indo-European languages: that semantic evolution is subdiffusive. Using an automated pipeline of diachronic distributional semantic embedding that controls for underlying symmetries, we show that words follow stochastic trajectories in meaning space with an anomalous diffusion exponent α = 0.45 ± 0.05 across languages, in contrast with diffusing particles that follow α = 1. Randomization methods indicate that preserving temporal correlations in semantic change directions is necessary to recover strongly subdiffusive behavior; however, correlations in change sizes play an important role too. We furthermore show that strong subdiffusion is a robust phenomenon under a wide variety of choices in data analysis and interpretation, such as the choice of fitting an ensemble average of displacements or averaging best-fit exponents of individual word trajectories.",Y,cc by
38,PMC11069517,10.1038/s41597-024-03317-w,A Dataset for Evaluating Contextualized Representation of Biomedical Concepts in Language Models.,"Rouhizadeh H, Nikishina I, Yazdani A, Bornet A, Zhang B, Ehrsam J, Gaudet-Blavignac C, Naderi N, Teodoro D.",2024,"Due to the complexity of the biomedical domain, the ability to capture semantically meaningful representations of terms in context is a long-standing challenge. Despite important progress in the past years, no evaluation benchmark has been developed to evaluate how well language models represent biomedical concepts according to their corresponding context. Inspired by the Word-in-Context (WiC) benchmark, in which word sense disambiguation is reformulated as a binary classification task, we propose a novel dataset, BioWiC, to evaluate the ability of language models to encode biomedical terms in context. BioWiC comprises 20'156 instances, covering over 7'400 unique biomedical terms, making it the largest WiC dataset in the biomedical domain. We evaluate BioWiC both intrinsically and extrinsically and show that it could be used as a reliable benchmark for evaluating context-dependent embeddings in biomedical corpora. In addition, we conduct several experiments using a variety of discriminative and generative large language models to establish robust baselines that can serve as a foundation for future research.",Y,cc by
39,PMC11076558,10.1038/s41598-024-61140-0,Universal and cultural factors shape body part vocabularies.,"Tjuka A, Forkel R, List JM.",2024,"Every human has a body. Yet, languages differ in how they divide the body into parts to name them. While universal naming strategies exist, there is also variation in the vocabularies of body parts across languages. In this study, we investigate the similarities and differences in naming two separate body parts with one word, i.e., colexifications. We use a computational approach to create networks of body part vocabularies across languages. The analyses focus on body part networks in large language families, on perceptual features that lead to colexifications of body parts, and on a comparison of network structures in different semantic domains. Our results show that adjacent body parts are colexified frequently. However, preferences for perceptual features such as shape and function lead to variations in body part vocabularies. In addition, body part colexification networks are less varied across language families than networks in the semantic domains of emotion and colour. The study presents the first large-scale comparison of body part vocabularies in 1,028 language varieties and provides important insights into the variability of a universal human domain.",Y,cc by
40,PMC11136954,10.1038/s41598-024-61981-9,An accurate semantic segmentation model for bean seedlings and weeds identification based on improved ERFnet.,"Gao H, Qi M, Du B, Yang S, Li H, Wang T, Zhong W, Tang Y.",2024,"In agricultural production activities, the growth of crops always accompanies the competition of weeds for nutrients and sunlight. In order to mitigate the adverse effects of weeds on yield, we apply semantic segmentation techniques to differentiate between seedlings and weeds, leading to precision weeding. The proposed EPAnet employs a loss function coupled with Cross-entropy loss and Dice loss to enhance attention to feature information. A multi-Decoder cooperative module based on ERFnet is designed to enhance information transfer during feature mapping. The SimAM is introduced to enhance position recognition. DO-CONV is used to replace the traditional convolution Feature Pyramid Networks (FPN) connection layer to integrate feature information, improving the model's performance on leaf edge processing, and is named FDPN. Moreover, the Overall Accuracy has been improved by 0.65%, the mean Intersection over Union (mIoU) by 1.91%, and the Frequency-Weighted Intersection over Union (FWIoU) by 1.19%. Compared to other advanced methods, EPAnet demonstrates superior image segmentation results in complex natural environments with uneven lighting, leaf interference, and shadows.",Y,cc by
41,PMC10928072,10.1038/s41598-024-56571-8,Shared structure of fundamental human experience revealed by polysemy network of basic vocabularies across languages.,"Liang Y, Xu K, Ran Q.",2024,"How are concepts related to fundamental human experiences organized within the human mind? Our insights are drawn from a semantic network created using the Cross-Linguistic Database of Polysemous Basic Vocabulary, which focuses on a broad range of senses extracted from dictionary entries. The database covers 60 basic vocabularies in 61 languages, providing 11,841 senses from 3736 entries, revealing cross-linguistic semantic connections through automatically generated weighted semantic maps. The network comprises 2941 nodes connected by 3573 edges. The nodes representing body parts, motions, and features closely related to human experience occupy wide fields or serve as crucial bridges across semantic domains in the network. The polysemous network of basic vocabularies across languages represents a shared cognitive network of fundamental human experiences, as these semantic connections should be conceived as generally independent of any specific language and are driven by universal characteristics of the real world as perceived by the human mind. The database holds the potential to contribute to research aimed at unraveling the nature of cognitive proximity.",Y,cc by
42,PMC10987776,10.3389/fpls.2024.1361716,A new model construction based on the knowledge graph for mining elite polyphenotype genes in crops.,"Zhang D, Zhao R, Xian G, Kou Y, Ma W.",2024,"Identifying polyphenotype genes that simultaneously regulate important agronomic traits (e.g., plant height, yield, and disease resistance) is critical for developing novel high-quality crop varieties. Predicting the associations between genes and traits requires the organization and analysis of multi-dimensional scientific data. The existing methods for establishing the relationships between genomic data and phenotypic data can only elucidate the associations between genes and individual traits. However, there are relatively few methods for detecting elite polyphenotype genes. In this study, a knowledge graph for traits regulating-genes was constructed by collecting data from the PubMed database and eight other databases related to the staple food crops rice, maize, and wheat as well as the model plant <i>Arabidopsis thaliana</i>. On the basis of the knowledge graph, a model for predicting traits regulating-genes was constructed by combining the data attributes of the gene nodes and the topological relationship attributes of the gene nodes. Additionally, a scoring method for predicting the genes regulating specific traits was developed to screen for elite polyphenotype genes. A total of 125,591 nodes and 547,224 semantic relationships were included in the knowledge graph. The accuracy of the knowledge graph-based model for predicting traits regulating-genes was 0.89, the precision rate was 0.91, the recall rate was 0.96, and the F1 value was 0.94. Moreover, 4,447 polyphenotype genes for 31 trait combinations were identified, among which the rice polyphenotype gene <i>IPA1</i> and the <i>A. thaliana</i> polyphenotype gene <i>CUC2</i> were verified via a literature search. Furthermore, the wheat gene <i>TraesCS5A02G275900</i> was revealed as a potential polyphenotype gene that will need to be further characterized. Meanwhile, the result of venn diagram analysis between the polyphenotype gene datasets (consists of genes that are predicted by our model) and the transcriptome gene datasets (consists of genes that were differential expression in response to disease, drought or salt) showed approximately 70% and 54% polyphenotype genes were identified in the transcriptome datasets of Arabidopsis and rice, respectively. The application of the model driven by knowledge graph for predicting traits regulating-genes represents a novel method for detecting elite polyphenotype genes.",Y,cc by
43,PMC11088250,10.1016/j.heliyon.2024.e30184,Artificial intelligence in judicial adjudication: Semantic biasness classification and identification in legal judgement (SBCILJ).,"Javed K, Li J.",2024,"History reveals that human societies have suffered in terms of social justice due to cognitive bias. Semantic bias tends to amplify cognitive bias. Therefore, the presence of cognitive biases in extensive historical data can potentially result in unethical and allegedly inhumane predictions since AI systems are trained on this data. The innovation of artificial intelligence and its rapid integration across disciplines has prompted questions regarding the subjectivity of the technology. Current research focuses the semantic bias in legal judgment to increase the legitimacy of training data. By the application of general-purpose Artificial Intelligence (AI) algorithms, we classify and detect the semantics bias that is present in the Chinese Artificial Intelligence and Law (CAIL) dataset. Our findings demonstrate that AI models acquire superior prediction power in the CAIL dataset, which is comprised of hundreds of cases, compared to a structured professional risk assessment tool. To assist legal practitioners during this process, innovative approaches that are based on AI may be implemented inside the legal arena. To accomplish this objective, we suggested a classification model for semantic bias that is related to the classification and identification of semantic biases in legal judgment. Our proposed model legal field uses the example of categorization along with the identification of the CAIL dataset. This will be accomplished by identifying the semantics biases in judicial decisions. We used different types of classifiers such as the Support Vector Machine (SVM), Naïve-Bayes (NB), Multi-Layer Perceptron (MLP), and the K-Nearest Neighbour (KNN) to come across the preferred results. SVM got 96.90 %, NB has 88.80 %, MLP has 86.75 % and KNN achieved 85.66 % accuracy whereas SVM achieved greater accuracy as compared to other models. Additionally, we demonstrate that we were able to get a relatively high classification performance when predicting outcomes based just on the semantic bias categorization in judicial judgments that determine the outcome of the case.",Y,cc by-nc-nd
44,PMC11178169,10.1371/journal.pone.0304835,Multi-source heterogeneous blockchain data quality assessment model for enterprise business activities.,"Zhang H, Zhang R, Li S, Du L, Song B, Ji W, Wang J.",2024,"Blockchain-based applications are becoming more and more widespread in business operations. In view of the shortcomings of existing enterprise blockchain evaluation methods, this paper proposes a multi-source heterogeneous blockchain data quality evaluation model for enterprise business activities, so as to achieve efficient evaluation of business activity information consistency, credibility and value. This paper proposes a multi-source heterogeneous blockchain data quality assessment method for enterprise business activities, aiming at the problems that most of the data in enterprise business activities come from different data sources, information representation is inconsistent, information ambiguity between the same block chain is serious, and it is difficult to evaluate the consistency, credibility and value of information. The method firstly proposes an entity information representation method based on the Representation learning for fusing entity category information (CEKGRL) model, which introduces the triad structure of related entities in blockchain, then associates them with enterprise business activity categories, and carries out similarity calculation through contextual information to achieve blockchain information consistency assessment. After that, a trustworthiness characterization method is proposed based on information sources, information comments, and information contents, to obtain the trustworthiness assessment of the business. Finally, based on the information trustworthiness characterization, a value assessment method is introduced to assess the total value of business activity information in the blockchain, and a blockchain quality assessment model is constructed. The experimental results show that the proposed model has great advantages over existing methods in assessing inter-block consistency, intra-block activity information trustworthiness and the value of blockchain.",Y,cc by
45,PMC10957894,10.1038/s41598-024-56897-3,The neural basis of naturalistic semantic and social cognition.,"Thye M, Hoffman P, Mirman D.",2024,"Decoding social environments and engaging meaningfully with other people are critical aspects of human cognition. Multiple cognitive systems, including social and semantic cognition, work alongside each other to support these processes. This study investigated shared processing between social and semantic systems using neuroimaging data collected during movie-viewing, which captures the multimodal environment in which social knowledge is exchanged. Semantic and social content from movie events (event-level) and movie transcripts (word-level) were used in parametric modulation analyses to test (1) the degree to which semantic and social information is processed within each respective network and (2) engagement of the same cross-network regions or the same domain-general hub located within the semantic network during semantic and social processing. Semantic word and event-level content engaged the same fronto-temporo-parietal network and a portion of the semantic hub in the anterior temporal lobe (ATL). Social word and event-level content engaged the supplementary motor area and right angular gyrus within the social network, but only social words engaged the domain-general semantic hub in left ATL. There was evidence of shared processing between the social and semantic systems in the dorsolateral portion of right ATL which was engaged by word and event-level semantic and social content. Overlap between the semantic and social word and event results was highly variable within and across participants, with the most consistent loci of overlap occurring in left inferior frontal, bilateral precentral and supramarginal gyri for social and semantic words and in bilateral superior temporal gyrus extending from ATL posteriorly into supramarginal gyri for social and semantic events. These results indicate a complex pattern of shared and distinct regions for social and semantic cognition during naturalistic processing. PROTOCOL REGISTRATION: The stage 1 protocol for this Registered Report was accepted in principle on October 11, 2022. The protocol, as accepted by the journal, can be found at: https://doi.org/10.17605/OSF.IO/ACWQY .",Y,cc by
46,PMC10824049,10.1162/jocn_a_02057,Evidence of Impaired Remote Experience-near Semantic Memory in Medial Temporal Lobe Amnesia.,"Grilli MD, Sabharwal-Siddiqi S, Thayer SC, Rapcsak SZ, Ekstrom AD.",2023,"Neuropsychological research suggests that ""experience-near"" semantic memory, meaning knowledge attached to a spatiotemporal or event context, is commonly impaired in individuals who have medial temporal lobe amnesia. It is not known if this impairment extends to remotely acquired experience-near knowledge, which is a question relevant to understanding hippocampal/medial temporal lobe functioning. In the present study, we administered a novel semantic memory task designed to target knowledge associated with remote, ""dormant"" concepts, in addition to knowledge associated with active concepts, to four individuals with medial temporal lobe amnesia and eight matched controls. We found that the individuals with medial temporal lobe amnesia generated significantly fewer experience-near semantic memories for both remote concepts and active concepts. In comparison, the generation of abstract or ""experience-far"" knowledge was largely spared in the individuals with medial temporal lobe amnesia, regardless of whether the targets for retrieval were remote or active concepts. We interpret these findings as evidence that the medial temporal lobes may have a sustained role in the retrieval of semantic memories associated with spatiotemporal and event contexts, which are cognitive features often ascribed to episodic memory. These results align with recent theoretical models proposing that the hippocampus/medial temporal lobes support cognitive processes that are involved in, but not exclusive to, episodic memory.",N,
47,PMC10914777,10.1038/s41598-024-56090-6,"Computer programmers show distinct, expertise-dependent brain responses to violations in form and meaning when reading code.","Kuo CH, Prat CS.",2024,"As computer programming becomes more central to the workforce, the need for better models of how it is effectively learned has become more apparent. The current study addressed this gap by recording electrophysiological brain responses as 62 Python programmers with varying skill levels read lines of code with manipulations of form (syntax) and meaning (semantics). At the group level, results showed that manipulations of form resulted in P600 effects, with syntactically invalid code generating more positive deflections in the 500-800 ms range than syntactically valid code. Meaning manipulations resulted in N400 effects, with semantically implausible code generating more negative deflections in the 300-500 ms range than semantically plausible code. Greater Python expertise within the group was associated with greater sensitivity to violations in form. These results support the notion that skilled programming, like skilled natural language learning, is associated with the incorporation of rule-based knowledge into online comprehension processes. Conversely, programmers at all skill levels showed neural sensitivity to meaning manipulations, suggesting that reliance on pre-existing semantic relationships facilitates code comprehension across skill levels.",Y,cc by
48,PMC10497750,10.3389/frai.2023.1225213,Profiling the barriers to the spreading of news using news headlines.,"Sittar A, Mladenić D, Grobelnik M.",2023,"News headlines can be a good data source for detecting the barriers to the spreading of news in news media, which can be useful in many real-world applications. In this study, we utilize semantic knowledge through the inference-based model COMET and the sentiments of news headlines for barrier classification. We consider five barriers, including cultural, economic, political, linguistic, and geographical and different types of news headlines, including health, sports, science, recreation, games, homes, society, shopping, computers, and business. To that end, we collect and label the news headlines automatically for the barriers using the metadata of news publishers. Then, we utilize the extracted common-sense inferences and sentiments as features to detect the barriers to the spreading of news. We compare our approach to the classical text classification methods, deep learning, and transformer-based methods. The results show that (1) the inference-based semantic knowledge provides distinguishable inferences across the 10 categories that can increase the effectiveness and enhance the speed of the classification model; (2) the news of positive sentiments cross the political barrier, whereas the news of negative sentiments cross the cultural, economic, linguistic, and geographical barriers; (3) the proposed approach using inferences-based semantic knowledge and sentiment improves performance compared with using only headlines in barrier classification. The average F1-score for 4 out of 5 barriers has significantly improved as follows: for cultural barriers from 0.41 to 0.47, for economic barriers from 0.39 to 0.55, for political barriers from 0.59 to 0.70 and for geographical barriers from 0.59 to 0.76.",Y,cc by
49,PMC10894874,10.1038/s41598-024-54096-8,A Siamese Swin-Unet for image change detection.,"Tang Y, Cao Z, Guo N, Jiang M.",2024,"The problem of change detection in remote sensing image processing is both difficult and important. It is extensively used in a variety of sectors, including land resource planning, monitoring and forecasting of agricultural plant health, and monitoring and assessment of natural disasters. Remote sensing images provide a large amount of long-term and fully covered data for earth environmental monitoring. A lot of progress has been made thanks to deep learning's quick development. But the majority of deep learning-based change detection techniques currently in use rely on the well-known Convolutional neural network (CNN). However, considering the locality of convolutional operation, CNN unable to master the interplay between global and distant semantic information. Some researches has employ Vision Transformer as a backbone in remote sensing field. Inspired by these researches, in this paper, we propose a network named Siam-Swin-Unet, which is a Siamesed pure Transformer with U-shape construction for remote sensing image change detection. Swin Transformer is a hierarchical vision transformer with shifted windows that can extract global feature. To learn local and global semantic feature information, the dual-time image are fed into Siam-Swin-Unet which is composed of Swin Transformer, Unet Siamesenet and two feature fusion module. Considered the Unet and Siamesenet are effective for change detection, We applied it to the model. The feature fusion module is designed for fusion of dual-time image features, and is efficient and low-compute confirmed by our experiments. Our network achieved 94.67 F1 on the CDD dataset (season varying).",Y,cc by
50,PMC10396762,10.1155/2023/9865767,Retracted: Painting Classification in Art Teaching under Machine Learning from the Perspective of Emotional Semantic Analysis.,Intelligence And Neuroscience C.,2023,[This retracts the article DOI: 10.1155/2022/9592050.].,Y,cc by
51,PMC10396458,10.1155/2023/9817102,Retracted: A Multi-RNN Research Topic Prediction Model Based on Spatial Attention and Semantic Consistency-Based Scientific Influence Modeling.,Intelligence And Neuroscience C.,2023,[This retracts the article DOI: 10.1155/2021/1766743.].,Y,cc by
52,PMC11021394,10.1038/s41597-024-03240-0,"A construction waste landfill dataset of two districts in Beijing, China from high resolution satellite images.","Lin S, Huang L, Liu X, Chen G, Fu Z.",2024,"Construction waste is unavoidable in the process of urban development, causing serious environmental pollution. Accurate assessment of municipal construction waste generation requires building construction waste identification models using deep learning technology. However, this process requires high-quality public datasets for model training and validation. This study utilizes Google Earth and GF-2 images as the data source to construct a specific dataset of construction waste landfills in the Changping and Daxing districts of Beijing, China. This dataset contains 3,653 samples of the original image areas and provides mask-labeled images in the semantic segmentation domains. Each pixel within a construction waste landfill is classified into 4 categories of the image areas, including background area, vacant landfillable area, engineering facility area, and waste dumping area. The dataset contains 237,115,531 pixels of construction waste and 49,724,513 pixels of engineering facilities. The pixel-level semantic segmentation labels are provided to quantify the construction waste yield, which can serve as the basic data for construction waste extraction and yield estimation both for academic and industrial research.",Y,cc by
53,PMC11094178,10.1038/s41598-024-61044-z,Mechanisms upholding the persistence of stigma across 100 years of historical text.,"Charlesworth TES, Hatzenbuehler ML.",2024,"Today, many social groups face negative stereotypes. Is such negativity a stable feature of society and, if so, what mechanisms maintain stability both within and across group targets? Answering these theoretically and practically important questions requires data on dozens of group stereotypes examined simultaneously over historical and societal scales, which is only possible through recent advances in Natural Language Processing. Across two studies, we use word embeddings from millions of English-language books over 100 years (1900-2000) and extract stereotypes for 58 stigmatized groups. Study 1 examines aggregate, societal-level trends in stereotype negativity by averaging across these groups. Results reveal striking persistence in aggregate negativity (no meaningful slope), suggesting that society maintains a stable level of negative stereotypes. Study 2 introduces and tests a new framework identifying potential mechanisms upholding stereotype negativity over time. We find evidence of two key sources of this aggregate persistence: within-group ""reproducibility"" (e.g., stereotype negativity can be maintained by using different traits with the same underlying meaning) and across-group ""replacement"" (e.g., negativity from one group is transferred to other related groups). These findings provide novel historical evidence of mechanisms upholding stigmatization in society and raise new questions regarding the possibility of future stigma change.",Y,cc by
54,PMC11033660,10.1136/bmjhci-2023-100935,Development of a scoring system to quantify errors from semantic characteristics in incident reports.,"Uematsu H, Uemura M, Kurihara M, Yamamoto H, Umemura T, Kitano F, Hiramatsu M, Nagao Y.",2024,"<h4>Objectives</h4>Incident reporting systems are widely used to identify risks and enable organisational learning. Free-text descriptions contain important information about factors associated with incidents. This study aimed to develop error scores by extracting information about the presence of error factors in incidents using an original decision-making model that partly relies on natural language processing techniques.<h4>Methods</h4>We retrospectively analysed free-text data from reports of incidents between January 2012 and December 2022 from Nagoya University Hospital, Japan. The sample data were randomly allocated to equal-sized training and validation datasets. We conducted morphological analysis on free text to segment terms from sentences in the training dataset. We calculated error scores for terms, individual reports and reports from staff groups according to report volume size and compared these with conventional classifications by patient safety experts. We also calculated accuracy, recall, precision and F-score values from the proposed 'report error score'.<h4>Results</h4>Overall, 114 013 reports were included. We calculated 36 131 'term error scores' from the 57 006 reports in the training dataset. There was a significant difference in error scores between reports of incidents categorised by experts as arising from errors (p<0.001, <i>d</i>=0.73 (large)) and other incidents. The accuracy, recall, precision and F-score values were 0.8, 0.82, 0.85 and 0.84, respectively. Group error scores were positively associated with expert ratings (correlation coefficient, 0.66; 95% CI 0.54 to 0.75, p<0.001) for all departments.<h4>Conclusion</h4>Our error scoring system could provide insights to improve patient safety using aggregated incident report data.",Y,cc by-nc
55,PMC11074738,10.21037/qims-23-1384,Semi-supervised learning in diagnosis of infant hip dysplasia towards multisource ultrasound images.,"Li X, Zhang R, Wang Z, Wang J.",2024,"<h4>Background</h4>Automated diagnosis of infant hip dysplasia is heavily affected by the individual differences among infants and ultrasound machines.<h4>Methods</h4>Hip sonographic images of 493 infants from various ultrasound machines were collected in the Department of Orthopedics in Yangzhou Maternal and Child Health Care Service Centre. Herein, we propose a semi-supervised learning method based on a feature pyramid network (FPN) and a contrastive learning scheme based on a Siamese architecture. A large amount of unlabeled data of ultrasound images was used via the Siamese network in the pre-training step, and then a small amount of annotated data for anatomical structures was adopted to train the model for landmark identification and standard plane recognition. The method was evaluated on our collected dataset.<h4>Results</h4>The method achieved a mean Dice similarity coefficient (DSC) of 0.7873 and a mean Hausdorff distance (HD) of 5.0102 in landmark identification, compared to the model without contrastive learning, which had a mean DSC of 0.7734 and a mean HD of 6.1586. The accuracy, precision, and recall of standard plane recognition were 95.4%, 91.64%, and 94.86%, respectively. The corresponding area under the curve (AUC) was 0.982.<h4>Conclusions</h4>This study proposes a semi-supervised deep learning method following Graf's principle, which can better utilize a large volume of ultrasound images from various devices and infants. This method can identify the landmarks of infant hips more accurately than manual operators, thereby improving the efficiency of diagnosis of infant hip dysplasia.",Y,cc by-nc-nd
56,PMC11111678,10.1038/s41598-024-62331-5,Polyp segmentation based on implicit edge-guided cross-layer fusion networks.,"Liu J, Zhang W, Liu Y, Zhang Q.",2024,"Polyps are abnormal tissue clumps growing primarily on the inner linings of the gastrointestinal tract. While such clumps are generally harmless, they can potentially evolve into pathological tumors, and thus require long-term observation and monitoring. Polyp segmentation in gastrointestinal endoscopy images is an important stage for polyp monitoring and subsequent treatment. However, this segmentation task faces multiple challenges: the low contrast of the polyp boundaries, the varied polyp appearance, and the co-occurrence of multiple polyps. So, in this paper, an implicit edge-guided cross-layer fusion network (IECFNet) is proposed for polyp segmentation. The codec pair is used to generate an initial saliency map, the implicit edge-enhanced context attention module aggregates the feature graph output from the encoding and decoding to generate the rough prediction, and the multi-scale feature reasoning module is used to generate final predictions. Polyp segmentation experiments have been conducted on five popular polyp image datasets (Kvasir, CVC-ClinicDB, ETIS, CVC-ColonDB, and CVC-300), and the experimental results show that the proposed method significantly outperforms a conventional method, especially with an accuracy margin of 7.9% on the ETIS dataset.",Y,cc by
57,PMC10824738,10.1038/s41531-024-00644-y,Author Correction: Neurocognitive correlates of semantic memory navigation in Parkinson's disease.,"Toro-Hernández FD, Migeot J, Marchant N, Olivares D, Ferrante F, González-Gómez R, González Campo C, Fittipaldi S, Rojas-Costa GM, Moguilner S, Slachevsky A, Chaná Cuevas P, Ibáñez A, Chaigneau S, García AM.",2024,,Y,cc by
58,PMC11055951,10.1038/s41598-024-60375-1,An improved semantic segmentation algorithm for high-resolution remote sensing images based on DeepLabv3.,"Wang Y, Yang L, Liu X, Yan P.",2024,"High-precision and high-efficiency Semantic segmentation of high-resolution remote sensing images is a challenge. Existing models typically require a significant amount of training data to achieve good classification results and have numerous training parameters. A novel model called MST-DeepLabv3+ was suggested in this paper for remote sensing image classification. It's based on the DeepLabv3+ and can produce better results with fewer train parameters. MST-DeepLabv3+ made three improvements: (1) Reducing the number of model parameters by substituting MobileNetV2 for the Xception in the DeepLabv3+'s backbone network. (2) Adding the attention mechanism module SENet to increase the precision of semantic segmentation. (3) Increasing Transfer Learning to enhance the model's capacity to recognize features, and raise the segmentation accuracy. MST-DeepLabv3+ was tested on international society for photogrammetry and remote sensing (ISPRS) dataset, Gaofen image dataset (GID), and practically applied to the Taikang cultivated land dataset. On the ISPRS dataset, the mean intersection over union (MIoU), overall accuracy (OA), Precision, Recall, and F1-score are 82.47%, 92.13%, 90.34%, 90.12%, and 90.23%, respectively. On the GID dataset, these values are 73.44%, 85.58%, 84.10%, 84.86%, and 84.48%, respectively. The results were as high as 90.77%, 95.47%, 95.28%, 95.02%, and 95.15% on the Taikang cultivated land dataset. The experimental results indicate that MST-DeepLabv3+ effectively improves the accuracy of semantic segmentation of remote sensing images, recognizes the edge information with more completeness, and significantly reduces the parameter size.",Y,cc by
59,PMC11139852,10.1038/s41598-024-63257-8,DASUNet: a deeply supervised change detection network integrating full-scale features.,"Miao R, Meng G, Zhou K, Li Y, Chang R, Zhang G.",2024,"The change detection (CD) technology has greatly improved the ability to interpret land surface changes. Deep learning (DL) methods have been widely used in the field of CD due to its high detection accuracy and application range. DL-based CD methods usually cannot fuse the extracted feature information at full scale, leaving out effective information, and commonly use transfer learning methods, which rely on the original dataset and training weights. To address the above issues, we propose a deeply supervised (DS) change detection network (DASUNet) that fuses full-scale features, which adopts a Siamese architecture, fuses full-scale feature information, and realizes end-to-end training. In order to obtain higher feature information, the network uses atrous spatial pyramid pooling (ASPP) module in the coding stage. In addition, the DS module is used in the decoding stage to exploit feature information at each scale in the final prediction. The experimental comparison shows that the proposed network has the current state-of-the-art performance on the CDD and the WHU-CD, reaching 94.32% and 90.37% on F1, respectively.",Y,cc by
60,PMC10957737,10.3389/fpubh.2024.1337107,Factors influencing public participation behavior relating to government microblogs on COVID-19 updates.,"Shao P, Li M.",2024,"<h4>Introduction</h4>During the global COVID-19 pandemic, densely populated megacities engaged in active international exchanges have faced the most severe impacts from both the disease and the associated infodemic. This study examines the factors influencing public participation behavior on government microblogs in these megacities during the pandemic. It guides megacities in disseminating epidemic information, promoting knowledge on epidemic prevention, managing public opinion, and addressing related matters.<h4>Methods</h4>Utilizing the elaboration likelihood model's central and peripheral routes, drawing on an empirical analysis of 6,677 epidemic-related microblogs from seven Chinese megacities, this study analyses the influence mechanisms influencing public participation behavior and reveals the regulatory role of confirmed case numbers. Meanwhile,a qualitative comparative analysis examines and discusses diferent confgurations of ixn fuential factors.<h4>Results</h4>The study reveals that microblog content richness demonstrates a U-shaped impact on public participation behavior. Conversely, content interaction, content length, and the number of fans positively impact participation, while update frequency has a negative impact. Additionally, the number of new confrmed cases positively regulates the impact of microblog content and publisher characteristics on public participation behavior. Public participation behavior also varies based on publishing time and content semantic features. This study further revealed the different confgurations of influential factors by QCA method.<h4>Conclusion</h4>This study reveals the impact mechanism of the microblog content and publisher characteristics on public participation behavior. It also demonstrates the regulatory role of newly confrmed cases in the way content and publishers' characteristics influence public participation behavior. This study is of great significance for the operation of government microblogs, the release of emergency information, and the promotion of public participation.",Y,cc by
61,PMC10412121,10.1155/2023/9797060,Retracted: An Intelligent Classification System for Cancer Detection Based on DNA Methylation Using ML and Semantic Knowledge in Healthcare.,Intelligence And Neuroscience C.,2023,[This retracts the article DOI: 10.1155/2022/4334852.].,Y,cc by
62,PMC11059262,10.1038/s41598-024-60668-5,Deep learning-aided 3D proxy-bridged region-growing framework for multi-organ segmentation.,"Chen Z, Yao L, Liu Y, Han X, Gong Z, Luo J, Zhao J, Fang G.",2024,"Accurate multi-organ segmentation in 3D CT images is imperative for enhancing computer-aided diagnosis and radiotherapy planning. However, current deep learning-based methods for 3D multi-organ segmentation face challenges such as the need for labor-intensive manual pixel-level annotations and high hardware resource demands, especially regarding GPU resources. To address these issues, we propose a 3D proxy-bridged region-growing framework specifically designed for the segmentation of the liver and spleen. Specifically, a key slice is selected from each 3D volume according to the corresponding intensity histogram. Subsequently, a deep learning model is employed to pinpoint the semantic central patch on this key slice, to calculate the growing seed. To counteract the impact of noise, segmentation of the liver and spleen is conducted on superpixel images created through proxy-bridging strategy. The segmentation process is then extended to adjacent slices by applying the same methodology iteratively, culminating in the comprehensive segmentation results. Experimental results demonstrate that the proposed framework accomplishes segmentation of the liver and spleen with an average Dice Similarity Coefficient of approximately 0.93 and a Jaccard Similarity Coefficient of around 0.88. These outcomes substantiate the framework's capability to achieve performance on par with that of deep learning methods, albeit requiring less guidance information and lower GPU resources.",Y,cc by
63,PMC10973504,10.1038/s41598-024-57419-x,Scoring method of English composition integrating deep learning in higher vocational colleges.,"Feng S, Yu L, Liu F.",2024,"Along with the progress of natural language processing technology and deep learning, the subjectivity, slow feedback, and long grading time of traditional English essay grading have been addressed. Intelligent English automatic scoring has been widely concerned by scholars. Given the limitations of topic relevance feature extraction methods and traditional automatic grading methods for English compositions, a topic decision model is proposed to calculate the topic relevance score of the topic richness in English composition. Then, based on the Score of Relevance Based on Topic Richness (TRSR) calculation method, an intelligent English composition scoring method combining artificial feature extraction and deep learning is designed. From the findings, the Topic Decision (TD) model achieved the best effect only when it was iterated 80 times. The corresponding accuracy, recall and F1 value were 0.97, 0.93 and 0.95 respectively. The model training loss finally stabilized at 0.03. The Intelligent English Composition Grading Method Integrating Deep Learning (DLIECG) method has the best overall performance and the best performance on dataset P. To sum up, the intelligent English composition scoring method has better effectiveness and reliability.",Y,cc by
64,PMC11150514,10.1038/s41598-024-63582-y,EMCMDA: predicting miRNA-disease associations via efficient matrix completion.,"Qin C, Zhang J, Ma L.",2024,"Abundant researches have consistently illustrated the crucial role of microRNAs (miRNAs) in a wide array of essential biological processes. Furthermore, miRNAs have been validated as promising therapeutic targets for addressing complex diseases. Given the costly and time-consuming nature of traditional biological experimental validation methods, it is imperative to develop computational methods. In the work, we developed a novel approach named efficient matrix completion (EMCMDA) for predicting miRNA-disease associations. First, we calculated the similarities across multiple sources for miRNA/disease pairs and combined this information to create a holistic miRNA/disease similarity measure. Second, we utilized this biological information to create a heterogeneous network and established a target matrix derived from this network. Lastly, we framed the miRNA-disease association prediction issue as a low-rank matrix-complete issue that was addressed via minimizing matrix truncated schatten p-norm. Notably, we improved the conventional singular value contraction algorithm through using a weighted singular value contraction technique. This technique dynamically adjusts the degree of contraction based on the significance of each singular value, ensuring that the physical meaning of these singular values is fully considered. We evaluated the performance of EMCMDA by applying two distinct cross-validation experiments on two diverse databases, and the outcomes were statistically significant. In addition, we executed comprehensive case studies on two prevalent human diseases, namely lung cancer and breast cancer. Following prediction and multiple validations, it was evident that EMCMDA proficiently forecasts previously undisclosed disease-related miRNAs. These results underscore the robustness and efficacy of EMCMDA in miRNA-disease association prediction.",Y,cc by
65,PMC10924897,10.1038/s41598-024-56518-z,Sentiment analysis of video danmakus based on MIBE-RoBERTa-FF-BiLSTM.,"Zhao J, Liu H, Wang Y, Zhang W, Zhang X, Li B, Sun T, Qi Y, Zhang S.",2024,"Danmakus are user-generated comments that overlay on videos, enabling real-time interactions between viewers and video content. The emotional orientation of danmakus can reflect the attitudes and opinions of viewers on video segments, which can help video platforms optimize video content recommendation and evaluate users' abnormal emotion levels. Aiming at the problems of low transferability of traditional sentiment analysis methods in the danmaku domain, low accuracy of danmaku text segmentation, poor consistency of sentiment annotation, and insufficient semantic feature extraction, this paper proposes a video danmaku sentiment analysis method based on MIBE-RoBERTa-FF-BiLSTM. This paper constructs a ""Bilibili Must-Watch List and Top Video Danmaku Sentiment Dataset"" by ourselves, covering 10,000 positive and negative sentiment danmaku texts of 18 themes. A new word recognition algorithm based on mutual information (MI) and branch entropy (BE) is used to discover 2610 irregular network popular new words from trigrams to heptagrams in the dataset, forming a domain lexicon. The Maslow's hierarchy of needs theory is applied to guide the consistent sentiment annotation. The domain lexicon is integrated into the feature fusion layer of the RoBERTa-FF-BiLSTM model to fully learn the semantic features of word information, character information, and context information of danmaku texts and perform sentiment classification. Comparative experiments on the dataset show that the model proposed in this paper has the best comprehensive performance among the mainstream models for video danmaku text sentiment classification, with an F1 value of 94.06%, and its accuracy and robustness are also better than other models. The limitations of this paper are that the construction of the domain lexicon still requires manual participation and review, the semantic information of danmaku video content and the positive case preference are ignored.",Y,cc by
66,PMC10848146,10.1145/3543873.3587601,Application of an ontology for model cards to generate computable artifacts for linking machine learning information from biomedical research.,"Amith MT, Cui L, Roberts K, Tao C.",2023,"Model card reports provide a transparent description of machine learning models which includes information about their evaluation, limitations, intended use, etc. Federal health agencies have expressed an interest in model cards report for research studies using machine-learning based AI. Previously, we have developed an ontology model for model card reports to structure and formalize these reports. In this paper, we demonstrate a Java-based library (OWL API, FaCT++) that leverages our ontology to publish computable model card reports. We discuss future directions and other use cases that highlight applicability and feasibility of ontology-driven systems to support FAIR challenges.",N,
67,PMC10943221,10.1038/s41598-024-56976-5,Application of the transformer model algorithm in chinese word sense disambiguation: a case study in chinese language.,"Li L, Li J, Wang H, Nie J.",2024,"This study aims to explore the research methodology of applying the Transformer model algorithm to Chinese word sense disambiguation, seeking to resolve word sense ambiguity in the Chinese language. The study introduces deep learning and designs a Chinese word sense disambiguation model based on the fusion of the Transformer with the Bi-directional Long Short-Term Memory (BiLSTM) algorithm. By utilizing the self-attention mechanism of Transformer and the sequence modeling capability of BiLSTM, this model efficiently captures semantic information and context relationships in Chinese sentences, leading to accurate word sense disambiguation. The model's evaluation is conducted using the PKU Paraphrase Bank, a Chinese text paraphrase dataset. The results demonstrate that the model achieves a precision rate of 83.71% in Chinese word sense disambiguation, significantly outperforming the Long Short-Term Memory algorithm. Additionally, the root mean squared error of this algorithm is less than 17, with a loss function value remaining around 0.14. Thus, this study validates that the constructed Transformer-fused BiLSTM-based Chinese word sense disambiguation model algorithm exhibits both high accuracy and robustness in identifying word senses in the Chinese language. The findings of this study provide valuable insights for advancing the intelligent development of word senses in Chinese language applications.",Y,cc by
68,PMC11106303,10.1038/s41598-024-62342-2,Time division multiplexing based multi-spectral semantic camera for LiDAR applications.,"Kim S, Jeong TI, Kim S, Choi E, Yang E, Song M, Eom TJ, Kim CS, Gliserin A, Kim S.",2024,"The recent progress in the development of measurement systems for autonomous recognition had a substantial impact on emerging technology in numerous fields, especially robotics and automotive applications. In particular, time-of-flight (TOF) based light detection and ranging (LiDAR) systems enable to map the surrounding environmental information over long distances and with high accuracy. The combination of advanced LiDAR with an artificial intelligence platform allows enhanced object recognition and classification, which however still suffers from limitations of inaccuracy and misidentification. Recently, multi-spectral LiDAR systems have been employed to increase the object recognition performance by additionally providing material information in the short-wave infrared (SWIR) range where the reflection spectrum characteristics are typically very sensitive to material properties. However, previous multi-spectral LiDAR systems utilized band-pass filters or complex dispersive optical systems and even required multiple photodetectors, adding complexity and cost. In this work, we propose a time-division-multiplexing (TDM) based multi-spectral LiDAR system for semantic object inference by the simultaneous acquisition of spatial and spectral information. By utilizing the TDM method, we enable the simultaneous acquisition of spatial and spectral information as well as a TOF based distance map with minimized optical loss using only a single photodetector. Our LiDAR system utilizes nanosecond pulses of five different wavelengths in the SWIR range to acquire sufficient material information in addition to 3D spatial information. To demonstrate the recognition performance, we map the multi-spectral image from a human hand, a mannequin hand, a fabric gloved hand, a nitrile gloved hand, and a printed human hand onto an RGB-color encoded image, which clearly visualizes spectral differences as RGB color depending on the material while having a similar shape. Additionally, the classification performance of the multi-spectral image is demonstrated with a convolution neural network (CNN) model using the full multi-spectral data set. Our work presents a compact novel spectroscopic LiDAR system, which provides increased recognition performance and thus a great potential to improve safety and reliability in autonomous driving.",Y,cc by
69,PMC10710517,10.1038/s41598-023-48922-8,Central emotions and hubs in a colexification network.,"Fukuya M, Matsumoto T, Shimada Y, Ikeguchi T.",2023,"By focusing on colexification, we detected central emotions sharing semantic commonalities with many other emotions in terms of a semantic relationship of both similarity and associativity. In analysis, we created colexification networks from multiple languages by assigning a concept to a vertex and colexification to an edge. We identify concepts of emotions with a large weight in the colexification network and specify central emotions by finding hub emotions. Our resultant central emotions are four: ""GOOD,"" ""WANT,"" ""BAD,"" and ""LOVE.""",Y,cc by
70,PMC10761856,10.1038/s41598-023-50687-z,Semantic and episodic processes differently predict false memories in the DRM task.,"Gatti D, Rinaldi L, Mazzoni G, Vecchi T.",2024,"There is a fervent debate about the processes underpinning false memories formation. Seminal theories have suggested that semantic memory would be involved in false memories production, while episodic memory would counter their formation. Yet, direct evidence corroborating such view is still lacking. Here, we tested this possibility by asking participants to perform the Deese-Roediger-McDermott (DRM) task, a typical false memory paradigm, in which they had to study lists of words and subsequently to recognize and distinguish them from new words (i.e., the false memory items). The same participants were also required to perform a semantic task and an episodic-source memory task. Our results showed that a higher number of false memories in the DRM task occurred for those participants with better semantic memory abilities, while a lower number of false memories occurred for participants with better episodic abilities. These findings support a key role of semantic processes in false memory formation and, more generally, help clarify the specific contribution of different memory systems to false recognitions.",Y,cc by
71,PMC10958035,10.1038/s41598-024-55612-6,BSI-MVS: multi-view stereo network with bidirectional semantic information.,"Jia R, Yu J, Hu Z, Yuan F.",2024,"The basic principle of multi-view stereo (MVS) is to perform 3D reconstruction by extracting depth information from multiple views. Most current SOTA MVS networks are based on Vision Transformer, which usually means expensive computational complexity. To reduce computational complexity and improve depth map accuracy, we propose a MVS network with Bidirectional Semantic Information (BSI-MVS). Firstly, we design a Multi-Level Spatial Pyramid module to generate multiple layers of feature map for extracting multi-scale information. Then we propose a 2D Bidirectional-LSTM module to capture bidirectional semantic information at different time steps in the horizontal and vertical directions, which contains abundant depth information. Finally, cost volumes are built based on various levels of feature maps to optimize the final depth map. We experiment on the DTU and BlendedMVS datasets. The result shows that our network, in terms of overall metrics, surpasses TransMVSNet, CasMVSNet, CVP-MVSNet, and AACVP-MVSNet respectively by 17.84%, 36.42%, 14.96%, and 4.86%, which also shows a noticeable performance enhancement in objective metrics and visualizations.",Y,cc by
72,PMC11126598,10.1038/s41598-024-61643-w,Segmentation of void defects in X-ray images of chip solder joints based on PCB-DeepLabV3 algorithm.,"Kong D, Hu X, Gong Z, Zhang D.",2024,"Defects within chip solder joints are usually inspected visually for defects using X-ray imaging to obtain images. The phenomenon of voids inside solder joints is one of the most likely types of defects in the soldering process, and accurate detection of voids becomes difficult due to their irregular shapes, varying sizes, and defocused edges. To address this problem, an X-ray void image segmentation algorithm based on improved PCB-DeepLabV3 is proposed. Firstly, to meet the demand for lightweight and easy deployment in industrial scenarios, mobilenetv2 is used as the feature extraction backbone network of the PCB-DeepLabV3 model; then, Attentional multi-scale two-space pyramid pooling network (AMTPNet) is designed to optimize the shallow feature edges and to improve the ability to capture detailed information; finally, image cropping and cleaning methods are designed to enhance the training dataset, and the improved PCB-DeepLabV3 is applied to the training dataset. The improved PCB-DeepLabV3 model is used to segment the void regions within the solder joints and compared with the classical semantic segmentation models such as Unet, SegNet, PSPNet, and DeeplabV3. The proposed new method enables the solder joint void inspection to get rid of the traditional way of visual inspection, realize intelligent upgrading, and effectively improve the problem of difficult segmentation of the target virtual edges, to obtain the inspection results with higher accuracy.",Y,cc by
73,PMC10866900,10.1038/s41539-024-00221-1,Harmonic memory signals in the human cerebral cortex induced by semantic relatedness of words.,Noguchi Y.,2024,"When we memorize multiple words simultaneously, semantic relatedness among those words assists memory. For example, the information about ""apple"", ""banana,"" and ""orange"" will be connected via a common concept of ""fruits"" and become easy to retain and recall. Neural mechanisms underlying this semantic integration in verbal working memory remain unclear. Here I used electroencephalography (EEG) and investigated neural signals when healthy human participants memorized five nouns semantically related (Sem trial) or not (NonSem trial). The regularity of oscillatory signals (8-30 Hz) during the retention period was found to be lower in NonSem than Sem trials, indicating that memorizing words unrelated to each other induced a non-harmonic (irregular) waveform in the temporal cortex. These results suggest that (i) semantic features of a word are retained as a set of neural oscillations at specific frequencies and (ii) memorizing words sharing a common semantic feature produces harmonic brain responses through a resonance or integration (sharing) of the oscillatory signals.",Y,cc by
74,PMC10322614,10.1155/2023/9760976,Retracted: Analysis of Machine Translation and Post-Translation Editing Ability Using Semantic Information Entropy Technology.,Environmental And Public Health JO.,2023,[This retracts the article DOI: 10.1155/2022/5932044.].,Y,cc by
75,PMC10447445,10.1038/s41597-023-02480-w,Author Correction: An fMRI Dataset for Concept Representation with Semantic Feature Annotations.,"Wang S, Zhang Y, Zhang X, Sun J, Lin N, Zhang J, Zong C.",2023,,Y,cc by
76,PMC11154504,10.3390/s24020510,Beyond Conventional Monitoring: A Semantic Segmentation Approach to Quantifying Traffic-Induced Dust on Unsealed Roads.,"de Silva A, Ranasinghe R, Sounthararajah A, Haghighi H, Kodikara J.",2024,"Road dust is a mixture of fine and coarse particles released into the air due to an external force, such as tire-ground friction or wind, which is harmful to human health when inhaled. Continuous dust emission from the road surfaces is detrimental to the road itself and the road users. Due to this, multiple dust monitoring and control techniques are currently adopted in the world. The current dust monitoring methods require expensive equipment and expertise. This study introduces a novel pragmatic and robust approach to quantifying traffic-induced road dust using a deep learning method called semantic segmentation. Based on the authors' previous works, the best-performing semantic segmentation machine learning models were selected and used to identify dust in an image pixel-wise. The total number of dust pixels was then correlated with real-world dust measurements obtained from a research-grade dust monitor. Our method shows that semantic segmentation can be adopted to quantify traffic-induced dust reasonably. Over 90% of the predictions from both correlations fall in true positive quadrant, indicating that when dust concentrations are below the threshold, the segmentation can accurately predict them. The results were validated and extended for real-time application. Our code implementation is publicly available.",Y,cc by
77,PMC10444781,10.1038/s41597-023-02479-3,Author Correction: A large dataset of semantic ratings and its computational extension.,"Wang S, Zhang Y, Shi W, Zhang G, Zhang J, Lin N, Zong C.",2023,,Y,cc by
78,PMC11116941,10.1111/hex.14082,'No matter what time of day': The value of joining Facebook groups supporting women's self-management of gestational diabetes mellitus.,"Pham S, Churruca K, Ellis LA, Braithwaite J.",2024,"<h4>Background</h4>Gestational diabetes mellitus (GDM) affects a significant and growing proportion of pregnant women each year. The condition entails additional monitoring, self-management and healthcare use during pregnancy, and some women also join GDM support groups on Facebook. Little is known about the practices inside these groups, but examining them may elucidate support needs, women's experience of healthcare and improve overall outcomes. The aims of this study were to explore motivations for joining GDM Facebook groups and the perceived value and benefits of such spaces.<h4>Design</h4>A cross-sectional design using a web-based survey collected data from two peer-led GDM Facebook groups; relevant quantitative and qualitative data were extracted from open and closed questions, and analysed using descriptive statistics and content analysis.<h4>Results</h4>A total of 340 women responded to the survey, with 306 (90%) tendering their motivations to join a GDM Facebook group. Their answers were classified into six categories: peer support; information and practical advice; lived experiences; community; a safe place to ask questions and being recommended. The most commonly reported benefits of membership were 'reading about food ideas' and 'finding helpful information and tips'. Respondents reported finding their group strongly sympathetic, sincere, compassionate, heart-felt, tolerant, sensitive, warm and supportive.<h4>Discussion and conclusions</h4>GDM Facebook groups are valuable for informational and emotional support, and the sharing and perusal of lived experiences; another key benefit for women is feeling belonging to a community. GDM Facebook groups provide women with access to more tailored and readily available support, filling gaps not addressed by healthcare providers.<h4>Patient contribution</h4>This study was led by a person with lived experience of GDM, and the survey was pilot tested with women who had also experienced GDM, which contributed to its development.",Y,cc by
79,PMC10806051,10.1038/s41598-024-52562-x,Verbal fluency patterns associated with the amnestic conversion from mild cognitive impairment to dementia.,"Cintoli S, Favilli L, Morganti R, Siciliano G, Ceravolo R, Tognoni G.",2024,"Patients with amnestic mild cognitive impairment (aMCI) are at a higher risk of converting to Alzheimer's disease. The aim of this study was to examine the potential use of Verbal Fluency (VF) measures as markers for predicting the conversion to dementia. At baseline, 61 aMCI, aged 65 to 80 years, underwent a comprehensive neuropsychological assessment, including phonemic (PVF) and semantic verbal fluency (SVF) tasks. After 18 months, 14 individuals with aMCI had progressed to a diagnosis of dementia. The findings revealed that aMCI-converter group had lower Mini Mental State Examination and Rey Auditory Verbal Learning Task scores than aMCI-no converter and produced fewer clusters in both VF tasks and a lower number of switches in PVF at baseline (p < 0.05). According to receiver operating characteristic curve analysis, the number of clusters in PVF had the highest predictive value (AUC = 0.80) with a threshold of 5.510 for identifying aMCI-converter at baseline. Additionally, participants with higher levels of education exhibited more clusters and switches in VF tasks (p < 0.05). These results suggest that qualitative measures of VF could serve as neuropsychological markers for predicting cognitive decline in individuals with aMCI. Furthermore, the study highlights the potential influence of the education level on cognitive performance in neuropsychological tasks.",Y,cc by
80,PMC10879509,10.1038/s41598-024-54823-1,Dynamics of task preparation processes revealed by effect course analysis on response times and error rates.,"Berger A, Kunde W, Kiefer M.",2024,"Cuing or executing a task impacts processing pathways for task-relevant information. While there is ample evidence that processing associated with task execution changes with practice, such evidence regarding cue-induced task preparation is scarce. Here we explored practice-related changes of processing pathways by task cuing in order to assess the plasticity of task preparation. We first developed and validated a new method for the study of practice-related changes, the effect course analysis. The effect course analysis is a model-free, non-parametric method designed to reveal effect changes within an experimental session on a continuous time scale. Then we applied this method to a new study in which cued task sets were supposed to remain activated during assessment of task-relevant pathways, as potential task execution was postponed at the end of the trial. The results showed that, with little practice, task cuing amplified task-relevant pathways, whereas this effect vanished with practice, suggesting that practice prompts fundamental changes of how task cues are used for task preparation. Hence, if one cannot be certain that cognitive processing is stationary, investigating the time course of experimental effects appears to be crucial to determine how cognitive processing is influenced by practice.",Y,cc by
81,PMC10322599,10.1155/2023/9892864,Retracted: Semantic Construal Mechanism of the Linguistic Environment Parameter Theory in Business English Translation.,Environmental And Public Health JO.,2023,[This retracts the article DOI: 10.1155/2022/8953181.].,Y,cc by
82,PMC10728182,10.1038/s41598-023-49858-9,Evidence for optimal semantic search throughout adulthood.,"Zemla JC, Gooding DC, Austerweil JL.",2023,"As people age, they learn and store new knowledge in their semantic memory. Despite learning a tremendous amount of information, people can still recall information relevant to the current situation with ease. To accomplish this, the mind must efficiently organize and search a vast store of information. It also must continue to retrieve information effectively despite changes in cognitive mechanisms due to healthy aging, including a general slowing in information processing and a decline in executive functioning. How effectively does the mind of an individual adjust its search to account for changes due to aging? We tested 746 people ages 25 through 69 on a semantic fluency task (free listing animals) and found that, on average, retrieval follows an optimal path through semantic memory. Participants tended to list a sequence of semantically related animals (e.g., lion, tiger, puma) before switching to a semantically unrelated animal (e.g., whale). We found that the timing of these transitions to semantically unrelated animals was remarkably consistent with an optimal strategy for maximizing the overall rate of retrieval (i.e., the number of animals listed per unit time). Age did not affect an individual's deviation from the optimal strategy given their general performance, suggesting that people adapt and continue to search memory optimally throughout their lives. We argue that this result is more likely due to compensating for a general slowing than a decline in executive functioning.",Y,cc by
83,PMC10957906,10.1038/s41598-024-57426-y,Language prediction in monolingual and bilingual speakers: an EEG study.,"Momenian M, Vaghefi M, Sadeghi H, Momtazi S, Meyer L.",2024,"Prediction of upcoming words is thought to be crucial for language comprehension. Here, we are asking whether bilingualism entails changes to the electrophysiological substrates of prediction. Prior findings leave it open whether monolingual and bilingual speakers predict upcoming words to the same extent and in the same manner. We address this issue with a naturalistic approach, employing an information-theoretic metric, surprisal, to predict and contrast the N400 brain potential in monolingual and bilingual speakers. We recruited 18 Iranian Azeri-Persian bilingual speakers and 22 Persian monolingual speakers. Subjects listened to a story in Persian while their electroencephalogram (EEG) was recorded. Bayesian item-level analysis was used. While in monolingual speakers N400 was sensitive to information-theoretic properties of both the current and previous words, in bilingual speakers N400 reflected the properties of the previous word only. Our findings show evidence for a processing delay in bilingual speakers which is consistent with prior research.",Y,cc by
84,PMC10586498,10.3389/frai.2023.1209507,CowMesh: a data-mesh architecture to unify dairy industry data for prediction and monitoring.,"Pakrashi A, Wallace D, Mac Namee B, Greene D, Guéret C.",2023,"Dairy is an economically significant industry that caters to the huge demand for food products in people's lives. To remain profitable, farmers need to manage their farms and the health of the dairy cows in their herds. There are, however, many risks to cow health that can lead to significant challenges to dairy farm management and have the potential to lead to significant losses. Such risks include cow udder infections (i.e., mastitis) and cow lameness. As automation and data recording become more common in the agricultural sector, dairy farms are generating increasing amounts of data. Recently, these data are being used to generate insights into farm and cow health, where the objective is to help farmers manage the health and welfare of dairy cows and reduce losses from cow health issues. Despite the level of data generation on dairy farms, this information is often difficult to access due to a lack of a single, central organization to collect data from individual farms. The prospect of such an organization, however, raises questions about data ownership, with some farmers reluctant to share their farm data for privacy reasons. In this study, we describe a new <i>data mesh</i> architecture designed for the dairy industry that focuses on facilitating access to data from farms in a decentralized fashion. This has the benefit of keeping the ownership of data with dairy farmers while bringing data together by providing a common and uniform set of protocols. Furthermore, this architecture will allow secure access to the data by research groups and product development groups, who can plug in new projects and applications built across the data. No similar framework currently exists in the dairy industry, and such a data mesh can help industry stakeholders by bringing the dairy farms of a country together in a decentralized fashion. This not only helps farmers, dairy researchers, and product builders but also facilitates an overview of all dairy farms which can help governments to decide on regulations to improve the dairy industry at a national level.",Y,cc by
85,PMC11184879,10.1186/s13040-024-00371-3,Using GPT-4 to write a scientific review article: a pilot evaluation study.,"Wang ZP, Bhandary P, Wang Y, Moore JH.",2024,"GPT-4, as the most advanced version of OpenAI's large language models, has attracted widespread attention, rapidly becoming an indispensable AI tool across various areas. This includes its exploration by scientists for diverse applications. Our study focused on assessing GPT-4's capabilities in generating text, tables, and diagrams for biomedical review papers. We also assessed the consistency in text generation by GPT-4, along with potential plagiarism issues when employing this model for the composition of scientific review papers. Based on the results, we suggest the development of enhanced functionalities in ChatGPT, aiming to meet the needs of the scientific community more effectively. This includes enhancements in uploaded document processing for reference materials, a deeper grasp of intricate biomedical concepts, more precise and efficient information distillation for table generation, and a further refined model specifically tailored for scientific diagram creation.",Y,cc by
86,PMC11137001,10.1038/s41597-024-03398-7,ChineseEEG: A Chinese Linguistic Corpora EEG Dataset for Semantic Alignment and Neural Decoding.,"Mou X, He C, Tan L, Yu J, Liang H, Zhang J, Tian Y, Yang YF, Xu T, Wang Q, Cao M, Chen Z, Hu CP, Wang X, Liu Q, Wu H.",2024,"An Electroencephalography (EEG) dataset utilizing rich text stimuli can advance the understanding of how the brain encodes semantic information and contribute to semantic decoding in brain-computer interface (BCI). Addressing the scarcity of EEG datasets featuring Chinese linguistic stimuli, we present the ChineseEEG dataset, a high-density EEG dataset complemented by simultaneous eye-tracking recordings. This dataset was compiled while 10 participants silently read approximately 13 hours of Chinese text from two well-known novels. This dataset provides long-duration EEG recordings, along with pre-processed EEG sensor-level data and semantic embeddings of reading materials extracted by a pre-trained natural language processing (NLP) model. As a pilot EEG dataset derived from natural Chinese linguistic stimuli, ChineseEEG can significantly support research across neuroscience, NLP, and linguistics. It establishes a benchmark dataset for Chinese semantic decoding, aids in the development of BCIs, and facilitates the exploration of alignment between large language models and human cognitive processes. It can also aid research into the brain's mechanisms of language processing within the context of the Chinese natural language.",Y,cc by
87,PMC10776628,10.1038/s41531-024-00630-4,Neurocognitive correlates of semantic memory navigation in Parkinson's disease.,"Toro-Hernández FD, Migeot J, Marchant N, Olivares D, Ferrante F, González-Gómez R, González Campo C, Fittipaldi S, Rojas-Costa GM, Moguilner S, Slachevsky A, Chaná Cuevas P, Ibáñez A, Chaigneau S, García AM.",2024,"Cognitive studies on Parkinson's disease (PD) reveal abnormal semantic processing. Most research, however, fails to indicate which conceptual properties are most affected and capture patients' neurocognitive profiles. Here, we asked persons with PD, healthy controls, and individuals with behavioral variant frontotemporal dementia (bvFTD, as a disease control group) to read concepts (e.g., 'sun') and list their features (e.g., hot). Responses were analyzed in terms of ten word properties (including concreteness, imageability, and semantic variability), used for group-level comparisons, subject-level classification, and brain-behavior correlations. PD (but not bvFTD) patients produced more concrete and imageable words than controls, both patterns being associated with overall cognitive status. PD and bvFTD patients showed reduced semantic variability, an anomaly which predicted semantic inhibition outcomes. Word-property patterns robustly classified PD (but not bvFTD) patients and correlated with disease-specific hypoconnectivity along the sensorimotor and salience networks. Fine-grained semantic assessments, then, can reveal distinct neurocognitive signatures of PD.",Y,cc by
88,PMC10995171,10.1038/s41598-024-58585-8,Typhoon disaster state information extraction for Chinese texts.,"Ye P, Zhang C, Chen M, Li S.",2024,"Typhoon disasters undergo a complex evolutionary process influenced by temporal changes, and investigating this process constitutes the central focus of geographical research. As a key node within the typhoon disaster process, the state serves as the foundation for gauging the dynamics of the disaster. The majority of current approaches to disaster information extraction rely on event extraction methods to acquire fundamental elements, including disaster-causing factors, disaster-bearing bodies, disaster-pregnant environment and the extent of damage. Due to the dispersion of various disaster information and the diversity of time and space, it is a challenge for supporting the analysis of the typhoon disaster process. In this paper, a typhoon disaster state information extraction (TDSIE) method for Chinese texts is proposed, which aims to facilitate the systematic integration of fragmented typhoon disaster information. First, the integration of part-of-speech tagging with spatio-temporal information extraction is employed to achieve the tagging of typhoon disaster texts. Second, within the framework of spatio-temporal semantic units, the typhoon disaster semantic vector is constructed to facilitate the identification of information elements of typhoon disaster states. Third, co-referential state information fusion is performed based on spatio-temporal cues. Experimental analysis, conducted using online news as the data source, reveals that the TDSIE achieves precision and recall rates consistently surpassing 85%. The typhoon disaster state information derived from the TDSIE allows for the analysis of spatio-temporal patterns, evolutionary characteristics, and activity modes of typhoon disasters across various scales. Therefore, TDSIE serves as valuable support for investigating the inherent process properties of typhoon disasters.",Y,cc by
89,PMC11014927,10.1038/s41597-024-03193-4,Cataract-1K Dataset for Deep-Learning-Assisted Analysis of Cataract Surgery Videos.,"Ghamsarian N, El-Shabrawi Y, Nasirihaghighi S, Putzgruber-Adamitsch D, Zinkernagel M, Wolf S, Schoeffmann K, Sznitman R.",2024,"In recent years, the landscape of computer-assisted interventions and post-operative surgical video analysis has been dramatically reshaped by deep-learning techniques, resulting in significant advancements in surgeons' skills, operation room management, and overall surgical outcomes. However, the progression of deep-learning-powered surgical technologies is profoundly reliant on large-scale datasets and annotations. In particular, surgical scene understanding and phase recognition stand as pivotal pillars within the realm of computer-assisted surgery and post-operative assessment of cataract surgery videos. In this context, we present the largest cataract surgery video dataset that addresses diverse requisites for constructing computerized surgical workflow analysis and detecting post-operative irregularities in cataract surgery. We validate the quality of annotations by benchmarking the performance of several state-of-the-art neural network architectures for phase recognition and surgical scene segmentation. Besides, we initiate the research on domain adaptation for instrument segmentation in cataract surgery by evaluating cross-domain instrument segmentation performance in cataract surgery videos. The dataset and annotations are publicly available in Synapse.",Y,cc by
90,PMC10957871,10.1038/s41539-024-00232-y,The neural and cognitive basis of expository text comprehension.,"Keller TA, Mason RA, Legg AE, Just MA.",2024,"As science and technology rapidly progress, it becomes increasingly important to understand how individuals comprehend expository technical texts that explain these advances. This study examined differences in individual readers' technical comprehension performance and differences among texts, using functional brain imaging to measure regional brain activity while students read passages on technical topics and then took a comprehension test. Better comprehension of the technical passages was related to higher activation in regions of the left inferior frontal gyrus, left superior parietal lobe, bilateral dorsolateral prefrontal cortex, and bilateral hippocampus. These areas are associated with the construction of a mental model of the passage and with the integration of new and prior knowledge in memory. Poorer comprehension of the passages was related to greater activation of the ventromedial prefrontal cortex and the precuneus, areas involved in autobiographical and episodic memory retrieval. More comprehensible passages elicited more brain activation associated with establishing links among different types of information in the text and activation associated with establishing conceptual coherence within the text representation. These findings converge with previous behavioral research in their implications for teaching technical learners to become better comprehenders and for improving the structure of instructional texts, to facilitate scientific and technological comprehension.",Y,cc by
91,PMC10976921,10.1007/s10278-023-00931-9,The Segmentation of Multiple Types of Uterine Lesions in Magnetic Resonance Images Using a Sequential Deep Learning Method with Image-Level Annotations.,"Cui YM, Wang HL, Cao R, Bai H, Sun D, Feng JX, Lu XF.",2024,"Fully supervised medical image segmentation methods use pixel-level labels to achieve good results, but obtaining such large-scale, high-quality labels is cumbersome and time consuming. This study aimed to develop a weakly supervised model that only used image-level labels to achieve automatic segmentation of four types of uterine lesions and three types of normal tissues on magnetic resonance images. The MRI data of the patients were retrospectively collected from the database of our institution, and the T2-weighted sequence images were selected and only image-level annotations were made. The proposed two-stage model can be divided into four sequential parts: the pixel correlation module, the class re-activation map module, the inter-pixel relation network module, and the Deeplab v3 + module. The dice similarity coefficient (DSC), the Hausdorff distance (HD), and the average symmetric surface distance (ASSD) were employed to evaluate the performance of the model. The original dataset consisted of 85,730 images from 316 patients with four different types of lesions (i.e., endometrial cancer, uterine leiomyoma, endometrial polyps, and atypical hyperplasia of endometrium). A total number of 196, 57, and 63 patients were randomly selected for model training, validation, and testing. After being trained from scratch, the proposed model showed a good segmentation performance with an average DSC of 83.5%, HD of 29.3 mm, and ASSD of 8.83 mm, respectively. As far as the weakly supervised methods using only image-level labels are concerned, the performance of the proposed model is equivalent to the state-of-the-art weakly supervised methods.",N,
92,PMC11133575,10.3389/fmed.2024.1365501,Artificial intelligence based data curation: enabling a patient-centric European health data space.,"de Zegher I, Norak K, Steiger D, Müller H, Kalra D, Scheenstra B, Cina I, Shulz S, Uma K, Kalendralis P, Lotmam EM, Benedikt M, Dumontier M, Celebi R.",2024,"The emerging European Health Data Space (EHDS) Regulation opens new prospects for large-scale sharing and re-use of health data. Yet, the proposed regulation suffers from two important limitations: it is designed to benefit the whole population with limited consideration for individuals, and the generation of secondary datasets from heterogeneous, unlinked patient data will remain burdensome. AIDAVA, a Horizon Europe project that started in September 2022, proposes to address both shortcomings by providing patients with an AI-based virtual assistant that maximises automation in the integration and transformation of their health data into an interoperable, longitudinal health record. This personal record can then be used to inform patient-related decisions at the point of care, whether this is the usual point of care or a possible cross-border point of care. The personal record can also be used to generate population datasets for research and policymaking. The proposed solution will enable a much-needed paradigm shift in health data management, implementing a 'curate once at patient level, use many times' approach, primarily for the benefit of patients and their care providers, but also for more efficient generation of high-quality secondary datasets. After 15 months, the project shows promising preliminary results in achieving automation in the integration and transformation of heterogeneous data of each individual patient, once the content of the data sources managed by the data holders has been formally described. Additionally, the conceptualization phase of the project identified a set of recommendations for the development of a patient-centric EHDS, significantly facilitating the generation of data for secondary use.",Y,cc by
93,PMC10876958,10.1038/s41598-024-54809-z,SEGCN: a subgraph encoding based graph convolutional network model for social bot detection.,"Liu F, Li Z, Yang C, Gong D, Lu H, Liu F.",2024,"Message passing neural networks such as graph convolutional networks (GCN) can jointly consider various types of features for social bot detection. However, the expressive power of GCN is upper-bounded by the 1st-order Weisfeiler-Leman isomorphism test, which limits the detection performance for the social bots. In this paper, we propose a subgraph encoding based GCN model, SEGCN, with stronger expressive power for social bot detection. Each node representation of this model is computed as the encoding of a surrounding induced subgraph rather than encoding of immediate neighbors only. Extensive experimental results on two publicly available datasets, Twibot-20 and Twibot-22, showed that the proposed model improves the accuracy of the state-of-the-art social bot detection models by around 2.4%, 3.1%, respectively.",Y,cc by
94,PMC10948387,10.1038/s41598-024-56956-9,Fully automated landmarking and facial segmentation on 3D photographs.,"Berends B, Bielevelt F, Schreurs R, Vinayahalingam S, Maal T, de Jong G.",2024,"Three-dimensional facial stereophotogrammetry provides a detailed representation of craniofacial soft tissue without the use of ionizing radiation. While manual annotation of landmarks serves as the current gold standard for cephalometric analysis, it is a time-consuming process and is prone to human error. The aim in this study was to develop and evaluate an automated cephalometric annotation method using a deep learning-based approach. Ten landmarks were manually annotated on 2897 3D facial photographs. The automated landmarking workflow involved two successive DiffusionNet models. The dataset was randomly divided into a training and test dataset. The precision of the workflow was evaluated by calculating the Euclidean distances between the automated and manual landmarks and compared to the intra-observer and inter-observer variability of manual annotation and a semi-automated landmarking method. The workflow was successful in 98.6% of all test cases. The deep learning-based landmarking method achieved precise and consistent landmark annotation. The mean precision of 1.69 ± 1.15 mm was comparable to the inter-observer variability (1.31 ± 0.91 mm) of manual annotation. Automated landmark annotation on 3D photographs was achieved with the DiffusionNet-based approach. The proposed method allows quantitative analysis of large datasets and may be used in diagnosis, follow-up, and virtual surgical planning.",Y,cc by
95,PMC10862086,10.1093/nsr/nwad317,Emergence of machine language: towards symbolic intelligence with neural networks.,"Wang Y, Zhang XY, Liu CL, Tan T, Zhang Z.",2024,"Inspired by human language, machine language is a novel discrete representation learned from visual data only through playing the speak, guess, and draw game.",Y,cc by
96,PMC11195943,10.1371/journal.pone.0299623,Enhancing semantic segmentation in chest X-ray images through image preprocessing: ps-KDE for pixel-wise substitution by kernel density estimation.,"Wang Y, Guo Y, Wang Z, Yu L, Yan Y, Gu Z.",2024,"<h4>Background</h4>In medical imaging, the integration of deep-learning-based semantic segmentation algorithms with preprocessing techniques can reduce the need for human annotation and advance disease classification. Among established preprocessing techniques, Contrast Limited Adaptive Histogram Equalization (CLAHE) has demonstrated efficacy in improving segmentation algorithms across various modalities, such as X-rays and CT. However, there remains a demand for improved contrast enhancement methods considering the heterogeneity of datasets and the various contrasts across different anatomic structures.<h4>Method</h4>This study proposes a novel preprocessing technique, ps-KDE, to investigate its impact on deep learning algorithms to segment major organs in posterior-anterior chest X-rays. Ps-KDE augments image contrast by substituting pixel values based on their normalized frequency across all images. We evaluate our approach on a U-Net architecture with ResNet34 backbone pre-trained on ImageNet. Five separate models are trained to segment the heart, left lung, right lung, left clavicle, and right clavicle.<h4>Results</h4>The model trained to segment the left lung using ps-KDE achieved a Dice score of 0.780 (SD = 0.13), while that of trained on CLAHE achieved a Dice score of 0.717 (SD = 0.19), p<0.01. ps-KDE also appears to be more robust as CLAHE-based models misclassified right lungs in select test images for the left lung model. The algorithm for performing ps-KDE is available at https://github.com/wyc79/ps-KDE.<h4>Discussion</h4>Our results suggest that ps-KDE offers advantages over current preprocessing techniques when segmenting certain lung regions. This could be beneficial in subsequent analyses such as disease classification and risk stratification.",Y,cc by
97,PMC11078994,10.1038/s41597-024-03280-6,Getting your DUCs in a row - standardising the representation of Digital Use Conditions.,"Jeanson F, Gibson SJ, Alper P, Bernier A, Woolley JP, Mietchen D, Strug A, Becker R, Kamerling P, Sanchez Gonzalez MDC, Mah N, Novakowski A, Wilkinson MD, Benhamed OM, Landi A, Krog GP, Müller H, Riaz U, Veal C, Holub P, van Enckevort E, Brookes AJ.",2024,"Improving patient care and advancing scientific discovery requires responsible sharing of research data, healthcare records, biosamples, and biomedical resources that must also respect applicable use conditions. Defining a standard to structure and manage these use conditions is a complex and challenging task. This is exemplified by a near unlimited range of asset types, a high variability of applicable conditions, and differing applications at the individual or collective level. Furthermore, the specifics and granularity required are likely to vary depending on the ultimate contexts of use. All these factors confound alignment of institutional missions, funding objectives, regulatory and technical requirements to facilitate effective sharing. The presented work highlights the complexity and diversity of the problem, reviews the current state of the art, and emphasises the need for a flexible and adaptable approach. We propose Digital Use Conditions (DUC) as a framework that addresses these needs by leveraging existing standards, striking a balance between expressiveness versus ambiguity, and considering the breadth of applicable information with their context of use.",Y,cc by
98,PMC10139678,10.7759/cureus.36718,Intraoperative Testing During the Mapping of the Language Cortex.,"Kabir SS, Jahangiri FR, Rinesmith C, Vilches CS, Chakarvarty S.",2023,"Intracranial lesions, particularly in the language-eloquent areas of the brain, can affect one's speaking ability. Despite advances in surgery, the excision of these lesions can be challenging. Intraoperative neurophysiological monitoring (IONM) during awake craniotomies can help identify language-eloquent areas and minimize postoperative impairments. Preoperative language testing is performed to establish a baseline before intraoperative language testing. This involves subjecting patients to predetermined tasks in the operating room to evaluate their phonological, semantic, and syntactic capabilities. The current state and future directions of intraoperative language testing procedures are discussed in this paper. The most common intraoperative tasks are counting and picture naming. However, some experts recommend utilizing more nuanced tasks that involve regions affected by infrequently occurring tumor patterns. Low-frequency bipolar Penfield stimulation is optimal for language mapping. Exception cases are discussed where awake craniotomies are not feasible. When dealing with multilingual patients, the patient's age of learning and skill level can be accounted for in terms of making informed task choices and mapping techniques to avoid any damage to language areas.",Y,cc by
99,PMC10753415,10.2196/50797,Transformation and Articulation of Clinical Data to Understand Students' and Health Professionals' Clinical Reasoning: Protocol for a Scoping Review.,"Deschênes MF, Fernandez N, Lechasseur K, Caty MÈ, Azimzadeh D, Mai TC, Lavoie P.",2023,"<h4>Background</h4>There are still unanswered questions regarding effective educational strategies to promote the transformation and articulation of clinical data while teaching and learning clinical reasoning. Additionally, understanding how this process can be analyzed and assessed is crucial, particularly considering the rapid growth of natural language processing in artificial intelligence.<h4>Objective</h4>The aim of this study is to map educational strategies to promote the transformation and articulation of clinical data among students and health care professionals and to explore the methods used to assess these individuals' transformation and articulation of clinical data.<h4>Methods</h4>This scoping review follows the Joanna Briggs Institute framework for scoping reviews and the PRISMA-ScR (Preferred Reporting Items for Systematic Reviews and Meta-Analyses Extension for Scoping Reviews) checklist for the analysis. A literature search was performed in November 2022 using 5 databases: CINAHL (EBSCOhost), MEDLINE (Ovid), Embase (Ovid), PsycINFO (Ovid), and Web of Science (Clarivate). The protocol was registered on the Open Science Framework in November 2023. The scoping review will follow the 9-step framework proposed by Peters and colleagues of the Joanna Briggs Institute. A data extraction form has been developed using key themes from the research questions.<h4>Results</h4>After removing duplicates, the initial search yielded 6656 results, and study selection is underway. The extracted data will be qualitatively analyzed and presented in a diagrammatic or tabular form alongside a narrative summary. The review will be completed by February 2024.<h4>Conclusions</h4>By synthesizing the evidence on semantic transformation and articulation of clinical data during clinical reasoning education, this review aims to contribute to the refinement of educational strategies and assessment methods used in academic and continuing education programs. The insights gained from this review will help educators develop more effective semantic approaches for teaching or learning clinical reasoning, as opposed to fragmented, purely symptom-based or probabilistic approaches. Besides, the results may suggest some ways to address challenges related to the assessment of clinical reasoning and ensure that the assessment tasks accurately reflect learners' developing competencies and educational progress.<h4>International registered report identifier (irrid)</h4>DERR1-10.2196/50797.",Y,cc by
