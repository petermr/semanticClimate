<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd"> 
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.3 20210610//EN?><?DTDIdentifier.IdentifierType public?><?SourceDTD.DTDName JATS-journalpublishing1-3.dtd?><?SourceDTD.Version 1.3?><?ConverterInfo.XSLTName jats2jats3.xsl?><?ConverterInfo.Version 1?><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>MDPI</publisher-name></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmcid">11154504</article-id><article-id pub-id-type="pmid">38257603</article-id>
<article-id pub-id-type="doi">10.3390/s24020510</article-id><article-id pub-id-type="publisher-id">sensors-24-00510</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Beyond Conventional Monitoring: A Semantic Segmentation Approach to Quantifying Traffic-Induced Dust on Unsealed Roads</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-3050-1290</contrib-id><name><surname>de Silva</surname><given-names>Asanka</given-names></name><xref rid="af1-sensors-24-00510" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-6001-512X</contrib-id><name><surname>Ranasinghe</surname><given-names>Rajitha</given-names></name><xref rid="af1-sensors-24-00510" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-0839-1045</contrib-id><name><surname>Sounthararajah</surname><given-names>Arooran</given-names></name><xref rid="af1-sensors-24-00510" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name><surname>Haghighi</surname><given-names>Hamed</given-names></name><xref rid="af2-sensors-24-00510" ref-type="aff">2</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-1725-7972</contrib-id><name><surname>Kodikara</surname><given-names>Jayantha</given-names></name><xref rid="af1-sensors-24-00510" ref-type="aff">1</xref><xref rid="c1-sensors-24-00510" ref-type="corresp">*</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name><surname>Penza</surname><given-names>Michele</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-24-00510"><label>1</label>ARC Industrial Transformation Research Hub (ITRH)&#x02014;SPARC Hub, Department of Civil Engineering, Monash University, Clayton Campus, Clayton, VIC 3800, Australia; <email>senarathna.desilva@monash.edu</email> (A.d.S.); <email>rajitha.ranasinghe@monash.edu</email> (R.R.); <email>arooran.sounthararajah@monash.edu</email> (A.S.)</aff><aff id="af2-sensors-24-00510"><label>2</label>Product Development Hub, Road Science, Downer EDI Works Pty Ltd., Somerton, VIC 3061, Australia; <email>hamed.haghighi@downergroup.com</email></aff><author-notes><corresp id="c1-sensors-24-00510"><label>*</label>Correspondence: <email>jayantha.kodikara@monash.edu</email></corresp></author-notes><pub-date pub-type="epub"><day>14</day><month>1</month><year>2024</year></pub-date><pub-date pub-type="collection"><month>1</month><year>2024</year></pub-date><volume>24</volume><issue>2</issue><elocation-id>0</elocation-id><history><date date-type="received"><day>03</day><month>11</month><year>2023</year></date><date date-type="rev-recd"><day>26</day><month>12</month><year>2023</year></date><date date-type="accepted"><day>12</day><month>1</month><year>2024</year></date></history><permissions><copyright-statement>&#x000a9; 2024 by the authors.</copyright-statement><copyright-year>2024</copyright-year><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><abstract><p>Road dust is a mixture of fine and coarse particles released into the air due to an external force, such as tire&#x02013;ground friction or wind, which is harmful to human health when inhaled. Continuous dust emission from the road surfaces is detrimental to the road itself and the road users. Due to this, multiple dust monitoring and control techniques are currently adopted in the world. The current dust monitoring methods require expensive equipment and expertise. This study introduces a novel pragmatic and robust approach to quantifying traffic-induced road dust using a deep learning method called semantic segmentation. Based on the authors&#x02019; previous works, the best-performing semantic segmentation machine learning models were selected and used to identify dust in an image pixel-wise. The total number of dust pixels was then correlated with real-world dust measurements obtained from a research-grade dust monitor. Our method shows that semantic segmentation can be adopted to quantify traffic-induced dust reasonably. Over 90% of the predictions from both correlations fall in true positive quadrant, indicating that when dust concentrations are below the threshold, the segmentation can accurately predict them. The results were validated and extended for real-time application. Our code implementation is publicly available.</p></abstract><kwd-group><kwd>dust</kwd><kwd>dust monitoring</kwd><kwd>dust quantification</kwd><kwd>deep learning</kwd><kwd>machine learning</kwd><kwd>road dust</kwd><kwd>semantic segmentation</kwd><kwd>traffic-induced dust</kwd><kwd>unsealed roads</kwd></kwd-group><funding-group><award-group><funding-source>Smart Pavements Australia Research Collaboration (SPARC) Hub</funding-source><award-id>IH18.12.1</award-id></award-group><award-group><funding-source>Australian Research Council (ARC) Industrial Transformation Research Hub (ITRH) Scheme</funding-source><award-id>IH180100010</award-id></award-group><funding-statement>This research work is also part of a research project (Project No. IH18.12.1) sponsored by the Smart Pavements Australia Research Collaboration (SPARC) Hub (<uri xlink:href="https://sparchub.org.au">https://sparchub.org.au</uri>) at the Department of Civil Engineering, Monash University, funded by the Australian Research Council (ARC) Industrial Transformation Research Hub (ITRH) Scheme (Project ID: IH180100010).</funding-statement></funding-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-24-00510"><title>1. Introduction</title><p>Dust emissions from unsealed roads occur due to the application of an external force or combined action of such forces. These external forces can be vehicle tire&#x02013;ground friction and shear due to vehicle movement and wind. When road dust is airborne, it is sometimes referred to as fugitive dust or particulate matter (PM). We use dust and particulate matter interchangeably in this article. PM<sub>x</sub> refers to particulate matter with a diameter of <italic toggle="yes">x</italic>
<inline-formula>
<mml:math id="mm2" overflow="scroll"><mml:mrow><mml:mi mathvariant="sans-serif">&#x003bc;</mml:mi></mml:mrow></mml:math>
</inline-formula>m or less [<xref rid="B1-sensors-24-00510" ref-type="bibr">1</xref>]. Due to the different sizes of PM causing different types of health issues, this study focuses on five particle sizes, PM<sub>30</sub>, PM<sub>10</sub>, PM<sub>4</sub>, PM<sub>2.5</sub>, and PM<sub>1</sub>. Notably, the inhalable (PM<sub>10</sub>) and respirable (PM<sub>2.5</sub>) PM can induce adverse health effects [<xref rid="B2-sensors-24-00510" ref-type="bibr">2</xref>], thus are often focused on more in dust-related studies [<xref rid="B3-sensors-24-00510" ref-type="bibr">3</xref>,<xref rid="B4-sensors-24-00510" ref-type="bibr">4</xref>,<xref rid="B5-sensors-24-00510" ref-type="bibr">5</xref>].</p><p>Dust emissions can occur due to aerodynamic entrainment, saltation bombardment, and aggregates disintegration, albeit the aerodynamic entrainment is often assumed to be negligible [<xref rid="B6-sensors-24-00510" ref-type="bibr">6</xref>]. Many questions relating to dust emissions, particularly due to traffic, remain unanswered as research groups primarily focus on wind-induced erosion events that cause dust emissions [<xref rid="B7-sensors-24-00510" ref-type="bibr">7</xref>,<xref rid="B8-sensors-24-00510" ref-type="bibr">8</xref>,<xref rid="B9-sensors-24-00510" ref-type="bibr">9</xref>]. The chaotic nature of dust emissions makes it difficult to use the empirical relationships [<xref rid="B10-sensors-24-00510" ref-type="bibr">10</xref>] derived from field experiments to explain the physics involved in the process. This paper approaches the problem by considering recent advancements in machine learning (ML) to find a more pragmatic solution [<xref rid="B11-sensors-24-00510" ref-type="bibr">11</xref>,<xref rid="B12-sensors-24-00510" ref-type="bibr">12</xref>]. There are other unconventional attempts to quantify dust or gravel loss using smartphones [<xref rid="B13-sensors-24-00510" ref-type="bibr">13</xref>] and machine learning algorithms [<xref rid="B14-sensors-24-00510" ref-type="bibr">14</xref>], but none have conducted extensive field experiments and correlated that with outputs from image processing. When dust particles are airborne, they form a cloud and depending on the intensity, it can be captured in an image or a video, allowing us to perform semantic segmentation. The segmented images can then be post-processed to calculate the number of dust pixels in them. In this study, we investigate if the calculated dust pixels can be correlated with the actual field dust measurements. This is further elaborated in <xref rid="sec2-sensors-24-00510" ref-type="sec">Section 2</xref>. The devised experimental procedures are explained in <xref rid="sec3-sensors-24-00510" ref-type="sec">Section 3</xref>.</p><sec id="sec1dot1-sensors-24-00510"><title>1.1. Machine Learning&#x000a0;Approach</title><p>In machine learning, segmentation refers to the process of dividing an image into segments and automatically identifying pre-learned objects or characteristics. Semantic segmentation involves splitting the image down to its smallest component, the pixel, and assigning a label to each pixel such that pixels with the same label share certain characteristics. This process effectively enables the model to understand and interpret every part of the image with detail.</p><p>The present study study introduces a novel approach to semantic segmentation, specifically tailored for the identification and analysis of dust clouds on unsealed road surfaces. Our methodology encompasses the development and training of machine learning models, which are optimized for semantic segmentation tasks. The process is outlined as follows:<list list-type="order"><list-item><p>Data collection: The initial phase involved acquiring high-resolution images of traffic-induced dust clouds from unsealed roads, exhibiting different levels of dust accumulation. These images constitute the foundational dataset required for training the semantic segmentation model. The dataset produced is called URDE and has been published [<xref rid="B15-sensors-24-00510" ref-type="bibr">15</xref>].</p></list-item><list-item><p>Annotation of dust particles: A critical step in data preparation was the manual annotation of dust particles in the collected images. Skilled annotators marked areas of dust accumulation, creating grayscale masks. These masks depicted the intensity of dust accumulation at the pixel level, thus providing a detailed representation of dust distribution within the cloud.</p></list-item><list-item><p>Production of binary masks: Since the semantic segmentation model necessitates binary masks, we utilized Otsu&#x02019;s thresholding method to transform the detailed grayscale masks into binary format. Otsu&#x02019;s method [<xref rid="B16-sensors-24-00510" ref-type="bibr">16</xref>], an adaptive thresholding technique, determines the optimal threshold value for binarization, resulting in binary masks that categorize each pixel as either &#x02018;dust&#x02019; or &#x02018;non-dust&#x02019;.</p></list-item><list-item><p>Model training: The binary masks were then used as ground truths in training our semantic segmentation models. We employed various deep learning frameworks, particularly leveraging Convolutional Neural Networks (CNNs), which are highly effective in image analysis and segmentation tasks. The models were trained to discern dust-laden areas from dust-free zones, thus learning to identify and segment dust particles within the environment.</p></list-item></list></p><p>Semantic segmentation has been used to solve road-related problems such as identifying road boundaries for autonomous vehicles [<xref rid="B17-sensors-24-00510" ref-type="bibr">17</xref>], preventing pedestrian collision [<xref rid="B18-sensors-24-00510" ref-type="bibr">18</xref>], and detecting road surface damages [<xref rid="B19-sensors-24-00510" ref-type="bibr">19</xref>]. Our case of road dust differs from the cases above in that the boundary of the object of interest, i.e., dust cloud, may not be as clear as a rigid object. Dust clouds have fuzzy boundaries that are difficult to determine, and current knowledge distillation methods fail to transfer boundary information, resulting in poor boundary identification explicitly. Image segmentation could accurately determine the exact boundary of the objects in the image. In recent years, knowledge distillation for semantic segmentation has been extensively studied to obtain satisfactory performance while reducing computational costs and developing advanced ML models. Ranging from simple encoder&#x02013;decoder structures to more densely connected convolutional networks [<xref rid="B20-sensors-24-00510" ref-type="bibr">20</xref>], the machine intelligence has pushed state-of-the-art forward to advanced ML models such as MobileNetV3 [<xref rid="B21-sensors-24-00510" ref-type="bibr">21</xref>], DeeplabV3 with resnet backbone, and fully convolutional network (FCN) with resnet backbone [<xref rid="B22-sensors-24-00510" ref-type="bibr">22</xref>,<xref rid="B23-sensors-24-00510" ref-type="bibr">23</xref>,<xref rid="B24-sensors-24-00510" ref-type="bibr">24</xref>]. As our primary goal is to quantify dust in real-time using semantic segmentation, emphasis was put on video sequences and static images. However, we were unable to use these image-based segmentation methods directly on a video sequence to independently segment each frame as the segmentation results will be inconsistent for different frames due to the lack of temporal coherence limitations. For video segmentation, spatial and temporal dimensions should be considered [<xref rid="B25-sensors-24-00510" ref-type="bibr">25</xref>]. This is further elaborated in <xref rid="sec4-sensors-24-00510" ref-type="sec">Section 4</xref>.</p></sec><sec id="sec1dot2-sensors-24-00510"><title>1.2. Objectives</title><p>The objectives of this study are to:<list list-type="order"><list-item><p>Employ advanced semantic segmentation algorithms, specifically DeepLabV3, FCN with ResNet101 backbone, and Lraspp_MobileNetV3_large, to accurately identify and quantify traffic-induced dust clouds in images of unsealed roads.</p></list-item><list-item><p>Establish correlations between the dust pixel features extracted from the semantic segmentation process and in situ dust measurements, thereby estimating the concentration of dust.</p></list-item><list-item><p>Validate the effectiveness of the semantic segmentation approach in dust quantification through comprehensive field experiments, ensuring the reliability and practical applicability of the methodology.</p></list-item><list-item><p>Demonstrate the potential of the proposed method in real-world scenarios, particularly in monitoring and managing dust levels on unsealed roads, thereby contributing to environmental studies.</p></list-item></list></p></sec><sec id="sec1dot3-sensors-24-00510"><title>1.3. Structure of the&#x000a0;Paper</title><p><xref rid="sec2-sensors-24-00510" ref-type="sec">Section 2</xref> defines the scope of a problem and formulates the question, &#x0201c;Is the relationship between the pixel representation of the dust cloud in an image and the actual dust cloud?&#x0201d;, and designs the experiments needed to be conducted to gather data to address said question. <xref rid="sec2-sensors-24-00510" ref-type="sec">Section 2</xref> also presents the procedure we used to incorporate dust segmentation for video sequences where dust is accurately segmented in each frame and combined to form a consistent video sequence. <xref rid="sec3-sensors-24-00510" ref-type="sec">Section 3</xref> presents the field experimental setup and equipment used. <xref rid="sec4-sensors-24-00510" ref-type="sec">Section 4</xref> develops the correlation between the actual dust measurements and the dust pixel percentage calculated from semantic segmentation. Results and validation are presented in <xref rid="sec5-sensors-24-00510" ref-type="sec">Section 5</xref>, where the effectiveness of the proposed method is demonstrated by validating the dust segmentation for independent video sequences and images, illustrating how the findings of this study can be implemented in the real world. Concluding remarks are presented in <xref rid="sec6-sensors-24-00510" ref-type="sec">Section 6</xref>.</p></sec></sec><sec id="sec2-sensors-24-00510"><title>2. Problem&#x000a0;Formulation</title><p>Suppose that a vehicle has produced a visible dust cloud. A camera was used to capture the dust emission from beginning to end, and a dust monitor was used to measure the aerosol. The process is illustrated in <xref rid="sensors-24-00510-f001" ref-type="fig">Figure 1</xref>. The problem is formulated based on the fact that the pictures do not and cannot represent a reality independent of the observer who sees them; thus, the following assumptions were made. The following boundary ranges were intentionally selected over discrete limits to introduce data variability.
<list list-type="bullet"><list-item><p>The dust cloud captured in the 2D image and the frame average of accumulated probability densities of dust <inline-formula>
<mml:math id="mm10" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>&#x003d5;</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math>
</inline-formula> calculated are representative of the actual dust cloud (Equation (<xref rid="FD1-sensors-24-00510" ref-type="disp-formula">1</xref>)). l is the distance from the camera to the dust cloud.</p></list-item><list-item><p>The dust measurement <inline-formula>
<mml:math id="mm11" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>&#x003bc;</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math>
</inline-formula> is representative of the actual dust cloud when the vehicle is in the vicinity of the dust monitor.</p></list-item><list-item><p>The field of view (FoV) of the video and images was selected so that the road segment is visible at least 20 m but no more than 200 m.</p></list-item><list-item><p>The dust monitor and the camera were located approximately 10&#x02013;15 m (<inline-formula>
<mml:math id="mm12" overflow="scroll"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math>
</inline-formula>) apart horizontally from each other.</p></list-item></list>
<fig position="anchor" id="sensors-24-00510-f001"><label>Figure 1</label><caption><p>Problem visualization. (<bold>a</bold>) Approach to field data collection, (<bold>b</bold>) correlating visual features extracted from images with in situ dust measurements from the dust monitor, (<bold>c</bold>) inference pipeline. I is the image frame in a video sequence, <inline-formula>
<mml:math id="mm74" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">F</mml:mi></mml:mrow></mml:math>
</inline-formula> is the trained ML model, <inline-formula>
<mml:math id="mm76" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:math>
</inline-formula> is the loss function, P is the prediction from ML model, Z is the Hadamard product between the green channel of I (<inline-formula>
<mml:math id="mm77" overflow="scroll"><mml:mrow><mml:msup><mml:mi>I</mml:mi><mml:mi>G</mml:mi></mml:msup></mml:mrow></mml:math>
</inline-formula>) and the inversion of the prediction (<inline-formula>
<mml:math id="mm78" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>P</mml:mi><mml:mo>&#x000af;</mml:mo></mml:mover></mml:mrow></mml:math>
</inline-formula>).</p></caption><graphic xlink:href="sensors-24-00510-g001" position="float"/></fig></p><p>In this study, we use our URDE dataset [<xref rid="B15-sensors-24-00510" ref-type="bibr">15</xref>], which was produced by field experiments conducted on unsealed roads in Victoria, Australia. In the repository, there are two subsets of data, dataset<inline-formula>
<mml:math id="mm13" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>_</mml:mo><mml:mn>100</mml:mn></mml:mrow></mml:mrow></mml:math>
</inline-formula> and dataset<inline-formula>
<mml:math id="mm14" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>_</mml:mo><mml:mn>897</mml:mn></mml:mrow></mml:mrow></mml:math>
</inline-formula>, which were truncated from the original dataset of approximately 7000 images. Images were extracted from video sequences which were recorded at 30 frames per second.</p><p>Let <inline-formula>
<mml:math id="mm15" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">V</mml:mi></mml:mrow></mml:math>
</inline-formula> = <inline-formula>
<mml:math id="mm16" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">V</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:msubsup><mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup></mml:mrow></mml:mrow></mml:math>
</inline-formula> be a labeled set with <italic toggle="yes">n</italic> number of samples, where each sample <inline-formula>
<mml:math id="mm17" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">V</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math>
</inline-formula> is a frame in a video with <inline-formula>
<mml:math id="mm18" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">V</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>W</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math>
</inline-formula>. Each <inline-formula>
<mml:math id="mm19" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">V</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math>
</inline-formula> is of height (<italic toggle="yes">H</italic>) = 1080, width (<italic toggle="yes">W</italic>) = 1920 and channel (<italic toggle="yes">C</italic>) = 3. Let <inline-formula>
<mml:math id="mm20" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">V</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:math>
</inline-formula> be the first frame. Let the <italic toggle="yes">r</italic>th pixel of <inline-formula>
<mml:math id="mm21" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">V</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math>
</inline-formula> is <inline-formula>
<mml:math id="mm22" overflow="scroll"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mrow></mml:math>
</inline-formula> where <inline-formula>
<mml:math id="mm23" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi>h</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi>H</mml:mi><mml:mo>,</mml:mo><mml:mi>w</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi>W</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math>
</inline-formula>. And <inline-formula>
<mml:math id="mm24" overflow="scroll"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math>
</inline-formula> is the probability of dust in pixel <inline-formula>
<mml:math id="mm25" overflow="scroll"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mrow></mml:math>
</inline-formula>.</p><p>Semantic segmentation of dust from video frames was undertaken from previously trained machine-learning models from our previous works [<xref rid="B26-sensors-24-00510" ref-type="bibr">26</xref>], and the three best-performing semantic segmentation machine-learning models were considered for the works presented in this paper. These three models are DeepLabV3_MobileNet_V3 _Large, FCN_Resnet101, and Lraspp_MobileNet_V3_Large. The selected models were trained for URDE dataset_897. Dust-segmented masks were then imposed onto each video frame. The accumulated probabilities of dust in pixels were recorded for each frame, and the average was obtained for 30 frames. Each original image has a corresponding manually annotated image called the ground truth. Original images and corresponding ground truths were used to train the machine learning models. From the processed video, the percentage dust pixel ratio (<inline-formula>
<mml:math id="mm26" overflow="scroll"><mml:mrow><mml:mi>&#x003c8;</mml:mi></mml:mrow></mml:math>
</inline-formula>) was calculated according to (Equation (<xref rid="FD2-sensors-24-00510" ref-type="disp-formula">2</xref>)).
<disp-formula id="FD1-sensors-24-00510">
<label>(1)</label>
<mml:math id="mm27" display="block" overflow="scroll"><mml:mrow><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>Average</mml:mi><mml:mspace width="4.pt"/><mml:mi>accumulated</mml:mi><mml:mspace width="4.pt"/><mml:mi>dust</mml:mi><mml:mspace width="4.pt"/><mml:mi>probabilities</mml:mi><mml:mspace width="4.pt"/><mml:mo>(</mml:mo><mml:mi>&#x003d5;</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo>&#x02211;</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mi>N</mml:mi></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math>
</disp-formula>
<disp-formula id="FD2-sensors-24-00510">
<label>(2)</label>
<mml:math id="mm28" display="block" overflow="scroll"><mml:mrow><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>Dust</mml:mi><mml:mspace width="4.pt"/><mml:mi>pixel</mml:mi><mml:mspace width="4.pt"/><mml:mi>ratio</mml:mi><mml:mspace width="4.pt"/><mml:mo>(</mml:mo><mml:mi>&#x003c8;</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>30</mml:mn></mml:mfrac><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mn>30</mml:mn></mml:munderover><mml:mfenced open="(" close=")"><mml:mi>&#x003d5;</mml:mi></mml:mfenced></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math>
</disp-formula>
where <inline-formula>
<mml:math id="mm29" overflow="scroll"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math>
</inline-formula> is the probability of dust in the <inline-formula>
<mml:math id="mm30" overflow="scroll"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math>
</inline-formula> pixel, and <italic toggle="yes">N</italic> is the total number of pixels of a video frame. The first Equation (<xref rid="FD1-sensors-24-00510" ref-type="disp-formula">1</xref>) calculates static frame dist pixel ratio from each pixel dust probability obtained from semantic segmentation prediction, and the second Equation (<xref rid="FD2-sensors-24-00510" ref-type="disp-formula">2</xref>) averages over 30 frames (approx 1 s) to reduce uncertainty noise.</p></sec><sec id="sec3-sensors-24-00510"><title>3. Experiments</title><sec id="sec3dot1-sensors-24-00510"><title>3.1. Instrumentation</title><sec id="sec3dot1dot1-sensors-24-00510"><title>3.1.1. Dust Monitor</title><p>A research-grade real-time dust monitor, the DustTrak&#x02122; DRX Aerosol Monitor 8533 from TSI, was used in this study. Using the light-scattering laser photometric method, it can simultaneously measure mass and size fractions continuously every second.</p></sec><sec id="sec3dot1dot2-sensors-24-00510"><title>3.1.2. Camera</title><p>The images were collected from videos captured by a Canon EOS 200D direct single-lens reflex (DSLR) camera mounted on a tripod at a fixed focal length and 30 frames per second (fps). The camera was positioned according to the test set-up shown in <xref rid="sensors-24-00510-f002" ref-type="fig">Figure 2</xref>.</p></sec><sec id="sec3dot1dot3-sensors-24-00510"><title>3.1.3. Test Vehicle</title><p>The test vehicle used for the experiments was a 2002 Toyota Corolla Sedan with an overall height of 1470 mm, an overall length of 4365 mm, an overall width of 1695 mm, a ground clearance (unladen) of 160 mm, a wheelbase of 2600 mm, a kerb weight of 1050 kg, and gross trailer weight brake of 1300 kg.</p></sec></sec><sec id="sec3dot2-sensors-24-00510"><title>3.2. Test Setups</title><p>The test vehicle was driven at different unsealed road segments to produce a dust cloud. The dust cloud was captured from multiple points of view for both travel directions, as shown in <xref rid="sensors-24-00510-f002" ref-type="fig">Figure 2</xref>. The dust monitor was placed in the wind direction so that the dust monitor&#x02019;s inlet sampled a representative portion of the dust cloud.</p></sec></sec><sec sec-type="methods" id="sec4-sensors-24-00510"><title>4. Methods</title><p>In order to develop a relationship between features extracted from the images, i.e., the percentage of dust pixels and the actual dust concentrations, first, we investigated the relationship between dust cloud annotation performed by hand and the prediction by an ML model. As shown in <xref rid="sensors-24-00510-f003" ref-type="fig">Figure 3</xref>, some differences exist between the hand-annotated image and the prediction. This is further illustrated in <xref rid="sensors-24-00510-f004" ref-type="fig">Figure 4</xref> for the dataset. <xref rid="sensors-24-00510-f004" ref-type="fig">Figure 4</xref> shows differences between machine learning (ML) predictions and manual (hand) annotations of dust pixels. These differences can be due to human error, as people might not always agree on what counts as a dust pixel, model limits as our ML model may miss some nuances in dust patterns, leading to less accurate predictions, limited diversity in data, differences in image resolution between hand annotations, and measurement mistakes. Therefore, it was prudent to develop two relationships for actual dust measurements: one with hand-annotated images and another with ML predictions. The first step encompasses an analysis of original high-resolution field images that capture diverse dust intensities on road surfaces. Accompanying these images are manually annotated grayscale masks, where each pixel&#x02019;s intensity is indicative of the quantity of dust present. In these masks, darker shades signify minimal or no dust presence (intensity value = 0), while progressively lighter shades up to white represent higher dust concentrations (intensity value = 1). The intermediate shades provide a gradient scale of dust intensity, informing varying dust levels. Additionally, binary segmentation masks are derived from the grayscale masks using Otsu&#x02019;s thresholding method. This binary format, which comprises only two pixel values (0 or 1), serves as a critical prerequisite for the training of our machine learning model. It simplifies the dust identification process by categorizing the segments into distinct classes. The effectiveness of this approach is further demonstrated in the predicted dust segmentation by the machine learning model Lraspp_MobileNet_V3_Large. This model showcases a capability in identifying dust-laden areas from the surrounding environment [<xref rid="B16-sensors-24-00510" ref-type="bibr">16</xref>].</p><p>The predictions from segmentation are validated by comparing them with respective field measurements, and air quality thresholds have been imposed on the predictions. Australia has established dust exposure limits of 10 mg/m<sup>3</sup> for workplaces, so that workers&#x02019; breathing zone should not cause adverse health effects or cause undue discomfort [<xref rid="B27-sensors-24-00510" ref-type="bibr">27</xref>]. The Australian Standard for PM<sub>10</sub> is 50 &#x000b5;g/m<sup>3</sup>, measured over a midnight-to-midnight 24 h period. However, most states adopt their own, sometimes stricter, PM<sub>10</sub> limits. There is currently no national standard for the average one-hour PM<sub>10</sub>. For one-hour PM<sub>10</sub>, Victoria uses the value 80 &#x000b5;g/m<sup>3</sup> to trigger a &#x02018;poor&#x02019; air quality category [<xref rid="B28-sensors-24-00510" ref-type="bibr">28</xref>]. The disease control and prevention centers in the United States have set the occupational exposure limit for total dust in flavoring-related work as 15 mg/m<sup>3</sup> over an 8-h time-weighted average. Multiple dust threshold levels have been established in various countries and jurisdictions. For our study, we opted to adhere to the guidelines set by OSHA [<xref rid="B29-sensors-24-00510" ref-type="bibr">29</xref>]. OSHA&#x02019;s legal airborne permissible exposure limit (PEL) is 15 mg/m<sup>3</sup> for total dust and 5 mg/m<sup>3</sup> for respirable dust, averaged over an 8-h work shift.</p><p>Correlation 1 was developed using the percentage of dust pixels obtained from manually annotated images. Correlation 2 was developed using the percentage of dust pixels obtained from images segmented with different ML models. The developed correlations are valid for images and videos obtained when the camera captures more than 20 m of the road and less than 100 m from its FoV and the total measured particulate matter is less than 30 mg/m<sup>3</sup>. These are reasonable limitations, because if the dust measurements exceed 30 mg/m<sup>3</sup> for a prolonged period of time, then the road segment obviously needs to address the dust emissions. If the image obtained captured the dust cloud too close, then it would be an error in selecting the control volume.</p><sec id="sec4dot1-sensors-24-00510"><title>4.1. Correlation 1</title><p>To match a dust cloud with an in situ dust measurement, the generated dust cloud needs to be captured when it is sampled by the dust monitor. A reference object should be in the captured video sequence or the image to make the comparison possible. To effectively solve this problem, we selected a dust monitor as the reference object and designed our field experiments in a way so that the distance between the dust monitor and the camera was the same. To keep the experiments consistent, reproducible, and extendable, the depth data were obtained with respect to a reference image taken beforehand.</p><p>Image statistics of the reference object, i.e., the dust monitor, were studied in different crops or angles within multiple images prior to extracting features from the image for the correlation. As the annotation of the images was performed by qualified people, the reference object detection was translation-invariant, viewpoint-invariant, size-invariant and illumination-invariant [<xref rid="B30-sensors-24-00510" ref-type="bibr">30</xref>]. There are multiple ways to compare the similarity of an object in different images. An image-based, direct similarity metric could be used, such as the sum of absolute distances, the sum of squared distances [<xref rid="B31-sensors-24-00510" ref-type="bibr">31</xref>] or normalized cross correlation [<xref rid="B32-sensors-24-00510" ref-type="bibr">32</xref>]. These approaches generalize to template matching, where a convolutional procedure is conducted. For simplicity, in this paper, we use the pixel density resultant ratio to determine similarity, as described below.</p><p>The pixel density resultant of the reference image was calculated according to Equation (<xref rid="FD3-sensors-24-00510" ref-type="disp-formula">3</xref>). The same for all other images was calculated according to Equation (<xref rid="FD4-sensors-24-00510" ref-type="disp-formula">4</xref>).
<disp-formula id="FD3-sensors-24-00510">
<label>(3)</label>
<mml:math id="mm43" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:msup><mml:mfenced separators="" open="[" close="]"><mml:mfrac><mml:msub><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi>p</mml:mi><mml:mi>x</mml:mi></mml:msub></mml:mfenced><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi>l</mml:mi><mml:mi>x</mml:mi></mml:msub></mml:mfenced><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mfenced><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mfenced separators="" open="[" close="]"><mml:mfrac><mml:msub><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi>p</mml:mi><mml:mi>z</mml:mi></mml:msub></mml:mfenced><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi>l</mml:mi><mml:mi>z</mml:mi></mml:msub></mml:mfenced><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mfenced><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msqrt></mml:mrow></mml:mrow></mml:math>
</disp-formula>
where <inline-formula>
<mml:math id="mm44" overflow="scroll"><mml:mrow><mml:msub><mml:mfenced separators="" open="[" close="]"><mml:msub><mml:mi>p</mml:mi><mml:mi>x</mml:mi></mml:msub></mml:mfenced><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math>
</inline-formula>, <inline-formula>
<mml:math id="mm45" overflow="scroll"><mml:mrow><mml:msub><mml:mfenced separators="" open="[" close="]"><mml:msub><mml:mi>l</mml:mi><mml:mi>x</mml:mi></mml:msub></mml:mfenced><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math>
</inline-formula>, <inline-formula>
<mml:math id="mm46" overflow="scroll"><mml:mrow><mml:msub><mml:mfenced separators="" open="[" close="]"><mml:msub><mml:mi>p</mml:mi><mml:mi>z</mml:mi></mml:msub></mml:mfenced><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math>
</inline-formula>, and <inline-formula>
<mml:math id="mm47" overflow="scroll"><mml:mrow><mml:msub><mml:mfenced separators="" open="[" close="]"><mml:msub><mml:mi>l</mml:mi><mml:mi>z</mml:mi></mml:msub></mml:mfenced><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math>
</inline-formula> are the number of pixels along the horizontal length of the dust monitor in the reference image, the horizontal length of the dust monitor in the reference image, the number of pixels along the vertical length of the dust monitor in the reference image, and the vertical length of the dust monitor in the reference image, respectively.
<disp-formula id="FD4-sensors-24-00510">
<label>(4)</label>
<mml:math id="mm48" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:msup><mml:mfenced separators="" open="[" close="]"><mml:mfrac><mml:msub><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi>p</mml:mi><mml:mi>x</mml:mi></mml:msub></mml:mfenced><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi>l</mml:mi><mml:mi>x</mml:mi></mml:msub></mml:mfenced><mml:mi>i</mml:mi></mml:msub></mml:mfrac></mml:mfenced><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mfenced separators="" open="[" close="]"><mml:mfrac><mml:msub><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi>p</mml:mi><mml:mi>z</mml:mi></mml:msub></mml:mfenced><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi>l</mml:mi><mml:mi>z</mml:mi></mml:msub></mml:mfenced><mml:mi>i</mml:mi></mml:msub></mml:mfrac></mml:mfenced><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msqrt></mml:mrow></mml:mrow></mml:math>
</disp-formula>
where <inline-formula>
<mml:math id="mm49" overflow="scroll"><mml:mrow><mml:msub><mml:mfenced separators="" open="[" close="]"><mml:msub><mml:mi>p</mml:mi><mml:mi>x</mml:mi></mml:msub></mml:mfenced><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math>
</inline-formula>, <inline-formula>
<mml:math id="mm50" overflow="scroll"><mml:mrow><mml:msub><mml:mfenced separators="" open="[" close="]"><mml:msub><mml:mi>l</mml:mi><mml:mi>x</mml:mi></mml:msub></mml:mfenced><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math>
</inline-formula>, <inline-formula>
<mml:math id="mm51" overflow="scroll"><mml:mrow><mml:msub><mml:mfenced separators="" open="[" close="]"><mml:msub><mml:mi>p</mml:mi><mml:mi>z</mml:mi></mml:msub></mml:mfenced><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math>
</inline-formula> and <inline-formula>
<mml:math id="mm52" overflow="scroll"><mml:mrow><mml:msub><mml:mfenced separators="" open="[" close="]"><mml:msub><mml:mi>l</mml:mi><mml:mi>z</mml:mi></mml:msub></mml:mfenced><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math>
</inline-formula> are the number of pixels along the horizontal length of the dust monitor in the image of interest, the horizontal length of the dust monitor in the same image, the vertical length of the dust monitor in the image of interest, and the vertical length of the dust monitor in the same image, respectively.</p><p>To keep the correlation independent of semantic segmentation, the correlation was developed using ground truths. Each dust pixel percentage obtained using ground truths (<inline-formula>
<mml:math id="mm53" overflow="scroll"><mml:mrow><mml:mi>&#x003d5;</mml:mi></mml:mrow></mml:math>
</inline-formula>) is modified using the ratio <inline-formula>
<mml:math id="mm54" overflow="scroll"><mml:mrow><mml:mfrac><mml:msub><mml:mi>R</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:math>
</inline-formula>, as shown in Equation (<xref rid="FD5-sensors-24-00510" ref-type="disp-formula">5</xref>).
<disp-formula id="FD5-sensors-24-00510">
<label>(5)</label>
<mml:math id="mm55" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#x003d5;</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>&#x003d5;</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mfrac><mml:msub><mml:mi>R</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:mrow></mml:math>
</disp-formula></p><p>Actual field dust measurements were then correlated with <inline-formula>
<mml:math id="mm56" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003d5;</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math>
</inline-formula>, as shown in <xref rid="sensors-24-00510-f005" ref-type="fig">Figure 5</xref>.</p></sec><sec id="sec4dot2-sensors-24-00510"><title>4.2. Correlation 2</title><p>Correlation 2 (<xref rid="sensors-24-00510-t001" ref-type="table">Table 1</xref>) was developed using the segmented results from different video sequences. The dust monitor was turned on in the field experiments before the vehicle started traveling to capture the background dust concentration. Therefore, each experiment has a much higher number of data points for background concentration than the data points for the actual dust event. If all the data points from an experiment were included in developing a correlation, it would be biased and inaccurate. To avoid that, dust events were generalized based on measurements obtained from the dust monitor, as shown in <xref rid="sensors-24-00510-f006" ref-type="fig">Figure 6</xref>a. It is evident that the dust even comprises &#x0201c;outlier&#x0201d; dust readings; therefore, we identified and isolated the dust event by identifying the origin and conclusion of a particular experiment through outlier sampling. The method used was similar to the rule of 1.5(IQR), where IQR is the interquartile range [<xref rid="B33-sensors-24-00510" ref-type="bibr">33</xref>]. The first quartile (<inline-formula>
<mml:math id="mm57" overflow="scroll"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math>
</inline-formula>) and third quartile (<inline-formula>
<mml:math id="mm58" overflow="scroll"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:math>
</inline-formula>) were calculated, and the IQR was obtained. However, instead of sampling outliers from the interquartile range from each quartile, we used the interquartile range directly. This approach was selected to ensure that adequate portions of the background dust and the dispersion were captured in the event. After dust events had been isolated for all experiments, the correlation was developed as shown in <xref rid="sensors-24-00510-f006" ref-type="fig">Figure 6</xref>b.</p></sec><sec id="sec4dot3-sensors-24-00510"><title>4.3. Graphical User Interface (GUI)</title><p>A graphical user interface (GUI) was developed to dynamically process video or image data and produce the above-discussed analysis on dust clouds. The code and software for this analytical tool are available at <uri xlink:href="https://github.com/RajithaRanasinghe/Dust-Cloud-Identification">https://github.com/RajithaRanasinghe/Dust-Cloud-Identification</uri> (accessed on 14 January 2024). A snapshot of the GUI is shown in <xref rid="sensors-24-00510-f007" ref-type="fig">Figure 7</xref>a. A snapshot of the GUI when a video is being processed is shown in <xref rid="sensors-24-00510-f007" ref-type="fig">Figure 7</xref>b. All training, testing and evaluation was performed in a Ryzen 9 5900HX CPU, 32GB RAM, and an NVIDIA RTX3080 Mobile 16GB GPU test environment. Similar platform specifications will be needed to run the executable provided in the above link.</p></sec></sec><sec sec-type="results" id="sec5-sensors-24-00510"><title>5. Results and Validation</title><p>The developed correlation was validated for an independent set of video sequences. An example of dust predictions for a video segment obtained from all three ML models with corresponding field dust measurements is shown in <xref rid="sensors-24-00510-f008" ref-type="fig">Figure 8</xref>. It is evident that all three ML outcomes are lower than the peak of the actual dust measurement suggesting the ML outcomes underestimate the maximum dust concentration. However, all three ML models overestimate the background dust concentration.</p><p>The accuracy of the developed correlations was evaluated by sensitivity (true positive) and specificity (true negative) using the threshold dust concentration <inline-formula>
<mml:math id="mm59" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>&#x003bc;</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math>
</inline-formula> 15 mg/m<sup>3</sup>. The true positives (TP) represent the dust predictions lower than <inline-formula>
<mml:math id="mm61" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>&#x003bc;</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math>
</inline-formula> and are actually lower than <inline-formula>
<mml:math id="mm62" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>&#x003bc;</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math>
</inline-formula> and depicted in quadrant 1. Conversely, the true negatives (TN) are the dust concentrations outside <inline-formula>
<mml:math id="mm63" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>&#x003bc;</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math>
</inline-formula> both in the segmentation predictions and the actual ground concentration and are depicted in quadrant 3. The false positives (FP) represent the dust concentrations that exceed <inline-formula>
<mml:math id="mm64" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>&#x003bc;</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math>
</inline-formula> but are predicted as less than <inline-formula>
<mml:math id="mm65" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>&#x003bc;</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math>
</inline-formula> and are depicted in quadrant 2. The false negatives (FN) are where actual dust predictions suggest that <inline-formula>
<mml:math id="mm66" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>&#x003bc;</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math>
</inline-formula> has been exceeded; however, the actual dust concentrations are less than <inline-formula>
<mml:math id="mm67" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>&#x003bc;</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math>
</inline-formula> and are shown in quadrant 4.</p><p>As shown in <xref rid="sensors-24-00510-f009" ref-type="fig">Figure 9</xref>, more than 90% of the data from both correlations fall in quadrant 1, meaning when the dust concentrations are less than the threshold, the segmentation can make an accurate prediction. If a stricter dust threshold is considered, for example, <inline-formula>
<mml:math id="mm68" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#x003bc;</mml:mi><mml:mi>d</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math>
</inline-formula> = 5 mg/m<sup>3</sup>, most of the data in both correlations would still fall in quadrant 1. The number of data points in quadrant 3 would increase in both correlations, suggesting that the correlations can adequately predict dust concentrations in both true positive and true negative cases. In the case of <inline-formula>
<mml:math id="mm70" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#x003bc;</mml:mi><mml:mi>d</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math>
</inline-formula> = 5 mg/m<sup>3</sup>, the rise in false negatives can be observed. This is not non-desirable as a slight overestimation of materials affecting air quality could be preferred among shrewd practitioners.</p><p>For both cases of <inline-formula>
<mml:math id="mm72" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#x003bc;</mml:mi><mml:mi>d</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math>
</inline-formula>, the number of data points in quadrant 2 does not change significantly, suggesting that although receiving a false positive is possible, it is negligible.</p></sec><sec sec-type="conclusions" id="sec6-sensors-24-00510"><title>6. Conclusions</title><p>This study provides a novel approach to quantifying traffic-induced dust in unsealed roads using semantic segmentation. The proposed method was validated by data collected from multiple field studies, where dust predictions from semantic segmentation were validated using actual in situ dust measurements. For established dust tolerance levels, i.e., 5 and 15 mg/m<sup>3</sup>, it was shown that the proposed method with reasonable accuracy predict traffic-induced dust. This study compares predictions from multiple different ML algorithms as a single algorithm may not make the perfect prediction for a given data set due to its limitations. The main highlights of this paper are summarized as follows.</p><list list-type="order"><list-item><p>The actual dust measurements from field experiments and dust concentrations predicted by the semantic segmentation demonstrate good correlations subjected to constraints established for field experiments. However, the proposed method is repeatable for different environments and testing equipment, as the main principal hypothesis is validated in this study.</p></list-item><list-item><p>Using the user interface we developed, one can go to a road segment of interest where dust emissions need to be assessed and record traffic-induced dust as a video sequence and determine if it exceeds relevant dust thresholds.</p></list-item><list-item><p>The developed method to quantify dust from images and videos can be used to assess dust severity with respect to dust exposure limits, thus, can be implemented as a real-world application.</p></list-item></list><p>Advanced semantic segmentation algorithms, namely DeepLabV3_MobileNetV3_Large, FCN_ResNet101, and Lraspp_MobileNetV3_large, were employed in conjunction with Otsu&#x02019;s thresholding method for the identification of dust-laden pixels in this study. This choice of algorithms and thresholding method, while not explicitly accounting for the variability in transmittance across the dust cloud, is aligned with our objective of estimating, rather than precisely quantifying, dust concentrations. The potential over or under-estimation of dust resulting from this approach is deemed acceptable within the parameters of an estimation-focused study. The robustness of our methodology was validated through a comparison with ground truths and actual field dust measurements. This validation process involved capturing images and corresponding dust events in the field alongside real-time dust measurements.</p><p>To quantify the correlation between the segmented dust in images and actual in situ measurements, the coefficient of determination (R<sup>2</sup>) was used, further strengthened by applying a 95% confidence interval. This statistical approach underscores the high degree of correlation between our image-based dust estimation and actual field measurements, thereby reinforcing the validity of our method. When compared to existing dust quantification methods, the image segmentation-based approach is recognized as a more cost-effective alternative. Traditional methods typically depend on either less accurate empirical models or expensive dust monitors, whereas the method presented in this study offers a viable solution for dust monitoring, particularly beneficial for applications in environmental monitoring, industrial settings, and public health studies. Future enhancements to the methodology are anticipated, especially in terms of incorporating variability in dust cloud transmittance. These improvements aim to refine the accuracy of dust concentration estimations, reducing any overestimation biases. Furthermore, the authors&#x02019; future work will be focused on the automation of scene understanding in unsealed roads in terms of road dust, soil type and traffic, making the approach wholesome.</p></sec><sec id="sec7-sensors-24-00510"><title>7. Future&#x000a0;Works</title><p>In this study, we have implemented and tested conventional machine learning models tailored for static image segmentation for dust identification, with each model trained on individual images. While this approach has yielded valuable insights, we anticipate significant enhancements by incorporating cutting-edge machine-learning frameworks capable of video-based analysis. Particularly, architectures like Convolutional Long Short-Term Memory (LSTM) networks, which excel in handling sequential data, and 3D Convolutional Neural Networks (3D CNNs), which extend the capabilities of 2D convolutions by adding time as a third dimension, could be leveraged. These models are adept at capturing the spatial-temporal dynamics in video sequences, making them well-suited for analyzing phenomena such as dust clouds exhibiting spatial and temporal variability. Further exploration into architectures such as Two-Stream Convolutional Networks, which combine spatial and temporal network streams for action recognition in videos, and the more recent Inflated 3D ConvNets (I3D), which inflate filters and pooling kernels into 3D, could provide even more sophisticated tools for dust cloud characterization.</p></sec></body><back><ack><title>Acknowledgments</title><p>The first author received a Monash Research Scholarship and a Faculty of Engineering International Postgraduate Research Scholarship to undertake this research project. The authors gratefully acknowledge the financial and in-kind support of Monash University, the SPARC Hub, and Downer EDI Works Pty Ltd.</p></ack><fn-group><fn><p><bold>Disclaimer/Publisher&#x02019;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>A.d.S., R.R., A.S., H.H. and J.K. set the research direction for the dust images collection. A.d.S. and R.R. carried out the experiments, interpreted the results, developed the supplementary dataset, and employed machine learning techniques for semantic segmentation on the dataset. A.d.S. developed correlations. A.d.S. drafted the entire manuscript. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>Not applicable.</p></notes><notes><title>Informed Consent Statement</title><p>Not applicable.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>The URDE dataset is publicly available at <uri xlink:href="https://doi.org/10.6084/m9.figshare.20459784.v3">https://doi.org/10.6084/m9.figshare.20459784.v3</uri> (accessed on 15 November 2022).</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>H.H. is currently employed by Downer EDI Works Pty Ltd., which has funded the research work in this study. However, it is declared that this employment has not influenced the interpretation or presentation of the results in this study. All findings and conclusions presented in this work are the result of objective analysis and interpretation of the data.</p></notes><glossary><title>Abbreviations</title><p>The following abbreviations are used in this manuscript:
<array><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">MDPI</td><td align="left" valign="middle" rowspan="1" colspan="1">Multidisciplinary Digital Publishing Institute</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">PM</td><td align="left" valign="middle" rowspan="1" colspan="1">Particulate Matter</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">USEPA</td><td align="left" valign="middle" rowspan="1" colspan="1">United States Environmental Protection Agency</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">ML</td><td align="left" valign="middle" rowspan="1" colspan="1">Machine Learning</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">FCN</td><td align="left" valign="middle" rowspan="1" colspan="1">Fully Convolutional Network</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">LR-ASPP</td><td align="left" valign="middle" rowspan="1" colspan="1">Lite Reduced Atrous Spatial Pyramid Pooling</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">DSC</td><td align="left" valign="middle" rowspan="1" colspan="1">Dice Similarity Coefficient</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">FoV</td><td align="left" valign="middle" rowspan="1" colspan="1">Field of View</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">URDE</td><td align="left" valign="middle" rowspan="1" colspan="1">Unsealed Road Dust Dataset</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">DSLR</td><td align="left" valign="middle" rowspan="1" colspan="1">Direct Single Lens Reflex</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">fps</td><td align="left" valign="middle" rowspan="1" colspan="1">frames per second</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">GUI</td><td align="left" valign="middle" rowspan="1" colspan="1">Graphical User Interface</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">GPU</td><td align="left" valign="middle" rowspan="1" colspan="1">Graphics Processing Unit</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">CPU</td><td align="left" valign="middle" rowspan="1" colspan="1">Central Processing Unit</td></tr></tbody></array></p></glossary><ref-list><title>References</title><ref id="B1-sensors-24-00510"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Khan</surname><given-names>R.K.</given-names></name>
<name><surname>Strand</surname><given-names>M.A.</given-names></name>
</person-group><article-title>Road dust and its effect on human health: A literature review</article-title><source>Epidemiol. Health</source><year>2018</year><volume>40</volume><fpage>e2018013</fpage><pub-id pub-id-type="doi">10.4178/epih.e2018013</pub-id><?supplied-pmid 29642653?><pub-id pub-id-type="pmid">29642653</pub-id>
</element-citation></ref><ref id="B2-sensors-24-00510"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Morakinyo</surname><given-names>O.M.</given-names></name>
<name><surname>Mokgobu</surname><given-names>M.I.</given-names></name>
<name><surname>Mukhola</surname><given-names>M.S.</given-names></name>
<name><surname>Hunter</surname><given-names>R.P.</given-names></name>
</person-group><article-title>Health Outcomes of Exposure to Biological and Chemical Components of Inhalable and Respirable Particulate Matter</article-title><source>Int. J. Environ. Res. Public Health</source><year>2016</year><volume>13</volume><elocation-id>592</elocation-id><pub-id pub-id-type="doi">10.3390/ijerph13060592</pub-id><?supplied-pmid 27314370?><pub-id pub-id-type="pmid">27314370</pub-id>
</element-citation></ref><ref id="B3-sensors-24-00510"><label>3.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Shi</surname><given-names>X.</given-names></name>
<name><surname>Jin</surname><given-names>N.</given-names></name>
</person-group><article-title>Xianyang City Atmospheric Dust Analysis in the Spring of 2008</article-title><source>Proceedings of the 2009 International Conference on Energy and Environment Technology</source><conf-loc>Guilin, China</conf-loc><conf-date>16&#x02013;18 October 2009</conf-date><volume>Volume 3</volume><fpage>147</fpage><lpage>150</lpage></element-citation></ref><ref id="B4-sensors-24-00510"><label>4.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>He</surname><given-names>H.</given-names></name>
<name><surname>Gao</surname><given-names>Y.</given-names></name>
<name><surname>Zhang</surname><given-names>Z.</given-names></name>
</person-group><article-title>The urban road dust monitoring system based on ZigBee</article-title><source>Proceedings of the 2016 Chinese Control and Decision Conference (CCDC)</source><conf-loc>Yinchuan, China</conf-loc><conf-date>28&#x02013;30 May 2016</conf-date><fpage>1793</fpage><lpage>1796</lpage></element-citation></ref><ref id="B5-sensors-24-00510"><label>5.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Xu</surname><given-names>D.</given-names></name>
<name><surname>Niu</surname><given-names>S.</given-names></name>
</person-group><article-title>Integrated ground-based monitoring of dust aerosol from a semi-arid site in North China during 2004&#x02013;2006</article-title><source>Proceedings of the 2009 17th International Conference on Geoinformatics</source><conf-loc>Fairfax, VA, USA</conf-loc><conf-date>12&#x02013;14 August 2009</conf-date><fpage>1</fpage><lpage>4</lpage></element-citation></ref><ref id="B6-sensors-24-00510"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zhang</surname><given-names>J.</given-names></name>
<name><surname>Teng</surname><given-names>Z.</given-names></name>
<name><surname>Huang</surname><given-names>N.</given-names></name>
<name><surname>Guo</surname><given-names>L.</given-names></name>
<name><surname>Shao</surname><given-names>Y.</given-names></name>
</person-group><article-title>Surface renewal as a significant mechanism for dust emission</article-title><source>Atmos. Chem. Phys.</source><year>2016</year><volume>16</volume><fpage>15517</fpage><lpage>15528</lpage><pub-id pub-id-type="doi">10.5194/acp-16-15517-2016</pub-id></element-citation></ref><ref id="B7-sensors-24-00510"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Lu</surname><given-names>H.</given-names></name>
<name><surname>Shao</surname><given-names>Y.</given-names></name>
</person-group><article-title>A new model for dust emission by saltation bombardment</article-title><source>J. Geophys. Res. Atmos.</source><year>1999</year><volume>104</volume><fpage>16827</fpage><lpage>16842</lpage><pub-id pub-id-type="doi">10.1029/1999JD900169</pub-id></element-citation></ref><ref id="B8-sensors-24-00510"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Ishizuka</surname><given-names>M.</given-names></name>
<name><surname>Mikami</surname><given-names>M.</given-names></name>
<name><surname>Leys</surname><given-names>J.</given-names></name>
<name><surname>Yamada</surname><given-names>Y.</given-names></name>
<name><surname>Heidenreich</surname><given-names>S.</given-names></name>
<name><surname>Shao</surname><given-names>Y.</given-names></name>
<name><surname>McTainsh</surname><given-names>G.H.</given-names></name>
</person-group><article-title>Effects of soil moisture and dried raindroplet crust on saltation and dust emission</article-title><source>J. Geophys. Res. Atmos.</source><year>2008</year><volume>113</volume><pub-id pub-id-type="doi">10.1029/2008JD009955</pub-id></element-citation></ref><ref id="B9-sensors-24-00510"><label>9.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>McEwan</surname><given-names>I.K.</given-names></name>
<name><surname>Willetts</surname><given-names>B.B.</given-names></name>
</person-group><article-title>Numerical model of the saltation cloud</article-title><source>Barndorff-Nielsen</source><person-group person-group-type="editor">
<name><surname>Ole</surname><given-names>E.</given-names></name>
<name><surname>Willetts</surname><given-names>B.B.</given-names></name>
</person-group><comment>Aeolian Grain Transport 1</comment><publisher-name>Springer</publisher-name><publisher-loc>Berlin/Heidelberg, Germany</publisher-loc><year>1991</year><fpage>53</fpage><lpage>66</lpage></element-citation></ref><ref id="B10-sensors-24-00510"><label>10.</label><element-citation publication-type="book"><person-group person-group-type="author">
<collab>USEPA</collab>
</person-group><source>AP-42, Compilation of Air Pollutant Emission Factors</source><publisher-name>John Wiley &#x00026; Sons, Inc.</publisher-name><publisher-loc>Hoboken, NJ, USA</publisher-loc><year>2016</year><fpage>137</fpage><lpage>140</lpage></element-citation></ref><ref id="B11-sensors-24-00510"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Garcia-Garcia</surname><given-names>A.</given-names></name>
<name><surname>Orts</surname><given-names>S.</given-names></name>
<name><surname>Oprea</surname><given-names>S.</given-names></name>
<name><surname>Villena-Martinez</surname><given-names>V.</given-names></name>
<name><surname>Rodr&#x000ed;guez</surname><given-names>J.</given-names></name>
</person-group><article-title>A Review on Deep Learning Techniques Applied to Semantic Segmentation</article-title><source>arXiv</source><year>2017</year><pub-id pub-id-type="arxiv">1704.06857</pub-id></element-citation></ref><ref id="B12-sensors-24-00510"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Guo</surname><given-names>Y.</given-names></name>
<name><surname>Liu</surname><given-names>Y.</given-names></name>
<name><surname>Georgiou</surname><given-names>T.</given-names></name>
<name><surname>Lew</surname><given-names>M.S.</given-names></name>
</person-group><article-title>A review of semantic segmentation using deep neural networks</article-title><source>Int. J. Multimed. Inf. Retr.</source><year>2018</year><volume>7</volume><fpage>87</fpage><lpage>93</lpage><pub-id pub-id-type="doi">10.1007/s13735-017-0141-z</pub-id></element-citation></ref><ref id="B13-sensors-24-00510"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Albatayneh</surname><given-names>O.</given-names></name>
<name><surname>Forsl&#x000f6;f</surname><given-names>L.</given-names></name>
<name><surname>Ksaibati</surname><given-names>K.</given-names></name>
</person-group><article-title>Developing and validating an image processing algorithm for evaluating gravel road dust</article-title><source>Int. J. Pavement Res. Technol.</source><year>2019</year><volume>12</volume><fpage>288</fpage><lpage>296</lpage><pub-id pub-id-type="doi">10.1007/s42947-019-0035-y</pub-id></element-citation></ref><ref id="B14-sensors-24-00510"><label>14.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Allan</surname><given-names>M.</given-names></name>
<name><surname>Henning</surname><given-names>T.F.P.</given-names></name>
<name><surname>Andrews</surname><given-names>M.</given-names></name>
</person-group><article-title>A Pragmatic Approach for Dust Monitoring on Unsealed Roads</article-title><source>Proceedings of the 12th International Conference on Low-Volume Roads, Unsealed Roads Management 2</source><conf-loc>Kalispell, MT, USA</conf-loc><conf-date>15&#x02013;18 September 2019</conf-date><fpage>439</fpage><lpage>444</lpage><comment>Available online: <ext-link xlink:href="https://www.trb.org/Publications/Blurbs/179567.aspx" ext-link-type="uri">https://www.trb.org/Publications/Blurbs/179567.aspx</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-01-14">(accessed on 14 January 2024)</date-in-citation></element-citation></ref><ref id="B15-sensors-24-00510"><label>15.</label><element-citation publication-type="webpage"><person-group person-group-type="author">
<name><surname>De Silva</surname><given-names>A.</given-names></name>
<name><surname>Ranasinghe</surname><given-names>R.</given-names></name>
</person-group><article-title>Unsealed Roads Dust Emissions (URDE), Figshare, 2022</article-title><comment>Available online: </comment><pub-id pub-id-type="doi">10.6084/m9.figshare.20459784</pub-id><date-in-citation content-type="access-date" iso-8601-date="2024-01-14">(accessed on 14 January 2024)</date-in-citation></element-citation></ref><ref id="B16-sensors-24-00510"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Otsu</surname><given-names>N.</given-names></name>
</person-group><article-title>A Threshold Selection Method from Gray-Level Histograms</article-title><source>IEEE Trans. Syst. Man Cybern.</source><year>1979</year><volume>9</volume><fpage>62</fpage><lpage>66</lpage><pub-id pub-id-type="doi">10.1109/TSMC.1979.4310076</pub-id></element-citation></ref><ref id="B17-sensors-24-00510"><label>17.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Pham</surname><given-names>T.</given-names></name>
</person-group><article-title>Semantic Road Segmentation using Deep Learning</article-title><source>Proceedings of the 2020 Applying New Technology in Green Buildings (ATiGB)</source><conf-loc>Da Nang, Vietnam</conf-loc><conf-date>12&#x02013;13 March 2021</conf-date><fpage>45</fpage><lpage>48</lpage></element-citation></ref><ref id="B18-sensors-24-00510"><label>18.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Jung</surname><given-names>H.</given-names></name>
<name><surname>Choi</surname><given-names>M.-K.</given-names></name>
<name><surname>Soon</surname><given-names>K.</given-names></name>
<name><surname>Jung</surname><given-names>W.Y.</given-names></name>
</person-group><article-title>End-to-end pedestrian collision warning system based on a convolutional neural network with semantic segmentation</article-title><source>Proceedings of the 2018 IEEE International Conference on Consumer Electronics (ICCE)</source><conf-loc>Las Vegas, NV, USA</conf-loc><conf-date>12&#x02013;14 January 2018</conf-date><fpage>1</fpage><lpage>3</lpage></element-citation></ref><ref id="B19-sensors-24-00510"><label>19.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Mouzinho</surname><given-names>F.A.L.N.</given-names></name>
<name><surname>Fukai</surname><given-names>H.</given-names></name>
</person-group><article-title>Hierarchical Semantic Segmentation Based Approach for Road Surface Damages and Markings Detection on Paved Road</article-title><source>Proceedings of the 2021 8th International Conference on Advanced Informatics: Concepts, Theory and Applications (ICAICTA)</source><conf-loc>Bandung, Indonesia</conf-loc><conf-date>29&#x02013;30 September 2021</conf-date><fpage>1</fpage><lpage>5</lpage></element-citation></ref><ref id="B20-sensors-24-00510"><label>20.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Huang</surname><given-names>G.</given-names></name>
<name><surname>Liu</surname><given-names>Z.</given-names></name>
<name><surname>Van Der Maaten</surname><given-names>L.</given-names></name>
<name><surname>Weinberger</surname><given-names>K.Q.</given-names></name>
</person-group><article-title>Densely Connected Convolutional Networks</article-title><source>Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Honolulu, HI, USA</conf-loc><conf-date>21&#x02013;26 July 2017</conf-date><fpage>2261</fpage><lpage>2269</lpage></element-citation></ref><ref id="B21-sensors-24-00510"><label>21.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Howard</surname><given-names>A.</given-names></name>
<name><surname>Sandler</surname><given-names>M.</given-names></name>
<name><surname>Chen</surname><given-names>B.</given-names></name>
<name><surname>Wang</surname><given-names>W.</given-names></name>
<name><surname>Chen</surname><given-names>L.-C.</given-names></name>
<name><surname>Tan</surname><given-names>M.</given-names></name>
<name><surname>Chu</surname><given-names>G.</given-names></name>
<name><surname>Vasudevan</surname><given-names>V.</given-names></name>
<name><surname>Zhu</surname><given-names>Y.</given-names></name>
<name><surname>Pang</surname><given-names>R.</given-names></name>
<etal/>
</person-group><article-title>Searching for MobileNetV3</article-title><source>Proceedings of the 2019 IEEE/CVF International Conference on Computer Vision (ICCV)</source><conf-loc>Seoul, Republic of Korea</conf-loc><conf-date>27 October&#x02013;2 November 2019</conf-date><fpage>1314</fpage><lpage>1324</lpage></element-citation></ref><ref id="B22-sensors-24-00510"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Chen</surname><given-names>L.-C.</given-names></name>
<name><surname>Papandreou</surname><given-names>G.</given-names></name>
<name><surname>Schroff</surname><given-names>F.</given-names></name>
<name><surname>Adam</surname><given-names>H.</given-names></name>
</person-group><article-title>Rethinking Atrous Convolution for Semantic Image Segmentation</article-title><source>arXiv</source><year>2017</year><pub-id pub-id-type="arxiv">1706.05587</pub-id></element-citation></ref><ref id="B23-sensors-24-00510"><label>23.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Shelhamer</surname><given-names>E.</given-names></name>
<name><surname>Long</surname><given-names>J.</given-names></name>
<name><surname>Darrell</surname><given-names>T.</given-names></name>
</person-group><article-title>Fully convolutional networks for semantic segmentation</article-title><source>Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Boston, MA, USA</conf-loc><conf-date>7&#x02013;12 June 2015</conf-date><fpage>3431</fpage><lpage>3440</lpage></element-citation></ref><ref id="B24-sensors-24-00510"><label>24.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>He</surname><given-names>K.</given-names></name>
<name><surname>Zhang</surname><given-names>X.</given-names></name>
<name><surname>Ren</surname><given-names>S.</given-names></name>
<name><surname>Sun</surname><given-names>J.</given-names></name>
</person-group><article-title>Deep Residual Learning for Image Recognition</article-title><source>Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Las Vegas, NV, USA</conf-loc><conf-date>27&#x02013;30 June 2016</conf-date><fpage>770</fpage><lpage>778</lpage></element-citation></ref><ref id="B25-sensors-24-00510"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Jiang</surname><given-names>H.</given-names></name>
<name><surname>Zhang</surname><given-names>G.</given-names></name>
<name><surname>Wang</surname><given-names>H.</given-names></name>
<name><surname>Bao</surname><given-names>H.</given-names></name>
</person-group><article-title>Spatio-Temporal Video Segmentation of Static Scenes and Its Applications</article-title><source>IEEE Trans. Multimed.</source><year>2015</year><volume>17</volume><fpage>3</fpage><lpage>15</lpage><pub-id pub-id-type="doi">10.1109/TMM.2014.2368273</pub-id></element-citation></ref><ref id="B26-sensors-24-00510"><label>26.</label><element-citation publication-type="webpage"><person-group person-group-type="author">
<name><surname>De Silva</surname><given-names>A.</given-names></name>
<name><surname>Ranasinghe</surname><given-names>R.</given-names></name>
<name><surname>Sounthararajah</surname><given-names>A.</given-names></name>
<name><surname>Haghighi</surname><given-names>H.</given-names></name>
<name><surname>Kodikara</surname><given-names>J.</given-names></name>
</person-group><article-title>Semantic Segmentation Model Performance on Vehicle-induced Dust Cloud Identification on Unsealed Roads</article-title><source>PREPRINT (Version 1) available at Research Square</source><comment>Available online: </comment><pub-id pub-id-type="doi">10.21203/rs.3.rs-2239765/v1</pub-id><date-in-citation content-type="access-date" iso-8601-date="2022-11-15">(accessed on 15 November 2022)</date-in-citation></element-citation></ref><ref id="B27-sensors-24-00510"><label>27.</label><element-citation publication-type="book"><person-group person-group-type="author">
<collab>WorkSafe Victoria</collab>
</person-group><article-title>Workplace exposure standards for airborne contaminants</article-title><source>Standard</source><publisher-name>WorkSafe Victoria, Geelong VIC</publisher-name><publisher-loc>Canberra, Australia</publisher-loc><year>2022</year><volume>Volume 2022</volume></element-citation></ref><ref id="B28-sensors-24-00510"><label>28.</label><element-citation publication-type="gov"><person-group person-group-type="author">
<collab>Federal Register of Legislation</collab>
</person-group><article-title>National Environment Protection (Ambient Air Quality) Measure, Compilation No. 3, 2021</article-title><comment>Available online: <ext-link xlink:href="https://www.legislation.gov.au/Details/F2021C00475" ext-link-type="uri">https://www.legislation.gov.au/Details/F2021C00475</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2022-11-15">(accessed on 15 November 2022)</date-in-citation></element-citation></ref><ref id="B29-sensors-24-00510"><label>29.</label><element-citation publication-type="gov"><person-group person-group-type="author">
<collab>Occupational Safety and Health Administration</collab>
</person-group><article-title>Annotated Permissible Exposure Limits (PELs)&#x02014;Table Z-1</article-title><comment>Available online: <ext-link xlink:href="https://www.osha.gov/annotated-pels/table-z-1" ext-link-type="uri">https://www.osha.gov/annotated-pels/table-z-1</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2023-12-08">(accessed on 8 December 2023)</date-in-citation></element-citation></ref><ref id="B30-sensors-24-00510"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Han</surname><given-names>Y.</given-names></name>
<name><surname>Roig</surname><given-names>G.</given-names></name>
<name><surname>Geiger</surname><given-names>G.</given-names></name>
<name><surname>Poggio</surname><given-names>T.</given-names></name>
</person-group><article-title>Scale and translation-invariance for novel objects in human vision</article-title><source>Sci. Rep.</source><year>2020</year><volume>10</volume><fpage>1411</fpage><pub-id pub-id-type="doi">10.1038/s41598-019-57261-6</pub-id><?supplied-pmid 31996698?><pub-id pub-id-type="pmid">31996698</pub-id>
</element-citation></ref><ref id="B31-sensors-24-00510"><label>31.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Kanade</surname><given-names>T.</given-names></name>
<name><surname>Kano</surname><given-names>H.</given-names></name>
<name><surname>Kimura</surname><given-names>S.</given-names></name>
<name><surname>Yoshida</surname><given-names>A.</given-names></name>
<name><surname>Oda</surname><given-names>K.</given-names></name>
</person-group><article-title>Development of a video-rate stereo machine</article-title><source>Proceedings of the 1995 IEEE/RSJ International Conference on Intelligent Robots and Systems. Human Robot Interaction and Cooperative Robots</source><conf-loc>Pittsburgh, PA, USA</conf-loc><conf-date>5&#x02013;9 August 1995</conf-date><volume>Volume 3</volume><fpage>95</fpage><lpage>100</lpage></element-citation></ref><ref id="B32-sensors-24-00510"><label>32.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Lewis</surname><given-names>J.P.</given-names></name>
</person-group><article-title>Fast Normalized Cross-Correlation</article-title><source>Ind. Light Magic</source><year>2001</year><volume>10</volume><fpage>120</fpage><lpage>123</lpage></element-citation></ref><ref id="B33-sensors-24-00510"><label>33.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Baron</surname><given-names>M.</given-names></name>
</person-group><source>Probability and Statistics for Computer Scientists</source><edition>2nd ed.</edition><publisher-name>CRC Press</publisher-name><publisher-loc>Boca Raton, FL, USA</publisher-loc><year>2014</year><isbn>9781439875902</isbn></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-24-00510-f002"><label>Figure 2</label><caption><p>Test setups. Eight different test configurations were used on the same road segment to cover both travel directions and two fields of view per direction. These configurations are as follows: (<bold>a</bold>) Vehicle traveling east with a northwest field of view, (<bold>b</bold>) Vehicle traveling east with a southwest field of view, (<bold>c</bold>) Vehicle traveling east with a northeast field of view, (<bold>d</bold>) Vehicle traveling east with a southeast field of view, (<bold>e</bold>) Vehicle traveling west with a northwest field of view, (<bold>f</bold>) Vehicle traveling west with a southwest field of view, (<bold>g</bold>) Vehicle traveling west with a northeast field of view, (<bold>h</bold>) Vehicle traveling west with a southeast field of view.</p></caption><graphic xlink:href="sensors-24-00510-g002" position="float"/></fig><fig position="float" id="sensors-24-00510-f003"><label>Figure 3</label><caption><p>Comparative analysis of dust cloud segmentation methods. (<bold>a</bold>) Original high-resolution field image, (<bold>b</bold>) manually annotated grayscale mask, (<bold>c</bold>) binary segmentation mask derived from the grayscale mask using Otsu&#x02019;s thresholding method, (<bold>d</bold>) predicted dust segmentation by the machine learning model Lraspp_MobileNet_V3_Large.</p></caption><graphic xlink:href="sensors-24-00510-g003" position="float"/></fig><fig position="float" id="sensors-24-00510-f004"><label>Figure 4</label><caption><p>Comparison of dust pixel percentage: machine learning vs. manual annotation.</p></caption><graphic xlink:href="sensors-24-00510-g004" position="float"/></fig><fig position="float" id="sensors-24-00510-f005"><label>Figure 5</label><caption><p>(<bold>a</bold>) Linear correlation between PM<sub>30</sub>, PM<sub>10</sub>, PM<sub>4</sub>, PM<sub>2.5</sub>, and PM<sub>1</sub>, and their respective percentage dust pixels from segmentation. (<bold>b</bold>) Linear correlation between PM<sub>10</sub> and percentage dust pixels from segmentation together with 95% prediction limits.</p></caption><graphic xlink:href="sensors-24-00510-g005" position="float"/></fig><fig position="float" id="sensors-24-00510-f006"><label>Figure 6</label><caption><p>(<bold>a</bold>) Generalized dust event, (<bold>b</bold>) Correlation 2.</p></caption><graphic xlink:href="sensors-24-00510-g006" position="float"/></fig><fig position="float" id="sensors-24-00510-f007"><label>Figure 7</label><caption><p>(<bold>a</bold>) GUI home page. (<bold>b</bold>) GUI when it is processing a video sequence.</p></caption><graphic xlink:href="sensors-24-00510-g007" position="float"/></fig><fig position="float" id="sensors-24-00510-f008"><label>Figure 8</label><caption><p>Validation. Field measurement and PM<sub>10</sub> prediction by the three ML models (correlation 2): DeeplabV3_MobileNetV3_Large, FCN_ResNet101, and Lraspp_MobileNetV3_Large, for video sequence 1.</p></caption><graphic xlink:href="sensors-24-00510-g008" position="float"/></fig><fig position="float" id="sensors-24-00510-f009"><label>Figure 9</label><caption><p>(<bold>a</bold>) Predicted dust concentration vs. actual dust measurement from dust monitor for correlation 1 for PM<sub>30</sub>, PM<sub>10</sub>, PM<sub>4</sub>, PM<sub>2.5</sub>, and PM<sub>1</sub>. (<bold>b</bold>) Predicted dust concentration vs. actual dust measurement from dust monitor for correlation 2 for PM<sub>30</sub>, PM<sub>10</sub>, PM<sub>4</sub>, PM<sub>2.5</sub>, and PM<sub>1</sub>. <inline-formula>
<mml:math id="mm96" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mn>2.5</mml:mn><mml:mo>,</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mn>4</mml:mn><mml:mo>,</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mn>10</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math>
</inline-formula> and 30.</p></caption><graphic xlink:href="sensors-24-00510-g009" position="float"/></fig><table-wrap position="float" id="sensors-24-00510-t001"><object-id pub-id-type="pii">sensors-24-00510-t001_Table 1</object-id><label>Table 1</label><caption><p>Correlation 2 regression data.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Equation</th><th colspan="5" align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1">
<inline-formula>
<mml:math id="mm97" overflow="scroll"><mml:mrow><mml:mstyle mathvariant="bold"><mml:mrow><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">a</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">a</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">a</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:msup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">a</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:msup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mn>3</mml:mn></mml:msup></mml:mrow></mml:mstyle></mml:mrow></mml:math>
</inline-formula>
</th></tr></thead><tbody><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Plot</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">PM<sub>1</sub></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">PM<sub>2.5</sub></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">PM<sub>4</sub></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">PM<sub>10</sub></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">PM<sub>30</sub></td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm103" overflow="scroll"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.207 &#x000b1; 0.053</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.231 &#x000b1; 0.056</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.304 &#x000b1; 0.067</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.543 &#x000b1; 0.109</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.612 &#x000b1; 0.126</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm104" overflow="scroll"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.913 &#x000b1; 0.110</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.970 &#x000b1; 0.116</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.152 &#x000b1; 0.138</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.594 &#x000b1; 0.225</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.868 &#x000b1; 0.260</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm105" overflow="scroll"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x02212;0.084 &#x000b1; 0.028</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x02212;0.089 &#x000b1; 0.029</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x02212;0.106 &#x000b1; 0.035</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x02212;0.134 &#x000b1; 0.056</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x02212;0.179 &#x000b1; 0.065</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm106" overflow="scroll"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.002 &#x000b1; 0.002</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.003 &#x000b1; 0.002</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.003 &#x000b1; 0.003</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.004 &#x000b1; 0.003</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.006 &#x000b1; 0.004</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Residual sum of squares</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">89.10</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">98.72</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">139.15</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">370.99</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">495.99</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm107" overflow="scroll"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.55</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.55</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.55</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.51</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.51</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Adjusted <inline-formula>
<mml:math id="mm108" overflow="scroll"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math>
</inline-formula></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.54</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.54</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.55</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.50</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.50</td></tr></tbody></table></table-wrap></floats-group></article>