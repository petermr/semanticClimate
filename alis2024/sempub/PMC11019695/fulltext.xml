<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd"> 
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?DTDIdentifier.IdentifierValue -//NLM//DTD Journal Publishing DTD v3.0 20080202//EN?><?DTDIdentifier.IdentifierType public?><?SourceDTD.DTDName journalpublishing3.dtd?><?SourceDTD.Version 3.0?><?ConverterInfo.XSLTName jats2jats3.xsl?><?ConverterInfo.Version 1?><?properties open_access?><?dpag newbhead?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Biomed Opt Express</journal-id><journal-id journal-id-type="iso-abbrev">Biomed Opt Express</journal-id><journal-id journal-id-type="publisher-id">BOE</journal-id><journal-title-group><journal-title>Biomedical Optics Express</journal-title></journal-title-group><issn pub-type="epub">2156-7085</issn><publisher><publisher-name>Optica Publishing Group</publisher-name></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmcid">11019695</article-id><article-id pub-id-type="publisher-id">510908</article-id><article-id pub-id-type="doi">10.1364/BOE.510908</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Improved dual-aggregation polyp segmentation network combining a pyramid vision transformer with a fully convolutional network</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Li</surname><given-names>Feng</given-names></name><xref rid="aff1" ref-type="aff">1</xref><xref rid="cor1" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><name><surname>Huang</surname><given-names>Zetao</given-names></name><xref rid="aff1" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name><surname>Zhou</surname><given-names>Lu</given-names></name><xref rid="aff2" ref-type="aff">2</xref></contrib><contrib contrib-type="author"><name><surname>Chen</surname><given-names>Yuyang</given-names></name><xref rid="aff1" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name><surname>Tang</surname><given-names>Shiqing</given-names></name><xref rid="aff1" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name><surname>Ding</surname><given-names>Pengchao</given-names></name><xref rid="aff1" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name><surname>Peng</surname><given-names>Haixia</given-names></name><xref rid="aff2" ref-type="aff">2</xref></contrib><contrib contrib-type="author"><name><surname>Chu</surname><given-names>Yimin</given-names></name><xref rid="aff2" ref-type="aff">2</xref></contrib><aff id="aff1"><label>1</label>School of Optical-Electrical and Computer Engineering, <institution>University of Shanghai for Science and Technology</institution>, Shanghai 200093, <country country="CN">China</country></aff><aff id="aff2"><label>2</label>Tongren Hospital, <institution>Shanghai Jiao Tong University School of Medicine</institution>, 1111 XianXia Road, Shanghai 200336, <country country="CN">China</country></aff></contrib-group><author-notes><corresp id="cor1">
<label>*</label>
<email>lifenggold@163.com</email>
</corresp></author-notes><pub-date pub-type="epub"><day>26</day><month>3</month><year>2024</year></pub-date><pub-date pub-type="collection"><day>01</day><month>4</month><year>2024</year></pub-date><volume>15</volume><issue>4</issue><fpage>2590</fpage><lpage>2621</lpage><history><date date-type="received"><day>31</day><month>10</month><year>2023</year></date><date date-type="rev-recd"><day>26</day><month>2</month><year>2024</year></date><date date-type="accepted"><day>08</day><month>3</month><year>2024</year></date></history><permissions><copyright-statement>&#x000a9; 2024 Optica Publishing Group</copyright-statement><copyright-year>2024</copyright-year><copyright-holder>Optica Publishing Group</copyright-holder><license><license-p>https://doi.org/10.1364/OA_License_v2#VOR-OA</license-p></license><license><license-p>&#x000a9; 2024 Optica Publishing Group under the terms of the <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1364/OA_License_v2#VOR-OA">Optica Open Access Publishing Agreement</ext-link></license-p></license></permissions><abstract><p>Automatic and precise polyp segmentation in colonoscopy images is highly valuable for diagnosis at an early stage and surgery of colorectal cancer. Nevertheless, it still posed a major challenge due to variations in the size and intricate morphological characteristics of polyps coupled with the indistinct demarcation between polyps and mucosas. To alleviate these challenges, we proposed an improved dual-aggregation polyp segmentation network, dubbed Dua-PSNet, for automatic and accurate full-size polyp prediction by combining both the transformer branch and a fully convolutional network (FCN) branch in a parallel style. Concretely, in the transformer branch, we adopted the B3 variant of pyramid vision transformer v2 (PVTv2-B3) as an image encoder for capturing multi-scale global features and modeling long-distant interdependencies between them whilst designing an innovative multi-stage feature aggregation decoder (MFAD) to highlight critical local feature details and effectively integrate them into global features. In the decoder, the adaptive feature aggregation (AFA) block was constructed for fusing high-level feature representations of different scales generated by the PVTv2-B3 encoder in a stepwise adaptive manner for refining global semantic information, while the ResidualBlock module was devised to mine detailed boundary cues disguised in low-level features. With the assistance of the selective global-to-local fusion head (SGLFH) module, the resulting boundary details were aggregated selectively with these global semantic features, strengthening these hierarchical features to cope with scale variations of polyps. The FCN branch embedded in the designed ResidualBlock module was used to encourage extraction of highly merged fine features to match the outputs of the Transformer branch into full-size segmentation maps. In this way, both branches were reciprocally influenced and complemented to enhance the discrimination capability of polyp features and enable a more accurate prediction of a full-size segmentation map. Extensive experiments on five challenging polyp segmentation benchmarks demonstrated that the proposed Dua-PSNet owned powerful learning and generalization ability and advanced the state-of-the-art segmentation performance among existing cutting-edge methods. These excellent results showed our Dua-PSNet had great potential to be a promising solution for practical polyp segmentation tasks in which wide variations of data typically occurred.</p></abstract><funding-group><award-group id="sp1"><funding-source country="CN">National Key Research and Development Program of China<named-content content-type="doi">10.13039/501100012166</named-content></funding-source><award-id>2021YFB2802303</award-id></award-group></funding-group></article-meta></front><body><sec sec-type="introduction" id="sec1"><label>1.</label><title>Introduction</title><p>Colorectal cancer (CRC) was the second most deadly cancer and third most common malignancy, which was estimated to account for approximately 9.4% of cancer-associated mortality worldwide and posed a serious threat to human health [<xref rid="r1" ref-type="bibr">1</xref>]. It often stemmed from small benign polyps progressing over time to gradually transform into malignant. Early diagnosis and excision of such diseased polyps could substantially decrease the occurrence and mortality of CRC and had become a worldwide public health priority [<xref rid="r2" ref-type="bibr">2</xref>]. In clinical practice, colonoscopy was frequently adopted means of examination and considered a gold standard for detecting colorectal lesions known as polyps or adenomas [<xref rid="r3" ref-type="bibr">3</xref>]. During the colonoscopy, physicians entailed visual inspection of the bowel by means of an endoscope to access location information and boundary details of polyps, whose accuracy heavily relied on the ability and tedious effort of physicians. Even skilled clinicians may also fail to reach an agreement on the segmentation for the same polyp image. Hence, a viable solution was to propose automated and accurate polyp segmentation strategies, which could provide physicians with great assistance for precisely locating and segmenting polyp areas to make further diagnosis.</p><p>Unfortunately, it still faced many challenges as a result of the following factors. First, high variations in polyp&#x02019;s appearance, e.g., color, size, shape, and complex morphological features of polyps, complicated the segmentation task, even if they were of the same type. Second, the fuzzy boundary contrasts between polyps and their surrounding mucosas made the polyps camouflaged against other endoluminal structures, while spot interference occurred when the light source of the lens was reflected on the tissue fluid of intestinal mucosa. Third, the inconsistency in scanning equipments and parameter standards leaded to a domain shift in polyp imaging across different medical institutions. These issues easily contributed to polyp segmentation negatively. Given these challenging factors, there was a growing demand for an automated and precise segmentation method that was ability of identifying nearly all potential polyps at their early stages from colonoscopy images, which could guide physicians to perform quick localization of polyp area and precisely delineate its boundary. To this end, this work intended to establish a polyp segmentation network which could represent polyp features better in a medical scenario for improving polyp segmentation performance and generalization capability with respect to domain shift.</p><p>In this context, with the backing of computer vision technologies, numerous automated polyp segmentation methods had been presented and obtained remarkable progress in the past several years. Among diverse polyp segmentation approaches, early learning-based approaches [<xref rid="r4" ref-type="bibr">4</xref>&#x02013;<xref rid="r7" ref-type="bibr">7</xref>] resorted to hand-crafted low-level features, including color, shape, texture, appearance, and some combination of these characteristics, etc. Yet, they usually yielded low-quality segmentation results and suffered from poor generalizability to complex scenarios, principally owing to the restricted representation ability of hand-crafted features when handling the high intra-class variations of polyps as well as low inter-class variations between polyps and hard mimics. Benefit from the rapid development of deep learning techniques in the field of medical image analysis, the automated polyp segmentation had progressively evolved from the early learning-based approaches to the deep learning methods, which could roughly fall into deep convolutional neural network (DCNN) based method [<xref rid="r8" ref-type="bibr">8</xref>&#x02013;<xref rid="r31" ref-type="bibr">31</xref>], Transformer based technique [<xref rid="r32" ref-type="bibr">32</xref>&#x02013;<xref rid="r37" ref-type="bibr">37</xref>], and hybrid architectures of Transformer and DCNN [<xref rid="r38" ref-type="bibr">38</xref>&#x02013;<xref rid="r44" ref-type="bibr">44</xref>]. In DCNN-based methods, a fully convolutional network (FCN) with a pre-trained VGG model was utilized for recognizing and segmenting polyp regions through pixel-level predictions, while a modified FCN combining the patch selection strategy was designed to increase the precision in polyp segmentation. Nevertheless, FCN approaches only depended on low-resolution characteristics to perform the final predictions, leading to coarse segmentation results and blurred edges. Currently, the advanced methods were largely based on U-shaped encoder-decoder CNN network, which had become prevalent in the segmentation of polyp images. For instance, the U-Net++ [<xref rid="r12" ref-type="bibr">12</xref>] extended the U-Net structure by inserting efficient and densely connected nested-decoder subnetworks while introduced a deep supervision mechanism to enhance the use of aggregative features across multiple semantic scales. Later, ResUNet++ [<xref rid="r13" ref-type="bibr">13</xref>] integrated residual computation, squeeze and excitation (SE), and attention mechanism to further boost segmentation ability. In recent studies, SFA [<xref rid="r19" ref-type="bibr">19</xref>] adopted a selective feature amalgamation structure along with a boundary-sensitive loss function to recovery the sharp boundary between a polyp and its surrounding mucosa. PraNet [<xref rid="r21" ref-type="bibr">21</xref>] merged high-level characteristic information for generating global feature maps and under their guidance, a parallel reverse attention block was utilized to progressively delve the boundary cues and build the correlations between regions and boundary clues. In spite of improving the segmentation accuracy, most of these methods directly used element-wise addition or concatenation operators to incorporate features at different levels gradually, resulting in diluting really useful features by excessive redundant information and weakening the complementary features among different levels. Moreover, the receptive field of the aforementioned methods [<xref rid="r8" ref-type="bibr">8</xref>&#x02013;<xref rid="r31" ref-type="bibr">31</xref>] were restricted and difficult to effectively summarize the global-level contextual information for handling size-varying polyps. Although some methods focused on capturing information from multiple scales and multiple receptive fields on the basis of the atrous spatial pyramid pooling (ASPP) block or DenseASPP, they would produce many extra parameters and computations, not to mention over-fitting. Different from DCNN-based methods, Transformer-based methods such as Polyp-PVT [<xref rid="r33" ref-type="bibr">33</xref>], SSFormer [<xref rid="r35" ref-type="bibr">35</xref>], and ESFPNet [<xref rid="r37" ref-type="bibr">37</xref>] etc., were able to perceive semantic distinctions from a global aspect via adaptively modeling long-distance relationships, and possessed stronger feature representation abilities by way of capturing global contextual information. Notwithstanding demonstrating impressive performance over DCNN-based alternatives, grabbing long-distance dependencies could destroy portion of task-specific critical local features and fail to model interactive features from the neighborhood, which could cause overly smooth segmentation for small/tiny polyps and vague boundaries between polyps. Apart from this, attention dispersion also occurred with the deepening of the Transformer model, whilst they often suffered from a dramatic performance decline on unseen out-of-distribution polyp data when domain shift issues existed, whose generalization capability remained lacked. The hybrid architectures of Transformer and DCNN including TransUNet [<xref rid="r38" ref-type="bibr">38</xref>], TransFuse [<xref rid="r39" ref-type="bibr">39</xref>], HS-Net [<xref rid="r41" ref-type="bibr">41</xref>], FCBFormer [<xref rid="r42" ref-type="bibr">42</xref>], TranSEFusionNet [<xref rid="r43" ref-type="bibr">43</xref>] and ECTransNet [<xref rid="r44" ref-type="bibr">44</xref>] etc., aimed to fuse the advantages of both models in a single architecture so as to strengthen underlying local features and restrict attention dispersion. However, feeding local feature information directly into the Transformer could not precisely deal with local contextual relationships, which could cause the local feature details to be overwhelmed by the dominant global context and generate inferior results in small/tiny polyp segmentation task. Another significant limitation of these hybrid architectures [<xref rid="r39" ref-type="bibr">39</xref>] fusing Transformer with DCNN was that the predicted segmentation maps were frequently lower in resolution compared with the input images, namely not full-size segmentation map. While the ECTransNet [<xref rid="r44" ref-type="bibr">44</xref>] had shown certain improvements in segmentation accuracy, there was still room for enhancement in building global contextual semantic relationships among pixels in the polyp feature maps.</p><p>To address the above-mentioned issues, inspired by encoder-decoder structures [<xref rid="r11" ref-type="bibr">11</xref>&#x02013;<xref rid="r14" ref-type="bibr">14</xref>], Transformers [<xref rid="r32" ref-type="bibr">32</xref>,<xref rid="r45" ref-type="bibr">45</xref>,<xref rid="r46" ref-type="bibr">46</xref>], residual learning [<xref rid="r47" ref-type="bibr">47</xref>] and the work by Zhang [<xref rid="r41" ref-type="bibr">41</xref>], we developed a novel improved dual-aggregation polyp segmentation network, termed as Dua-PSNet, to enhance feature representation ability in network fusion and global information modelling by combining both the Transformer branch and the FCN branch in parallel, thereby achieving automatic and accurate full-size polyp segmentation from colonoscopy images. Our solution adapted to both inductive bias and powerful representation of global context, catching local and long-range characteristics adequately on the polyp. Specifically, in the Transformer branch, we exploited the B3 variant of pyramid vision transformer v2 (PVTv2-B3) [<xref rid="r46" ref-type="bibr">46</xref>] as the hierarchical encoder for capturing multi-scale features and establish global correlations between them. Then, we constructed a novel multi-stage feature fusion decoder (MFAD) to put emphasis on important local feature cues and selectively extend them into global features. In the MFAD, we made an attempt to design an adaptive feature aggregation (AFA) block for aggregating adjacent high-level feature representations of different scales derived from PVTv2-B3 encoder in a stepwise adaptive fashion, generating critical global semantic features. After that, we developed the ResidualBlock module to strength local boundary details camouflaged in low-level features while devised the selective global-to-local fusion head (SGLFH) module to selectively merge them with the global semantic features for eliminating the semantic gap between low-level and high-level features and constraining attention dispersion, enhancing the efficiency of feature fusion across different levels. In the FCN branch, we exploited the designed ResidualBlock module instead of the original residual block (RB) module to refine highly merged multi-scale features at full-size and supplement output of Transformer branch into full-size prediction mask. These two branches influenced and complemented each other for refining the semantic information of the network to improve the recognition of boundary ambiguous features and boost the network&#x02019;s performance in dealing with the scale variations of polyps. Our major contributions were summarized as follows: <list list-type="simple" id="L1"><list-item><label>1.</label><p>We newly developed a dual-aggregation polyp segmentation network, named Dua-PSNet for achieving automatic and accurate segmentation of polyp areas at full-size in colonoscopy images, which combined the Transformer branch and the FCN branch in parallel. The fine features extracted by the FCN branch along with important features outputted by Transformer branch complemented reciprocally to enhance the distinctive ability for target information and weaken the background noises.</p></list-item><list-item><label>2.</label><p>We constructed adaptive feature aggregation (AFA) module to merge and optimize adjacent high-level semantic characteristics at different scales from the hierarchical PVTv2-B3 encoder in a progressive adaptive way, which stimulated the most important global semantic feature extraction for polyp segmentation.</p></list-item><list-item><label>3.</label><p>We added the ResidualBlock module into low-level feature information processing stage for improving the perception capability of local fine boundary cues, and then the selective global-to-local fusion head (SGLFH) module at the end of the Transformer branch for selectively injecting them into global semantic features and filling semantic gap between low-level and high-level characteristics.</p></list-item><list-item><label>4.</label><p>Extensive experiments on five challenging polyp segmentation benchmark datasets demonstrated that our Dua-PSNet outperformed other advanced polyp segmentation methods and manifested the new state-of-the-art performance. The cross-dataset generalizability analysis validated its stronger generalization ability than current cutting-edge polyp segmentation approaches. By conducting thorough ablation studies, we confirmed the validity of all critical components in the proposed Dua-PSNet.</p></list-item></list></p></sec><sec sec-type="other1" id="sec2"><label>2.</label><title>Related works</title><p>In this part, we presented an overview of recent efforts in relation to our work briefly from the following two perspectives: polyp segmentation, as well as multi-scale and multi-level amalgamation.</p><sec id="sec2-1"><label>2.1</label><title>Polyp segmentation</title><p>Nowadays, polyp segmentation had been progressed by leaps and bounds and numerous exceptional works had paid attention to this area. Generally, the current polyp segmentation approaches were mainly divided into four types, including traditional learning-based approaches [<xref rid="r4" ref-type="bibr">4</xref>&#x02013;<xref rid="r7" ref-type="bibr">7</xref>], DCNN-based methods [<xref rid="r8" ref-type="bibr">8</xref>&#x02013;<xref rid="r31" ref-type="bibr">31</xref>], Transformer-based techniques [<xref rid="r32" ref-type="bibr">32</xref>&#x02013;<xref rid="r37" ref-type="bibr">37</xref>], hybrid architecture combing Transformer and DCNN [<xref rid="r38" ref-type="bibr">38</xref>&#x02013;<xref rid="r44" ref-type="bibr">44</xref>].</p><sec id="sec2-1-1"><title>Traditional learning-based methods.</title><p>Early polyp segmentation solutions highly depended on hand-crafted low-level features, such as color, shape, texture, appearance, or some combination of these characteristics [<xref rid="r4" ref-type="bibr">4</xref>&#x02013;<xref rid="r7" ref-type="bibr">7</xref>]. Following the acquisition of manually designed characteristics, a classifier was often trained for detection or segmentation of polyp region from its surrounding tissue. For instance, Ameling et al. [<xref rid="r4" ref-type="bibr">4</xref>] extracted texture features with Grey-Level-Co-occurence and Local-Binary-Patterns for conducting polyp segmentation. Karkanis et al. [<xref rid="r5" ref-type="bibr">5</xref>] leveraged the covariances of the second-order textural measures computed over the wavelet frame decomposition of different color bands to extract color image features for representing different polyp areas, while performing classification using a step-wise linear discriminant analysis (LDA). Mamonov et al. [<xref rid="r6" ref-type="bibr">6</xref>] utilized the information about both the texture and the geometry to acquire a binary classification algorithm with pre-selection. Further, Tajbakhsh et al. [<xref rid="r7" ref-type="bibr">7</xref>] applied context and shape information to get rid of non-polyp structures and achieve reliably polyp location. Yet, these traditional learning-based methods often suffered from unsatisfactory segmentation results and lacked generalizability to real-world scenarios as a result of the poor representation capacity of hand-crafted characteristics.</p></sec><sec id="sec2-1-2"><title>DCNN-based methods.</title><p>As the continuous development of DCNN, it had gained considerable attention with excellent learning ability and greatly promoted the development in polyp segmentation field. The introduction of FCN [<xref rid="r8" ref-type="bibr">8</xref>] provided a true solution to the issue of polyp segmentation at the pixel level. For example, Akbari et al. [<xref rid="r9" ref-type="bibr">9</xref>] adopted an FCN fusing image patch selection strategy and Otsu thresholding for polyp segmentation, yielding better segmentation results against traditional learning-based solutions. Similarly, Brandao et al. [<xref rid="r10" ref-type="bibr">10</xref>] converted VGG and ResNets into FCNs, and fine-tuned them for achieving polyp segmentation, which incorporated depth information restored using shape-from-shading strategy to provide a richer feature representation. However, most of these FCN methods [<xref rid="r8" ref-type="bibr">8</xref>&#x02013;<xref rid="r10" ref-type="bibr">10</xref>] only resorted on low resolution feature information to carry out the final prediction of polyp regions, suffering from fuzzy boundaries and rough segmentation results. As a milestone work, U-shaped encoder-decoder structures instead of a single encoder in FCN had gradually come to be the mainstream network architecture with superior performance, e.g., U-Net [<xref rid="r11" ref-type="bibr">11</xref>], U-Net++ [<xref rid="r12" ref-type="bibr">12</xref>] and ResUNet++ [<xref rid="r13" ref-type="bibr">13</xref>]. Yeung et al. [<xref rid="r14" ref-type="bibr">14</xref>] introduced a dual attention-based deep neural network called Focus U-Net to promote selective learning on polyp characteristics by way of combining efficiently spatial and channel attentions into the sole focus gate block. Nevertheless, these skip connections operated with feature maps yielded by only a specific level of the encoder, which was not conducive to information transmission between the encoder and decoder and degraded the segmentation performance. To address this problem, Zhou et al. [<xref rid="r12" ref-type="bibr">12</xref>] reconstructed the dense skip connections to integrate feature maps between the encoder and the decoder in U-Net and established U-Net++, obtaining promising segmentation performance. Further, Tomar et al. [<xref rid="r15" ref-type="bibr">15</xref>] presented a dual decoder attention network built upon ResUNet++ to segment polyp regions. Yet, they mostly exploited a fixed size of kernel at each layer and only considered the feature maps from final decoder layer to conduct the reconstruction, which constrained the model&#x02019;s ability of exploring representational characteristics from different receptive fields. Based on the encoding-decoding network architecture, Sun et al. [<xref rid="r16" ref-type="bibr">16</xref>] introduced a dilated convolution for extracting and aggregating high-level semantic characteristics without resolution reduction, thereby improving the encoder network. Banik et al. [<xref rid="r17" ref-type="bibr">17</xref>] incorporated 2-D dual tree complex wavelet transform pooling with multiple skip connections to establish an enriched version of CNN named Polyp-Net for automatic polyp segmentation. Mahmud et al. [<xref rid="r18" ref-type="bibr">18</xref>] integrated depth dilated inception (DDI) module, deep fusion skip module (DFSM) and deep reconstruction module (DRM) into PolypSegNet for precise automated segmentation of polyp areas. Song et al. [<xref rid="r24" ref-type="bibr">24</xref>] proposed a parallel medical segmentation framework-AMNet to gain an improvement in polyp segmentation performance. Lin et al. [<xref rid="r25" ref-type="bibr">25</xref>] designed a bit-slicing context attention network (BSCA-Net) for improving performance in polyp segmentation by combining bit slice context attention (BSCA) module, split-squeeze-bottleneck-union (SSBU) block, multipath concatenation attention decoder (MCAD) as well as multi-path attention concatenation encoder (MACE). Shen et al. [<xref rid="r26" ref-type="bibr">26</xref>] presented a multi-scale coded colon polyp segmentation network, in which the attention mechanism integrating both spatial and channel dimensions was inserted into the encoding and decoding modules to enable the model to focus more on small polyp segmentation task. Lately, Li et al. [<xref rid="r27" ref-type="bibr">27</xref>] built the multiple feature association network (MFA-Net) with global attention mechanisms for improving the gains in segmentation performance of polyps. Further, more other U-shaped encoder-decoder based works related to polyp segmentation could be found in these literatures [<xref rid="r11" ref-type="bibr">11</xref>&#x02013;<xref rid="r24" ref-type="bibr">24</xref>]. Notwithstanding achieving remarkable segmentation effect, the vast majority of these methods [<xref rid="r11" ref-type="bibr">11</xref>&#x02013;<xref rid="r22" ref-type="bibr">22</xref>] lacked spatial details following pooling, and modeled long-range interdependencies insufficiently after performing multiple convolution operators. At the same time, most of these methods were prone to overfitting and suffering from weak generalization ability. Besides, there were also some methods [<xref rid="r19" ref-type="bibr">19</xref>&#x02013;<xref rid="r22" ref-type="bibr">22</xref>] built upon U-Net, which recovered edge cues for polyp segmentation through establishing the association between the edge and area characteristics. Murugesan et al. [<xref rid="r20" ref-type="bibr">20</xref>] designed Psi-Net containing three parallel decoders to segment polyps, one of which was used for prediction, whereas the other two decoders were responsible for detecting contour and estimating distance map for attaining polyp shape and boundary information. But the correlation between them was not fully explored. Fang et al. [<xref rid="r19" ref-type="bibr">19</xref>] put forward a selective feature aggregation network (SFA) consisted of a shared encoder together with two reciprocally restrained decoders, to speculate areas and boundaries of polyps. In contrast, Fan et al. [<xref rid="r21" ref-type="bibr">21</xref>] developed a parallel reverse attention network (PraNet) for segmenting polyps, which utilized a parallel partial decoder (PPD) to produce a global map to serve as the guidance region along with a reverse attention (RA) block to probe the contour details and build the relationships between areas and edges. More recently, Song et al. [<xref rid="r24" ref-type="bibr">24</xref>] used a parallel attention block together with a reverse fusion component to establish associations between areas and edges for refining the edge information and improving polyp segmentation accuracy. Zhou et al. [<xref rid="r28" ref-type="bibr">28</xref>] constructed a cross-level feature aggregation network (CFA-Net) incorporating a boundary prediction module using a layer-wise strategy, to enhance hierarchical features to refine segmentation maps on polyps. Li et al. [<xref rid="r31" ref-type="bibr">31</xref>] utilized channel and spatial fusion block in conjunction with spatial and channel attention mechanism, feature complementary module, and shape block to construct a multi-scale channel spatial fusion network (MCSF-Net) for real-time polyp segmentation, enhancing lesion boundary feature extraction and the fusion of multi-scale features together with exhibiting excellent segmentation and real-time performance. However, the boundary information derived from these methods [<xref rid="r19" ref-type="bibr">19</xref>&#x02013;<xref rid="r22" ref-type="bibr">22</xref>] was often ambiguous, resulting in sub-optimal performance in polyp segmentation task. In addition, these DCNN-based methods [<xref rid="r8" ref-type="bibr">8</xref>&#x02013;<xref rid="r31" ref-type="bibr">31</xref>] were good at capturing local neighboring feature details by means of a local receptive field, yet could be powerless to establish long-distance interdependencies effectively, with restricted feature representation capacity to tackle size-varying polyps.</p><p>Our network combined both a Transformer branch built upon U-shaped encoder-decoder architecture with a FCN branch in parallel to capture the long-range contexts and local low-level feature details from different viewpoints, which provided a comprehensive insight into polyp characteristics and brought a segmentation performance improvement.</p></sec><sec id="sec2-1-3"><title>Transformer-based techniques.</title><p>Different from DCNN, the Transformer consisting of self-attention gained similarities between all pairs of patches via calculating the dot product between their respective vectors so that features between all patches could be adaptively extracted and mixed, which enabled it to possess a strong capability of modeling long-term relations and reduce inductive bias. As the Transformer model emerged, Visual Transformer (ViT) [<xref rid="r32" ref-type="bibr">32</xref>] regarded each image as a sequence of patches (tokens) with a fixed size and subsequently forwarded them into multiple Transformer layers for seeking the association between each other. Due to only a single scale of output feature map with low resolution, it was challenging to directly adapt it to polyp segmentation task. Based this consideration, several methods [<xref rid="r33" ref-type="bibr">33</xref>&#x02013;<xref rid="r37" ref-type="bibr">37</xref>] incorporated the pyramid structure in CNN into the design of Transformers, presenting a hierarchical Transformer with different stages. Dong et al. [<xref rid="r33" ref-type="bibr">33</xref>] utilized a pyramid vision Transformer (PVT) as backbone encoder and presented a polyp segmentation architecture called Polyp-PVT. Tang et al. [<xref rid="r34" ref-type="bibr">34</xref>] proposed a Dual-Aggregation Transformer Network (DuAT) to segment polyp regions, which adapted the PVT as the encoder for capturing richer feature cues. Wang et al. [<xref rid="r35" ref-type="bibr">35</xref>] proposed SSFormer comprising a PVT encoder and progressive locality decoder (PLD) to enhance robust and generalization ability for polyp segmentation. Nachmani et al. [<xref rid="r36" ref-type="bibr">36</xref>] proposed ResPVT framework to segment polyp, which used Transformer as an encoder to capture more feature representations about polyps. Chang et al. [<xref rid="r37" ref-type="bibr">37</xref>] established the ESFPNet architecture for segmenting polyp lesions, which exploited the Mix Transformer (MiT) as the backbone encoder and an efficient stage-wise feature pyramid (ESFP) in the decoder to enhance the usage of high-level semantic information. Despite manifesting notably impressive results, the Transformer network struggled to establish interactive characteristics from the neighborhoods adequately and represent fine-grained feature details accurately, as a result of the low spatial inductive bias and strong global receptive field. In addition, as the Transformer model was deepened, the global feature information was persistently mingled and accumulated, susceptible to attention diffusion. In our network design, we upgraded multi-stage feature aggregation decoder (MFAD) to adapt to the PVTv2-B3 backbone encoder, forming the Transformer branch, together with the FCN to underline local contextual relationship and constrain attention divergence.</p></sec><sec id="sec2-1-4"><title>Hybrid architectures combing Transformer and DCNN.</title><p>Aiming at exploiting strengths of both designs, substantial works sought to improve model&#x02019;s ability of global contextual relations while maintaining a strong extraction of local detailed features by integrating Transformer and DCNN. For example, TransUNet [<xref rid="r38" ref-type="bibr">38</xref>] was developed by leveraging ResNet50 network and ViT as the encoder of U-Net for feature extraction and global interactions. But, it could result in a complex and bloated architecture which was inclined to overfitting. Zhang et al. [<xref rid="r39" ref-type="bibr">39</xref>] constructed TransFuse network to combine the Transformer and CNN parallelly so that global dependencies along with local spatial detailed information could be efficiently collected. Cai et al. [<xref rid="r40" ref-type="bibr">40</xref>] combined a shallow CNN encoder with a deep Transformer encoder for extracting richer feature details, and erected the PP-guided self-attention in the decoder so as to guide self-attention with the help of prediction maps, enhancing the model&#x02019;s sensitivity to polyp boundaries. Zhang et al. [<xref rid="r41" ref-type="bibr">41</xref>] investigated a hybrid semantic network (HSNet) that incorporated both the Transformer and CNN with a dual-branch framework to improve polyp segmentation, which introduced a hybrid semantic complementary module to capture long-term relationships and local appearance cues. Sanderson et al. [<xref rid="r42" ref-type="bibr">42</xref>] combined Transformer and FCN into a single structure to form FCN-Transformer hybrid network (FCBFormer) for achieving polyp segmentation. Zhang et al. [<xref rid="r43" ref-type="bibr">43</xref>] took full advantage of the combination of the CNN and Transformer on top of the U-Net structure to build a hybrid CNN-Transformer architecture called TranSEFusionNet, improving accuracy in polyp lesion segmentation. Li et al. [<xref rid="r44" ref-type="bibr">44</xref>] presented a novel polyp image segmentation network called ECTransNet by incorporating the local feature extraction ability of DCNN with the global contextual semantic correlation construction capability of Transformer, which achieved promising segmentation accuracy and generalization performance. Yet, there was still a need for further improvement in capturing long-range dependencies among pixels. In these methods [<xref rid="r39" ref-type="bibr">39</xref>,<xref rid="r41" ref-type="bibr">41</xref>], it could be imprecise to deal with local contextual relations by feeding local features directly into the Transformer, which made the local information be deluged by the dominant global context. Concurrently, they only combined the structures in a simple manner and overlooked the interaction between the two semantic representations, whereas down-sampling operations of these methods could cause the loss of local information, resulting in blurred boundary. What&#x02019;s more, most of these approaches [<xref rid="r39" ref-type="bibr">39</xref>] were not able to perform full-size segmentation map prediction. Inspired by this [<xref rid="r40" ref-type="bibr">40</xref>,<xref rid="r42" ref-type="bibr">42</xref>], we introduced Transformer and FCN in a parallel way to generate dual-aggregation polyp segmentation network (Dua-PSNet) for full-size polyp prediction. In our Dua-PSNet, the Transformer branch was developed to learn critical global semantic information, whereas the FCN branch was used for emphasizing fine local boundary details. These two branches influenced and complemented each other, enhancing the distinguished capability for polyps and achieving full-size segmentation.</p></sec></sec><sec id="sec2-2"><label>2.2</label><title>Multi-scale and multi-level amalgamation</title><p>Multi-scale feature representation offered a viable mean for tackling variations in scales with respect to segmentation task, whilst multi-level fusion provided multiple granularities for semantic segmentation. For instance, TransFuse [<xref rid="r39" ref-type="bibr">39</xref>] created a BiFusion module to incorporate multiple levels of features from both Transformer and CNN branches. SFA [<xref rid="r19" ref-type="bibr">19</xref>] embedded selective kernel module (SKM) into convolutional layers for dynamically extracting multi-scale and multi-receptive-field features from kernels of different sizes, and added up-concatenations between encoder and decoder in U-Net structure to aggregate these features. UACANet [<xref rid="r23" ref-type="bibr">23</xref>] conducted the atrous convolution with regard to high-level features, which could enlarge the perceptual field and degrade the loss of global spatial feature information to a certain degree, yet the boundary space information of subtle polyps could be lost owing to null convolution. PraNet [<xref rid="r21" ref-type="bibr">21</xref>] aggregated multiple high-level features extracted from Res2Net-based backbone network via a PPD component. MSNet [<xref rid="r29" ref-type="bibr">29</xref>] used the subtractive units for yielding different characteristics between contiguous levels of the network, which provided different perceptual fields concerning different subtractive units in a feature pyramid way to acquire abundant feature differences of multiple scales. M<sup>2</sup>SNet [<xref rid="r30" ref-type="bibr">30</xref>] equipped with both the inter-layer and intra-layer subtraction structures to collect multi-scale complementary information among different levels, and effectively aggregated specific level features and multi-path cross-level distinctive features for generating the final prediction, thereby amplifying the perceptions of polyp regions. Yet, it was prone to lose the boundary details of small/tiny polyps and affected segmentation accuracy. AMNet [<xref rid="r24" ref-type="bibr">24</xref>] built a multi-scale interactive fusion network for obtaining richer local and global feature details on polyps via merging high-level and low-level characteristics of different scales working together to supplement the position and spatial information, and introduced the parallel network of attention mechanisms for augmenting the model&#x02019;s segmentation capability. MFA-Net [<xref rid="r27" ref-type="bibr">27</xref>] integrated a parallelly dilated convolutions arrangement (PDCA) block between the encoder and the decoder to excavate critical feature representations and introduced a multi-scale feature restructuring module (MFRM) to reorganize and merge semantic information at different scales from the encoder, while cascaded global attention stacking (GAS) block in the decoder to enhance global attention perception and guide shallower features through the use of deeper features. Polyp-Net [<xref rid="r17" ref-type="bibr">17</xref>] amalgamated 2-D dual-tree complex wavelet transform pooling with local gradient weighting-embedded level-set method to achieve multi-model pixel-level fusion, suppressing high-intensity false area and ensuring smoothness of polyp contour. PolypSegNet [<xref rid="r18" ref-type="bibr">18</xref>] utilized deep fusion skip module (DFSM) for fusing different scales of features produced by different levels of the encoder, and created deep reconstruction module (DRM) to restore and optimize decoded feature maps of multiple scales from different levels of decoders, enabling skip inter-connections establishment between encoder and decoder as well as the semantic gap between them reduction. Segformer [<xref rid="r48" ref-type="bibr">48</xref>] predicted features of different scales and depths individually by simple up-sampling, and then parallelly fuse them using a multi-stage feature aggregation algorithm. DuAT [<xref rid="r34" ref-type="bibr">34</xref>] used global-to-local spatial aggregation (GLSA) module to simultaneously mine local spatial details and global spatial semantic information, and constructed selective boundary aggregation (SBA) module for fine-tuning polyp boundaries. SSFormer [<xref rid="r35" ref-type="bibr">35</xref>] used a pyramid Transformer encoder for multi-scale feature extraction and progressive locality decoder for multi-stage feature fusion, enabling characteristics of different depths and representation capabilities to guide reciprocally. The outstanding performance illustrated that the decoder approach of multi-stage feature amalgamation was helpful to boosting the performance of the Transformer in the task of dense prediction. Lately, MCSF-Net [<xref rid="r31" ref-type="bibr">31</xref>] introduced the channel and spatial fusion module to amalgamate high-level multi-scale features through using channel and spatial attention mechanisms while developed a feature complementary module to fuse low-level multi-scale feature maps, which not only effectively captured the spatial positional information of polyps but also accurately preserved lesion boundaries. ResPVT [<xref rid="r36" ref-type="bibr">36</xref>] implemented PVT as backbone encoder to extract multi-stage feature information and adopted fusion module (FM) to meld the high-level features. ECTransNet [<xref rid="r44" ref-type="bibr">44</xref>] embedded an edge complementary module to fuse and complement polyp feature maps at multiple resolutions for effectively mining crucial edge information of polyps, and employed a residual-based feature aggregation decoder to adaptively merge high-level and low-level features, thereby better retaining spatial position information and reinforcing the model&#x02019;s ability to segment local details. On a similar note as [<xref rid="r31" ref-type="bibr">31</xref>,<xref rid="r36" ref-type="bibr">36</xref>,<xref rid="r44" ref-type="bibr">44</xref>], our Transformer branch employed a stepwise adaptive method to generate global semantic information and focus on local edge details, and then selectively aggregated them, improving the feature information processing capability of different levels and achieving more effective feature fusion. Besides, more studies on multi-scale and multi-level amalgamation could refer to these literatures [<xref rid="r8" ref-type="bibr">8</xref>&#x02013;<xref rid="r16" ref-type="bibr">16</xref>,<xref rid="r20" ref-type="bibr">20</xref>,<xref rid="r22" ref-type="bibr">22</xref>,<xref rid="r25" ref-type="bibr">25</xref>,<xref rid="r26" ref-type="bibr">26</xref>,<xref rid="r28" ref-type="bibr">28</xref>]. The vast majority of these methods combined different level features by utilization of simple element-wise addition or concatenation operations, which tended to create substantially excessive information to attenuate the really useful characteristics and weaken the complementary feature cues between different levels.</p><p>In our network design, we utilized the PVTv2-B3 as backbone encoder for capturing different levels of features from input images and explore the correlations between them. In the decoder, we constructed the AFA module to aggregate resultant high-level characteristics from the encoder to form global semantic features in a stepwise adaptive manner, while designed the ResidualBlock module to enhance extraction capability of local boundary details from low-level features. Further, we proposed the SGLFH module to selectively inject local boundary information into global semantic features, which could enable characteristics of different representation abilities to guide mutually and the fusion more efficient.</p></sec></sec><sec sec-type="methods" id="sec3"><label>3.</label><title>Methodology</title><p>In this part, we initially provided an overview of the presented Dua-PSNet for full-size polyp segmentation. Then, we presented two key branches, including the Transformer branch and FCN branch. In the Transformer branch, we introduced PVTv2-B3 encoder and multi-stage feature aggregation decoder (MFAD), respectively, while elaborately depicted the AFA, ResidualBlock and SGLFH components in the MFAD. At last, we developed the entire loss function.</p><sec id="sec3-1"><label>3.1</label><title>Overall architecture</title><p>The whole architecture of the developed Dua-PSNet for polyp segmentation was illustrated in <xref rid="g001" ref-type="fig">Fig.&#x000a0;1(a)</xref>. The model involved two key parallel branches starting from a <inline-formula>
<mml:math id="m1" display="inline" overflow="scroll"><mml:mi>H</mml:mi><mml:mo>&#x000d7;<!-- &#x000d7; --></mml:mo><mml:mi>W</mml:mi></mml:math>
</inline-formula> input images, one of which was the Transformer branch and returned reduced-size <inline-formula>
<mml:math id="m2" display="inline" overflow="scroll"><mml:mfrac><mml:mi>H</mml:mi><mml:mn>4</mml:mn></mml:mfrac><mml:mo>&#x000d7;<!-- &#x000d7; --></mml:mo><mml:mfrac><mml:mi>W</mml:mi><mml:mn>4</mml:mn></mml:mfrac></mml:math>
</inline-formula> feature maps, the other one of which was an FCN branch and returned full-size <inline-formula>
<mml:math id="m3" display="inline" overflow="scroll"><mml:mi>H</mml:mi><mml:mo>&#x000d7;<!-- &#x000d7; --></mml:mo><mml:mi>W</mml:mi></mml:math>
</inline-formula> feature maps. Specifically, a colonoscopy image with a resolution of <inline-formula>
<mml:math id="m4" display="inline" overflow="scroll"><mml:mi>h</mml:mi><mml:mo>&#x000d7;<!-- &#x000d7; --></mml:mo><mml:mi>&#x003c9;<!-- &#x003c9; --></mml:mi></mml:math>
</inline-formula> was fed into Transformer and FCN branches simultaneously. In the Transformer branch, we leveraged PVTv2-B3 as an image encoder to derive 4 pyramid features <inline-formula>
<mml:math id="m5" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>&#x02208;<!-- &#x02208; --></mml:mo><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mfrac><mml:mi>H</mml:mi><mml:mrow><mml:mrow><mml:msup><mml:mn>2</mml:mn><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:mfrac><mml:mo>&#x000d7;<!-- &#x000d7; --></mml:mo><mml:mfrac><mml:mi>W</mml:mi><mml:mrow><mml:mrow><mml:msup><mml:mn>2</mml:mn><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:mfrac><mml:mo>&#x000d7;<!-- &#x000d7; --></mml:mo><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:math>
</inline-formula> and model long-range dependencies between different features, where <inline-formula>
<mml:math id="m6" display="inline" overflow="scroll"><mml:mi>i</mml:mi><mml:mo>&#x02208;<!-- &#x02208; --></mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mn>4</mml:mn></mml:mrow><mml:mo fence="false" stretchy="false">}</mml:mo></mml:math>
</inline-formula> and <inline-formula>
<mml:math id="m7" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>&#x02208;<!-- &#x02208; --></mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mrow><mml:mrow><mml:mn>64</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mn>128</mml:mn><mml:mo>,</mml:mo><mml:mrow><mml:mn>320</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mn>512</mml:mn></mml:mrow><mml:mo fence="false" stretchy="false">}</mml:mo></mml:math>
</inline-formula>. For accurate polyp segmentation, we constructed the MFAD to conduct stepwise adaptive aggregation on high-level features from PVTv2-B3 encoder for extracting the most important global semantic features as well as focus more on local boundary cues from low-level features. In the MFAD, we used convolution units to reduce the number of channels from the last 3 feature maps <inline-formula>
<mml:math id="m8" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math>
</inline-formula>, <inline-formula>
<mml:math id="m9" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:math>
</inline-formula> and <inline-formula>
<mml:math id="m10" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mn>4</mml:mn></mml:msub></mml:mrow></mml:math>
</inline-formula> to 64, while fed them into the developed AFA module to implement high-level feature fusion in a progressive adaptive way generating global feature map <inline-formula>
<mml:math id="m11" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>O</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>&#x02208;<!-- &#x02208; --></mml:mo><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mfrac><mml:mi>H</mml:mi><mml:mn>4</mml:mn></mml:mfrac><mml:mo>&#x000d7;<!-- &#x000d7; --></mml:mo><mml:mfrac><mml:mi>W</mml:mi><mml:mn>4</mml:mn></mml:mfrac><mml:mo>&#x000d7;<!-- &#x000d7; --></mml:mo><mml:mn>64</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math>
</inline-formula>. Meanwhile, was forwarded into the designed ResidualBlock module producing <inline-formula>
<mml:math id="m12" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>O</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>&#x02208;<!-- &#x02208; --></mml:mo><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mfrac><mml:mi>H</mml:mi><mml:mn>4</mml:mn></mml:mfrac><mml:mo>&#x000d7;<!-- &#x000d7; --></mml:mo><mml:mfrac><mml:mi>W</mml:mi><mml:mn>4</mml:mn></mml:mfrac><mml:mo>&#x000d7;<!-- &#x000d7; --></mml:mo><mml:mn>64</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math>
</inline-formula>, which enhanced local edge perception ability from low-level features. Afterwards, we presented the SGLFH module to selectively aggregate feature maps <inline-formula>
<mml:math id="m13" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>O</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math>
</inline-formula> and <inline-formula>
<mml:math id="m14" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>O</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math>
</inline-formula>, resulting in the feature map <inline-formula>
<mml:math id="m15" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>T</mml:mi></mml:msub></mml:mrow><mml:mo>&#x02208;<!-- &#x02208; --></mml:mo><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mfrac><mml:mi>H</mml:mi><mml:mn>4</mml:mn></mml:mfrac><mml:mo>&#x000d7;<!-- &#x000d7; --></mml:mo><mml:mfrac><mml:mi>W</mml:mi><mml:mn>4</mml:mn></mml:mfrac><mml:mo>&#x000d7;<!-- &#x000d7; --></mml:mo><mml:mn>64</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math>
</inline-formula>, which could encourage semantic information of different expression powers to guide mutually and bridge the semantic gap between high-level and low-level features. The resulting feature map <inline-formula>
<mml:math id="m16" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>T</mml:mi></mml:msub></mml:mrow><mml:mo>&#x02208;<!-- &#x02208; --></mml:mo><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mfrac><mml:mi>H</mml:mi><mml:mn>4</mml:mn></mml:mfrac><mml:mo>&#x000d7;<!-- &#x000d7; --></mml:mo><mml:mfrac><mml:mi>W</mml:mi><mml:mn>4</mml:mn></mml:mfrac><mml:mo>&#x000d7;<!-- &#x000d7; --></mml:mo><mml:mn>64</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math>
</inline-formula> was up-sampled into full-size <inline-formula>
<mml:math id="m17" display="inline" overflow="scroll"><mml:msubsup><mml:mi>F</mml:mi><mml:mi>T</mml:mi><mml:mrow><mml:mrow><mml:mi class="MJX-variant" mathvariant="normal">&#x02032;<!-- &#x02032; --></mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:mo>&#x02208;<!-- &#x02208; --></mml:mo><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mo>&#x000d7;<!-- &#x000d7; --></mml:mo><mml:mi>W</mml:mi><mml:mo>&#x000d7;<!-- &#x000d7; --></mml:mo><mml:mn>64</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math>
</inline-formula> and concatenated with the feature map <inline-formula>
<mml:math id="m18" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow><mml:mo>&#x02208;<!-- &#x02208; --></mml:mo><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mo>&#x000d7;<!-- &#x000d7; --></mml:mo><mml:mi>W</mml:mi><mml:mo>&#x000d7;<!-- &#x000d7; --></mml:mo><mml:mrow><mml:mn>32</mml:mn></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:math>
</inline-formula> generated by the FCN branch along the channel dimension. Ultimately, the concatenated feature map was tackled into the final full-size polyp segmentation map through the prediction head module.</p><fig position="float" id="g001" fig-type="figure"><label>Fig. 1.</label><caption><p>Overall architecture of the developed Dua-PSNet for polyp segmentation (a), including two critical parts: Transformer branch (b) and FCN branch (c). AFA: adaptive feature aggregation module; SGLFH: selective global-to-local fusion head module.</p></caption><graphic xlink:href="boe-15-4-2590-g001" position="float"/></fig></sec><sec id="sec3-2"><label>3.2</label><title>Transformer branch</title><p>Our implementation of the Transformer branch was shown in <xref rid="g002" ref-type="fig">Fig.&#x000a0;2</xref>, which used a pre-trained PVTv2-B3 on ImageNet [<xref rid="r49" ref-type="bibr">49</xref>] as an image encoder and designed the AFA, ResidualBlock and SGLFH modules to constitute a MFAD. The PVTv2-B3 encoder produced a 4-level feature pyramid, and then served as the input to the MFAD. In the MFAD, three high levels of feature pyramids were coped with by the AFA module in a stepwise adaptive manner to learn important global semantic context, whereas the remain one low-level pyramid was processed by the ResidualBlock module which improved the ability in representing local fine edge characteristics. In the following, the resulting local features were selectively injected into global semantic features through the SGLFH module, thereby filling the semantic gap between different level features and restricting attention dispersion.</p><fig position="float" id="g002" fig-type="figure"><label>Fig. 2.</label><caption><p>The architecture of the Transformer branch, including the PVTv2-B3 encoder and multi-stage feature aggregation decoder (MFAD). AFA: adaptive feature aggregation module; SGLFH: selective global-to-local fusion head module.</p></caption><graphic xlink:href="boe-15-4-2590-g002" position="float"/></fig><sec id="sec3-2-1"><label>3.2.1</label><title>Transformer encoder</title><p>To ensure our model enough generalization and powerful multi-scale feature extraction capability in polyp segmentation task, we utilized the PVTv2-B3 (See <xref rid="g002" ref-type="fig">Fig.&#x000a0;2(a)</xref>) built upon a progressive shrinking pyramid structure as the image encoder. In contrast with traditional Transformer, the PVTv2-B3 used a linear spatial reduction attention (SRA) layer for reducing the computational burden, and an overlapping patch embedding via strided convolution to obtain more consistency of spatial information, while introduced zero-padding position encoding into the PVT in convolutional feed-forward network to more easily handle inputs with varying resolutions. In specific, the entire PVTv2-B3 encoder was split into 4 stages, each of which embodied both an overlapping patch embedding layer and a linear-layer Transformer encoder. The feature pyramid with four stages was generated from a rough level (4-stride) to a fine level (32-stride) in a progressive way. In the first stage, given an input image with the size of <inline-formula>
<mml:math id="m19" display="inline" overflow="scroll"><mml:mi>H</mml:mi><mml:mo>&#x000d7;<!-- &#x000d7; --></mml:mo><mml:mi>W</mml:mi><mml:mo>&#x000d7;<!-- &#x000d7; --></mml:mo><mml:mn>3</mml:mn></mml:math>
</inline-formula>, the image was firstly separated into <inline-formula>
<mml:math id="m20" display="inline" overflow="scroll"><mml:mfrac><mml:mrow><mml:mi>H</mml:mi><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:msup><mml:mn>4</mml:mn><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:mfrac></mml:math>
</inline-formula> patches, and each of size was <inline-formula>
<mml:math id="m21" display="inline" overflow="scroll"><mml:mn>4</mml:mn><mml:mo>&#x000d7;<!-- &#x000d7; --></mml:mo><mml:mn>4</mml:mn><mml:mo>&#x000d7;<!-- &#x000d7; --></mml:mo><mml:mn>3</mml:mn></mml:math>
</inline-formula>. Then, the flattened patches were provided into a linear projection to acquire embedded patches of size <inline-formula>
<mml:math id="m22" display="inline" overflow="scroll"><mml:mfrac><mml:mrow><mml:mi>H</mml:mi><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:msup><mml:mn>4</mml:mn><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:mfrac><mml:mo>&#x000d7;<!-- &#x000d7; --></mml:mo><mml:mn>64</mml:mn></mml:math>
</inline-formula>. Subsequently, these embedded patches together with corresponding position embedding were fed into a linear-layer Transformer encoder with <inline-formula>
<mml:math id="m23" display="inline" overflow="scroll"><mml:mi>L</mml:mi><mml:mn>1</mml:mn></mml:math>
</inline-formula> layers, and the output was reshaped to a feature map <inline-formula>
<mml:math id="m24" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>&#x02208;<!-- &#x02208; --></mml:mo><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mfrac><mml:mi>H</mml:mi><mml:mn>4</mml:mn></mml:mfrac><mml:mo>&#x000d7;<!-- &#x000d7; --></mml:mo><mml:mfrac><mml:mi>W</mml:mi><mml:mn>4</mml:mn></mml:mfrac><mml:mo>&#x000d7;<!-- &#x000d7; --></mml:mo><mml:mn>64</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math>
</inline-formula>. In the same way, taking the feature map resulting from the preceding stage as the input, we could derive the following three feature maps of different scales <inline-formula>
<mml:math id="m25" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>&#x02208;<!-- &#x02208; --></mml:mo><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mfrac><mml:mi>H</mml:mi><mml:mrow><mml:mrow><mml:msup><mml:mn>2</mml:mn><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:mfrac><mml:mo>&#x000d7;<!-- &#x000d7; --></mml:mo><mml:mfrac><mml:mi>W</mml:mi><mml:mrow><mml:mrow><mml:msup><mml:mn>2</mml:mn><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:mfrac><mml:mo>&#x000d7;<!-- &#x000d7; --></mml:mo><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:math>
</inline-formula>, where <inline-formula>
<mml:math id="m26" display="inline" overflow="scroll"><mml:mi>i</mml:mi><mml:mo>&#x02208;<!-- &#x02208; --></mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mn>4</mml:mn></mml:mrow><mml:mo fence="false" stretchy="false">}</mml:mo></mml:math>
</inline-formula> and <inline-formula>
<mml:math id="m27" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>&#x02208;<!-- &#x02208; --></mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mn>128</mml:mn><mml:mo>,</mml:mo><mml:mrow><mml:mn>320</mml:mn><mml:mo>,</mml:mo><mml:mn>512</mml:mn><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mrow></mml:math>
</inline-formula>. Among these feature maps, <inline-formula>
<mml:math id="m28" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math>
</inline-formula>, <inline-formula>
<mml:math id="m29" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:math>
</inline-formula> and <inline-formula>
<mml:math id="m30" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mn>4</mml:mn></mml:msub></mml:mrow></mml:math>
</inline-formula> provided high-level semantic information related with polyps, whilst <inline-formula>
<mml:math id="m31" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math>
</inline-formula> contained low-level local details.</p></sec><sec id="sec3-2-2"><label>3.2.2</label><title>Multi-stage feature aggregation decoder (MFAD)</title><p>Considering the most existing Transformer models for polyp segmentation lacked local low-level detailed feature processing capability, we proposed a novel MFAD for feature pyramids to stepwise aggregate multiple stages of features. The MFAD mainly consisted of the AFA, ResidualBlock and SGLFH modules, which could be depicted in <xref rid="g002" ref-type="fig">Fig.&#x000a0;2(b)</xref>.</p><sec id="sec3-2-2-1"><label>3.2.2.1</label><title>Adaptive feature aggregation (AFA) module</title><p>Taking inspiration from the work [<xref rid="r29" ref-type="bibr">29</xref>,<xref rid="r30" ref-type="bibr">30</xref>,<xref rid="r41" ref-type="bibr">41</xref>], we created the AFA module to gather more varied and distinctive high-level semantic information by stepwise adaptively aggregating three high-level feature maps from Transformer encoder, as shown in <xref rid="g003" ref-type="fig">Fig.&#x000a0;3</xref>. Specifically, we firstly reduced the number of channels to 32 for <inline-formula>
<mml:math id="m32" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math>
</inline-formula>, <inline-formula>
<mml:math id="m33" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:math>
</inline-formula> and <inline-formula>
<mml:math id="m34" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mn>4</mml:mn></mml:msub></mml:mrow></mml:math>
</inline-formula> by use of local emphasis (LE) module comprising <inline-formula>
<mml:math id="m35" display="inline" overflow="scroll"><mml:mn>3</mml:mn><mml:mo>&#x000d7;<!-- &#x000d7; --></mml:mo><mml:mn>3</mml:mn></mml:math>
</inline-formula> convolution operators with padding of 1, group normalization (GN) and SiLU activation functions in sequence, resulting in <inline-formula>
<mml:math id="m36" display="inline" overflow="scroll"><mml:msubsup><mml:mi>F</mml:mi><mml:mn>2</mml:mn><mml:mrow><mml:mrow><mml:mi class="MJX-variant" mathvariant="normal">&#x02032;<!-- &#x02032; --></mml:mi></mml:mrow></mml:mrow></mml:msubsup></mml:math>
</inline-formula>, <inline-formula>
<mml:math id="m37" display="inline" overflow="scroll"><mml:msubsup><mml:mi>F</mml:mi><mml:mn>3</mml:mn><mml:mrow><mml:mrow><mml:mi class="MJX-variant" mathvariant="normal">&#x02032;<!-- &#x02032; --></mml:mi></mml:mrow></mml:mrow></mml:msubsup></mml:math>
</inline-formula> and <inline-formula>
<mml:math id="m38" display="inline" overflow="scroll"><mml:msubsup><mml:mi>F</mml:mi><mml:mn>4</mml:mn><mml:mrow><mml:mrow><mml:mi class="MJX-variant" mathvariant="normal">&#x02032;<!-- &#x02032; --></mml:mi></mml:mrow></mml:mrow></mml:msubsup></mml:math>
</inline-formula>, respectively. At the same time, our model could also refocus attention on neighboring features for decreasing attention dispersion by utilization of the local receptive field within the LE module, thus amplifying the weights of the neighboring patches associated with the center patch and highlighting critical local characteristics of each patch. Then, we fused the feature maps of <inline-formula>
<mml:math id="m39" display="inline" overflow="scroll"><mml:msubsup><mml:mi>F</mml:mi><mml:mn>3</mml:mn><mml:mrow><mml:mrow><mml:mi class="MJX-variant" mathvariant="normal">&#x02032;<!-- &#x02032; --></mml:mi></mml:mrow></mml:mrow></mml:msubsup></mml:math>
</inline-formula> and <inline-formula>
<mml:math id="m40" display="inline" overflow="scroll"><mml:msubsup><mml:mi>F</mml:mi><mml:mn>4</mml:mn><mml:mrow><mml:mrow><mml:mi class="MJX-variant" mathvariant="normal">&#x02032;<!-- &#x02032; --></mml:mi></mml:mrow></mml:mrow></mml:msubsup></mml:math>
</inline-formula>, as follows: we up-sampled <inline-formula>
<mml:math id="m41" display="inline" overflow="scroll"><mml:msubsup><mml:mi>F</mml:mi><mml:mn>4</mml:mn><mml:mrow><mml:mrow><mml:mi class="MJX-variant" mathvariant="normal">&#x02032;<!-- &#x02032; --></mml:mi></mml:mrow></mml:mrow></mml:msubsup></mml:math>
</inline-formula> by 2 and forwarded the resulting feature map through the LE module, generating <inline-formula>
<mml:math id="m42" display="inline" overflow="scroll"><mml:msubsup><mml:mi>F</mml:mi><mml:mn>4</mml:mn><mml:mrow><mml:mrow><mml:mrow><mml:mi class="MJX-variant" mathvariant="normal">&#x02032;<!-- &#x02032; --></mml:mi><mml:mi class="MJX-variant" mathvariant="normal">&#x02032;<!-- &#x02032; --></mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msubsup></mml:math>
</inline-formula>. Meanwhile, <inline-formula>
<mml:math id="m43" display="inline" overflow="scroll"><mml:msubsup><mml:mi>F</mml:mi><mml:mn>4</mml:mn><mml:mrow><mml:mrow><mml:mrow><mml:mi class="MJX-variant" mathvariant="normal">&#x02032;<!-- &#x02032; --></mml:mi><mml:mi class="MJX-variant" mathvariant="normal">&#x02032;<!-- &#x02032; --></mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msubsup></mml:math>
</inline-formula> was multiplied by <inline-formula>
<mml:math id="m44" display="inline" overflow="scroll"><mml:msubsup><mml:mi>F</mml:mi><mml:mn>3</mml:mn><mml:mrow><mml:mrow><mml:mi class="MJX-variant" mathvariant="normal">&#x02032;<!-- &#x02032; --></mml:mi></mml:mrow></mml:mrow></mml:msubsup></mml:math>
</inline-formula>, and then the resultant feature map was further concatenated with <inline-formula>
<mml:math id="m45" display="inline" overflow="scroll"><mml:msubsup><mml:mi>F</mml:mi><mml:mn>4</mml:mn><mml:mrow><mml:mrow><mml:mrow><mml:mi class="MJX-variant" mathvariant="normal">&#x02032;<!-- &#x02032; --></mml:mi><mml:mi class="MJX-variant" mathvariant="normal">&#x02032;<!-- &#x02032; --></mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msubsup></mml:math>
</inline-formula>. After that, we processed the concatenated feature via the LE module and then performed double up-sampled operation, obtaining the aggregated feature map <inline-formula>
<mml:math id="m46" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math>
</inline-formula>. The above processing steps could be represented by the follows: <disp-formula id="e1">
<label>(1)</label>
<mml:math id="m47" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mi>U</mml:mi><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>L</mml:mi><mml:mi>E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>L</mml:mi><mml:mi>E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x02297;<!-- &#x02297; --></mml:mo><mml:mi>L</mml:mi><mml:mi>E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>U</mml:mi><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>L</mml:mi><mml:mi>E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mn>4</mml:mn></mml:msub></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>L</mml:mi><mml:mi>E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>U</mml:mi><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>L</mml:mi><mml:mi>E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mn>4</mml:mn></mml:msub></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:math>
</disp-formula> where <inline-formula>
<mml:math id="m48" display="inline" overflow="scroll"><mml:mi>U</mml:mi><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math>
</inline-formula> and <inline-formula>
<mml:math id="m49" display="inline" overflow="scroll"><mml:mi>U</mml:mi><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mn>4</mml:mn></mml:msub></mml:mrow></mml:math>
</inline-formula> denoted a 2-fold and 4-fold nearest neighbour up-sampling operation, respectively, <inline-formula>
<mml:math id="m50" display="inline" overflow="scroll"><mml:mrow><mml:mo>&#x02297;<!-- &#x02297; --></mml:mo></mml:mrow></mml:math>
</inline-formula> stood for element-wise product, <inline-formula>
<mml:math id="m51" display="inline" overflow="scroll"><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:math>
</inline-formula> represented a channel-wise concatenation operation, and <inline-formula>
<mml:math id="m52" display="inline" overflow="scroll"><mml:mi>L</mml:mi><mml:mi>E</mml:mi></mml:math>
</inline-formula> was a sequence of operations comprising a <inline-formula>
<mml:math id="m53" display="inline" overflow="scroll"><mml:mn>3</mml:mn><mml:mo>&#x000d7;<!-- &#x000d7; --></mml:mo><mml:mn>3</mml:mn></mml:math>
</inline-formula> convolutional layer, GN, followed by SiLU activation function.</p><fig position="float" id="g003" fig-type="figure"><label>Fig. 3.</label><caption><p>Detailed structure of the developed adaptive feature aggregation (AFA) component. LE: local emphasis module.</p></caption><graphic xlink:href="boe-15-4-2590-g003" position="float"/></fig><p>Similarly, we aggregated the feature maps of <inline-formula>
<mml:math id="m54" display="inline" overflow="scroll"><mml:msubsup><mml:mi>F</mml:mi><mml:mn>2</mml:mn><mml:mrow><mml:mrow><mml:mi class="MJX-variant" mathvariant="normal">&#x02032;<!-- &#x02032; --></mml:mi></mml:mrow></mml:mrow></mml:msubsup></mml:math>
</inline-formula>, <inline-formula>
<mml:math id="m55" display="inline" overflow="scroll"><mml:msubsup><mml:mi>F</mml:mi><mml:mn>3</mml:mn><mml:mrow><mml:mrow><mml:mi class="MJX-variant" mathvariant="normal">&#x02032;<!-- &#x02032; --></mml:mi></mml:mrow></mml:mrow></mml:msubsup></mml:math>
</inline-formula> and <inline-formula>
<mml:math id="m56" display="inline" overflow="scroll"><mml:msubsup><mml:mi>F</mml:mi><mml:mn>4</mml:mn><mml:mrow><mml:mrow><mml:mi class="MJX-variant" mathvariant="normal">&#x02032;<!-- &#x02032; --></mml:mi></mml:mrow></mml:mrow></mml:msubsup></mml:math>
</inline-formula>, as follows: we conducted a 4-fold and 2-fold up-sampling on <inline-formula>
<mml:math id="m57" display="inline" overflow="scroll"><mml:msubsup><mml:mi>F</mml:mi><mml:mn>4</mml:mn><mml:mrow><mml:mrow><mml:mi class="MJX-variant" mathvariant="normal">&#x02032;<!-- &#x02032; --></mml:mi></mml:mrow></mml:mrow></mml:msubsup></mml:math>
</inline-formula> and <inline-formula>
<mml:math id="m58" display="inline" overflow="scroll"><mml:msubsup><mml:mi>F</mml:mi><mml:mn>3</mml:mn><mml:mrow><mml:mrow><mml:mi class="MJX-variant" mathvariant="normal">&#x02032;<!-- &#x02032; --></mml:mi></mml:mrow></mml:mrow></mml:msubsup></mml:math>
</inline-formula>, separately. And then, the resulting feature maps were streamed into the LE module, leading to feature maps <inline-formula>
<mml:math id="m59" display="inline" overflow="scroll"><mml:msubsup><mml:mi>F</mml:mi><mml:mn>4</mml:mn><mml:mrow><mml:mrow><mml:mrow><mml:mi class="MJX-variant" mathvariant="normal">&#x02032;<!-- &#x02032; --></mml:mi><mml:mi class="MJX-variant" mathvariant="normal">&#x02032;<!-- &#x02032; --></mml:mi><mml:mi class="MJX-variant" mathvariant="normal">&#x02032;<!-- &#x02032; --></mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msubsup></mml:math>
</inline-formula> and <inline-formula>
<mml:math id="m60" display="inline" overflow="scroll"><mml:msubsup><mml:mi>F</mml:mi><mml:mn>3</mml:mn><mml:mrow><mml:mrow><mml:mrow><mml:mi class="MJX-variant" mathvariant="normal">&#x02032;<!-- &#x02032; --></mml:mi><mml:mi class="MJX-variant" mathvariant="normal">&#x02032;<!-- &#x02032; --></mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msubsup></mml:math>
</inline-formula>, individually. Further, we carried out an element-wise multiplication operation on <inline-formula>
<mml:math id="m61" display="inline" overflow="scroll"><mml:msubsup><mml:mi>F</mml:mi><mml:mn>2</mml:mn><mml:mrow><mml:mrow><mml:mi class="MJX-variant" mathvariant="normal">&#x02032;<!-- &#x02032; --></mml:mi></mml:mrow></mml:mrow></mml:msubsup></mml:math>
</inline-formula>, <inline-formula>
<mml:math id="m62" display="inline" overflow="scroll"><mml:msubsup><mml:mi>F</mml:mi><mml:mn>3</mml:mn><mml:mrow><mml:mrow><mml:mrow><mml:mi class="MJX-variant" mathvariant="normal">&#x02032;<!-- &#x02032; --></mml:mi><mml:mi class="MJX-variant" mathvariant="normal">&#x02032;<!-- &#x02032; --></mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msubsup></mml:math>
</inline-formula> and <inline-formula>
<mml:math id="m63" display="inline" overflow="scroll"><mml:msubsup><mml:mi>F</mml:mi><mml:mn>4</mml:mn><mml:mrow><mml:mrow><mml:mrow><mml:mi class="MJX-variant" mathvariant="normal">&#x02032;<!-- &#x02032; --></mml:mi><mml:mi class="MJX-variant" mathvariant="normal">&#x02032;<!-- &#x02032; --></mml:mi><mml:mi class="MJX-variant" mathvariant="normal">&#x02032;<!-- &#x02032; --></mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msubsup></mml:math>
</inline-formula> to obtain the feature map <inline-formula>
<mml:math id="m64" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math>
</inline-formula>, thus the fused feature map could be represented as follows: <disp-formula id="e2">
<label>(2)</label>
<mml:math id="m65" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mi>L</mml:mi><mml:mi>E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x02297;<!-- &#x02297; --></mml:mo><mml:mi>L</mml:mi><mml:mi>E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>U</mml:mi><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>L</mml:mi><mml:mi>E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x02297;<!-- &#x02297; --></mml:mo><mml:mi>L</mml:mi><mml:mi>E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>U</mml:mi><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mn>4</mml:mn></mml:msub></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>L</mml:mi><mml:mi>E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mn>4</mml:mn></mml:msub></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:math>
</disp-formula></p><p>At last, we concatenated between <inline-formula>
<mml:math id="m66" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math>
</inline-formula> and <inline-formula>
<mml:math id="m67" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math>
</inline-formula>, and the resultant feature map was passed through the LE module for obtaining the fused global feature map <inline-formula>
<mml:math id="m68" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>O</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math>
</inline-formula>, which could be depicted by: <disp-formula id="e3">
<label>(3)</label>
<mml:math id="m69" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mi>O</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mi>L</mml:mi><mml:mi>E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:math>
</disp-formula></p><p>Note that through the aforementioned procedures, the AFA module was able to aggregate adjacent high-level features from the Transformer encoder in a stepwise adaptive fashion, generating the most important global semantic information.</p></sec><sec id="sec3-2-2-2"><label>3.2.2.2</label><title>ResidualBlock module</title><p>In order to boost the extraction ability of local low-level feature details for Transformer feature pyramid, we constructed residual blocks named ResidualBlock module through the incorporation of the LE modules as well as a skip connection with convolutional layer with <inline-formula>
<mml:math id="m70" display="inline" overflow="scroll"><mml:mn>1</mml:mn><mml:mo>&#x000d7;<!-- &#x000d7; --></mml:mo><mml:mn>1</mml:mn></mml:math>
</inline-formula> kernel (denoted as <inline-formula>
<mml:math id="m71" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x000d7;<!-- &#x000d7; --></mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math>
</inline-formula>) and GN, as displayed in <xref rid="g004" ref-type="fig">Fig.&#x000a0;4(a)</xref>. The LE module consisted of <inline-formula>
<mml:math id="m72" display="inline" overflow="scroll"><mml:mn>3</mml:mn><mml:mo>&#x000d7;<!-- &#x000d7; --></mml:mo><mml:mn>3</mml:mn></mml:math>
</inline-formula> convolution, GN and SiLU activation function in an order arrangement. Compared to original RB module (See <xref rid="g004" ref-type="fig">Fig.&#x000a0;4(b)</xref>) in FCBFormer network, we made small changes in the order of <inline-formula>
<mml:math id="m73" display="inline" overflow="scroll"><mml:mn>3</mml:mn><mml:mo>&#x000d7;<!-- &#x000d7; --></mml:mo><mml:mn>3</mml:mn></mml:math>
</inline-formula> convolution along with adding <inline-formula>
<mml:math id="m74" display="inline" overflow="scroll"><mml:mn>1</mml:mn><mml:mo>&#x000d7;<!-- &#x000d7; --></mml:mo><mml:mn>1</mml:mn></mml:math>
</inline-formula> convolution and GN in residual connection. This sequence was designed to optimally leverage the effects of these three operations. We first performed <inline-formula>
<mml:math id="m75" display="inline" overflow="scroll"><mml:mn>3</mml:mn><mml:mo>&#x000d7;<!-- &#x000d7; --></mml:mo><mml:mn>3</mml:mn></mml:math>
</inline-formula> convolution operation on high-resolution low-level features from PVTv2-B3 encoder to extract the local boundary information related to polyp lesions. Then, GN was applied immediately to help in normalizing the distribution of the resulting features and mitigating internal covariate shift. Finally, we introduced a SiLU activation function to the normalized features, fostering local edge feature representation power. Through equipping with two units (LE modules) consisting of <inline-formula>
<mml:math id="m76" display="inline" overflow="scroll"><mml:mn>3</mml:mn><mml:mo>&#x000d7;<!-- &#x000d7; --></mml:mo><mml:mn>3</mml:mn></mml:math>
</inline-formula> convolution, GN and SiLU activation function in sequence, the boundary details of low-level features could be emphasized. In addition, we placed <inline-formula>
<mml:math id="m77" display="inline" overflow="scroll"><mml:mn>1</mml:mn><mml:mo>&#x000d7;<!-- &#x000d7; --></mml:mo><mml:mn>1</mml:mn></mml:math>
</inline-formula> convolution and GN in the residual connection for strengthening cross-channel interaction of boundary information in high-resolution low-level features and scaling their feature channel dimensions. By doing so, it could more efficiently cope with edge details of polyp lesions. Concretely, we first provided the low-level feature map <inline-formula>
<mml:math id="m78" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math>
</inline-formula> into two sequential LE modules and an unit with <inline-formula>
<mml:math id="m79" display="inline" overflow="scroll"><mml:mn>1</mml:mn><mml:mo>&#x000d7;<!-- &#x000d7; --></mml:mo><mml:mn>1</mml:mn></mml:math>
</inline-formula> convolutional layer and GN, respectively, generating feature maps <inline-formula>
<mml:math id="m80" display="inline" overflow="scroll"><mml:msubsup><mml:mi>F</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mi class="MJX-variant" mathvariant="normal">&#x02032;<!-- &#x02032; --></mml:mi></mml:mrow></mml:msubsup></mml:math>
</inline-formula> and <inline-formula>
<mml:math id="m81" display="inline" overflow="scroll"><mml:msubsup><mml:mi>F</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mi class="MJX-variant" mathvariant="normal">&#x02032;<!-- &#x02032; --></mml:mi><mml:mi class="MJX-variant" mathvariant="normal">&#x02032;<!-- &#x02032; --></mml:mi></mml:mrow></mml:msubsup></mml:math>
</inline-formula>. Next, we directly implemented an element-wise addition operation on the resulting feature maps followed by a SiLU activation function, obtaining the feature map <inline-formula>
<mml:math id="m82" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>O</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math>
</inline-formula>, which could be expressed as follows: <disp-formula id="e4">
<label>(4)</label>
<mml:math id="m83" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mi>O</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mi>S</mml:mi><mml:mrow><mml:mtext>iLU</mml:mtext></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>L</mml:mi><mml:mi>E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>L</mml:mi><mml:mi>E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x02297;<!-- &#x02297; --></mml:mo><mml:mi>G</mml:mi><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x000d7;<!-- &#x000d7; --></mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:math>
</disp-formula></p><p>As such, more fine boundary details from low-level features could be underlined and the detailed information processing capability of our model could be strengthened.</p><fig position="float" id="g004" fig-type="figure"><label>Fig. 4.</label><caption><p>The proposed ResidualBlock module (a) and the original RB module in FCBFormer (b).</p></caption><graphic xlink:href="boe-15-4-2590-g004" position="float"/></fig></sec><sec id="sec3-2-2-3"><label>3.2.2.3</label><title>Selective global-to-local fusion head (SGLFH) module</title><p>With the goal of making effective integration between high-level and low-level characteristics, we implemented selectively fusion on feature maps <inline-formula>
<mml:math id="m84" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>O</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math>
</inline-formula> and <inline-formula>
<mml:math id="m85" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>O</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math>
</inline-formula>, by designing the selective global-to-local fusion head (SGLFH) module. In the SGLFH module, we utilized two symmetric attention units (AUs) to adaptively pick up mutual representations from two inputs (<inline-formula>
<mml:math id="m86" display="inline" overflow="scroll"><mml:msubsup><mml:mi>O</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mi class="MJX-variant" mathvariant="normal">&#x02032;<!-- &#x02032; --></mml:mi></mml:mrow></mml:msubsup></mml:math>
</inline-formula>, <inline-formula>
<mml:math id="m87" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>O</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math>
</inline-formula>) before fusion, thereby exhibiting its selective characteristic, as shown in <xref rid="g005" ref-type="fig">Fig.&#x000a0;5</xref>. The low-level and high-level feature information was fed into the two AUs in different ways for compensating the missing critical spatial edge information of the high-level semantic features and the loss of semantic information of low-level features, followed by concatenation operation to aggregate the outputs of two AUs. As given in <xref rid="g005" ref-type="fig">Fig.&#x000a0;5</xref>, the AU function <inline-formula>
<mml:math id="m88" display="inline" overflow="scroll"><mml:mi>P</mml:mi><mml:mi>A</mml:mi><mml:mi>U</mml:mi></mml:math>
</inline-formula> process could be expressed as follows: <disp-formula id="e5">
<label>(5)</label>
<mml:math id="m89" display="block" overflow="scroll"><mml:msubsup><mml:mi>T</mml:mi><mml:mi>H</mml:mi><mml:mrow><mml:mrow><mml:mi class="MJX-variant" mathvariant="normal">&#x02032;<!-- &#x02032; --></mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>&#x003c3;<!-- &#x003c3; --></mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>L</mml:mi><mml:mi>E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msubsup><mml:mi>O</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mrow><mml:mi class="MJX-variant" mathvariant="normal">&#x02032;<!-- &#x02032; --></mml:mi></mml:mrow></mml:mrow></mml:msubsup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:msubsup><mml:mi>T</mml:mi><mml:mi>L</mml:mi><mml:mrow><mml:mrow><mml:mi class="MJX-variant" mathvariant="normal">&#x02032;<!-- &#x02032; --></mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>&#x003c3;<!-- &#x003c3; --></mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>L</mml:mi><mml:mi>E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>O</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:math>
</disp-formula>
<disp-formula id="e6">
<label>(6)</label>
<mml:math id="m90" display="block" overflow="scroll"><mml:mi>P</mml:mi><mml:mi>A</mml:mi><mml:mi>U</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>O</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:msubsup><mml:mi>O</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mrow><mml:mi class="MJX-variant" mathvariant="normal">&#x02032;<!-- &#x02032; --></mml:mi></mml:mrow></mml:mrow></mml:msubsup></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msubsup><mml:mi>T</mml:mi><mml:mi>L</mml:mi><mml:mrow><mml:mrow><mml:mi class="MJX-variant" mathvariant="normal">&#x02032;<!-- &#x02032; --></mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:mo>&#x02297;<!-- &#x02297; --></mml:mo><mml:mrow><mml:msub><mml:mi>O</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x02295;<!-- &#x02295; --></mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msubsup><mml:mi>T</mml:mi><mml:mi>H</mml:mi><mml:mrow><mml:mrow><mml:mi class="MJX-variant" mathvariant="normal">&#x02032;<!-- &#x02032; --></mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:mo>&#x02297;<!-- &#x02297; --></mml:mo><mml:msubsup><mml:mi>O</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mrow><mml:mi class="MJX-variant" mathvariant="normal">&#x02032;<!-- &#x02032; --></mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:mo>&#x02297;<!-- &#x02297; --></mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="normal">&#x00398;<!-- &#x00398; --></mml:mi><mml:msubsup><mml:mi>T</mml:mi><mml:mi>L</mml:mi><mml:mrow><mml:mrow><mml:mi class="MJX-variant" mathvariant="normal">&#x02032;<!-- &#x02032; --></mml:mi></mml:mrow></mml:mrow></mml:msubsup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x02295;<!-- &#x02295; --></mml:mo><mml:mrow><mml:msub><mml:mi>O</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math>
</disp-formula> where <inline-formula>
<mml:math id="m91" display="inline" overflow="scroll"><mml:msubsup><mml:mi>O</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mi class="MJX-variant" mathvariant="normal">&#x02032;<!-- &#x02032; --></mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>U</mml:mi><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>O</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:math>
</inline-formula> and <inline-formula>
<mml:math id="m92" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>O</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math>
</inline-formula> referred to the input features. <inline-formula>
<mml:math id="m93" display="inline" overflow="scroll"><mml:mi>U</mml:mi><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math>
</inline-formula> indicated a 2-fold up-sampling operation. <inline-formula>
<mml:math id="m94" display="inline" overflow="scroll"><mml:mi>L</mml:mi><mml:mi>E</mml:mi></mml:math>
</inline-formula> described a sequence of operations comprising a <inline-formula>
<mml:math id="m95" display="inline" overflow="scroll"><mml:mn>3</mml:mn><mml:mo>&#x000d7;<!-- &#x000d7; --></mml:mo><mml:mn>3</mml:mn></mml:math>
</inline-formula> convolutional layer, GN, followed by SiLU activation function. <inline-formula>
<mml:math id="m96" display="inline" overflow="scroll"><mml:mi>&#x003c3;<!-- &#x003c3; --></mml:mi></mml:math>
</inline-formula> denoted sigmoid activation function. <inline-formula>
<mml:math id="m97" display="inline" overflow="scroll"><mml:mrow><mml:mo>&#x02297;<!-- &#x02297; --></mml:mo></mml:mrow></mml:math>
</inline-formula> stood for element-wise multiplication. <inline-formula>
<mml:math id="m98" display="inline" overflow="scroll"><mml:mi mathvariant="normal">&#x00398;<!-- &#x00398; --></mml:mi></mml:math>
</inline-formula> represented the reverse operation by subtracting the feature <inline-formula>
<mml:math id="m99" display="inline" overflow="scroll"><mml:msubsup><mml:mi>T</mml:mi><mml:mi>L</mml:mi><mml:mrow><mml:mi class="MJX-variant" mathvariant="normal">&#x02032;<!-- &#x02032; --></mml:mi></mml:mrow></mml:msubsup></mml:math>
</inline-formula>, refining the imprecise and coarse estimation into an accurate and complete prediction map. At last, the output features of the two AUs were concatenated along the channel dimension, which could be described by: <disp-formula id="e7">
<label>(7)</label>
<mml:math id="m100" display="block" overflow="scroll"><mml:mi>Z</mml:mi><mml:mo>=</mml:mo><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>P</mml:mi><mml:mi>A</mml:mi><mml:mi>U</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msubsup><mml:mi>O</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mrow><mml:mi class="MJX-variant" mathvariant="normal">&#x02032;<!-- &#x02032; --></mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mi>O</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>P</mml:mi><mml:mi>A</mml:mi><mml:mi>U</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>O</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:msubsup><mml:mi>O</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mrow><mml:mi class="MJX-variant" mathvariant="normal">&#x02032;<!-- &#x02032; --></mml:mi></mml:mrow></mml:mrow></mml:msubsup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:math>
</disp-formula> where <inline-formula>
<mml:math id="m101" display="inline" overflow="scroll"><mml:msubsup><mml:mi>O</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mi class="MJX-variant" mathvariant="normal">&#x02032;<!-- &#x02032; --></mml:mi></mml:mrow></mml:msubsup></mml:math>
</inline-formula> contained high-level semantic information while <inline-formula>
<mml:math id="m102" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>O</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math>
</inline-formula> comprised rich boundary details. The resulting concatenated feature maps were further fed into two sequential units composed of convolutional layer with <inline-formula>
<mml:math id="m103" display="inline" overflow="scroll"><mml:mn>1</mml:mn><mml:mo>&#x000d7;<!-- &#x000d7; --></mml:mo><mml:mn>1</mml:mn></mml:math>
</inline-formula> kernel, GN and SiLU activation function for generating ultimate output feature map <inline-formula>
<mml:math id="m104" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>T</mml:mi></mml:msub></mml:mrow></mml:math>
</inline-formula> in the Transformer branch. By designing SGLFH module with attention units, the boundary information and semantic information could be selectively aggregated to depict more fine-grained contours of polyp lesions and recalibrate their locations. The above process could be summarized as: <disp-formula id="e8">
<label>(8)</label>
<mml:math id="m105" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>T</mml:mi></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mi>S</mml:mi><mml:mrow><mml:mtext>iLU</mml:mtext><mml:mo stretchy="false">(</mml:mo></mml:mrow><mml:mi>G</mml:mi><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mrow><mml:mtext>&#x000a0;</mml:mtext><mml:mo>&#x000d7;<!-- &#x000d7; --></mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>S</mml:mi><mml:mrow><mml:mtext>iLU</mml:mtext><mml:mo stretchy="false">(</mml:mo></mml:mrow><mml:mi>G</mml:mi><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mrow><mml:mtext>&#x000a0;</mml:mtext><mml:mo>&#x000d7;<!-- &#x000d7; --></mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>Z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:math>
</disp-formula></p><fig position="float" id="g005" fig-type="figure"><label>Fig. 5.</label><caption><p>The flowchart of the proposed selective global-to-local fusion head (SGLFH) module. AU: attention unit.</p></caption><graphic xlink:href="boe-15-4-2590-g005" position="float"/></fig><p>In addition, the feature map <inline-formula>
<mml:math id="m106" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>T</mml:mi></mml:msub></mml:mrow></mml:math>
</inline-formula> was further implemented 4-fold up-sampling operation to generate <inline-formula>
<mml:math id="m107" display="inline" overflow="scroll"><mml:msubsup><mml:mi>F</mml:mi><mml:mi>T</mml:mi><mml:mrow><mml:mi class="MJX-variant" mathvariant="normal">&#x02032;<!-- &#x02032; --></mml:mi></mml:mrow></mml:msubsup></mml:math>
</inline-formula>, which maintained consistent size with the input image.</p></sec></sec></sec><sec id="sec3-3"><label>3.3</label><title>FCN branch</title><p>The FCN branch (See <xref rid="g001" ref-type="fig">Fig.&#x000a0;1(c)</xref>) was characterized as a composition of the ResidualBlock module, strided convolutional layers for down-sampling, nearest neighbour interpolation for up-sampling, and dense U-Net type skip connections. This design encouraged extraction of highly merged features of different scales required for matching outputs of the Transformer branch into full-size segmentation maps. Through the encoder of FCN branch, the feature map <inline-formula>
<mml:math id="m108" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo>&#x02208;<!-- &#x02208; --></mml:mo><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mfrac><mml:mi>H</mml:mi><mml:mrow><mml:mrow><mml:msup><mml:mn>2</mml:mn><mml:mi>j</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:mfrac><mml:mo>&#x000d7;<!-- &#x000d7; --></mml:mo><mml:mfrac><mml:mi>W</mml:mi><mml:mrow><mml:mrow><mml:msup><mml:mn>2</mml:mn><mml:mi>j</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:mfrac><mml:mo>&#x000d7;<!-- &#x000d7; --></mml:mo><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:math>
</inline-formula> was attained in sequence from the down-sampling layer 1 to 5, where <inline-formula>
<mml:math id="m109" display="inline" overflow="scroll"><mml:mi>j</mml:mi><mml:mo>&#x02208;<!-- &#x02208; --></mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>&#x02026;<!-- &#x02026; --></mml:mo><mml:mo>,</mml:mo><mml:mn>5</mml:mn></mml:mrow><mml:mo fence="false" stretchy="false">}</mml:mo></mml:math>
</inline-formula> and <inline-formula>
<mml:math id="m110" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo>&#x02208;<!-- &#x02208; --></mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mrow><mml:mn>32</mml:mn><mml:mo>,</mml:mo><mml:mn>32</mml:mn><mml:mo>,</mml:mo><mml:mn>64</mml:mn><mml:mo>,</mml:mo><mml:mn>128</mml:mn><mml:mo>,</mml:mo><mml:mn>512</mml:mn></mml:mrow><mml:mo fence="false" stretchy="false">}</mml:mo></mml:math>
</inline-formula>. On the contrary, by way of the decoder of FCN branch, the feature resolution was progressively restored by a factor of 2 from the up-sampling layer 1 to 5, whereas the number of feature channels was reduced consecutively by a factor of 2 ranging from the second to fifth up-sampling layer.</p></sec><sec id="sec3-4"><label>3.4</label><title>PredictionHead (PH) module</title><p>Compared with the PredictionHead (PH) module in FCBFormer network [<xref rid="r42" ref-type="bibr">42</xref>] (See <xref rid="g006" ref-type="fig">Fig.&#x000a0;6(b)</xref>), we made small modification on it including substituting the designed ResidualBlock module for the RB module, as illustrated in <xref rid="g006" ref-type="fig">Fig.&#x000a0;6(a)</xref>. The PH module received a full-size feature tensor derived from stitching output tensors from the Transformer branch and FCN branch to perform polyp segmentation mask prediction at full-size. Concretely, the stitched full-size feature tensor was passed through two consecutive ResidualBlock modules followed by <inline-formula>
<mml:math id="m111" display="inline" overflow="scroll"><mml:mn>1</mml:mn><mml:mo>&#x000d7;<!-- &#x000d7; --></mml:mo><mml:mn>1</mml:mn></mml:math>
</inline-formula> convolutional layer and SiLU activation function to yield a full-size segmentation map. Each layer of PH module produced 64 channels, apart from the prediction layer (<inline-formula>
<mml:math id="m112" display="inline" overflow="scroll"><mml:mn>1</mml:mn><mml:mo>&#x000d7;<!-- &#x000d7; --></mml:mo><mml:mn>1</mml:mn></mml:math>
</inline-formula> convolution&#x02009;+&#x02009;SiLU) returning a single channel. Through aggregating complementary features generated by the Transformer branch along with the FCN branch, the PH module was able to achieve accurate prediction of polyp segmentation map.</p><fig position="float" id="g006" fig-type="figure"><label>Fig. 6.</label><caption><p>The PredictionHead (PH) module in the Dua-PSNet (a), and the FCBFormer (b).</p></caption><graphic xlink:href="boe-15-4-2590-g006" position="float"/></fig></sec><sec id="sec3-5"><label>3.5</label><title>Loss function</title><p>It was reported by works [<xref rid="r50" ref-type="bibr">50</xref>] that using joint loss functions with adaptive weight coefficients could boost the model&#x02019;s performance with faster convergence speed. Hence, we combined weighted binary cross-entropy loss (<inline-formula>
<mml:math id="m113" display="inline" overflow="scroll"><mml:msubsup><mml:mi>L</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mi>C</mml:mi><mml:mi>E</mml:mi></mml:mrow><mml:mi>&#x003c9;<!-- &#x003c9; --></mml:mi></mml:msubsup></mml:math>
</inline-formula>) with the weighted Dice loss (<inline-formula>
<mml:math id="m114" display="inline" overflow="scroll"><mml:msubsup><mml:mi>L</mml:mi><mml:mrow><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mi>&#x003c9;<!-- &#x003c9; --></mml:mi></mml:msubsup></mml:math>
</inline-formula>) for supervision of the network training. The entire loss function <inline-formula>
<mml:math id="m115" display="inline" overflow="scroll"><mml:mi>L</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>S</mml:mi><mml:mo>,</mml:mo><mml:mi>G</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:math>
</inline-formula> for the proposed Dua-PSNet could be formulated as follows: <disp-formula id="e9">
<label>(9)</label>
<mml:math id="m116" display="block" overflow="scroll"><mml:mi>L</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>S</mml:mi><mml:mo>,</mml:mo><mml:mi>G</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>&#x003bb;<!-- &#x003bb; --></mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:msubsup><mml:mi>L</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mi>C</mml:mi><mml:mi>E</mml:mi></mml:mrow><mml:mi>&#x003c9;<!-- &#x003c9; --></mml:mi></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>S</mml:mi><mml:mo>,</mml:mo><mml:mi>G</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mi>&#x003bb;<!-- &#x003bb; --></mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:msubsup><mml:mi>L</mml:mi><mml:mrow><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mi>&#x003c9;<!-- &#x003c9; --></mml:mi></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>S</mml:mi><mml:mo>,</mml:mo><mml:mi>G</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:math>
</disp-formula> where <italic>S</italic> referred to the prediction masks and <italic>G</italic> represented the ground truths, respectively. <inline-formula>
<mml:math id="m117" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003bb;<!-- &#x003bb; --></mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math>
</inline-formula> and <inline-formula>
<mml:math id="m118" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003bb;<!-- &#x003bb; --></mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math>
</inline-formula> indicated the adjustable weighting coefficients. It was noteworthy that <inline-formula>
<mml:math id="m119" display="inline" overflow="scroll"><mml:msubsup><mml:mi>L</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mi>C</mml:mi><mml:mi>E</mml:mi></mml:mrow><mml:mi>&#x003c9;<!-- &#x003c9; --></mml:mi></mml:msubsup></mml:math>
</inline-formula> was able to put more emphasis on hard pixels rather than giving all pixel equal weights and <inline-formula>
<mml:math id="m120" display="inline" overflow="scroll"><mml:msubsup><mml:mi>L</mml:mi><mml:mrow><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mi>&#x003c9;<!-- &#x003c9; --></mml:mi></mml:msubsup></mml:math>
</inline-formula> could increase weighted coefficients of hard pixels to emphasize their importance, which may tackle the foreground-background imbalance problem.</p></sec></sec><sec sec-type="results" id="sec4"><label>4.</label><title>Experiments and results</title><p>In this part, we provided the details on datasets and evaluation indicators, as well as implementation details. Then, we performed comparison between the developed Dua-PSNet with the cutting-edge polyp segmentation methods from quantitative and qualitative perspectives. After that, we measured our model&#x02019;s generalization ability across different datasets. Finally, aiming at clarifying the effectiveness of the principal components in the proposed Dua-PSNet, we carried out a series of ablation experiments.</p><sec id="sec4-1"><label>4.1</label><title>Datasets and evaluation indicators</title><sec id="sec4-1-1"><label>4.1.1</label><title>Datasets</title><p>In order to assess the ability of the presented Dua-PSNet to segment polyps, we conducted a series of experiments on five benchmark datasets, each of which was described in detail as follows:</p><p><bold>Kvasir-SEG</bold> [<xref rid="r51" ref-type="bibr">51</xref>]<bold>:</bold> This dataset was created by the Vestre Viken Health Trust of Norway, and contained 1,000 polyp images gathered from multiple colonoscopy video sequences, where the image resolution distribution varied between 332&#x02009;&#x000d7;&#x02009;487 and 1920&#x02009;&#x000d7;&#x02009;1072. It provided a more fine-grained pixel-level segmentation annotation validated by experienced gastroenterologists.</p><p><bold>CVC-ClinicDB</bold> [<xref rid="r52" ref-type="bibr">52</xref>]<bold>:</bold> This dataset was provided by the Hospital Clinic of Barcelona and comprised 612 images with a resolution of 288&#x02009;&#x000d7;&#x02009;384, which were captured from 23 colonoscopy video sequences of 13 different patients.</p><p><bold>CVC-ColonDB</bold> [<xref rid="r6" ref-type="bibr">6</xref>]<bold>:</bold> It consisted of 380 images with a fixed resolution of 500&#x02009;&#x000d7;&#x02009;570 extracted from 15 colonoscopy videos with a sample of 20 frames at random from each sequence, in which each image was accompanied by a binary mask for putting emphasis on the polyp regions.</p><p><bold>ETIS-LaribPolypDB</bold> [<xref rid="r2" ref-type="bibr">2</xref>]<bold>:</bold> ETIS was an early polyp segmentation dataset, which involved 196 polyp images with a resolution of 966&#x02009;&#x000d7;&#x02009;1225.</p><p><bold>CVC-300</bold> [<xref rid="r53" ref-type="bibr">53</xref>]<bold>:</bold> It was a cross-domain dataset, and embraced 60 polyp images with a resolution of 500&#x02009;&#x000d7;&#x02009;574.</p></sec><sec id="sec4-1-2"><label>4.1.2</label><title>Evaluation protocols</title><p>Following recommendations from common methods for polyp segmentation [<xref rid="r21" ref-type="bibr">21</xref>,<xref rid="r42" ref-type="bibr">42</xref>], we utilized mean Dice (<inline-formula>
<mml:math id="m121" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:math>
</inline-formula>), mean IoU (<inline-formula>
<mml:math id="m122" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:mi>U</mml:mi></mml:math>
</inline-formula>), mean absolute error (MAE) [<xref rid="r54" ref-type="bibr">54</xref>], weighted F-measure (<inline-formula>
<mml:math id="m123" display="inline" overflow="scroll"><mml:msubsup><mml:mi>F</mml:mi><mml:mi>&#x003b2;<!-- &#x003b2; --></mml:mi><mml:mi>w</mml:mi></mml:msubsup></mml:math>
</inline-formula>) [<xref rid="r55" ref-type="bibr">55</xref>], S-measure (<inline-formula>
<mml:math id="m124" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>&#x003b1;<!-- &#x003b1; --></mml:mi></mml:msub></mml:mrow></mml:math>
</inline-formula>) [<xref rid="r56" ref-type="bibr">56</xref>], and mean E-measure (<inline-formula>
<mml:math id="m125" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mi>&#x003d5;<!-- &#x003d5; --></mml:mi></mml:msub></mml:mrow></mml:math>
</inline-formula>) [<xref rid="r57" ref-type="bibr">57</xref>] as our assessment indicators for comprehensively investigating our model&#x02019;s performance. Among these indicators, <inline-formula>
<mml:math id="m126" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:math>
</inline-formula> and <inline-formula>
<mml:math id="m127" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:mi>U</mml:mi></mml:math>
</inline-formula> were regional level similarity measures and mainly highlighted the internal consistency of segmented objects. MAE was a pixel-by-pixel comparison metric, while <inline-formula>
<mml:math id="m128" display="inline" overflow="scroll"><mml:msubsup><mml:mi>F</mml:mi><mml:mi>&#x003b2;<!-- &#x003b2; --></mml:mi><mml:mi>w</mml:mi></mml:msubsup></mml:math>
</inline-formula> comprehensively considered the precision and recall. <inline-formula>
<mml:math id="m129" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>&#x003b1;<!-- &#x003b1; --></mml:mi></mml:msub></mml:mrow></mml:math>
</inline-formula> concentrated on the structural similarity at the region and object level, and <inline-formula>
<mml:math id="m130" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mi>&#x003d5;<!-- &#x003d5; --></mml:mi></mml:msub></mml:mrow></mml:math>
</inline-formula> paid attention to the segmentation performance at the pixel and image level.</p></sec></sec><sec id="sec4-2"><label>4.2</label><title>Implementation details</title><p>The developed Dua-PSNet was built under the PyTorch framework, which was trained utilizing a single NVIDIA RTX3060 GPU with 12 GB memory. Different data augmentation strategies were applied during training, including a gaussian blur, color jitter, horizontal and vertical flips, and affine transforms. Aiming to assess the learning and generalization power of the proposed Dua-PSNet, following the same settings as in [<xref rid="r21" ref-type="bibr">21</xref>,<xref rid="r42" ref-type="bibr">42</xref>] we separately partitioned Kvasir-SEG and CVC-ClinicDB into training, validation and test sets with a ratio of 8:1:1 in our experiment A. Similarly, we randomly chose 1,288 and 162 images from CVC-ClinicDB and Kvasir-SEG to form the training and validation sets, respectively, while an overall 798 images (100 images selected from Kvasir-SEG dataset, 62 images chose from CVC-ClinicDB benchmark, 380 images collected from CVC-ColonDB benchmark, 196 images gathered from ETIS-LaribPolypDB dataset, and the remain 60 images from CVC-300) from the used five benchmark datasets were adopted as the test set in our experiment B. The specific data partitioning was illustrated in <xref rid="t001" ref-type="table">Table&#x000a0;1</xref>. For the ablation studies, we also exploited the Kvasir-SEG and CVC-ClinicDB to evaluate the baseline performance of FCN along with PVTv2-B3, and then investigate the effect with successive additions of PVTv2-B3, AFA, ResidualBlock, and SGLFH modules. Besides, we adjusted the size of the input images into 352&#x02009;&#x000d7;&#x02009;352, whilst normalized the RGB values into a range between -1 and 1. The values of <inline-formula>
<mml:math id="m131" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003bb;<!-- &#x003bb; --></mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math>
</inline-formula> and <inline-formula>
<mml:math id="m132" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003bb;<!-- &#x003bb; --></mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math>
</inline-formula> for loss function were assigned into 1 and 0.2, empirically. The AdamW [<xref rid="r58" ref-type="bibr">58</xref>] optimizer with an initial learning rate of <inline-formula>
<mml:math id="m133" display="inline" overflow="scroll"><mml:mn>1</mml:mn><mml:mi>e</mml:mi><mml:mo>&#x02212;<!-- &#x02212; --></mml:mo><mml:mn>4</mml:mn></mml:math>
</inline-formula> was adopted for optimization of the proposed model during the training. In order to accelerate the model&#x02019;s convergence, the learning rate was stepped down by a factor of 2 when the <inline-formula>
<mml:math id="m134" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:math>
</inline-formula> on the validation set did not further boost over 10 epochs until hitting a minimum of <inline-formula>
<mml:math id="m135" display="inline" overflow="scroll"><mml:mn>1</mml:mn><mml:mi>e</mml:mi><mml:mo>&#x02212;<!-- &#x02212; --></mml:mo><mml:mn>6</mml:mn></mml:math>
</inline-formula>. Eventually, the model with the highest validation <inline-formula>
<mml:math id="m136" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:math>
</inline-formula> was considered as the final one. All models were trained with a mini-batch size of 4 for more than 200 epochs, unless otherwise specified. During the testing stage, our model predicted full-size binary segmentation maps for RGB images with <inline-formula>
<mml:math id="m137" display="inline" overflow="scroll"><mml:mi>H</mml:mi><mml:mo>&#x000d7;<!-- &#x000d7; --></mml:mo><mml:mi>W</mml:mi></mml:math>
</inline-formula> spatial dimensions.</p><table-wrap position="float" id="t001"><label>Table 1.</label><caption><title>Datasets used in this study.</title></caption><table frame="hsides" rules="all"><colgroup span="1"><?tabw 21pc?><col align="left" width="17%" span="1"/><col align="left" width="17%" span="1"/><col align="left" width="17%" span="1"/><col align="left" width="17%" span="1"/><col align="left" width="17%" span="1"/><col align="left" width="17%" span="1"/></colgroup><thead><tr><th valign="top" align="left" colspan="6" rowspan="1">Experiment A</th></tr><tr><th valign="top" align="center" colspan="6" rowspan="1">
<hr/>
</th></tr><tr><th valign="top" align="left" rowspan="1" colspan="1">Dataset</th><th valign="top" align="left" rowspan="1" colspan="1">Images</th><th valign="top" align="left" rowspan="1" colspan="1">Input size</th><th valign="top" align="left" rowspan="1" colspan="1">Train</th><th valign="top" align="left" rowspan="1" colspan="1">Valid</th><th valign="top" align="left" rowspan="1" colspan="1">Test</th></tr></thead><tbody><tr><td valign="top" align="left" rowspan="1" colspan="1">Kvasir-SEG [<xref rid="r51" ref-type="bibr">51</xref>]</td><td valign="top" align="left" rowspan="1" colspan="1">1000</td><td valign="top" align="left" rowspan="1" colspan="1">Variable</td><td valign="top" align="left" rowspan="1" colspan="1">800</td><td valign="top" align="left" rowspan="1" colspan="1">100</td><td valign="top" align="left" rowspan="1" colspan="1">100</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">CVC-ClinicDB [<xref rid="r52" ref-type="bibr">52</xref>]</td><td valign="top" align="left" rowspan="1" colspan="1">612</td><td valign="top" align="left" rowspan="1" colspan="1">288&#x02009;&#x000d7;&#x02009;384</td><td valign="top" align="left" rowspan="1" colspan="1">490</td><td valign="top" align="left" rowspan="1" colspan="1">61</td><td valign="top" align="left" rowspan="1" colspan="1">61</td></tr><tr><td valign="top" align="center" colspan="6" rowspan="1">
<hr/>
</td></tr><tr><td valign="top" align="left" colspan="6" rowspan="1">Experiment B</td></tr><tr><td valign="top" align="center" colspan="6" rowspan="1">
<hr/>
</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Dataset</td><td valign="top" align="left" rowspan="1" colspan="1">Images</td><td valign="top" align="left" rowspan="1" colspan="1">Input size</td><td valign="top" align="left" rowspan="1" colspan="1">Train</td><td valign="top" align="left" rowspan="1" colspan="1">Valid</td><td valign="top" align="left" rowspan="1" colspan="1">Test</td></tr><tr><td valign="top" align="center" colspan="6" rowspan="1">
<hr/>
</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Kvasir-SEG [<xref rid="r51" ref-type="bibr">51</xref>]</td><td valign="top" align="left" rowspan="1" colspan="1">1000</td><td valign="top" align="left" rowspan="1" colspan="1">Variable</td><td valign="top" align="left" rowspan="1" colspan="1">800</td><td valign="top" align="left" rowspan="1" colspan="1">100</td><td valign="top" align="left" rowspan="1" colspan="1">100</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">CVC-ClinicDB [<xref rid="r52" ref-type="bibr">52</xref>]</td><td valign="top" align="left" rowspan="1" colspan="1">612</td><td valign="top" align="left" rowspan="1" colspan="1">288&#x02009;&#x000d7;&#x02009;384</td><td valign="top" align="left" rowspan="1" colspan="1">488</td><td valign="top" align="left" rowspan="1" colspan="1">62</td><td valign="top" align="left" rowspan="1" colspan="1">62</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">CVC-ColonDB [<xref rid="r6" ref-type="bibr">6</xref>]</td><td valign="top" align="left" rowspan="1" colspan="1">380</td><td valign="top" align="left" rowspan="1" colspan="1">500&#x02009;&#x000d7;&#x02009;570</td><td valign="top" align="left" rowspan="1" colspan="1">-</td><td valign="top" align="left" rowspan="1" colspan="1">-</td><td valign="top" align="left" rowspan="1" colspan="1">380</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">ETIS-LaribPolypDB [<xref rid="r2" ref-type="bibr">2</xref>]</td><td valign="top" align="left" rowspan="1" colspan="1">196</td><td valign="top" align="left" rowspan="1" colspan="1">966&#x02009;&#x000d7;&#x02009;1225</td><td valign="top" align="left" rowspan="1" colspan="1">-</td><td valign="top" align="left" rowspan="1" colspan="1">-</td><td valign="top" align="left" rowspan="1" colspan="1">196</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">CVC-300 [<xref rid="r53" ref-type="bibr">53</xref>]</td><td valign="top" align="left" rowspan="1" colspan="1">60</td><td valign="top" align="left" rowspan="1" colspan="1">500&#x02009;&#x000d7;&#x02009;574</td><td valign="top" align="left" rowspan="1" colspan="1">-</td><td valign="top" align="left" rowspan="1" colspan="1">-</td><td valign="top" align="left" rowspan="1" colspan="1">60</td></tr></tbody></table></table-wrap></sec><sec id="sec4-3"><label>4.3</label><title>Comparison with state-of-the-art approaches</title><sec id="sec4-3-1"><label>4.3.1</label><title>Comparison approaches</title><p>In an effort to investigate the effectiveness of the developed Dua-PSNet, we compared it with a variety of current state-of-the-art counterparts in the domain of polyp segmentation, including U-Net [<xref rid="r11" ref-type="bibr">11</xref>], U-Net++ [<xref rid="r12" ref-type="bibr">12</xref>], PraNet [<xref rid="r21" ref-type="bibr">21</xref>], MCSF-Net [<xref rid="r31" ref-type="bibr">31</xref>], SSFormer [<xref rid="r35" ref-type="bibr">35</xref>], FCBFormer [<xref rid="r42" ref-type="bibr">42</xref>], SFA [<xref rid="r19" ref-type="bibr">19</xref>], MSNet [<xref rid="r29" ref-type="bibr">29</xref>], SANet [<xref rid="r22" ref-type="bibr">22</xref>], M2SNet [<xref rid="r30" ref-type="bibr">30</xref>], Polyp-PVT [<xref rid="r33" ref-type="bibr">33</xref>], CFA-Net [<xref rid="r28" ref-type="bibr">28</xref>], ESFPNet [<xref rid="r37" ref-type="bibr">37</xref>], TranSEFusionNet [<xref rid="r43" ref-type="bibr">43</xref>] and ECTransNet [<xref rid="r44" ref-type="bibr">44</xref>]. Note that, for these methods, we performed retraining and testing using their publicly available codes and recommended parameter settings as well as default data augmentation methods reported in their original literatures. Among data augmentation strategies used in these models and our approach, there only existed slight differences. These slightly different augmentation methods did not significantly affect the comparison of models, which could ensure the relative fairness of comparisons.</p></sec><sec id="sec4-3-2"><label>4.3.2</label><title>Quantitative comparison</title><p><xref rid="t002" ref-type="table">Table&#x000a0;2</xref> provided the quantitative comparison results between our model and five advanced methods on both CVC-ClinicDB and Kvasir-SEG benchmark datasets in experiment A. On the CVC-ClinicDB benchmark, we could observe that our model outperformed all compared approaches by a large margin in terms of <inline-formula>
<mml:math id="m138" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:math>
</inline-formula> and <inline-formula>
<mml:math id="m139" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:mi>U</mml:mi></mml:math>
</inline-formula> metrics. Specifically, in comparison with DCNN-based approaches (such as U-Net [<xref rid="r11" ref-type="bibr">11</xref>], U-Net++ [<xref rid="r12" ref-type="bibr">12</xref>], and PraNet [<xref rid="r21" ref-type="bibr">21</xref>]), our model had more obvious advantages. For instance, our model brought considerable performance improvements of 3.7%, 10.6%, 1.6% and 4.3%, 15.2%, 2.2% in <inline-formula>
<mml:math id="m140" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:math>
</inline-formula> and <inline-formula>
<mml:math id="m141" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:mi>U</mml:mi></mml:math>
</inline-formula>, against U-Net, U-Net++ and PraNet, respectively. When compared to other Transformer-based methods (including SSFormer [<xref rid="r35" ref-type="bibr">35</xref>] and FCBFormer [<xref rid="r42" ref-type="bibr">42</xref>]), our model also manifested excellent learning ability, observing an increase of 2.0% and 3.1% in <inline-formula>
<mml:math id="m142" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:math>
</inline-formula> and <inline-formula>
<mml:math id="m143" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:mi>U</mml:mi></mml:math>
</inline-formula> over SSFormer, and an improvement of 1.5% and 1.4% against FCBFormer. In the Kvasir-SEG dataset, our model also consistently gained the best segmentation results in terms of <inline-formula>
<mml:math id="m144" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:math>
</inline-formula>. For example, our model achieved 0.6% and 1.3% gains over SSFormer with respect to <inline-formula>
<mml:math id="m145" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:math>
</inline-formula> and <inline-formula>
<mml:math id="m146" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:mi>U</mml:mi></mml:math>
</inline-formula>, respectively, while performed competitively (<inline-formula>
<mml:math id="m147" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:math>
</inline-formula>: 0.9253 vs 0.9231, <inline-formula>
<mml:math id="m148" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:mi>U</mml:mi></mml:math>
</inline-formula>: 0.8746 vs 0.8752) against FCBFormer. In addition, FCBFormer and SSFormer obtained relatively better segmentation results compared to the other DCNN-based approaches on both datasets. According to these results, the effectiveness of our model could be demonstrated.</p><table-wrap position="float" id="t002"><label>Table 2.</label><caption><title>Quantitative comparison results with different advanced approaches for polyp segmentation on the CVC-ClinicDB and Kvasir-SEG benchmarks in experiment A. The bold fonts denoted the best results.</title></caption><table frame="hsides" rules="all"><colgroup span="1"><?tabw 22pc?><col align="left" width="20%" span="1"/><col align="left" width="20%" span="1"/><col align="left" width="20%" span="1"/><col align="left" width="20%" span="1"/><col align="left" width="20%" span="1"/></colgroup><thead><tr><th valign="top" align="left" rowspan="3" colspan="1">Datasets</th><th valign="top" align="left" colspan="2" rowspan="1">CVC-ClinicDB [<xref rid="r52" ref-type="bibr">52</xref>]</th><th valign="top" align="left" colspan="2" rowspan="1">Kvasir-SEG [<xref rid="r51" ref-type="bibr">51</xref>]</th></tr><tr><th valign="top" align="center" colspan="4" rowspan="1">
<hr/>
</th></tr><tr><th valign="top" align="left" rowspan="1" colspan="1">mDice</th><th valign="top" align="left" rowspan="1" colspan="1">mIoU</th><th valign="top" align="left" rowspan="1" colspan="1">mDice</th><th valign="top" align="left" rowspan="1" colspan="1">mIoU</th></tr></thead><tbody><tr><td valign="top" align="left" rowspan="1" colspan="1">U-Net [<xref rid="r11" ref-type="bibr">11</xref>]</td><td valign="top" align="left" rowspan="1" colspan="1">0.9145</td><td valign="top" align="left" rowspan="1" colspan="1">0.8654</td><td valign="top" align="left" rowspan="1" colspan="1">0.8629</td><td valign="top" align="left" rowspan="1" colspan="1">0.8176</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">U-Net++ [<xref rid="r12" ref-type="bibr">12</xref>]</td><td valign="top" align="left" rowspan="1" colspan="1">0.8453</td><td valign="top" align="left" rowspan="1" colspan="1">0.7559</td><td valign="top" align="left" rowspan="1" colspan="1">0.7475</td><td valign="top" align="left" rowspan="1" colspan="1">0.6313</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">PraNet [<xref rid="r21" ref-type="bibr">21</xref>]</td><td valign="top" align="left" rowspan="1" colspan="1">0.9358</td><td valign="top" align="left" rowspan="1" colspan="1">0.8867</td><td valign="top" align="left" rowspan="1" colspan="1">0.9011</td><td valign="top" align="left" rowspan="1" colspan="1">0.8403</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">SSFormer [<xref rid="r35" ref-type="bibr">35</xref>]</td><td valign="top" align="left" rowspan="1" colspan="1">0.9318</td><td valign="top" align="left" rowspan="1" colspan="1">0.8777</td><td valign="top" align="left" rowspan="1" colspan="1">0.9196</td><td valign="top" align="left" rowspan="1" colspan="1">0.8616</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">FCBFormer [<xref rid="r42" ref-type="bibr">42</xref>]</td><td valign="top" align="left" rowspan="1" colspan="1">0.9362</td><td valign="top" align="left" rowspan="1" colspan="1">0.8943</td><td valign="top" align="left" rowspan="1" colspan="1">0.9231</td><td valign="top" align="left" rowspan="1" colspan="1">
<bold>0.8752</bold>
</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Ours</td><td valign="top" align="left" rowspan="1" colspan="1">
<bold>0.9514</bold></td><td valign="top" align="left" rowspan="1" colspan="1">
<bold>0.9083</bold></td><td valign="top" align="left" rowspan="1" colspan="1">
<bold>0.9253</bold></td><td valign="top" align="left" rowspan="1" colspan="1">0.8746</td></tr></tbody></table></table-wrap><p><xref rid="t003" ref-type="table">Table&#x000a0;3</xref> summarized the quantitative comparison results of our model against thirteen cutting-edge methods on the CVC-ClinicDB and Kvasir-SEG benchmarks in experiment B. From the comparison results in <xref rid="t003" ref-type="table">Table&#x000a0;3</xref>, it could be seen that our model likewise achieved considerable performance gains over other segmentation methods. In particular, comparing to SSFormer, our model attained 3.3%, 4%, 2.3%, 2.2%, and 1.8% score improvements on average in terms of <inline-formula>
<mml:math id="m149" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:math>
</inline-formula>, <inline-formula>
<mml:math id="m150" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:mi>U</mml:mi></mml:math>
</inline-formula>, <inline-formula>
<mml:math id="m151" display="inline" overflow="scroll"><mml:msubsup><mml:mi>F</mml:mi><mml:mi>&#x003b2;<!-- &#x003b2; --></mml:mi><mml:mi>w</mml:mi></mml:msubsup></mml:math>
</inline-formula>, <inline-formula>
<mml:math id="m152" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>&#x003b1;<!-- &#x003b1; --></mml:mi></mml:msub></mml:mrow></mml:math>
</inline-formula>, and <inline-formula>
<mml:math id="m153" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mi>&#x003d5;<!-- &#x003d5; --></mml:mi></mml:msub></mml:mrow></mml:math>
</inline-formula> on the CVC-ClinicDB dataset. Among all evaluation indicators, it performed the best on <inline-formula>
<mml:math id="m154" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:math>
</inline-formula>, <inline-formula>
<mml:math id="m155" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:mi>U</mml:mi></mml:math>
</inline-formula>, <inline-formula>
<mml:math id="m156" display="inline" overflow="scroll"><mml:msubsup><mml:mi>F</mml:mi><mml:mi>&#x003b2;<!-- &#x003b2; --></mml:mi><mml:mi>w</mml:mi></mml:msubsup></mml:math>
</inline-formula>, and <inline-formula>
<mml:math id="m157" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>&#x003b1;<!-- &#x003b1; --></mml:mi></mml:msub></mml:mrow></mml:math>
</inline-formula> metrics, reaching up to 0.939, 0.895, 0.936 and 0.950, respectively. On the Kvasir-SEG dataset, our model gained overall performance increases of 1.8%, 2.9%, 2.9%, 0.9% and 1.8% against FCBFormer, respectively, concerning <inline-formula>
<mml:math id="m158" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:math>
</inline-formula>, <inline-formula>
<mml:math id="m159" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:mi>U</mml:mi></mml:math>
</inline-formula>, <inline-formula>
<mml:math id="m160" display="inline" overflow="scroll"><mml:msubsup><mml:mi>F</mml:mi><mml:mi>&#x003b2;<!-- &#x003b2; --></mml:mi><mml:mi>w</mml:mi></mml:msubsup></mml:math>
</inline-formula>, <inline-formula>
<mml:math id="m161" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>&#x003b1;<!-- &#x003b1; --></mml:mi></mml:msub></mml:mrow></mml:math>
</inline-formula>, and <inline-formula>
<mml:math id="m162" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mi>&#x003d5;<!-- &#x003d5; --></mml:mi></mml:msub></mml:mrow></mml:math>
</inline-formula>. In contrast, the MAE score of our model reduced by 0.5%. Here, our model manifested best results (<inline-formula>
<mml:math id="m163" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:math>
</inline-formula>:0.922 and <inline-formula>
<mml:math id="m164" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:mi>U</mml:mi></mml:math>
</inline-formula>:0.874) in <inline-formula>
<mml:math id="m165" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:math>
</inline-formula> and <inline-formula>
<mml:math id="m166" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:mi>U</mml:mi></mml:math>
</inline-formula>, exceeding the Polyp-PVT by 0.9% and 1.1%, separately. Even when compared to recently developed state-of-the-art DCNN-based M2SNet, Transformer-based ESFPNet and hybrid CNN-Transformer-based TranSEFusionNet, our model still obtained consistently best segmentation results in all evaluation metrics on both datasets. Notably, our model was slightly inferior to recently proposed CFA-Net (<inline-formula>
<mml:math id="m167" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mi>&#x003d5;<!-- &#x003d5; --></mml:mi></mml:msub></mml:mrow></mml:math>
</inline-formula>:0.983 vs 0.989 and MAE:0.008 vs 0.007 on the CVC-ClinicDB dataset; <inline-formula>
<mml:math id="m168" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mi>&#x003d5;<!-- &#x003d5; --></mml:mi></mml:msub></mml:mrow></mml:math>
</inline-formula>:0.960 vs 0.962 and MAE:0.024 vs 0.023 on the Kvasir-SEG dataset) in <inline-formula>
<mml:math id="m169" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mi>&#x003d5;<!-- &#x003d5; --></mml:mi></mml:msub></mml:mrow></mml:math>
</inline-formula> and MAE indices, but it still maintained advantages over the CFA-Net in <inline-formula>
<mml:math id="m170" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:math>
</inline-formula>, <inline-formula>
<mml:math id="m171" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:mi>U</mml:mi></mml:math>
</inline-formula>, <inline-formula>
<mml:math id="m172" display="inline" overflow="scroll"><mml:msubsup><mml:mi>F</mml:mi><mml:mi>&#x003b2;<!-- &#x003b2; --></mml:mi><mml:mi>w</mml:mi></mml:msubsup></mml:math>
</inline-formula>, and <inline-formula>
<mml:math id="m173" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>&#x003b1;<!-- &#x003b1; --></mml:mi></mml:msub></mml:mrow></mml:math>
</inline-formula>. Further, on the CVC-ClinicDB dataset, our model brought significant gains of 1.6% and 1.7% in <inline-formula>
<mml:math id="m174" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:math>
</inline-formula> and <inline-formula>
<mml:math id="m175" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:mi>U</mml:mi></mml:math>
</inline-formula> as comparison with the ECTransNet, respectively, whereas maintained excellent segmentation performance consistent with the MCSF-Net (<inline-formula>
<mml:math id="m176" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:math>
</inline-formula>: 0.939 vs 0.941, and <inline-formula>
<mml:math id="m177" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:mi>U</mml:mi></mml:math>
</inline-formula>: 0.895 vs 0.895). On the Kvasir-SEG dataset, our model consistently surpassed the ECTransNet and the MCSF-Net, with 2.1% and 1.1% increases in <inline-formula>
<mml:math id="m178" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:math>
</inline-formula> and 2.7% and 1.3% boosts in <inline-formula>
<mml:math id="m179" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:mi>U</mml:mi></mml:math>
</inline-formula>, respectively. On the basis of these results, we could conclude that our model had relatively better overall segmentation performance against the highly competitive ECTransNet and MCSF-Net. Again, these results indicated that our model had strong feature learning power, proving its superiority.</p><table-wrap position="float" id="t003"><label>Table 3.</label><caption><title>Quantitative comparison results with different advanced approaches for polyp segmentation on the CVC-ClinicDB and Kvasir-SEG benchmarks in experiment B. The bold fonts denoted the best results.</title></caption><table frame="hsides" rules="all"><colgroup span="1"><col align="left" width="14%" span="1"/><col align="left" width="14%" span="1"/><col align="left" width="14%" span="1"/><col align="left" width="14%" span="1"/><col align="left" width="14%" span="1"/><col align="left" width="14%" span="1"/><col align="left" width="14%" span="1"/></colgroup><thead><tr><th valign="top" align="left" rowspan="3" colspan="1">Datasets</th><th valign="top" align="left" colspan="6" rowspan="1">CVC-ClinicDB [<xref rid="r52" ref-type="bibr">52</xref>]</th></tr><tr><th valign="top" align="center" colspan="6" rowspan="1">
<hr/>
</th></tr><tr><th valign="top" align="left" rowspan="1" colspan="1">mDice</th><th valign="top" align="left" rowspan="1" colspan="1">mIoU</th><th valign="top" align="left" rowspan="1" colspan="1">&#x000a0;<inline-formula>
<mml:math id="m180" display="inline" overflow="scroll"><mml:msubsup><mml:mi>F</mml:mi><mml:mi>&#x003b2;<!-- &#x003b2; --></mml:mi><mml:mi>&#x003c9;<!-- &#x003c9; --></mml:mi></mml:msubsup></mml:math>
</inline-formula>
</th><th valign="top" align="left" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="m181" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>&#x003b1;<!-- &#x003b1; --></mml:mi></mml:msub></mml:mrow></mml:math>
</inline-formula>
</th><th valign="top" align="left" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="m182" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mi mathvariant="normal">&#x02205;<!-- &#x02205; --></mml:mi></mml:msub></mml:mrow></mml:math>
</inline-formula>
</th><th valign="top" align="left" rowspan="1" colspan="1">MAE</th></tr></thead><tbody><tr><td valign="top" align="left" rowspan="1" colspan="1">SFA [<xref rid="r19" ref-type="bibr">19</xref>]</td><td valign="top" align="left" rowspan="1" colspan="1">0.700</td><td valign="top" align="left" rowspan="1" colspan="1">0.607</td><td valign="top" align="left" rowspan="1" colspan="1">0.647</td><td valign="top" align="left" rowspan="1" colspan="1">0.793</td><td valign="top" align="left" rowspan="1" colspan="1">0.840</td><td valign="top" align="left" rowspan="1" colspan="1">0.042</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">PraNet [<xref rid="r21" ref-type="bibr">21</xref>]</td><td valign="top" align="left" rowspan="1" colspan="1">0.899</td><td valign="top" align="left" rowspan="1" colspan="1">0.849</td><td valign="top" align="left" rowspan="1" colspan="1">0.896</td><td valign="top" align="left" rowspan="1" colspan="1">0.936</td><td valign="top" align="left" rowspan="1" colspan="1">0.963</td><td valign="top" align="left" rowspan="1" colspan="1">0.009</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">MSNet [<xref rid="r29" ref-type="bibr">29</xref>]</td><td valign="top" align="left" rowspan="1" colspan="1">0.918</td><td valign="top" align="left" rowspan="1" colspan="1">0.869</td><td valign="top" align="left" rowspan="1" colspan="1">0.913</td><td valign="top" align="left" rowspan="1" colspan="1">0.946</td><td valign="top" align="left" rowspan="1" colspan="1">0.973</td><td valign="top" align="left" rowspan="1" colspan="1">0.008</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">SANet [<xref rid="r22" ref-type="bibr">22</xref>]</td><td valign="top" align="left" rowspan="1" colspan="1">0.916</td><td valign="top" align="left" rowspan="1" colspan="1">0.859</td><td valign="top" align="left" rowspan="1" colspan="1">0.909</td><td valign="top" align="left" rowspan="1" colspan="1">0.939</td><td valign="top" align="left" rowspan="1" colspan="1">0.971</td><td valign="top" align="left" rowspan="1" colspan="1">0.012</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">SSFormer [<xref rid="r35" ref-type="bibr">35</xref>]</td><td valign="top" align="left" rowspan="1" colspan="1">0.906</td><td valign="top" align="left" rowspan="1" colspan="1">0.855</td><td valign="top" align="left" rowspan="1" colspan="1">0.913</td><td valign="top" align="left" rowspan="1" colspan="1">0.928</td><td valign="top" align="left" rowspan="1" colspan="1">0.965</td><td valign="top" align="left" rowspan="1" colspan="1">0.008</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">FCBFormer [<xref rid="r42" ref-type="bibr">42</xref>]</td><td valign="top" align="left" rowspan="1" colspan="1">0.934</td><td valign="top" align="left" rowspan="1" colspan="1">0.883</td><td valign="top" align="left" rowspan="1" colspan="1">0.932</td><td valign="top" align="left" rowspan="1" colspan="1">0.946</td><td valign="top" align="left" rowspan="1" colspan="1">0.978</td><td valign="top" align="left" rowspan="1" colspan="1">0.010</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">M<sup>2</sup>SNet [<xref rid="r30" ref-type="bibr">30</xref>]</td><td valign="top" align="left" rowspan="1" colspan="1">0.919</td><td valign="top" align="left" rowspan="1" colspan="1">0.870</td><td valign="top" align="left" rowspan="1" colspan="1">0.917</td><td valign="top" align="left" rowspan="1" colspan="1">0.945</td><td valign="top" align="left" rowspan="1" colspan="1">0.974</td><td valign="top" align="left" rowspan="1" colspan="1">0.009</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Polyp-PVT [<xref rid="r33" ref-type="bibr">33</xref>]</td><td valign="top" align="left" rowspan="1" colspan="1">0.932</td><td valign="top" align="left" rowspan="1" colspan="1">0.889</td><td valign="top" align="left" rowspan="1" colspan="1">0.933</td><td valign="top" align="left" rowspan="1" colspan="1">0.948</td><td valign="top" align="left" rowspan="1" colspan="1">0.982</td><td valign="top" align="left" rowspan="1" colspan="1">0.008</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">CFA-Net [<xref rid="r28" ref-type="bibr">28</xref>]</td><td valign="top" align="left" rowspan="1" colspan="1">0.933</td><td valign="top" align="left" rowspan="1" colspan="1">0.883</td><td valign="top" align="left" rowspan="1" colspan="1">0.924</td><td valign="top" align="left" rowspan="1" colspan="1">
<bold>0.950</bold>
</td><td valign="top" align="left" rowspan="1" colspan="1">
<bold>0.989</bold></td><td valign="top" align="left" rowspan="1" colspan="1">
<bold>0.007</bold></td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">ESFPNet [<xref rid="r37" ref-type="bibr">37</xref>]</td><td valign="top" align="left" rowspan="1" colspan="1">0.908</td><td valign="top" align="left" rowspan="1" colspan="1">0.857</td><td valign="top" align="left" rowspan="1" colspan="1">0.907</td><td valign="top" align="left" rowspan="1" colspan="1">0.931</td><td valign="top" align="left" rowspan="1" colspan="1">0.962</td><td valign="top" align="left" rowspan="1" colspan="1">0.010</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">TranSEFusionNet [<xref rid="r43" ref-type="bibr">43</xref>]</td><td valign="top" align="left" rowspan="1" colspan="1">0.744</td><td valign="top" align="left" rowspan="1" colspan="1">0.654</td><td valign="top" align="left" rowspan="1" colspan="1">0.712</td><td valign="top" align="left" rowspan="1" colspan="1">0.841</td><td valign="top" align="left" rowspan="1" colspan="1">0.879</td><td valign="top" align="left" rowspan="1" colspan="1">0.033</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">ECTransNet [<xref rid="r44" ref-type="bibr">44</xref>]</td><td valign="top" align="left" rowspan="1" colspan="1">0.923</td><td valign="top" align="left" rowspan="1" colspan="1">0.878</td><td valign="top" align="left" rowspan="1" colspan="1">0.926</td><td valign="top" align="left" rowspan="1" colspan="1">
<bold>0.950</bold></td><td valign="top" align="left" rowspan="1" colspan="1">0.976</td><td valign="top" align="left" rowspan="1" colspan="1">0.011</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">MCSF-Net [<xref rid="r31" ref-type="bibr">31</xref>]</td><td valign="top" align="left" rowspan="1" colspan="1">
<bold>0.941</bold></td><td valign="top" align="left" rowspan="1" colspan="1">
<bold>0.895</bold></td><td valign="top" align="left" rowspan="1" colspan="1">0.925</td><td valign="top" align="left" rowspan="1" colspan="1">0.945</td><td valign="top" align="left" rowspan="1" colspan="1">0.973</td><td valign="top" align="left" rowspan="1" colspan="1">0.010</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Ours</td><td valign="top" align="left" rowspan="1" colspan="1">0.939</td><td valign="top" align="left" rowspan="1" colspan="1">
<bold>0.895</bold></td><td valign="top" align="left" rowspan="1" colspan="1">
<bold>0.936</bold></td><td valign="top" align="left" rowspan="1" colspan="1">
<bold>0.950</bold></td><td valign="top" align="left" rowspan="1" colspan="1">0.983</td><td valign="top" align="left" rowspan="1" colspan="1">0.008</td></tr><tr><td valign="top" align="center" colspan="7" rowspan="1">
<hr/>
</td></tr><tr><td valign="top" align="left" rowspan="2" colspan="1">Datasets</td><td valign="top" align="left" colspan="6" rowspan="1">Kvasir-SEG [<xref rid="r51" ref-type="bibr">51</xref>]<hr/></td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">mDice</td><td valign="top" align="left" rowspan="1" colspan="1">mIoU</td><td valign="top" align="left" rowspan="1" colspan="1">&#x000a0;<inline-formula>
<mml:math id="m183" display="inline" overflow="scroll"><mml:msubsup><mml:mi>F</mml:mi><mml:mi>&#x003b2;<!-- &#x003b2; --></mml:mi><mml:mi>&#x003c9;<!-- &#x003c9; --></mml:mi></mml:msubsup></mml:math>
</inline-formula></td><td valign="top" align="left" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="m184" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>&#x003b1;<!-- &#x003b1; --></mml:mi></mml:msub></mml:mrow></mml:math>
</inline-formula>
</td><td valign="top" align="left" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="m185" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mi mathvariant="normal">&#x02205;<!-- &#x02205; --></mml:mi></mml:msub></mml:mrow></mml:math>
</inline-formula>
</td><td valign="top" align="left" rowspan="1" colspan="1">MAE</td></tr><tr><td valign="top" align="center" colspan="7" rowspan="1">
<hr/>
</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">SFA [<xref rid="r19" ref-type="bibr">19</xref>]</td><td valign="top" align="left" rowspan="1" colspan="1">0.723</td><td valign="top" align="left" rowspan="1" colspan="1">0.611</td><td valign="top" align="left" rowspan="1" colspan="1">0.670</td><td valign="top" align="left" rowspan="1" colspan="1">0.782</td><td valign="top" align="left" rowspan="1" colspan="1">0.834</td><td valign="top" align="left" rowspan="1" colspan="1">0.075</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">PraNet [<xref rid="r21" ref-type="bibr">21</xref>]</td><td valign="top" align="left" rowspan="1" colspan="1">0.898</td><td valign="top" align="left" rowspan="1" colspan="1">0.840</td><td valign="top" align="left" rowspan="1" colspan="1">0.885</td><td valign="top" align="left" rowspan="1" colspan="1">0.915</td><td valign="top" align="left" rowspan="1" colspan="1">0.944</td><td valign="top" align="left" rowspan="1" colspan="1">0.030</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">MSNet [<xref rid="r29" ref-type="bibr">29</xref>]</td><td valign="top" align="left" rowspan="1" colspan="1">0.905</td><td valign="top" align="left" rowspan="1" colspan="1">0.849</td><td valign="top" align="left" rowspan="1" colspan="1">0.892</td><td valign="top" align="left" rowspan="1" colspan="1">0.923</td><td valign="top" align="left" rowspan="1" colspan="1">0.947</td><td valign="top" align="left" rowspan="1" colspan="1">0.028</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">SANet [<xref rid="r22" ref-type="bibr">22</xref>]</td><td valign="top" align="left" rowspan="1" colspan="1">0.904</td><td valign="top" align="left" rowspan="1" colspan="1">0.847</td><td valign="top" align="left" rowspan="1" colspan="1">0.892</td><td valign="top" align="left" rowspan="1" colspan="1">0.915</td><td valign="top" align="left" rowspan="1" colspan="1">0.949</td><td valign="top" align="left" rowspan="1" colspan="1">0.028</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">SSFormer [<xref rid="r35" ref-type="bibr">35</xref>]</td><td valign="top" align="left" rowspan="1" colspan="1">0.917</td><td valign="top" align="left" rowspan="1" colspan="1">0.864</td><td valign="top" align="left" rowspan="1" colspan="1">
<bold>0.916</bold></td><td valign="top" align="left" rowspan="1" colspan="1">0.922</td><td valign="top" align="left" rowspan="1" colspan="1">0.958</td><td valign="top" align="left" rowspan="1" colspan="1">
<bold>0.022</bold></td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">FCBFormer [<xref rid="r42" ref-type="bibr">42</xref>]</td><td valign="top" align="left" rowspan="1" colspan="1">0.904</td><td valign="top" align="left" rowspan="1" colspan="1">0.845</td><td valign="top" align="left" rowspan="1" colspan="1">0.880</td><td valign="top" align="left" rowspan="1" colspan="1">0.916</td><td valign="top" align="left" rowspan="1" colspan="1">0.942</td><td valign="top" align="left" rowspan="1" colspan="1">0.029</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">M<sup>2</sup>SNet [<xref rid="r30" ref-type="bibr">30</xref>]</td><td valign="top" align="left" rowspan="1" colspan="1">0.908</td><td valign="top" align="left" rowspan="1" colspan="1">0.852</td><td valign="top" align="left" rowspan="1" colspan="1">0.901</td><td valign="top" align="left" rowspan="1" colspan="1">0.922</td><td valign="top" align="left" rowspan="1" colspan="1">0.950</td><td valign="top" align="left" rowspan="1" colspan="1">0.025</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Polyp-PVT [<xref rid="r33" ref-type="bibr">33</xref>]</td><td valign="top" align="left" rowspan="1" colspan="1">0.913</td><td valign="top" align="left" rowspan="1" colspan="1">0.863</td><td valign="top" align="left" rowspan="1" colspan="1">0.910</td><td valign="top" align="left" rowspan="1" colspan="1">0.924</td><td valign="top" align="left" rowspan="1" colspan="1">0.956</td><td valign="top" align="left" rowspan="1" colspan="1">0.023</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">CFA-Net [<xref rid="r28" ref-type="bibr">28</xref>]</td><td valign="top" align="left" rowspan="1" colspan="1">0.915</td><td valign="top" align="left" rowspan="1" colspan="1">0.861</td><td valign="top" align="left" rowspan="1" colspan="1">0.903</td><td valign="top" align="left" rowspan="1" colspan="1">0.924</td><td valign="top" align="left" rowspan="1" colspan="1">
<bold>0.962</bold></td><td valign="top" align="left" rowspan="1" colspan="1">0.023</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">ESFPNet [<xref rid="r37" ref-type="bibr">37</xref>]</td><td valign="top" align="left" rowspan="1" colspan="1">0.880</td><td valign="top" align="left" rowspan="1" colspan="1">0.812</td><td valign="top" align="left" rowspan="1" colspan="1">0.870</td><td valign="top" align="left" rowspan="1" colspan="1">0.891</td><td valign="top" align="left" rowspan="1" colspan="1">0.933</td><td valign="top" align="left" rowspan="1" colspan="1">0.040</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">TranSEFusionNet [<xref rid="r43" ref-type="bibr">43</xref>]</td><td valign="top" align="left" rowspan="1" colspan="1">0.783</td><td valign="top" align="left" rowspan="1" colspan="1">0.686</td><td valign="top" align="left" rowspan="1" colspan="1">0.744</td><td valign="top" align="left" rowspan="1" colspan="1">0.842</td><td valign="top" align="left" rowspan="1" colspan="1">0.878</td><td valign="top" align="left" rowspan="1" colspan="1">0.058</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">ECTransNet [<xref rid="r44" ref-type="bibr">44</xref>]</td><td valign="top" align="left" rowspan="1" colspan="1">0.901</td><td valign="top" align="left" rowspan="1" colspan="1">0.847</td><td valign="top" align="left" rowspan="1" colspan="1">0.873</td><td valign="top" align="left" rowspan="1" colspan="1">0.913</td><td valign="top" align="left" rowspan="1" colspan="1">0.941</td><td valign="top" align="left" rowspan="1" colspan="1">0.032</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">MCSF-Net [<xref rid="r31" ref-type="bibr">31</xref>]</td><td valign="top" align="left" rowspan="1" colspan="1">0.911</td><td valign="top" align="left" rowspan="1" colspan="1">0.861</td><td valign="top" align="left" rowspan="1" colspan="1">0.880</td><td valign="top" align="left" rowspan="1" colspan="1">0.915</td><td valign="top" align="left" rowspan="1" colspan="1">0.943</td><td valign="top" align="left" rowspan="1" colspan="1">0.030</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Ours</td><td valign="top" align="left" rowspan="1" colspan="1">
<bold>0.922</bold></td><td valign="top" align="left" rowspan="1" colspan="1">
<bold>0.874</bold></td><td valign="top" align="left" rowspan="1" colspan="1">0.909</td><td valign="top" align="left" rowspan="1" colspan="1">
<bold>0.925</bold></td><td valign="top" align="left" rowspan="1" colspan="1">0.960</td><td valign="top" align="left" rowspan="1" colspan="1">0.024</td></tr></tbody></table></table-wrap></sec><sec id="sec4-3-3"><label>4.3.3</label><title>Qualitative comparison</title><p><xref rid="g007" ref-type="fig">Figure&#x000a0;7</xref> depicted the qualitative comparison results of our model over five cutting-edge polyp segmentation approaches in experiment A. On the basis of the visualizable comparison results, it could be observed that the results of our model were in most agreement with the ground truths, while surpassed the other compared approaches under different challenging conditions. Concretely, for small size of polyps in the first and second rows, our model could perform accurate segmentation on them. Nevertheless, U-Net++ completely failed to segment them, and U-Net produced some over-segmented regions. In this case, FCBFormer, SSFormer and PraNet generated several un-related or over-segmented areas, resulting from the neglect of the edge feature details from the deep characteristics. With respect to polyps with variable shapes and large sizes in the third and fourth rows, U-Net and U-Net++ manifested worse than the other methods, while FCBFormer, SSFormer and PraNet produced some errors with un-related or under-segmented regions. In the case of blurred boundaries between the polyps and background in the fifth and sixth rows making it more challenging to identify them, our model was able to focus more on the boundary details and segment them more accurately by contrast to the other methods. On the contrary, U-Net and U-Net++ conducted the worst among these methods. And yet, the predicted edges by PraNet were overly smooth, whilst FCBFormer and SSFormer missed some fine details. In a nutshell, the qualitative comparison results in experiment A demonstrated that our model had a good capability to deal with different challenging cases with respect to polyp segmentation.</p><fig position="float" id="g007" fig-type="figure"><label>Fig. 7.</label><caption><p>Qualitative comparisons of polyp segmentation results obtained from different cutting-edge approaches in experiment A, encompassing FCBFormer [<xref rid="r42" ref-type="bibr">42</xref>], SSFormer [<xref rid="r35" ref-type="bibr">35</xref>], PraNet [<xref rid="r21" ref-type="bibr">21</xref>], U-Net++ [<xref rid="r12" ref-type="bibr">12</xref>] and U-Net [<xref rid="r11" ref-type="bibr">11</xref>].</p></caption><graphic xlink:href="boe-15-4-2590-g007" position="float"/></fig><p>Moreover, we also displayed prediction results of polyp segmentation from our model and recent cutting-edge methods in experiment B, as shown in <xref rid="g008" ref-type="fig">Fig.&#x000a0;8</xref>. As can be seen in <xref rid="g008" ref-type="fig">Fig.&#x000a0;8</xref>, our model provided consistently best performance under various challenging cases, which kept the polyp boundary segmentation intact and clear while decreasing the miss segmentation inside the polyp. Among these advanced methods, SFA performed almost the worst and completely failed to segment polyps under different challenges. When polyp regions were small (such as in the second row), TranSEFusionNet, ESFPNet, Polyp-PVT, M<sup>2</sup>SNet, FCBFormer, SSFormer and PraNet predicted some un-related segmentation areas. In segmentation tasks of polyps with irregularity shapes and large sizes in the third and fourth rows, SSFormer and SANet completely failed in segmenting them, whereas CFA-Net, Polyp-PVT, M<sup>2</sup>SNet, and FCBFormer yielded some incorrect segmentation regions. As for TranSEFusionNet and ESFPNet, there still existed some incomplete and false positive situations. When the edges of the polyp areas were similar to the surrounding tissues in the fifth and sixth rows, ESFPNet encountered some mis-segmentations and PraNet produced over-segmented areas, while CFA-Net, M<sup>2</sup>SNet and MSNet experienced fuzzy boundaries resulting from the loss of fine edge feature details. In this context, TranSEFusionNet manifested incomplete segmentation and Polyp-PVT only discriminated a portion of polyp area and suffered from under-segmentation, whilst FCBFormer as well as SSFormer neglected some fine detailed feature information. Conversely, the segmented mask predicted by our model held a best agreement with the ground truth under all these challenging conditions. Simultaneously, we could also notice from <xref rid="g008" ref-type="fig">Fig.&#x000a0;8</xref> that when coping with extremely small polyp lesions (such as in the second row), the ECTransNet and MCSF-Net produced some un-related segmentation regions. In addition, when facing polyps with irregularity shapes in the third row or accompanied by excessive mucus along with the indistinct demarcation with the surrounding tissues (See the sixth row), the ECTransNet tended to overlook a small portion of polyp lesion area while the MCSF-Net failed to correctly outline polyp samples, leading to incomplete segmentation. In contrast, our model delineated more accurate segmentation regions of polyp lesions with fine boundaries in tackling different challenging scenes, which closely aligned with the ground truth maps and showcased its superior identification ability. In summary, the qualitative results of polyp segmentation in experiment B signified again that our model was owned to the outstanding strength in accurately predicting the polyp segmentation masks under varied size, shape and contrast scenarios.</p><fig position="float" id="g008" fig-type="figure"><label>Fig. 8.</label><caption><p>Qualitative comparisons of polyp segmentation results generated by our model and thirteen different state-of-the-art methods in experiment B, containing ECTransNet [<xref rid="r44" ref-type="bibr">44</xref>], MCSF-Net [<xref rid="r31" ref-type="bibr">31</xref>], TranSEFusionNet [<xref rid="r43" ref-type="bibr">43</xref>], ESFPNet [<xref rid="r37" ref-type="bibr">37</xref>], CFA-Net [<xref rid="r28" ref-type="bibr">28</xref>], Polyp-PVT [<xref rid="r33" ref-type="bibr">33</xref>], M2SNet [<xref rid="r30" ref-type="bibr">30</xref>], FCBFormer [<xref rid="r42" ref-type="bibr">42</xref>], SSFormer [<xref rid="r35" ref-type="bibr">35</xref>], SANet [<xref rid="r22" ref-type="bibr">22</xref>], MSNet [<xref rid="r29" ref-type="bibr">29</xref>], PraNet [<xref rid="r21" ref-type="bibr">21</xref>] and SFA [<xref rid="r19" ref-type="bibr">19</xref>].</p></caption><graphic xlink:href="boe-15-4-2590-g008" position="float"/></fig></sec></sec><sec id="sec4-4"><label>4.4</label><title>Generalization ability</title><p>We measured the generalization ability of the proposed Dua-PSNet using two cross-dataset testing schemes. The one was to use the CVC-ClinicDB/Kvasir-SEG training set to train our model and test on the corresponding unseen Kvasir-SEG/CVC-ClinicDB benchmarks, which was referred to testing scheme A (See <xref rid="t004" ref-type="table">Table&#x000a0;4</xref>). The other one was to utilize the training set built from part of the CVC-ClinicDB and Kvasir-SEG benchmarks for training, while evaluate the trained model on the unseen CVC-ConlonDB, ETIS, and CVC-300 benchmark datasets, respectively, which was used as testing scheme B (See <xref rid="t005" ref-type="table">Table&#x000a0;5</xref>). As can be seen in <xref rid="t004" ref-type="table">Table&#x000a0;4</xref>, U-Net and U-Net ++ owed poor generalization ability on both datasets, especially U-Net++ decreased sharply in <inline-formula>
<mml:math id="m186" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:math>
</inline-formula> and <inline-formula>
<mml:math id="m187" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:mi>U</mml:mi></mml:math>
</inline-formula> metrics. Relative to the SSFormer, our model attained considerable performance improvements, with <inline-formula>
<mml:math id="m188" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:math>
</inline-formula> by 7.9% and <inline-formula>
<mml:math id="m189" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:mi>U</mml:mi></mml:math>
</inline-formula> by 9.5% on the Kvasir-SEG dataset, and <inline-formula>
<mml:math id="m190" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:math>
</inline-formula> by 12.4% and <inline-formula>
<mml:math id="m191" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:mi>U</mml:mi></mml:math>
</inline-formula> by 13.5% on the CVC-ClinicDB benchmark. In addition, our model exhibited the best generalization power on the CVC-ClinicDB dataset, reaching up to 0.9203 <inline-formula>
<mml:math id="m192" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:math>
</inline-formula> and 0.8582 <inline-formula>
<mml:math id="m193" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:mi>U</mml:mi></mml:math>
</inline-formula>. These results illustrated that our model exhibited particular advantages in handling images from a somewhat divergent distribution over that considered during training.</p><table-wrap position="float" id="t004"><label>Table 4.</label><caption><title>Generalizability comparisons of the proposed Dua-PSNet with current mainstream approaches in the testing scheme A. The bold fonts denoted the best results.</title></caption><table frame="hsides" rules="all"><colgroup span="1"><col align="left" width="20%" span="1"/><col align="left" width="20%" span="1"/><col align="left" width="20%" span="1"/><col align="left" width="20%" span="1"/><col align="left" width="20%" span="1"/></colgroup><thead><tr><th valign="top" align="left" rowspan="1" colspan="1">TrainDatasets</th><th valign="top" align="left" colspan="2" rowspan="1">CVC-ClinicDB [<xref rid="r52" ref-type="bibr">52</xref>]</th><th valign="top" align="left" colspan="2" rowspan="1">Kvasir-SEG [<xref rid="r51" ref-type="bibr">51</xref>]</th></tr><tr><th valign="top" align="center" colspan="5" rowspan="1">
<hr/>
</th></tr><tr><th valign="top" align="left" rowspan="1" colspan="1">TestDatasets</th><th valign="top" align="left" colspan="2" rowspan="1">Kvasir-SEG [<xref rid="r51" ref-type="bibr">51</xref>]</th><th valign="top" align="left" colspan="2" rowspan="1">CVC-ClinicDB [<xref rid="r52" ref-type="bibr">52</xref>]</th></tr><tr><th valign="top" align="center" colspan="5" rowspan="1">
<hr/>
</th></tr><tr><th valign="top" align="left" rowspan="1" colspan="1">-</th><th valign="top" align="left" rowspan="1" colspan="1">mDice</th><th valign="top" align="left" rowspan="1" colspan="1">mIoU</th><th valign="top" align="left" rowspan="1" colspan="1">mDice</th><th valign="top" align="left" rowspan="1" colspan="1">mIoU</th></tr></thead><tbody><tr><td valign="top" align="left" rowspan="1" colspan="1">U-Net [<xref rid="r11" ref-type="bibr">11</xref>]</td><td valign="top" align="left" rowspan="1" colspan="1">0.6222</td><td valign="top" align="left" rowspan="1" colspan="1">0.4588</td><td valign="top" align="left" rowspan="1" colspan="1">0.7172</td><td valign="top" align="left" rowspan="1" colspan="1">0.6133</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">U-Net++ [<xref rid="r12" ref-type="bibr">12</xref>]</td><td valign="top" align="left" rowspan="1" colspan="1">0.5926</td><td valign="top" align="left" rowspan="1" colspan="1">0.4564</td><td valign="top" align="left" rowspan="1" colspan="1">0.4265</td><td valign="top" align="left" rowspan="1" colspan="1">0.3345</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">PraNet [<xref rid="r21" ref-type="bibr">21</xref>]</td><td valign="top" align="left" rowspan="1" colspan="1">0.7950</td><td valign="top" align="left" rowspan="1" colspan="1">0.7073</td><td valign="top" align="left" rowspan="1" colspan="1">0.7912</td><td valign="top" align="left" rowspan="1" colspan="1">0.7119</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">SSFormer [<xref rid="r35" ref-type="bibr">35</xref>]</td><td valign="top" align="left" rowspan="1" colspan="1">0.7790</td><td valign="top" align="left" rowspan="1" colspan="1">0.6977</td><td valign="top" align="left" rowspan="1" colspan="1">0.7966</td><td valign="top" align="left" rowspan="1" colspan="1">0.7229</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">FCBFormer [<xref rid="r42" ref-type="bibr">42</xref>]</td><td valign="top" align="left" rowspan="1" colspan="1">0.8514</td><td valign="top" align="left" rowspan="1" colspan="1">0.7803</td><td valign="top" align="left" rowspan="1" colspan="1">0.9070</td><td valign="top" align="left" rowspan="1" colspan="1">0.8470</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Ours</td><td valign="top" align="left" rowspan="1" colspan="1">
<bold>0</bold>
<bold>.</bold>
<bold>8576</bold>
</td><td valign="top" align="left" rowspan="1" colspan="1">
<bold>0.7929</bold></td><td valign="top" align="left" rowspan="1" colspan="1">
<bold>0.9203</bold></td><td valign="top" align="left" rowspan="1" colspan="1">
<bold>0.8582</bold></td></tr></tbody></table></table-wrap><table-wrap position="float" id="t005"><label>Table 5.</label><caption><title>Generalizability comparisons of the proposed Dua-PSNet with current mainstream approaches in the testing scheme B. The bold fonts denoted the best results.</title></caption><table frame="hsides" rules="all"><?pag \renewcommand{\arraystretch}{1.33}?><colgroup span="1"><col align="left" width="14%" span="1"/><col align="left" width="14%" span="1"/><col align="left" width="14%" span="1"/><col align="left" width="14%" span="1"/><col align="left" width="14%" span="1"/><col align="left" width="14%" span="1"/><col align="left" width="14%" span="1"/></colgroup><thead><tr><th valign="top" align="left" rowspan="3" colspan="1">Datasets</th><th valign="top" align="left" colspan="6" rowspan="1">CVC-ColonDB [<xref rid="r6" ref-type="bibr">6</xref>]</th></tr><tr><th valign="top" align="center" colspan="6" rowspan="1">
<hr/>
</th></tr><tr><th valign="top" align="left" rowspan="1" colspan="1">mDice</th><th valign="top" align="left" rowspan="1" colspan="1">mIoU</th><th valign="top" align="left" rowspan="1" colspan="1">&#x000a0;<inline-formula>
<mml:math id="m194" display="inline" overflow="scroll"><mml:msubsup><mml:mi>F</mml:mi><mml:mi>&#x003b2;<!-- &#x003b2; --></mml:mi><mml:mi>&#x003c9;<!-- &#x003c9; --></mml:mi></mml:msubsup></mml:math>
</inline-formula>
</th><th valign="top" align="left" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="m195" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>&#x003b1;<!-- &#x003b1; --></mml:mi></mml:msub></mml:mrow></mml:math>
</inline-formula>
</th><th valign="top" align="left" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="m196" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mi mathvariant="normal">&#x02205;<!-- &#x02205; --></mml:mi></mml:msub></mml:mrow></mml:math>
</inline-formula>
</th><th valign="top" align="left" rowspan="1" colspan="1">MAE</th></tr></thead><tbody><tr><td valign="top" align="left" rowspan="1" colspan="1">SFA [<xref rid="r19" ref-type="bibr">19</xref>]</td><td valign="top" align="left" rowspan="1" colspan="1">0.456</td><td valign="top" align="left" rowspan="1" colspan="1">0.337</td><td valign="top" align="left" rowspan="1" colspan="1">0.366</td><td valign="top" align="left" rowspan="1" colspan="1">0.628</td><td valign="top" align="left" rowspan="1" colspan="1">0.661</td><td valign="top" align="left" rowspan="1" colspan="1">0.094</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">PraNet [<xref rid="r21" ref-type="bibr">21</xref>]</td><td valign="top" align="left" rowspan="1" colspan="1">0.712</td><td valign="top" align="left" rowspan="1" colspan="1">0.640</td><td valign="top" align="left" rowspan="1" colspan="1">0.699</td><td valign="top" align="left" rowspan="1" colspan="1">0.820</td><td valign="top" align="left" rowspan="1" colspan="1">0.847</td><td valign="top" align="left" rowspan="1" colspan="1">0.043</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">MSNet [<xref rid="r29" ref-type="bibr">29</xref>]</td><td valign="top" align="left" rowspan="1" colspan="1">0.751</td><td valign="top" align="left" rowspan="1" colspan="1">0.671</td><td valign="top" align="left" rowspan="1" colspan="1">0.736</td><td valign="top" align="left" rowspan="1" colspan="1">0.838</td><td valign="top" align="left" rowspan="1" colspan="1">0.872</td><td valign="top" align="left" rowspan="1" colspan="1">0.041</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">SANet [<xref rid="r22" ref-type="bibr">22</xref>]</td><td valign="top" align="left" rowspan="1" colspan="1">0.753</td><td valign="top" align="left" rowspan="1" colspan="1">0.670</td><td valign="top" align="left" rowspan="1" colspan="1">0.726</td><td valign="top" align="left" rowspan="1" colspan="1">0.837</td><td valign="top" align="left" rowspan="1" colspan="1">0.869</td><td valign="top" align="left" rowspan="1" colspan="1">0.043</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">SSFormer [<xref rid="r35" ref-type="bibr">35</xref>]</td><td valign="top" align="left" rowspan="1" colspan="1">0.772</td><td valign="top" align="left" rowspan="1" colspan="1">0.697</td><td valign="top" align="left" rowspan="1" colspan="1">0.766</td><td valign="top" align="left" rowspan="1" colspan="1">0.843</td><td valign="top" align="left" rowspan="1" colspan="1">0.880</td><td valign="top" align="left" rowspan="1" colspan="1">0.036</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">FCBFormer [<xref rid="r42" ref-type="bibr">42</xref>]</td><td valign="top" align="left" rowspan="1" colspan="1">0.798</td><td valign="top" align="left" rowspan="1" colspan="1">0.723</td><td valign="top" align="left" rowspan="1" colspan="1">0.775</td><td valign="top" align="left" rowspan="1" colspan="1">0.850</td><td valign="top" align="left" rowspan="1" colspan="1">0.902</td><td valign="top" align="left" rowspan="1" colspan="1">0.033</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">M<sup>2</sup>SNet [<xref rid="r30" ref-type="bibr">30</xref>]</td><td valign="top" align="left" rowspan="1" colspan="1">0.756</td><td valign="top" align="left" rowspan="1" colspan="1">0.678</td><td valign="top" align="left" rowspan="1" colspan="1">0.737</td><td valign="top" align="left" rowspan="1" colspan="1">0.843</td><td valign="top" align="left" rowspan="1" colspan="1">0.873</td><td valign="top" align="left" rowspan="1" colspan="1">0.038</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Polyp-PVT [<xref rid="r33" ref-type="bibr">33</xref>]</td><td valign="top" align="left" rowspan="1" colspan="1">0.801</td><td valign="top" align="left" rowspan="1" colspan="1">0.725</td><td valign="top" align="left" rowspan="1" colspan="1">0.791</td><td valign="top" align="left" rowspan="1" colspan="1">0.847</td><td valign="top" align="left" rowspan="1" colspan="1">0.908</td><td valign="top" align="left" rowspan="1" colspan="1">0.031</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">CFA-Net [<xref rid="r28" ref-type="bibr">28</xref>]</td><td valign="top" align="left" rowspan="1" colspan="1">0.743</td><td valign="top" align="left" rowspan="1" colspan="1">0.665</td><td valign="top" align="left" rowspan="1" colspan="1">0.728</td><td valign="top" align="left" rowspan="1" colspan="1">0.835</td><td valign="top" align="left" rowspan="1" colspan="1">0.898</td><td valign="top" align="left" rowspan="1" colspan="1">0.039</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">ESFPNet [<xref rid="r37" ref-type="bibr">37</xref>]</td><td valign="top" align="left" rowspan="1" colspan="1">0.767</td><td valign="top" align="left" rowspan="1" colspan="1">0.679</td><td valign="top" align="left" rowspan="1" colspan="1">0.747</td><td valign="top" align="left" rowspan="1" colspan="1">0.837</td><td valign="top" align="left" rowspan="1" colspan="1">0.880</td><td valign="top" align="left" rowspan="1" colspan="1">0.037</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">TranSEFusionNet [<xref rid="r43" ref-type="bibr">43</xref>]</td><td valign="top" align="left" rowspan="1" colspan="1">0.614</td><td valign="top" align="left" rowspan="1" colspan="1">0.510</td><td valign="top" align="left" rowspan="1" colspan="1">0.572</td><td valign="top" align="left" rowspan="1" colspan="1">0.758</td><td valign="top" align="left" rowspan="1" colspan="1">0.801</td><td valign="top" align="left" rowspan="1" colspan="1">0.052</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">ECTransNet [<xref rid="r44" ref-type="bibr">44</xref>]</td><td valign="top" align="left" rowspan="1" colspan="1">0.766</td><td valign="top" align="left" rowspan="1" colspan="1">0.687</td><td valign="top" align="left" rowspan="1" colspan="1">0.710</td><td valign="top" align="left" rowspan="1" colspan="1">0.831</td><td valign="top" align="left" rowspan="1" colspan="1">0.846</td><td valign="top" align="left" rowspan="1" colspan="1">0.045</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">MCSF-Net [<xref rid="r31" ref-type="bibr">31</xref>]</td><td valign="top" align="left" rowspan="1" colspan="1">0.765</td><td valign="top" align="left" rowspan="1" colspan="1">0.692</td><td valign="top" align="left" rowspan="1" colspan="1">0.700</td><td valign="top" align="left" rowspan="1" colspan="1">0.824</td><td valign="top" align="left" rowspan="1" colspan="1">0.853</td><td valign="top" align="left" rowspan="1" colspan="1">0.041</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Ours</td><td valign="top" align="left" rowspan="1" colspan="1">
<bold>0</bold>
<bold>.</bold>
<bold>821</bold>
</td><td valign="top" align="left" rowspan="1" colspan="1">
<bold>0.741</bold></td><td valign="top" align="left" rowspan="1" colspan="1">
<bold>0.805</bold></td><td valign="top" align="left" rowspan="1" colspan="1">
<bold>0.869</bold></td><td valign="top" align="left" rowspan="1" colspan="1">
<bold>0.916</bold></td><td valign="top" align="left" rowspan="1" colspan="1">
<bold>0.029</bold></td></tr><tr><td valign="top" align="center" colspan="7" rowspan="1">
<hr/>
</td></tr><tr><td valign="top" align="left" rowspan="2" colspan="1">Datasets</td><td valign="top" align="left" colspan="6" rowspan="1">ETIS-LaribPolypDB [<xref rid="r2" ref-type="bibr">2</xref>]<hr/></td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">mDice</td><td valign="top" align="left" rowspan="1" colspan="1">mIoU</td><td valign="top" align="left" rowspan="1" colspan="1">&#x000a0;<inline-formula>
<mml:math id="m197" display="inline" overflow="scroll"><mml:msubsup><mml:mi>F</mml:mi><mml:mi>&#x003b2;<!-- &#x003b2; --></mml:mi><mml:mi>&#x003c9;<!-- &#x003c9; --></mml:mi></mml:msubsup></mml:math>
</inline-formula></td><td valign="top" align="left" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="m198" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>&#x003b1;<!-- &#x003b1; --></mml:mi></mml:msub></mml:mrow></mml:math>
</inline-formula>
</td><td valign="top" align="left" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="m199" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mi mathvariant="normal">&#x02205;<!-- &#x02205; --></mml:mi></mml:msub></mml:mrow></mml:math>
</inline-formula>
</td><td valign="top" align="left" rowspan="1" colspan="1">MAE</td></tr><tr><td valign="top" align="center" colspan="7" rowspan="1">
<hr/>
</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">SFA [<xref rid="r19" ref-type="bibr">19</xref>]</td><td valign="top" align="left" rowspan="1" colspan="1">0.297</td><td valign="top" align="left" rowspan="1" colspan="1">0.217</td><td valign="top" align="left" rowspan="1" colspan="1">0.231</td><td valign="top" align="left" rowspan="1" colspan="1">0.557</td><td valign="top" align="left" rowspan="1" colspan="1">0.531</td><td valign="top" align="left" rowspan="1" colspan="1">0.109</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">PraNet [<xref rid="r21" ref-type="bibr">21</xref>]</td><td valign="top" align="left" rowspan="1" colspan="1">0.628</td><td valign="top" align="left" rowspan="1" colspan="1">0.567</td><td valign="top" align="left" rowspan="1" colspan="1">0.600</td><td valign="top" align="left" rowspan="1" colspan="1">0.794</td><td valign="top" align="left" rowspan="1" colspan="1">0.808</td><td valign="top" align="left" rowspan="1" colspan="1">0.031</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">MSNet [<xref rid="r29" ref-type="bibr">29</xref>]</td><td valign="top" align="left" rowspan="1" colspan="1">0.723</td><td valign="top" align="left" rowspan="1" colspan="1">0.652</td><td valign="top" align="left" rowspan="1" colspan="1">0.677</td><td valign="top" align="left" rowspan="1" colspan="1">0.845</td><td valign="top" align="left" rowspan="1" colspan="1">0.875</td><td valign="top" align="left" rowspan="1" colspan="1">0.020</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">SANet [<xref rid="r22" ref-type="bibr">22</xref>]</td><td valign="top" align="left" rowspan="1" colspan="1">0.750</td><td valign="top" align="left" rowspan="1" colspan="1">0.654</td><td valign="top" align="left" rowspan="1" colspan="1">0.685</td><td valign="top" align="left" rowspan="1" colspan="1">0.849</td><td valign="top" align="left" rowspan="1" colspan="1">0.881</td><td valign="top" align="left" rowspan="1" colspan="1">0.015</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">SSFormer [<xref rid="r35" ref-type="bibr">35</xref>]</td><td valign="top" align="left" rowspan="1" colspan="1">0.767</td><td valign="top" align="left" rowspan="1" colspan="1">0.697</td><td valign="top" align="left" rowspan="1" colspan="1">0.736</td><td valign="top" align="left" rowspan="1" colspan="1">0.863</td><td valign="top" align="left" rowspan="1" colspan="1">0.889</td><td valign="top" align="left" rowspan="1" colspan="1">0.016</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">FCBFormer [<xref rid="r42" ref-type="bibr">42</xref>]</td><td valign="top" align="left" rowspan="1" colspan="1">0.773</td><td valign="top" align="left" rowspan="1" colspan="1">0.689</td><td valign="top" align="left" rowspan="1" colspan="1">0.723</td><td valign="top" align="left" rowspan="1" colspan="1">0.860</td><td valign="top" align="left" rowspan="1" colspan="1">0.896</td><td valign="top" align="left" rowspan="1" colspan="1">0.017</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">M<sup>2</sup>SNet [<xref rid="r30" ref-type="bibr">30</xref>]</td><td valign="top" align="left" rowspan="1" colspan="1">0.746</td><td valign="top" align="left" rowspan="1" colspan="1">0.668</td><td valign="top" align="left" rowspan="1" colspan="1">0.712</td><td valign="top" align="left" rowspan="1" colspan="1">0.853</td><td valign="top" align="left" rowspan="1" colspan="1">0.880</td><td valign="top" align="left" rowspan="1" colspan="1">0.017</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Polyp-PVT [<xref rid="r33" ref-type="bibr">33</xref>]</td><td valign="top" align="left" rowspan="1" colspan="1">0.781</td><td valign="top" align="left" rowspan="1" colspan="1">0.705</td><td valign="top" align="left" rowspan="1" colspan="1">0.748</td><td valign="top" align="left" rowspan="1" colspan="1">0.870</td><td valign="top" align="left" rowspan="1" colspan="1">
<bold>0</bold>
<bold>.</bold>
<bold>901</bold>
</td><td valign="top" align="left" rowspan="1" colspan="1">0.015</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">CFA-Net [<xref rid="r28" ref-type="bibr">28</xref>]</td><td valign="top" align="left" rowspan="1" colspan="1">0.732</td><td valign="top" align="left" rowspan="1" colspan="1">0.655</td><td valign="top" align="left" rowspan="1" colspan="1">0.693</td><td valign="top" align="left" rowspan="1" colspan="1">0.845</td><td valign="top" align="left" rowspan="1" colspan="1">0.892</td><td valign="top" align="left" rowspan="1" colspan="1">
<bold>0.014</bold></td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">ESFPNet [<xref rid="r37" ref-type="bibr">37</xref>]</td><td valign="top" align="left" rowspan="1" colspan="1">0.760</td><td valign="top" align="left" rowspan="1" colspan="1">0.676</td><td valign="top" align="left" rowspan="1" colspan="1">0.723</td><td valign="top" align="left" rowspan="1" colspan="1">0.850</td><td valign="top" align="left" rowspan="1" colspan="1">0.895</td><td valign="top" align="left" rowspan="1" colspan="1">0.019</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">TranSEFusionNet [<xref rid="r43" ref-type="bibr">43</xref>]</td><td valign="top" align="left" rowspan="1" colspan="1">0.474</td><td valign="top" align="left" rowspan="1" colspan="1">0.393</td><td valign="top" align="left" rowspan="1" colspan="1">0.431</td><td valign="top" align="left" rowspan="1" colspan="1">0.717</td><td valign="top" align="left" rowspan="1" colspan="1">0.731</td><td valign="top" align="left" rowspan="1" colspan="1">0.032</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">ECTransNet [<xref rid="r44" ref-type="bibr">44</xref>]</td><td valign="top" align="left" rowspan="1" colspan="1">0.728</td><td valign="top" align="left" rowspan="1" colspan="1">0.655</td><td valign="top" align="left" rowspan="1" colspan="1">0.683</td><td valign="top" align="left" rowspan="1" colspan="1">0.847</td><td valign="top" align="left" rowspan="1" colspan="1">0.864</td><td valign="top" align="left" rowspan="1" colspan="1">0.017</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">MCSF-Net [<xref rid="r31" ref-type="bibr">31</xref>]</td><td valign="top" align="left" rowspan="1" colspan="1">0.765</td><td valign="top" align="left" rowspan="1" colspan="1">0.690</td><td valign="top" align="left" rowspan="1" colspan="1">0.601</td><td valign="top" align="left" rowspan="1" colspan="1">0.800</td><td valign="top" align="left" rowspan="1" colspan="1">0.837</td><td valign="top" align="left" rowspan="1" colspan="1">0.019</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Ours</td><td valign="top" align="left" rowspan="1" colspan="1">
<bold>0.794</bold></td><td valign="top" align="left" rowspan="1" colspan="1">
<bold>0.719</bold></td><td valign="top" align="left" rowspan="1" colspan="1">
<bold>0.750</bold></td><td valign="top" align="left" rowspan="1" colspan="1">
<bold>0.882</bold></td><td valign="top" align="left" rowspan="1" colspan="1">0.894</td><td valign="top" align="left" rowspan="1" colspan="1">0.015</td></tr><tr><td valign="top" align="center" colspan="7" rowspan="1">
<hr/>
</td></tr><tr><td valign="top" align="left" rowspan="2" colspan="1">Datasets</td><td valign="top" align="left" colspan="6" rowspan="1">CVC-300 [<xref rid="r53" ref-type="bibr">53</xref>]<hr/></td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">mDice</td><td valign="top" align="left" rowspan="1" colspan="1">mIoU</td><td valign="top" align="left" rowspan="1" colspan="1">&#x000a0;<inline-formula>
<mml:math id="m200" display="inline" overflow="scroll"><mml:msubsup><mml:mi>F</mml:mi><mml:mi>&#x003b2;<!-- &#x003b2; --></mml:mi><mml:mi>&#x003c9;<!-- &#x003c9; --></mml:mi></mml:msubsup></mml:math>
</inline-formula></td><td valign="top" align="left" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="m201" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>&#x003b1;<!-- &#x003b1; --></mml:mi></mml:msub></mml:mrow></mml:math>
</inline-formula>
</td><td valign="top" align="left" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="m202" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mi mathvariant="normal">&#x02205;<!-- &#x02205; --></mml:mi></mml:msub></mml:mrow></mml:math>
</inline-formula>
</td><td valign="top" align="left" rowspan="1" colspan="1">MAE</td></tr><tr><td valign="top" align="center" colspan="7" rowspan="1">
<hr/>
</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">SFA [<xref rid="r19" ref-type="bibr">19</xref>]</td><td valign="top" align="left" rowspan="1" colspan="1">0.467</td><td valign="top" align="left" rowspan="1" colspan="1">0.329</td><td valign="top" align="left" rowspan="1" colspan="1">0.341</td><td valign="top" align="left" rowspan="1" colspan="1">0.640</td><td valign="top" align="left" rowspan="1" colspan="1">0.644</td><td valign="top" align="left" rowspan="1" colspan="1">0.065</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">PraNet [<xref rid="r21" ref-type="bibr">21</xref>]</td><td valign="top" align="left" rowspan="1" colspan="1">0.871</td><td valign="top" align="left" rowspan="1" colspan="1">0.797</td><td valign="top" align="left" rowspan="1" colspan="1">0.843</td><td valign="top" align="left" rowspan="1" colspan="1">0.925</td><td valign="top" align="left" rowspan="1" colspan="1">0.950</td><td valign="top" align="left" rowspan="1" colspan="1">0.010</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">MSNet [<xref rid="r29" ref-type="bibr">29</xref>]</td><td valign="top" align="left" rowspan="1" colspan="1">0.865</td><td valign="top" align="left" rowspan="1" colspan="1">0.799</td><td valign="top" align="left" rowspan="1" colspan="1">0.848</td><td valign="top" align="left" rowspan="1" colspan="1">0.926</td><td valign="top" align="left" rowspan="1" colspan="1">0.945</td><td valign="top" align="left" rowspan="1" colspan="1">0.010</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">SANet [<xref rid="r22" ref-type="bibr">22</xref>]</td><td valign="top" align="left" rowspan="1" colspan="1">0.888</td><td valign="top" align="left" rowspan="1" colspan="1">0.815</td><td valign="top" align="left" rowspan="1" colspan="1">0.859</td><td valign="top" align="left" rowspan="1" colspan="1">0.928</td><td valign="top" align="left" rowspan="1" colspan="1">0.962</td><td valign="top" align="left" rowspan="1" colspan="1">0.008</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">SSFormer [<xref rid="r35" ref-type="bibr">35</xref>]</td><td valign="top" align="left" rowspan="1" colspan="1">0.887</td><td valign="top" align="left" rowspan="1" colspan="1">0.821</td><td valign="top" align="left" rowspan="1" colspan="1">0.869</td><td valign="top" align="left" rowspan="1" colspan="1">0.929</td><td valign="top" align="left" rowspan="1" colspan="1">0.959</td><td valign="top" align="left" rowspan="1" colspan="1">
<bold>0.007</bold></td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">FCBFormer [<xref rid="r42" ref-type="bibr">42</xref>]</td><td valign="top" align="left" rowspan="1" colspan="1">0.897</td><td valign="top" align="left" rowspan="1" colspan="1">0.833</td><td valign="top" align="left" rowspan="1" colspan="1">0.877</td><td valign="top" align="left" rowspan="1" colspan="1">0.931</td><td valign="top" align="left" rowspan="1" colspan="1">0.969</td><td valign="top" align="left" rowspan="1" colspan="1">0.009</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">M<sup>2</sup>SNet [<xref rid="r30" ref-type="bibr">30</xref>]</td><td valign="top" align="left" rowspan="1" colspan="1">0.899</td><td valign="top" align="left" rowspan="1" colspan="1">0.833</td><td valign="top" align="left" rowspan="1" colspan="1">0.881</td><td valign="top" align="left" rowspan="1" colspan="1">0.940</td><td valign="top" align="left" rowspan="1" colspan="1">0.971</td><td valign="top" align="left" rowspan="1" colspan="1">
<bold>0.007</bold></td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Polyp-PVT [<xref rid="r33" ref-type="bibr">33</xref>]</td><td valign="top" align="left" rowspan="1" colspan="1">0.899</td><td valign="top" align="left" rowspan="1" colspan="1">0.831</td><td valign="top" align="left" rowspan="1" colspan="1">0.884</td><td valign="top" align="left" rowspan="1" colspan="1">0.932</td><td valign="top" align="left" rowspan="1" colspan="1">0.973</td><td valign="top" align="left" rowspan="1" colspan="1">0.008</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">CFA-Net [<xref rid="r28" ref-type="bibr">28</xref>]</td><td valign="top" align="left" rowspan="1" colspan="1">0.893</td><td valign="top" align="left" rowspan="1" colspan="1">0.827</td><td valign="top" align="left" rowspan="1" colspan="1">0.875</td><td valign="top" align="left" rowspan="1" colspan="1">0.938</td><td valign="top" align="left" rowspan="1" colspan="1">
<bold>0.978</bold></td><td valign="top" align="left" rowspan="1" colspan="1">0.008</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">ESFPNet [<xref rid="r37" ref-type="bibr">37</xref>]</td><td valign="top" align="left" rowspan="1" colspan="1">0.874</td><td valign="top" align="left" rowspan="1" colspan="1">0.809</td><td valign="top" align="left" rowspan="1" colspan="1">0.858</td><td valign="top" align="left" rowspan="1" colspan="1">0.923</td><td valign="top" align="left" rowspan="1" colspan="1">0.944</td><td valign="top" align="left" rowspan="1" colspan="1">0.009</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">TranSEFusionNet [<xref rid="r43" ref-type="bibr">43</xref>]</td><td valign="top" align="left" rowspan="1" colspan="1">0.754</td><td valign="top" align="left" rowspan="1" colspan="1">0.649</td><td valign="top" align="left" rowspan="1" colspan="1">0.707</td><td valign="top" align="left" rowspan="1" colspan="1">0.857</td><td valign="top" align="left" rowspan="1" colspan="1">0.902</td><td valign="top" align="left" rowspan="1" colspan="1">0.016</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">ECTransNet [<xref rid="r44" ref-type="bibr">44</xref>]</td><td valign="top" align="left" rowspan="1" colspan="1">0.907</td><td valign="top" align="left" rowspan="1" colspan="1">0.840</td><td valign="top" align="left" rowspan="1" colspan="1">0.865</td><td valign="top" align="left" rowspan="1" colspan="1">0.937</td><td valign="top" align="left" rowspan="1" colspan="1">0.962</td><td valign="top" align="left" rowspan="1" colspan="1">
<bold>0.007</bold></td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">MCSF-Net [<xref rid="r31" ref-type="bibr">31</xref>]</td><td valign="top" align="left" rowspan="1" colspan="1">0.901</td><td valign="top" align="left" rowspan="1" colspan="1">0.834</td><td valign="top" align="left" rowspan="1" colspan="1">0.859</td><td valign="top" align="left" rowspan="1" colspan="1">0.934</td><td valign="top" align="left" rowspan="1" colspan="1">0.960</td><td valign="top" align="left" rowspan="1" colspan="1">0.008</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Ours</td><td valign="top" align="left" rowspan="1" colspan="1">
<bold>0.910</bold></td><td valign="top" align="left" rowspan="1" colspan="1">
<bold>0.845</bold></td><td valign="top" align="left" rowspan="1" colspan="1">
<bold>0.890</bold></td><td valign="top" align="left" rowspan="1" colspan="1">
<bold>0.941</bold></td><td valign="top" align="left" rowspan="1" colspan="1">0.976</td><td valign="top" align="left" rowspan="1" colspan="1">
<bold>0.007</bold></td></tr></tbody></table></table-wrap><p>In <xref rid="t005" ref-type="table">Table&#x000a0;5</xref>, it should be noticed that our model consistently outperformed other mainstream models in terms of nearly all indicators on the CVC-ConlonDB, ETIS, and CVC-300 benchmark datasets. For instance, compared with the FCBFormer, it provided 2.3% improvement in <inline-formula>
<mml:math id="m203" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:math>
</inline-formula>, 1.8% improvement in <inline-formula>
<mml:math id="m204" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:mi>U</mml:mi></mml:math>
</inline-formula>, 3.0% improvement in <inline-formula>
<mml:math id="m205" display="inline" overflow="scroll"><mml:msubsup><mml:mi>F</mml:mi><mml:mi>&#x003b2;<!-- &#x003b2; --></mml:mi><mml:mi>w</mml:mi></mml:msubsup></mml:math>
</inline-formula>, 1.9% improvement in <inline-formula>
<mml:math id="m206" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>&#x003b1;<!-- &#x003b1; --></mml:mi></mml:msub></mml:mrow></mml:math>
</inline-formula>, and 1.4% improvement in <inline-formula>
<mml:math id="m207" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mi>&#x003d5;<!-- &#x003d5; --></mml:mi></mml:msub></mml:mrow></mml:math>
</inline-formula>, respectively, on the CVC-ConlonDB dataset. Yet, the MAE of our model reduced by 0.4%. On the CVC-300 dataset, our model exceeded the Polyp-PVT by a large margin, improving <inline-formula>
<mml:math id="m208" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:math>
</inline-formula> from 0.899 to 0.910 (1.1% improvement), and <inline-formula>
<mml:math id="m209" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:mi>U</mml:mi></mml:math>
</inline-formula> from 0.831 to 0.845 (1.4% improvement), respectively. Even in the most challenging ETIS dataset, it also improved the state-of-the-art performance, with a <inline-formula>
<mml:math id="m210" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:math>
</inline-formula> score of 0.794, a <inline-formula>
<mml:math id="m211" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:mi>U</mml:mi></mml:math>
</inline-formula> score of 0.719, a <inline-formula>
<mml:math id="m212" display="inline" overflow="scroll"><mml:msubsup><mml:mi>F</mml:mi><mml:mi>&#x003b2;<!-- &#x003b2; --></mml:mi><mml:mi>w</mml:mi></mml:msubsup></mml:math>
</inline-formula> score of 0.750 and a <inline-formula>
<mml:math id="m213" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>&#x003b1;<!-- &#x003b1; --></mml:mi></mml:msub></mml:mrow></mml:math>
</inline-formula> score of 0.882. Furthermore, our method was consistently superior to recently proposed best-performing M2SNet, CFA-Net, ESFPNet, and TranSEFusionNet for nearly all evaluation indicators on these unseen datasets. However, another one notable finding was that SFA and TranSEFusionNet exhibited a dramatic decline in segmentation performance on these unseen datasets, partially evidencing that their generalization abilities were weak. When compared with the ECTransNet and the MCSF-Net, our model manifested better generalization ability. For instance, in terms of <inline-formula>
<mml:math id="m214" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:math>
</inline-formula> and <inline-formula>
<mml:math id="m215" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:mi>U</mml:mi></mml:math>
</inline-formula>, our model obtained substantial improvements of 5.5% and 5.4% over the ECTransNet on the CVC-ColonDB dataset, while 6.6% and 6.4% on the ETIS-LabribPolypDB dataset, respectively. Even on the CVC-300 dataset, our model still outperformed the ECTransNet with 0.3% higher <inline-formula>
<mml:math id="m216" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:math>
</inline-formula> and 0.5% higher <inline-formula>
<mml:math id="m217" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:mi>U</mml:mi></mml:math>
</inline-formula>. In contrast, our model significantly exceeded the MCSF-Net by 5.6% <inline-formula>
<mml:math id="m218" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:math>
</inline-formula> and 4.9% <inline-formula>
<mml:math id="m219" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:mi>U</mml:mi></mml:math>
</inline-formula> on the CVC-ColonDB dataset, 2.9% <inline-formula>
<mml:math id="m220" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:math>
</inline-formula> and 2.9% <inline-formula>
<mml:math id="m221" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:mi>U</mml:mi></mml:math>
</inline-formula> for the ETIS-LabribPolypDB dataset, and 0.9% <inline-formula>
<mml:math id="m222" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:math>
</inline-formula> and 1.1% <inline-formula>
<mml:math id="m223" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:mi>U</mml:mi></mml:math>
</inline-formula> for the CVC-300 dataset. These results validated that our model owned excellent segmentation capability even when confronting with different source datasets. Again, these results revealed that by contrast with the current state-of-the-art approaches, our models still manifested remarkable generalization capability in tackling images with a different distribution from that utilized for training, which could be an efficient option for real clinical practice with considerable variations in data.</p></sec><sec id="sec4-5"><label>4.5</label><title>Ablation studies</title><p>In this part, we verified the effectiveness of each critical component in our Dua-PSNet through a series of ablation experiments on the CVC-ClinicDB and Kvasir-SEG datasets, and the quantitative and qualitative results were tabulated in <xref rid="t006" ref-type="table">Table&#x000a0;6</xref> and <xref rid="g009" ref-type="fig">Fig.&#x000a0;9</xref>, respectively. In the ablation experiments, we selected the FCN and PVTv2-B3 as the baseline (No.1 and No.2) and used the ResidualBlock module instead of RB module in FCN (No.3) to evaluate the role of ResidualBlock module, and then investigate the effect with successive additions of PVTv2-B3 (No.4), AFA module (No.5), ResidualBlock module (No.6) and SGLFH module (No.7).</p><fig position="float" id="g009" fig-type="figure"><label>Fig. 9.</label><caption><p>Visualizable comparison results for verifying the strength of different key components.</p></caption><graphic xlink:href="boe-15-4-2590-g009" position="float"/></fig><table-wrap position="float" id="t006"><label>Table 6.</label><caption><title>Quantitative segmentation results from different primary modules in the ablation study utilizing the CVC-ClinicDB and Kvasir-SEG benchmarks.</title></caption><table frame="hsides" rules="all"><colgroup span="1"><col align="left" width="20%" span="1"/><col align="left" width="20%" span="1"/><col align="left" width="20%" span="1"/><col align="left" width="20%" span="1"/><col align="left" width="20%" span="1"/></colgroup><thead><tr><th valign="top" align="left" rowspan="3" colspan="1">Datasets</th><th valign="top" align="left" colspan="2" rowspan="1">CVC-ClinicDB [<xref rid="r52" ref-type="bibr">52</xref>]</th><th valign="top" align="left" colspan="2" rowspan="1">Kvasir-SEG [<xref rid="r51" ref-type="bibr">51</xref>]</th></tr><tr><th valign="top" align="center" colspan="4" rowspan="1">
<hr/>
</th></tr><tr><th valign="top" align="left" rowspan="1" colspan="1">mDice</th><th valign="top" align="left" rowspan="1" colspan="1">mIoU</th><th valign="top" align="left" rowspan="1" colspan="1">mDice</th><th valign="top" align="left" rowspan="1" colspan="1">mIoU</th></tr></thead><tbody><tr><td valign="top" align="left" rowspan="1" colspan="1">No.1 FCN Only with RB</td><td valign="top" align="left" rowspan="1" colspan="1">0.8896</td><td valign="top" align="left" rowspan="1" colspan="1">0.8123</td><td valign="top" align="left" rowspan="1" colspan="1">0.8569</td><td valign="top" align="left" rowspan="1" colspan="1">0.7783</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">No.2 PVTv2-B3 Only</td><td valign="top" align="left" rowspan="1" colspan="1">0.9099</td><td valign="top" align="left" rowspan="1" colspan="1">0.8488</td><td valign="top" align="left" rowspan="1" colspan="1">0.8825</td><td valign="top" align="left" rowspan="1" colspan="1">0.8129</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">No.3 FCN Only with ResidualBlock</td><td valign="top" align="left" rowspan="1" colspan="1">0.9002</td><td valign="top" align="left" rowspan="1" colspan="1">0.8282</td><td valign="top" align="left" rowspan="1" colspan="1">0.8695</td><td valign="top" align="left" rowspan="1" colspan="1">0.7992</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">No.4 FCN&#x02009;+&#x02009;PVTv2-B3</td><td valign="top" align="left" rowspan="1" colspan="1">0.9220</td><td valign="top" align="left" rowspan="1" colspan="1">0.8701</td><td valign="top" align="left" rowspan="1" colspan="1">0.8926</td><td valign="top" align="left" rowspan="1" colspan="1">0.8267</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">No.5 FCN&#x02009;+&#x02009;PVTv2-B3&#x02009;+&#x02009;AFA</td><td valign="top" align="left" rowspan="1" colspan="1">0.9346</td><td valign="top" align="left" rowspan="1" colspan="1">0.8861</td><td valign="top" align="left" rowspan="1" colspan="1">0.9065</td><td valign="top" align="left" rowspan="1" colspan="1">0.8478</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">No.6 FCN&#x02009;+&#x02009;PVTv2-B3&#x02009;+&#x02009;AFA&#x02009;+&#x02009;ResidualBlock</td><td valign="top" align="left" rowspan="1" colspan="1">0.9438</td><td valign="top" align="left" rowspan="1" colspan="1">0.8966</td><td valign="top" align="left" rowspan="1" colspan="1">0.9174</td><td valign="top" align="left" rowspan="1" colspan="1">0.8556</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">No.7 FCN&#x02009;+&#x02009;PVTv2-B3&#x02009;+&#x02009;AFA&#x02009;+&#x02009;ResidualBlock&#x02009;+&#x02009;SGLFH</td><td valign="top" align="left" rowspan="1" colspan="1">0.9514</td><td valign="top" align="left" rowspan="1" colspan="1">0.9083</td><td valign="top" align="left" rowspan="1" colspan="1">0.9253</td><td valign="top" align="left" rowspan="1" colspan="1">0.8746</td></tr></tbody></table></table-wrap><sec id="sec4-5-1"><title>Effectiveness of ResidualBlock Module.</title><p>We investigated the validity of the ResidualBlock module. As shown in <xref rid="t006" ref-type="table">Table&#x000a0;6</xref>, compared No.1 with No.3, it could be clearly observed that No.3 using the ResidualBlock module could significantly improve the segmentation performance by 1.1% <inline-formula>
<mml:math id="m224" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:math>
</inline-formula> and 1.6% <inline-formula>
<mml:math id="m225" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:mi>U</mml:mi></mml:math>
</inline-formula> on the CVC-ClinicDB dataset, and 1.3% <inline-formula>
<mml:math id="m226" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:math>
</inline-formula> and 2.1% <inline-formula>
<mml:math id="m227" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:mi>U</mml:mi></mml:math>
</inline-formula> on the Kvasir-SEG dataset. In addition, from No.6 vs No.5, similar trends could be inspected. From the visualizable comparison results in <xref rid="g009" ref-type="fig">Fig.&#x000a0;9</xref>, it could be witnessed that the perception of more local information was enhanced. In short, these results suggested that the ResidualBlock module was benefit for boosting model&#x02019;s performance.</p></sec><sec id="sec4-5-2"><title>Effectiveness of combination of FCN and PVTv2-B3.</title><p>In order to evaluate the combination of FCN and PVTv2-B3 branches, we tested the performance of No.4 (FCN&#x02009;+&#x02009;PVTv2-B3). As can be seen from <xref rid="t006" ref-type="table">Table&#x000a0;6</xref> and <xref rid="g009" ref-type="fig">Fig.&#x000a0;9</xref>, No.4 brought dramatic performance improvements against No.3 and No.2 on both datasets. Specifically, by comparing No.4 with No.3 in <xref rid="t005" ref-type="table">Table&#x000a0;5</xref>, the <inline-formula>
<mml:math id="m228" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:math>
</inline-formula> and <inline-formula>
<mml:math id="m229" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:mi>U</mml:mi></mml:math>
</inline-formula> obtained an increment of 2.2% and 4.2% on the CVC-ClinicDB dataset, while 2.3% and 2.8% on the Kvasir-SEG dataset. In contrast to No.2, No.4 attained a performance gain of 1.2% and 2.1% with respect to <inline-formula>
<mml:math id="m230" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:math>
</inline-formula> and <inline-formula>
<mml:math id="m231" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:mi>U</mml:mi></mml:math>
</inline-formula>, on the CVC-ClinicDB dataset, whilst 1% and 1.4% on the Kvasir-SEG dataset. These improvements confirmed that the work together of both branches rather than only using a single branch structure could enable our model to more accurately recognize true polyp regions, revealing that complementary hierarchical semantic information could be employed for boosting the model&#x02019;s segmentation performance, which was also demonstrated by the visualizable comparison results in <xref rid="g009" ref-type="fig">Fig.&#x000a0;9</xref>.</p></sec><sec id="sec4-5-3"><title>Effectiveness of Adaptive Feature Aggregation (AFA) Module.</title><p>We further investigated the importance of AFA module. According to the No.4 vs No.5 in <xref rid="t006" ref-type="table">Table&#x000a0;6</xref>, we could notice that No.5 showed a significant increase in the performance, improving <inline-formula>
<mml:math id="m232" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:math>
</inline-formula> score from 0.9220 to 0.9346 (1.3% ascension) and <inline-formula>
<mml:math id="m233" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:mi>U</mml:mi></mml:math>
</inline-formula> score from 0.8701 to 0.8861 (1.6% ascension), on the CVC-ClinicDB benchmark, and <inline-formula>
<mml:math id="m234" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:math>
</inline-formula> score from 0.8926 to 0.9065 (1.4% ascension) and <inline-formula>
<mml:math id="m235" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:mi>U</mml:mi></mml:math>
</inline-formula> score from 0.8267 to 0.8478 (2.1% ascension), on the Kvasir-SEG benchmark. This indicated the significance of the AFA module on segmentation performance enhancement. Likewise, the visualizable comparison results in <xref rid="g009" ref-type="fig">Fig.&#x000a0;9</xref> also implied that the devised AFA block could contribute to the improvement in the performance of polyp segmentation.</p></sec><sec id="sec4-5-4"><title>Effectiveness of Selective Global-to-Local Fusion Head (SGLFH) Module.</title><p>Finally, we assessed the contribution of SGLFH component. In <xref rid="t006" ref-type="table">Table&#x000a0;6</xref>, it was apparent that No.7 integrated the SGLFH module achieved better segmentation performance than No.6 on all the evaluation criterias, especially with <inline-formula>
<mml:math id="m236" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:mi>U</mml:mi></mml:math>
</inline-formula> metric reaching 0.9083 and 0.8746 on the CVC-ClinicDB and Kvasir-SEG benchmarks, respectively, which were 1.2% and 1.9% higher than those gained with No.6. This demonstrated that the introduction of the SGLFH module helped improve segmentation accuracy a lot, highlighting the effectiveness of the presented SGLFH module, which was achieved by allowing the model to dynamically aggregate local feature details and the global semantic features for narrowing information gap between them. As displayed in <xref rid="g009" ref-type="fig">Fig.&#x000a0;9</xref>, some edge detailed information could not be precisely identified without utilizing the developed SGLFH module.</p></sec></sec></sec><sec sec-type="discussion" id="sec5"><label>5.</label><title>Discussion</title><p>In this work, we developed a novel architecture Dua-PSNet for accurate and automated polyp segmentation at full-size from colonoscopy images in many challenging cases. One of the strengths was that it took full use of the advantages of the Transformer with global-receptive-field on modeling long-range relationship and the FCN with local-receptive-field on establishing local spatial correlation in dense prediction. Combining them using two parallel branches allowed our model to pay attention to global semantic information without missing local spatial details. In this way, they could be reciprocally constrained and complemented for enabling more accurate full-size predictions. In the Transformer branch, we constructed the ResidualBlock module to deeper excavate polyp edge details disguised in low-level features and improve the recognition of edge ambiguous features. Further, we proposed two feature fusion modules, AFA module and SGLFH module, which could aggregate high-level features in a stepwise adaptive fashion and selectively incorporate edge cues into global semantic information, bridging semantic gap between high-level and low-level features. The Transformer was a sequence-to-sequence prediction model which attended to global contextual information. However, their predicted segmentation maps were typical types of a lower resolution than the input images, lacking detailed localization information. Our FCN branch allowed for the extraction of highly merged multi-scale fine boundary characteristics at full-size, compensating for output of the Transformer branch into full-size segmentation map. From quantitative and qualitative comparison results with state-of-the-art methods (See <xref rid="t002" ref-type="table">Table&#x000a0;2</xref> and <xref rid="t003" ref-type="table">Table&#x000a0;3</xref> as well as <xref rid="g007" ref-type="fig">Fig.&#x000a0;7</xref> and <xref rid="g008" ref-type="fig">Fig.&#x000a0;8</xref>) together with generalizability analysis (See <xref rid="t004" ref-type="table">Table&#x000a0;4</xref> and <xref rid="t005" ref-type="table">Table&#x000a0;5</xref>), it was demonstrated that our model consistently exceeded these advanced methods [<xref rid="r11" ref-type="bibr">11</xref>,<xref rid="r12" ref-type="bibr">12</xref>,<xref rid="r19" ref-type="bibr">19</xref>,<xref rid="r21" ref-type="bibr">21</xref>,<xref rid="r22" ref-type="bibr">22</xref>,<xref rid="r28" ref-type="bibr">28</xref>&#x02013;<xref rid="r31" ref-type="bibr">31</xref>,<xref rid="r33" ref-type="bibr">33</xref>,<xref rid="r35" ref-type="bibr">35</xref>,<xref rid="r37" ref-type="bibr">37</xref>,<xref rid="r42" ref-type="bibr">42</xref>&#x02013;<xref rid="r44" ref-type="bibr">44</xref>] and located polyp areas more accurately with clear boundary contours, even though they were varied, which put emphasis on its powerful strengths in learning and generalization capability. Hence, we could conclude that the proposed Dua-PSNet had great potential to act as an &#x0201c;extra pair of eyes&#x0201d; for endoscopists providing additional objective diagnostic information during colonoscopy, and aid in making a feasible decision for further treatment.</p><p>The developed Dua-PSNet performed better than a variety of current advanced methods, comprising DCNN-based methods [<xref rid="r11" ref-type="bibr">11</xref>,<xref rid="r12" ref-type="bibr">12</xref>,<xref rid="r19" ref-type="bibr">19</xref>,<xref rid="r21" ref-type="bibr">21</xref>,<xref rid="r22" ref-type="bibr">22</xref>,<xref rid="r28" ref-type="bibr">28</xref>&#x02013;<xref rid="r31" ref-type="bibr">31</xref>], Transformed-based approaches [<xref rid="r33" ref-type="bibr">33</xref>,<xref rid="r35" ref-type="bibr">35</xref>,<xref rid="r37" ref-type="bibr">37</xref>] and hybrid architecture combining Transformer and CNN [<xref rid="r42" ref-type="bibr">42</xref>&#x02013;<xref rid="r44" ref-type="bibr">44</xref>], in the task of polyp segmentation. As reported in <xref rid="t001" ref-type="table">Table&#x000a0;1</xref>, our model had best segmentation accuracy among all five methods, on the CVC-ClinicDB dataset, with <inline-formula>
<mml:math id="m237" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:math>
</inline-formula> score of 0.9514 and <inline-formula>
<mml:math id="m238" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:mi>U</mml:mi></mml:math>
</inline-formula> score of 0.9083. Despite the <inline-formula>
<mml:math id="m239" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:mi>U</mml:mi></mml:math>
</inline-formula> value was not highest on the Kvasir-SEG dataset, it was slightly inferior to second-best FCBFormer (<inline-formula>
<mml:math id="m240" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:mi>U</mml:mi></mml:math>
</inline-formula>: 0.8746 vs 0.8752). In <xref rid="t002" ref-type="table">Table&#x000a0;2</xref>, our model also attained best scores in nearly all evaluation indicators compared with SFA [<xref rid="r19" ref-type="bibr">19</xref>], PraNet [<xref rid="r21" ref-type="bibr">21</xref>], MSNet [<xref rid="r29" ref-type="bibr">29</xref>], SANet [<xref rid="r22" ref-type="bibr">22</xref>], M2SNet [<xref rid="r30" ref-type="bibr">30</xref>], SSFormer [<xref rid="r35" ref-type="bibr">35</xref>], ESFPNet [<xref rid="r37" ref-type="bibr">37</xref>], and TranSEFusionNet [<xref rid="r43" ref-type="bibr">43</xref>] on both datasets. In addition to this, the <inline-formula>
<mml:math id="m241" display="inline" overflow="scroll"><mml:msubsup><mml:mi>F</mml:mi><mml:mi>&#x003b2;<!-- &#x003b2; --></mml:mi><mml:mi>w</mml:mi></mml:msubsup></mml:math>
</inline-formula> value of Polyp-PVT [<xref rid="r33" ref-type="bibr">33</xref>] on the Kvasir-SEG was 0.1% marginally higher than our model, whereas the MAE value of Polyp-PVT was 0.1% slightly lower. Through comparison between our model and CFA-Net, a similar observation also existed (<inline-formula>
<mml:math id="m242" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mi>&#x003d5;<!-- &#x003d5; --></mml:mi></mml:msub></mml:mrow></mml:math>
</inline-formula>:0.960 vs 0.962; MAE:0.024 vs 0.023) in terms of <inline-formula>
<mml:math id="m243" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mi>&#x003d5;<!-- &#x003d5; --></mml:mi></mml:msub></mml:mrow></mml:math>
</inline-formula> and MAE. We believed the reason for this result was due to too small dataset and abundant noise, which made it difficult to learn sufficient feature details and have an adverse impact on our model. In contrast, the <inline-formula>
<mml:math id="m244" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:math>
</inline-formula> and <inline-formula>
<mml:math id="m245" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:mi>U</mml:mi></mml:math>
</inline-formula> scores of our model were higher by 0.9% and 1.1%, 0.7% and 1.3%, respectively. To show our model intuitively, we also provided segmentation results on different testing datasets, and compared testing performance with several representative advanced methods. From visualizable prediction results of different methods in <xref rid="g007" ref-type="fig">Fig.&#x000a0;7</xref> and <xref rid="g008" ref-type="fig">Fig.&#x000a0;8</xref>, it could also be well visualized that the proposed Dua-PSNet identified a more comprehensive range of polyp regions with smooth and clear boundaries. Even for the challenging scenarios such as irregular shape, varied size as well as blurred boundary between the polyp and its surrounding tissue, it could also deal with well and produce considerably better segmentation mask, proving its strong learning ability. These excellent segmentation results could be attributed to the fact that our model had a powerful ability to fully mine the multi-scale feature representation and dynamically fuse different level of features for tackling the scale variations of polyps, and simultaneously provide fine local edge cues to guide for locating its boundary, thereby boosting the segmentation performance. With the aid of our model, an accurate and operator-independent estimate of the polyp size could be provided to assist in making feasible decisions required during colonoscopy.</p><p>Considering stitching features directly using the skip connection of U-Net may cause some relevant information lost, we first restored the feature sizes of three high levels to the same size and then aggregated them in a stepwise adaptive way through the constructed AFA block in the Transformer branch, highlighting important and coarse global contextual features. As listed in <xref rid="t006" ref-type="table">Table&#x000a0;6</xref> (No.5 vs No.4), the model adding the AFA component yielded better segmentation results than one without this module, with 1.3% and 1.4% higher <inline-formula>
<mml:math id="m246" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:math>
</inline-formula> and 1.6% and 2.1% higher <inline-formula>
<mml:math id="m247" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:mi>U</mml:mi></mml:math>
</inline-formula> on the CVC-ClinicDB and Kvasir-SEG benchmarks, respectively. Notwithstanding preserving most information in global semantic features, the transmission of these semantic information from high-level to low-level was still weakened by simple up-sampling operation. Accordingly, we constructed ResidualBlock module to emphasize critical local boundary details, and proposed SGLFH module to selectively add these fine local details into global semantic features again at the end of Transformer branch, such that the original characteristics of images could be conserved to the maximum extent. From the visualizable results in <xref rid="g009" ref-type="fig">Fig.&#x000a0;9</xref>, it could be visually perceived that the overall segmentation effect on polyps of <xref rid="g009" ref-type="fig">Fig.&#x000a0;9</xref> (No.3) with ResidualBlock module was relatively better than that of <xref rid="g009" ref-type="fig">Fig.&#x000a0;9</xref> (No.1) using only FCN with RB module. From No.1 vs No.3 in <xref rid="t006" ref-type="table">Table&#x000a0;6</xref>, the same observation was obtained. When further adding the SGLFH block, the identification of focus regions was more precise in <xref rid="g009" ref-type="fig">Fig.&#x000a0;9</xref> (No.7 vs No.6), and the boundary of <xref rid="g009" ref-type="fig">Fig.&#x000a0;9</xref> (No.7) was sharper and smoother compared to <xref rid="g009" ref-type="fig">Fig.&#x000a0;9</xref> (No.6). In the segmentation task of polyps, low contrast between the target features and the background made it hard to accurately distinguish them, and most Transformer-based methods only predicted a lower resolution segmentation map (not full-size) for the input image. To enhance the discrimination capability and perform full-size prediction, we combined the Transformer and FCN branches in parallel, where the fine-grained features extracted by FCN branch as well as important and rough features outputted by Transformer branch complemented each other to enhance the target information and weaken the background characteristics for dense prediction at full-size. From the segmentation results in <xref rid="t006" ref-type="table">Table&#x000a0;6</xref> (No.2 vs No.4, and No.3 vs No.4) together with <xref rid="g009" ref-type="fig">Fig.&#x000a0;9</xref> (No.2), <xref rid="g009" ref-type="fig">Fig.&#x000a0;9</xref> (No.3) and <xref rid="g009" ref-type="fig">Fig.&#x000a0;9</xref> (No.4), we could find that the model combining two branches stimulated segmentation performance gains by a significant increment (On the CVC-ClinicDB dataset, No.4 vs No.2: 1.2% <inline-formula>
<mml:math id="m248" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:math>
</inline-formula> and 2.1% <inline-formula>
<mml:math id="m249" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:mi>U</mml:mi></mml:math>
</inline-formula> improvements, No.4 vs No.3: 2.2% <inline-formula>
<mml:math id="m250" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:math>
</inline-formula> and 4.2% <inline-formula>
<mml:math id="m251" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:mi>U</mml:mi></mml:math>
</inline-formula> improvements; On the Kvasir-SEG dataset, No.4 vs No.2: 1.0% <inline-formula>
<mml:math id="m252" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:math>
</inline-formula> and 1.4% <inline-formula>
<mml:math id="m253" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:mi>U</mml:mi></mml:math>
</inline-formula> boosts, No.4 vs No.3: 2.3% <inline-formula>
<mml:math id="m254" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:math>
</inline-formula> and 2.8% <inline-formula>
<mml:math id="m255" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:mi>U</mml:mi></mml:math>
</inline-formula> boosts). The gradual superposition of each component provided consistent improvement of performance, and combining the presented all components together allowed our model to achieve the highest performance in all evaluation metrics. This justified that all components we devised contributed to performance boost in the entire Dua-PSNet, and excluding any one of them would lead to a decline in the performance of polyp segmentation.</p><p>The model&#x02019;s generalizability was also a challenge in the domain of medical image analysis. We argued that the most important angle of the model&#x02019;s generalizability lied in its ability to adapt to different types of datasets. Most of existed methods for polyp segmentation were only evaluated on a single dataset, which was not capable of directly reflecting the model&#x02019;s generalizability. On the contrary, cross-dataset evaluation could be implemented for investigating the generalizability of different networks. In view of the generalizability of U-Net, the Dua-PSNet was also built upon this model for innovation, and the segmentation results on the unseen three datasets implied its superior generalization ability. As can be seen in <xref rid="t005" ref-type="table">Table&#x000a0;5</xref>, on both the CVC-ColonDB and CVC-300 datasets, our model exhibited best segmentation results, exceeding the second-best Polyp-PVT by 2.0% and 1.6% with respect to <inline-formula>
<mml:math id="m256" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:math>
</inline-formula> and <inline-formula>
<mml:math id="m257" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:mi>U</mml:mi></mml:math>
</inline-formula> for the CVC-ColonDB dataset, while 1.1% and 1.4% for the CVC-300 dataset. Even on the most challenging ETIS benchmark, our model was also better in comparison with the latest Polyp-PVT, FCBFormer and ESFPNet apart from <inline-formula>
<mml:math id="m258" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mi>&#x003d5;<!-- &#x003d5; --></mml:mi></mml:msub></mml:mrow></mml:math>
</inline-formula> indicator, which was only slightly lower by 0.7%, 0.2%, and 0.1%, respectively. It was marginally 0.1% higher than CFA-Net in MAE indictor, yet exhibited obviously better in other evaluation metrics. Across all datasets, the proposed Dua-PSNet generalized well with consistently accurate segmentations, which made it more suitable for practical applications in which wide variations of data happened frequently. Furthermore, we also displayed the comparison results on <inline-formula>
<mml:math id="m259" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:math>
</inline-formula> and <inline-formula>
<mml:math id="m260" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:mi>U</mml:mi></mml:math>
</inline-formula> in <xref rid="g010" ref-type="fig">Fig.&#x000a0;10</xref>, which further evidenced the stability and leadings of our model. The excellent generalizability may be owed to the successful mixture of the merits of the Transformer and FCN in the proposed Dua-PSNet, resulting in the main region of polyp being handled by the Transformer branch whilst a reliable and fine full-size edge around this main area being guaranteed by the FCN branch.</p><fig position="float" id="g010" fig-type="figure"><label>Fig. 10.</label><caption><p>Model generalizability validation using Max Dice (a) and Max IoU (b) metrics. The <bold>bold</bold> fonts denoted the best results.</p></caption><graphic xlink:href="boe-15-4-2590-g010" position="float"/></fig><p>By comparison with the ECTransNet [<xref rid="r44" ref-type="bibr">44</xref>] and MCSF-Net [<xref rid="r31" ref-type="bibr">31</xref>], two latest segmentation methods proposed recently, the outstanding segmentation performance of our model was demonstrated as reported in <xref rid="t003" ref-type="table">Table&#x000a0;3</xref>. With respect to ECTransNet and MCSF-Net, we performed retraining and testing using their publicly available codes and recommended parameter settings as well as default data augmentation methods reported in their original literatures. Among data augmentation strategies used in these models and our approach, there only existed slight differences. Moreover, we also conducted an additional experiment to retrain and test ECTransNet and MCSF-Net based on the released codes and the recommended parameters using our data augmentation techniques in experiment B, and determined the p-values for comparisons of the Dice and IoU metrics, in which five repetitions were adopted for statistical significance evaluation. The gains in Dice and IoU for ECTransNet and MCSF-Net using our data augmentation method (compared to counterparts using respective data augmentation strategies reported in their original literatures) were statistically insignificant with p-values far greater than 0.05, which indicated that slightly different data augmentation methods did not significantly affect the comparison of these models. Even in some cases, the Dice and IoU values for ECTransNet and MCSF-Net using our data augmentation method were relatively lower than those using their respective data augmentation techniques. Under this circumstance, we leveraged their best scores attained using their default data augmentation methods to avoid the bias introduced in model re-training, which could guarantee relatively fairness of comparisons of the performance of different models. Concretely, our model gained the highest 0.922 <inline-formula>
<mml:math id="m261" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:math>
</inline-formula> and 0.874 <inline-formula>
<mml:math id="m262" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:mi>U</mml:mi></mml:math>
</inline-formula> scores on the Kvasir-SEG dataset. Notably, on the CVC-ClinicDB dataset, our model achieved a <inline-formula>
<mml:math id="m263" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:math>
</inline-formula> value of 0.2% lower slightly than the MCSF-Net, whereas maintaining the same best level as that in the <inline-formula>
<mml:math id="m264" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:mi>U</mml:mi></mml:math>
</inline-formula> metric. And we believed that this difference was very small and its advantage in polyp segmentation accuracy in the face of challenges arising from variations in size and morphology was still substantiated. Moreover, in the case of the unseen datasets CVC-ColonDB, ETIS-LaribPolypDB, and CVC-300, our model achieved the consistently better segmentation performance across nearly all metrics than the ECTransNet and MCSF-Net, as listed in <xref rid="t005" ref-type="table">Table&#x000a0;5</xref>. For the CVC-300 dataset, the improvements of our model in <inline-formula>
<mml:math id="m265" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:math>
</inline-formula> and <inline-formula>
<mml:math id="m266" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:mi>U</mml:mi></mml:math>
</inline-formula> values were not obvious, with 0.3% and 0.5% increases over the ECTransNet, and 0.9% and 1.1% improvements over the MCSF-Net. Perhaps, utilizing domain adaptation technique or data augmentation strategy could contribute to further generalization capability improvements. <xref rid="g008" ref-type="fig">Figure&#x000a0;8</xref> also provided a visual comparison between our model and these two methods. We could see that the proposed Dua-PSNet consistently predicted relatively accurate segmentation maps under various challenges, whilst the ECTransNet and MCSF-Net occasionally generated un-related or incomplete segmentation effects. Different from the ECTransNet and MCSF-Net, our Dua-PSNet combined both Transformer branch and FCN branch in a parallel pattern for modeling multi-scale global contextual semantic relationships and extracting fine low-level features to relieve the loss of useful information. Additionally, the model incorporated the AFA block in the multi-scale fusion process of high-level features to progressively refine global semantic information, thereby enhancing the representation ability of such characteristics and obtaining spatial and location information related to polyps. The ResidualBlock module enabled the extraction of richer local boundary cues of low-level features. The SGLFH module aggregated the resulting local boundary information into global semantic features for delineating more fine-grained contours of polyp lesions and recalibrating their locations. As a result, our Dua-PSNet was able to achieve strong learning and generalization capability.</p><p>Despite generating promising performance, this work had also some limitations and rooms for further enhancement. Firstly, during the model training, we needed to resize the input images into a uniform size of 352&#x02009;&#x000d7;&#x02009;352 so that the model&#x02019;s complexity could be decreased, inevitably resulting in the loss of some information in the images and influencing the overall performance of the model. A future study will attempt to explore the data augmentation (such as Generative Adversarial Network (GAN) [<xref rid="r59" ref-type="bibr">59</xref>] or Conditional Variational Autoencoders (CVAE) [<xref rid="r60" ref-type="bibr">60</xref>] or SAM [<xref rid="r61" ref-type="bibr">61</xref>]) and feature extraction technology [<xref rid="r62" ref-type="bibr">62</xref>] for alleviating information loss. Secondly, the introducing of the Transformer increased the model&#x02019;s complexity. In the future, we plan to seek more efficient strategies [<xref rid="r63" ref-type="bibr">63</xref>] for parameter reduction to reduce the computation complexity of our model without compromising its performance. Finally, our network lacked a comprehensive evaluation under a real-world clinical scenario. The future work is to consider new data from complex real clinical settings to further test the model&#x02019;s generalization ability and thoroughly investigate its potential of practical clinical applications.</p><p>In conclusion, we developed a novel improved dual-aggregation polyp segmentation network called Dua-PSNet. One of advantages of this model was the usage of both parallel branches characterized by Transformer and FCN to work together for enhancing polyp segmentation performance. Beyond that, in the decoder (MFAD) of Transformer branch, we also introduced the AFA module to aggregate multi-scale high-level semantic characteristics and added the ResidualBlock module to focus on local fine edge information, whilst designed the SGLFH component to selectively incorporate low-level fine boundary cues with high-level semantic information. Simultaneously, the FCN branch extracted highly merged multi-scale characteristics at full-size, and compensated for outputs of the Transformer branch into full-size prediction. The experimental results on five publicly available benchmark datasets revealed that our model achieved better segmentation performance while exhibiting higher generalization capabilities compared to existing state-of-the-art approaches. We expect that this work will furnish a new perspective on network architecture design for polyp image segmentation and be further extended for other relative fields of medical image segmentation.</p></sec></body><back><ack><title>Acknowledgements</title><p>We thank Tongren Hospital (Shanghai Jiao Tong University School of Medicine) for their invaluable help.</p></ack><sec sec-type="funding" id="sec6"><title>Funding</title><p><funding-source rid="sp1">National Key Research and Development Program of China<named-content content-type="doi">10.13039/501100012166</named-content></funding-source> (<award-id rid="sp1">2021YFB2802303</award-id>).</p></sec><sec sec-type="COI-statement" id="sec7"><title>Disclosures</title><p>The authors declare no conflict of interest related to this article.</p></sec><sec sec-type="data-availability" id="sec8"><title>Data availability</title><p>The CVC-ClinicDB dataset is accessible at [<xref rid="r64" ref-type="bibr">64</xref>]. The Kvasir-SEG dataset is publicly available at [<xref rid="r65" ref-type="bibr">65</xref>]. The CVC-ColonDB dataset is acquired by [<xref rid="r6" ref-type="bibr">6</xref>]. The ETIS-LaribPolypDB dataset is acquired by [<xref rid="r2" ref-type="bibr">2</xref>]. The CVC-300 dataset is acquired by [<xref rid="r53" ref-type="bibr">53</xref>].</p><p>The source codes will be released at [<xref rid="r66" ref-type="bibr">66</xref>].</p></sec><ref-list><title>References</title><ref id="r1"><label>1</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Xi</surname><given-names>Y.</given-names></name><name><surname>Xu</surname><given-names>P.</given-names></name></person-group>, &#x0201c;<article-title>Global colorectal cancer burden in 2020 and projections to 2040</article-title>,&#x0201d; <source>Transl. Oncol.</source>
<volume>14</volume>(<issue>10</issue>), <fpage>101174</fpage> (<year>2021</year>).<pub-id pub-id-type="doi">10.1016/j.tranon.2021.101174</pub-id>
<pub-id pub-id-type="pmid">34243011</pub-id>
</mixed-citation></ref><ref id="r2"><label>2</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Silva</surname><given-names>J.</given-names></name><name><surname>Histace</surname><given-names>A.</given-names></name><name><surname>Romain</surname><given-names>O.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>Towards embedded detection of polyps in WCE images for early diagnosis of colorectal cancer</article-title>,&#x0201d; <source>Int. J. CARS</source>
<volume>9</volume>(<issue>2</issue>), <fpage>283</fpage>&#x02013;<lpage>293</lpage> (<year>2014</year>).<pub-id pub-id-type="doi">10.1007/s11548-013-0926-3</pub-id>
</mixed-citation></ref><ref id="r3"><label>3</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Tanaka</surname><given-names>S.</given-names></name><name><surname>Saitoh</surname><given-names>Y.</given-names></name><name><surname>Matsuda</surname><given-names>T.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>Evidence-based clinical practice guidelines for management of colorectal polyps</article-title>,&#x0201d; <source>J. Gastroenterol.</source>
<volume>50</volume>(<issue>3</issue>), <fpage>252</fpage>&#x02013;<lpage>260</lpage> (<year>2015</year>).<pub-id pub-id-type="doi">10.1007/s00535-014-1021-4</pub-id>
<pub-id pub-id-type="pmid">25559129</pub-id>
</mixed-citation></ref><ref id="r4"><label>4</label><mixed-citation publication-type="confproc">
<person-group person-group-type="author"><name><surname>Ameling</surname><given-names>S.</given-names></name><name><surname>Wirth</surname><given-names>S.</given-names></name><name><surname>Paulus</surname><given-names>D.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>Texture-based Polyp Detection in Colonoscopy</article-title>,&#x0201d; in <conf-name>Bildverarbeitung f&#x000fc;r die Medizin 2009: Algorithmen-Systeme-Anwendungen Proceedings des Workshops vom 22. bis 25. M&#x000e4;rz 2009</conf-name> in Heidelberg. Springer, pp. <fpage>346</fpage>&#x02013;<lpage>350</lpage>, <year>2009</year>.</mixed-citation></ref><ref id="r5"><label>5</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Karkanis</surname><given-names>S. A.</given-names></name><name><surname>Iakovidis</surname><given-names>D. K.</given-names></name><name><surname>Maroulis</surname><given-names>D. E.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>Computer-aided tumor detection in endoscopic video using color wavelet features</article-title>,&#x0201d; <source>IEEE Trans. Inform. Technol. Biomed.</source>
<volume>7</volume>(<issue>3</issue>), <fpage>141</fpage>&#x02013;<lpage>152</lpage> (<year>2003</year>).<pub-id pub-id-type="doi">10.1109/TITB.2003.813794</pub-id>
</mixed-citation></ref><ref id="r6"><label>6</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Mamonov</surname><given-names>A. V.</given-names></name><name><surname>Figueiredo</surname><given-names>I. N.</given-names></name><name><surname>Figueiredo</surname><given-names>P. N.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>Automated polyp detection in colon capsule endoscopy</article-title>,&#x0201d; <source>IEEE Trans. Med. Imaging</source>
<volume>33</volume>(<issue>7</issue>), <fpage>1488</fpage>&#x02013;<lpage>1502</lpage> (<year>2014</year>).<pub-id pub-id-type="doi">10.1109/TMI.2014.2314959</pub-id>
<pub-id pub-id-type="pmid">24710829</pub-id>
</mixed-citation></ref><ref id="r7"><label>7</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Tajbakhsh</surname><given-names>N.</given-names></name><name><surname>Gurudu</surname><given-names>S. R.</given-names></name><name><surname>Liang</surname><given-names>J.</given-names></name></person-group>, &#x0201c;<article-title>Automated Polyp Detection in Colonoscopy Videos Using Shape and Context Information</article-title>,&#x0201d; <source>IEEE Trans Med. Imaging.</source>
<volume>35</volume>(<issue>2</issue>), <fpage>630</fpage>&#x02013;<lpage>644</lpage> (<year>2015</year>).<pub-id pub-id-type="doi">10.1109/TMI.2015.2487997</pub-id>
<pub-id pub-id-type="pmid">26462083</pub-id>
</mixed-citation></ref><ref id="r8"><label>8</label><mixed-citation publication-type="confproc">
<person-group person-group-type="author"><name><surname>Long</surname><given-names>J.</given-names></name><name><surname>Shelhamer</surname><given-names>E.</given-names></name><name><surname>Darrell</surname><given-names>T.</given-names></name></person-group>, &#x0201c;<article-title>Fully Convolutional Networks for Semantic Segmentation</article-title>,&#x0201d; in <conf-name>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</conf-name>, pp. <fpage>3431</fpage>&#x02013;<lpage>3440</lpage>, <year>2015</year>.</mixed-citation></ref><ref id="r9"><label>9</label><mixed-citation publication-type="confproc">
<person-group person-group-type="author"><name><surname>Akbari</surname><given-names>M.</given-names></name><name><surname>Mohrekesh</surname><given-names>M.</given-names></name><name><surname>Nasr-Esfahani</surname><given-names>E.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>Polyp Segmentation in Colonoscopy Images Using Fully Convolutional Network</article-title>,&#x0201d; in <conf-name>2018 40th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)</conf-name>. <publisher-name>IEEE</publisher-name>, pp. <fpage>69</fpage>&#x02013;<lpage>72</lpage>, <year>2018</year>.</mixed-citation></ref><ref id="r10"><label>10</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Brandao</surname><given-names>P.</given-names></name><name><surname>Zisimopoulos</surname><given-names>O.</given-names></name><name><surname>Mazomenos</surname><given-names>E.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>Towards a computed-aided diagnosis system in colonoscopy: automatic polyp segmentation using convolution neural networks</article-title>,&#x0201d; <source>J. Med. Robot. Res.</source>
<volume>03</volume>(<issue>02</issue>), <fpage>1840002</fpage> (<year>2018</year>).<pub-id pub-id-type="doi">10.1142/S2424905X18400020</pub-id>
</mixed-citation></ref><ref id="r11"><label>11</label><mixed-citation publication-type="confproc">
<person-group person-group-type="author"><name><surname>Ronneberger</surname><given-names>O.</given-names></name><name><surname>Fischer</surname><given-names>P.</given-names></name><name><surname>Brox</surname><given-names>T.</given-names></name></person-group>, &#x0201c;<article-title>U-Net: Convolutional Networks for Biomedical Image Segmentation</article-title>,&#x0201d; in <conf-name>Medical Image Computing and Computer-Assisted Intervention (MICCAI) 2015: 18th International Conference</conf-name>, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18. <publisher-name>Springer</publisher-name>, pp. <fpage>234</fpage>&#x02013;<lpage>241</lpage>, <year>2015</year>.</mixed-citation></ref><ref id="r12"><label>12</label><mixed-citation publication-type="confproc">
<person-group person-group-type="author"><name><surname>Zhou</surname><given-names>Z.W.</given-names></name><name><surname>Rahman Siddiquee</surname><given-names>M. M.</given-names></name><name><surname>Tajbakhsh</surname><given-names>N.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>UNet++: A Nested U-Net Architecture for Medical Image Segmentation</article-title>,&#x0201d; in <conf-name>Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support: 4th International Workshop (DLMIA) 2018, and 8th International Workshop</conf-name>, ML-CDS 2018, Held in <italic>Conjunction with MICCAI</italic> 2018, Granada, Spain, September 20, 2018, Proceedings 4. <publisher-name>Springer</publisher-name>, pp. <fpage>3</fpage>&#x02013;<lpage>11</lpage>, <year>2018</year>.</mixed-citation></ref><ref id="r13"><label>13</label><mixed-citation publication-type="confproc">
<person-group person-group-type="author"><name><surname>Jha</surname><given-names>D.</given-names></name><name><surname>Smedsrud</surname><given-names>P. H.</given-names></name><name><surname>Riegler</surname><given-names>M. A.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>ResUNet++: An Advanced Architecture for Medical Image Segmentation</article-title>,&#x0201d; in <conf-name>2019 IEEE International Symposium on Multimedia (ISM)</conf-name>. <publisher-name>IEEE</publisher-name>, pp. <fpage>225</fpage>&#x02013;<lpage>2255</lpage>, <year>2019</year>.</mixed-citation></ref><ref id="r14"><label>14</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Yeung</surname><given-names>M.</given-names></name><name><surname>Sala</surname><given-names>E.</given-names></name><name><surname>Sch&#x000f6;nlieb</surname><given-names>C. B.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>Focus U-Net: A novel dual attention-gated CNN for polyp segmentation during colonoscopy</article-title>,&#x0201d; <source>Comput. Biol. Med.</source>
<volume>137</volume>, <fpage>104815</fpage> (<year>2021</year>).<pub-id pub-id-type="doi">10.1016/j.compbiomed.2021.104815</pub-id>
<pub-id pub-id-type="pmid">34507156</pub-id>
</mixed-citation></ref><ref id="r15"><label>15</label><mixed-citation publication-type="confproc">
<person-group person-group-type="author"><name><surname>Tomar</surname><given-names>N. K.</given-names></name><name><surname>Jha</surname><given-names>D.</given-names></name><name><surname>Ali</surname><given-names>S.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>DDANet: Dual Decoder Attention Network for Automatic Polyp Segmentation</article-title>,&#x0201d; in <conf-name>PATTERN RECOGN. ICPR International Workshops and Challenges: Virtual Event</conf-name>, January 10-15, 2021, Proceedings, Part VIII. <publisher-name>Springer</publisher-name>, pp. <fpage>307</fpage>&#x02013;<lpage>314</lpage>, <year>2021</year>.</mixed-citation></ref><ref id="r16"><label>16</label><mixed-citation publication-type="confproc">
<person-group person-group-type="author"><name><surname>Sun</surname><given-names>X.Z.</given-names></name><name><surname>Zhang</surname><given-names>P.F.</given-names></name><name><surname>Wang</surname><given-names>D.C.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>Colorectal Polyp Segmentation by U-Net with Dilation Convolution</article-title>,&#x0201d; in <conf-name>2019 18th IEEE International Conference on Machine Learning and Applications (ICMLA)</conf-name>. <publisher-name>IEEE</publisher-name>, pp. <fpage>851</fpage>&#x02013;<lpage>858</lpage>, <year>2019</year>.</mixed-citation></ref><ref id="r17"><label>17</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Banik</surname><given-names>D.</given-names></name><name><surname>Roy</surname><given-names>K.</given-names></name><name><surname>Bhattacharjee</surname><given-names>D.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>Polyp-Net: A Multimodel Fusion Network for Polyp Segmentation</article-title>,&#x0201d; <source>IEEE Trans. Instrum. Meas.</source>
<volume>70</volume>, <fpage>1</fpage>&#x02013;<lpage>12</lpage> (<year>2020</year>).<pub-id pub-id-type="doi">10.1109/TIM.2020.3015607</pub-id>
<pub-id pub-id-type="pmid">33776080</pub-id>
</mixed-citation></ref><ref id="r18"><label>18</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Mahmud</surname><given-names>T.</given-names></name><name><surname>Paul</surname><given-names>B.</given-names></name><name><surname>Fattah</surname><given-names>S. A.</given-names></name></person-group>, &#x0201c;<article-title>PolypSegNet: A modified encoder-decoder architecture for automated polyp segmentation from colonoscopy images</article-title>,&#x0201d; <source>Comput. Biol. Med.</source>
<volume>128</volume>, <fpage>104119</fpage> (<year>2021</year>).<pub-id pub-id-type="doi">10.1016/j.compbiomed.2020.104119</pub-id>
<pub-id pub-id-type="pmid">33254083</pub-id>
</mixed-citation></ref><ref id="r19"><label>19</label><mixed-citation publication-type="confproc">
<person-group person-group-type="author"><name><surname>Fang</surname><given-names>Y.</given-names></name><name><surname>Chen</surname><given-names>C.</given-names></name><name><surname>Yuan</surname><given-names>Y.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>Selective Feature Aggregation Network with Area-Boundary Constraints for Polyp Segmentation</article-title>,&#x0201d; in <conf-name>Medical Image Computing and Computer Assisted Intervention (MICCAI) 2019: 22nd International Conference</conf-name>, Shenzhen, China, October 13-17, 2019, Proceedings, Part I 22. <publisher-name>Springer</publisher-name>, pp. <fpage>302</fpage>&#x02013;<lpage>310</lpage>, <year>2019</year>.</mixed-citation></ref><ref id="r20"><label>20</label><mixed-citation publication-type="confproc">
<person-group person-group-type="author"><name><surname>Murugesan</surname><given-names>B.</given-names></name><name><surname>Sarveswaran</surname><given-names>K.</given-names></name><name><surname>Shankaranarayana</surname><given-names>S. M.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>Psi-Net: Shape and boundary aware joint multi-task deep network for medical image segmentation</article-title>,&#x0201d; in <conf-name>2019 41st Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)</conf-name>. <publisher-name>IEEE</publisher-name>, pp. <fpage>7223</fpage>&#x02013;<lpage>7226</lpage>, <year>2019</year>.</mixed-citation></ref><ref id="r21"><label>21</label><mixed-citation publication-type="confproc">
<person-group person-group-type="author"><name><surname>Fan</surname><given-names>D.P.</given-names></name><name><surname>Ji</surname><given-names>G.P.</given-names></name><name><surname>Zhou</surname><given-names>T.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>PraNet: Parallel Reverse Attention Network for Polyp Segmentation</article-title>,&#x0201d; in <conf-name>International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI)</conf-name>. <publisher-name>Springer</publisher-name>, pp. <fpage>263</fpage>&#x02013;<lpage>273</lpage>, <year>2020</year>.</mixed-citation></ref><ref id="r22"><label>22</label><mixed-citation publication-type="confproc">
<person-group person-group-type="author"><name><surname>Wei</surname><given-names>J.</given-names></name><name><surname>Hu</surname><given-names>Y.W.</given-names></name><name><surname>Zhang</surname><given-names>R.M.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>Shallow Attention Network for Polyp Segmentation</article-title>,&#x0201d; in <conf-name>Medical Image Computing and Computer Assisted Intervention (MICCAI) 2021: 24th International Conference</conf-name>, Strasbourg, France, September 27-October 1, 2021, Proceedings, Part I 24. <publisher-name>Springer</publisher-name>, pp. <fpage>699</fpage>&#x02013;<lpage>708</lpage>, <year>2021</year>.</mixed-citation></ref><ref id="r23"><label>23</label><mixed-citation publication-type="confproc">
<person-group person-group-type="author"><name><surname>Kim</surname><given-names>T.</given-names></name><name><surname>Lee</surname><given-names>H.</given-names></name><name><surname>Kim</surname><given-names>D.</given-names></name></person-group>, &#x0201c;<article-title>UACANet: Uncertainty Augmented Context Attention for Polyp Segmentation</article-title>,&#x0201d; in <conf-name>Proceedings of the 29th ACM International Conference on Multimedia</conf-name>, pp. <fpage>2167</fpage>&#x02013;<lpage>2175</lpage>, <year>2021</year>.</mixed-citation></ref><ref id="r24"><label>24</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Song</surname><given-names>P. F.</given-names></name><name><surname>Li</surname><given-names>J. J.</given-names></name><name><surname>Fan</surname><given-names>H.</given-names></name></person-group>, &#x0201c;<article-title>Attention based multi-scale parallel network for polyp segmentation</article-title>,&#x0201d; <source>Comput. Biol. Med.</source>
<volume>146</volume>, <fpage>105476</fpage> (<year>2022</year>).<pub-id pub-id-type="doi">10.1016/j.compbiomed.2022.105476</pub-id>
<pub-id pub-id-type="pmid">35483226</pub-id>
</mixed-citation></ref><ref id="r25"><label>25</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Lin</surname><given-names>Y.</given-names></name><name><surname>Wu</surname><given-names>J. C.</given-names></name><name><surname>Xiao</surname><given-names>G. B.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>BSCA-Net: Bit Slicing Context Attention Network for Polyp Segmentation</article-title>,&#x0201d; <source>Pattern Recogn.</source>
<volume>132</volume>, <fpage>108917</fpage> (<year>2022</year>).<pub-id pub-id-type="doi">10.1016/j.patcog.2022.108917</pub-id>
</mixed-citation></ref><ref id="r26"><label>26</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Shen</surname><given-names>T. P.</given-names></name><name><surname>Li</surname><given-names>X. G.</given-names></name></person-group>, &#x0201c;<article-title>Automatic polyp image segmentation and cancer prediction based on deep learning</article-title>,&#x0201d; <source>Front. Oncol.</source>
<volume>12</volume>, <fpage>1087438</fpage> (<year>2023</year>).<pub-id pub-id-type="doi">10.3389/fonc.2022.1087438</pub-id>
<pub-id pub-id-type="pmid">36713495</pub-id>
</mixed-citation></ref><ref id="r27"><label>27</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Li</surname><given-names>Z. X.</given-names></name><name><surname>Zhang</surname><given-names>N.</given-names></name><name><surname>Gong</surname><given-names>H. L.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>MFA-Net: Multiple Feature Association Network for medical image segmentation</article-title>,&#x0201d; <source>Comput. Biol. Med.</source>
<volume>158</volume>, <fpage>106834</fpage> (<year>2023</year>).<pub-id pub-id-type="doi">10.1016/j.compbiomed.2023.106834</pub-id>
<pub-id pub-id-type="pmid">37003067</pub-id>
</mixed-citation></ref><ref id="r28"><label>28</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Zhou</surname><given-names>T.</given-names></name><name><surname>Zhou</surname><given-names>Y.</given-names></name><name><surname>He</surname><given-names>K. L.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>Cross-level feature aggregation network for polyp segmentation</article-title>,&#x0201d; <source>Pattern Recogn.</source>
<volume>140</volume>, <fpage>109555</fpage> (<year>2023</year>).<pub-id pub-id-type="doi">10.1016/j.patcog.2023.109555</pub-id>
</mixed-citation></ref><ref id="r29"><label>29</label><mixed-citation publication-type="confproc">
<person-group person-group-type="author"><name><surname>Zhao</surname><given-names>X.Q.</given-names></name><name><surname>Zhang</surname><given-names>L.H.</given-names></name><name><surname>Lu</surname><given-names>H.C.</given-names></name></person-group>, &#x0201c;<article-title>Automatic Polyp Segmentation via Multi-scale Subtraction Network</article-title>,&#x0201d; in <conf-name>Medical Image Computing and Computer Assisted Intervention (MICCAI) 2021: 24th International Conference</conf-name>, Strasbourg, France, September 27-October 1, 2021, Proceedings, Part I 24. <publisher-name>Springer</publisher-name>, pp. <fpage>120</fpage>&#x02013;<lpage>130</lpage>, <year>2021</year>.</mixed-citation></ref><ref id="r30"><label>30</label><mixed-citation publication-type="preprint">
<person-group person-group-type="author"><name><surname>Zhao</surname><given-names>X.Q.</given-names></name><name><surname>Jia</surname><given-names>H.P.</given-names></name><name><surname>Pang</surname><given-names>Y.W.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>M<sup>2</sup>SNet: Multi-scale in Multi-scale Subtraction Network for Medical Image Segmentation</article-title>,&#x0201d; <source>arXiv</source>, , (<year>2023</year>).<pub-id pub-id-type="doi">10.48550/arXiv.2303.10894</pub-id>
</mixed-citation></ref><ref id="r31"><label>31</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Liu</surname><given-names>W. K.</given-names></name><name><surname>Li</surname><given-names>Z. G.</given-names></name><name><surname>Xia</surname><given-names>J. A.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>MCSF-Net: a multi-scale channel spatial fusion network for real-time polyp segmentation</article-title>,&#x0201d; <source>Phys. Med. Biol.</source>
<volume>31</volume>(<issue>17</issue>), <fpage>175041</fpage> (<year>2023</year>).<pub-id pub-id-type="doi">10.1088/1361-6560/acf090</pub-id>
</mixed-citation></ref><ref id="r32"><label>32</label><mixed-citation publication-type="preprint">
<person-group person-group-type="author"><name><surname>Dosovitskiy</surname><given-names>A.</given-names></name><name><surname>Beyer</surname><given-names>L.</given-names></name><name><surname>Kolesnikov</surname><given-names>A.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>An image is worth 16&#x000d7;16 words: Transformers for image recognition at scale</article-title>,&#x0201d; <source>arXiv</source>, , (<year>2020</year>).<pub-id pub-id-type="doi">10.48550/arXiv.2010.11929</pub-id>
</mixed-citation></ref><ref id="r33"><label>33</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Dong</surname><given-names>B.</given-names></name><name><surname>Wang</surname><given-names>W. H.</given-names></name><name><surname>Fan</surname><given-names>D. P.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>Polyp-PVT: Polyp Segmentation with Pyramid Vision Transformers</article-title>,&#x0201d; <source>CAAI Artificial Intelligence Research</source>
<volume>2</volume>, <fpage>9150015</fpage> (<year>2023</year>).<pub-id pub-id-type="doi">10.26599/AIR.2023.9150015</pub-id>
</mixed-citation></ref><ref id="r34"><label>34</label><mixed-citation publication-type="preprint">
<person-group person-group-type="author"><name><surname>Tang</surname><given-names>F.L.</given-names></name><name><surname>Huang</surname><given-names>Q.M.</given-names></name><name><surname>Wang</surname><given-names>J.F.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>DuAT: Dual-Aggregation Transformer Network for Medical Image Segmentation</article-title>,&#x0201d; <source>arXiv</source>, , (<year>2022</year>).<pub-id pub-id-type="doi">10.48550/arXiv.2212.11677</pub-id>
</mixed-citation></ref><ref id="r35"><label>35</label><mixed-citation publication-type="confproc">
<person-group person-group-type="author"><name><surname>Wang</surname><given-names>J.F.</given-names></name><name><surname>Huang</surname><given-names>Q.M.</given-names></name><name><surname>Tang</surname><given-names>F.L.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>Stepwise Feature Fusion: Local Guides Global</article-title>,&#x0201d; in <conf-name>International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI)</conf-name>. <publisher-name>Springer</publisher-name>, pp. <fpage>110</fpage>&#x02013;<lpage>120</lpage>, <year>2022</year>.</mixed-citation></ref><ref id="r36"><label>36</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Nachmani</surname><given-names>R.</given-names></name><name><surname>Nidal</surname><given-names>I.</given-names></name><name><surname>Robinson</surname><given-names>D.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>Segmentation of polyps based on pyramid vision transformers and residual block for real-time endoscopy imaging</article-title>,&#x0201d; <source>Journal of Pathology Informatics</source>
<volume>14</volume>, <fpage>100197</fpage> (<year>2023</year>).<pub-id pub-id-type="doi">10.1016/j.jpi.2023.100197</pub-id>
<pub-id pub-id-type="pmid">36844703</pub-id>
</mixed-citation></ref><ref id="r37"><label>37</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Chang</surname><given-names>Q.</given-names></name><name><surname>Ahmad</surname><given-names>D.</given-names></name><name><surname>Toth</surname><given-names>J.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>ESFPNet: efficient deep learning architecture for real-time lesion segmentation in autofluorescence bronchoscopic video</article-title>,&#x0201d; <source>Medical Imaging 2023: Biomedical Applications in Molecular, Structural, and Functional Imaging</source>
<volume>12468</volume>, <fpage>1246803</fpage> (<year>2023</year>).</mixed-citation></ref><ref id="r38"><label>38</label><mixed-citation publication-type="preprint">
<person-group person-group-type="author"><name><surname>Chen</surname><given-names>J.N.</given-names></name><name><surname>Lu</surname><given-names>Y.Y.</given-names></name><name><surname>Yu</surname><given-names>Q.H.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>TransUNet: Transformers Make Strong Encoders for Medical Image Segmentation</article-title>,&#x0201d; <source>arXiv</source>, , (<year>2021</year>).<pub-id pub-id-type="doi">10.48550/arXiv.2102.04306</pub-id>
</mixed-citation></ref><ref id="r39"><label>39</label><mixed-citation publication-type="confproc">
<person-group person-group-type="author"><name><surname>Zhang</surname><given-names>Y.D.</given-names></name><name><surname>Liu</surname><given-names>H.Y.</given-names></name><name><surname>Hu</surname><given-names>Q.</given-names></name></person-group>, &#x0201c;<article-title>TransFuse: Fusing Transformers and CNNs for Medical Image Segmentation</article-title>,&#x0201d; in <conf-name>Medical Image Computing and Computer Assisted Intervention (MICCAI) 2021: 24th International Conference</conf-name>, Strasbourg, France, September 27-October 1, 2021, Proceedings, Part I 24. <publisher-name>Springer</publisher-name>, pp. <fpage>14</fpage>&#x02013;<lpage>24</lpage>, <year>2021</year>.</mixed-citation></ref><ref id="r40"><label>40</label><mixed-citation publication-type="confproc">
<person-group person-group-type="author"><name><surname>Cai</surname><given-names>L.H.</given-names></name><name><surname>Wu</surname><given-names>M.J.</given-names></name><name><surname>Chen</surname><given-names>L.J.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>Using Guided Self-attention with Local Information for Polyp Segmentation</article-title>,&#x0201d; in <conf-name>International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI)</conf-name>. <publisher-name>Springer</publisher-name>, pp. <fpage>629</fpage>&#x02013;<lpage>638</lpage>, <year>2022</year>.</mixed-citation></ref><ref id="r41"><label>41</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Zhang</surname><given-names>W. C.</given-names></name><name><surname>Fu</surname><given-names>C.</given-names></name><name><surname>Zheng</surname><given-names>Y.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>HSNet: A hybrid semantic network for polyp segmentation</article-title>,&#x0201d; <source>Comput. Biol. Med.</source>
<volume>150</volume>, <fpage>106173</fpage> (<year>2022</year>).<pub-id pub-id-type="doi">10.1016/j.compbiomed.2022.106173</pub-id>
<pub-id pub-id-type="pmid">36257278</pub-id>
</mixed-citation></ref><ref id="r42"><label>42</label><mixed-citation publication-type="confproc">
<person-group person-group-type="author"><name><surname>Sanderson</surname><given-names>E.</given-names></name><name><surname>Matuszewski</surname><given-names>B. J.</given-names></name></person-group>, &#x0201c;<article-title>FCN-Transformer Feature Fusion for Polyp Segmentation</article-title>,&#x0201d; in <conf-name>Annual Conference on Medical Image Understanding and Analysis (MIUA)</conf-name>. <publisher-name>Springer</publisher-name>, pp. <fpage>892</fpage>&#x02013;<lpage>907</lpage>, <year>2022</year>.</mixed-citation></ref><ref id="r43"><label>43</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Zhang</surname><given-names>Y. Y.</given-names></name><name><surname>Liu</surname><given-names>L.</given-names></name><name><surname>Han</surname><given-names>Z. Y.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>TranSEFusionNet: Deep fusion network for colorectal polyp segmentation</article-title>,&#x0201d; <source>Biomed. Signal Proces.</source>
<volume>86</volume>, <fpage>105133</fpage> (<year>2023</year>).<pub-id pub-id-type="doi">10.1016/j.bspc.2023.105133</pub-id>
</mixed-citation></ref><ref id="r44"><label>44</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Liu</surname><given-names>W. K.</given-names></name><name><surname>Li</surname><given-names>Z. G.</given-names></name><name><surname>Li</surname><given-names>C. Y.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>ECTransNet: An Automatic Polyp Segmentation Network Based on Multi-scale Edge Complementary</article-title>,&#x0201d; <source>J. Digit. Imaging</source>
<volume>36</volume>(<issue>6</issue>), <fpage>2427</fpage>&#x02013;<lpage>2440</lpage> (<year>2023</year>).<pub-id pub-id-type="doi">10.1007/s10278-023-00885-y</pub-id>
<pub-id pub-id-type="pmid">37491542</pub-id>
</mixed-citation></ref><ref id="r45"><label>45</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Vaswani</surname><given-names>A.</given-names></name><name><surname>Shazeer</surname><given-names>N.</given-names></name><name><surname>Parmar</surname><given-names>N.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>Attention is all you need</article-title>,&#x0201d; <source>Advances in Neural Information Processing Systems (NeurIPS)</source>
<volume>30</volume>, <fpage>1</fpage> (<year>2017</year>).</mixed-citation></ref><ref id="r46"><label>46</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Wang</surname><given-names>W. H.</given-names></name><name><surname>Xie</surname><given-names>E. Z.</given-names></name><name><surname>Li</surname><given-names>X.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>PVT v2: Improved baselines with Pyramid Vision Transformer</article-title>,&#x0201d; <source>Computational Visual Media</source>
<volume>8</volume>(<issue>3</issue>), <fpage>415</fpage>&#x02013;<lpage>424</lpage> (<year>2022</year>).<pub-id pub-id-type="doi">10.1007/s41095-022-0274-8</pub-id>
</mixed-citation></ref><ref id="r47"><label>47</label><mixed-citation publication-type="confproc">
<person-group person-group-type="author"><name><surname>He</surname><given-names>K.M.</given-names></name><name><surname>Zhang</surname><given-names>X.Y.</given-names></name><name><surname>Ren</surname><given-names>S.Q.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>Deep Residual Learning for Image Recognition</article-title>,&#x0201d; in <conf-name>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</conf-name>, pp. <fpage>770</fpage>&#x02013;<lpage>778</lpage>, <year>2016</year>.</mixed-citation></ref><ref id="r48"><label>48</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Xie</surname><given-names>E. Z.</given-names></name><name><surname>Wang</surname><given-names>W. H.</given-names></name><name><surname>Yu</surname><given-names>Z. D.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>Segformer: Simple and efficient design for semantic segmentation with transformers</article-title>,&#x0201d; <source>Advances in Neural Information Processing Systems (NeurIPS)</source>
<volume>34</volume>, <fpage>12077</fpage>&#x02013;<lpage>12090</lpage> (<year>2021</year>).</mixed-citation></ref><ref id="r49"><label>49</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Russakovsky</surname><given-names>O.</given-names></name><name><surname>Deng</surname><given-names>J.</given-names></name><name><surname>Su</surname><given-names>H.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>ImageNet Large Scale Visual Recognition Challenge</article-title>,&#x0201d; <source>Int. J. Comput. Vision</source>
<volume>115</volume>(<issue>3</issue>), <fpage>211</fpage>&#x02013;<lpage>252</lpage> (<year>2015</year>).<pub-id pub-id-type="doi">10.1007/s11263-015-0816-y</pub-id>
</mixed-citation></ref><ref id="r50"><label>50</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Yang</surname><given-names>D. D.</given-names></name><name><surname>Li</surname><given-names>Y. Y.</given-names></name><name><surname>Yu</surname><given-names>J. K.</given-names></name></person-group>, &#x0201c;<article-title>Multi-task thyroid tumor segmentation based on the joint loss function</article-title>,&#x0201d; <source>Biomed. Signal Proces</source>
<volume>79</volume>, <fpage>104249</fpage> (<year>2023</year>).<pub-id pub-id-type="doi">10.1016/j.bspc.2022.104249</pub-id>
</mixed-citation></ref><ref id="r51"><label>51</label><mixed-citation publication-type="confproc">
<person-group person-group-type="author"><name><surname>Jha</surname><given-names>D.</given-names></name><name><surname>Smedsrud</surname><given-names>P. H.</given-names></name><name><surname>Riegler</surname><given-names>M. A.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>Kvasir-SEG: A Segmented Polyp Dataset</article-title>,&#x0201d; in <conf-name>MultiMedia Modeling: 26th International Conference (MMM) 2020</conf-name>, Daejeon, South Korea, January 5-8, 2020, Proceedings, Part II 26. <publisher-name>Springer</publisher-name>, pp. <fpage>451</fpage>&#x02013;<lpage>462</lpage>, <year>2020</year>.</mixed-citation></ref><ref id="r52"><label>52</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Bernal</surname><given-names>J.</given-names></name><name><surname>S&#x000e1;nchez</surname><given-names>F. J.</given-names></name><name><surname>Fern&#x000e1;ndez-Esparrach</surname><given-names>G.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>WM-DOVA maps for accurate polyp highlighting in colonoscopy: Validation vs. saliency maps from physicians</article-title>,&#x0201d; <source>Comput. Med. Imag. Grap</source>
<volume>43</volume>, <fpage>99</fpage>&#x02013;<lpage>111</lpage> (<year>2015</year>).<pub-id pub-id-type="doi">10.1016/j.compmedimag.2015.02.007</pub-id>
</mixed-citation></ref><ref id="r53"><label>53</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>V&#x000e1;zquez</surname><given-names>D.</given-names></name><name><surname>Bernal</surname><given-names>J.</given-names></name><name><surname>S&#x000e1;nchez</surname><given-names>F. J.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>A Benchmark for Endoluminal Scene Segmentation of Colonoscopy Images</article-title>,&#x0201d; <source>J. Healthc. Eng.</source>
<volume>2017</volume>, <fpage>4037190</fpage> (<year>2017</year>).<pub-id pub-id-type="doi">10.1155/2017/4037190</pub-id>
<pub-id pub-id-type="pmid">29065595</pub-id>
</mixed-citation></ref><ref id="r54"><label>54</label><mixed-citation publication-type="confproc">
<person-group person-group-type="author"><name><surname>Perazzi</surname><given-names>F.</given-names></name><name><surname>Kr&#x000e4;henb&#x000fc;hl</surname><given-names>P.</given-names></name><name><surname>Pritch</surname><given-names>Y.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>Saliency filters: Contrast based filtering for salient region detection</article-title>,&#x0201d; in <conf-name>2012 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</conf-name>. <publisher-name>IEEE</publisher-name>, pp. <fpage>733</fpage>&#x02013;<lpage>740</lpage>, <year>2012</year>.</mixed-citation></ref><ref id="r55"><label>55</label><mixed-citation publication-type="preprint">
<person-group person-group-type="author"><name><surname>Fan</surname><given-names>D.P.</given-names></name><name><surname>Gong</surname><given-names>C.</given-names></name><name><surname>Cao</surname><given-names>Y.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>Enhanced-alignment Measure for Binary Foreground Map Evaluation</article-title>,&#x0201d; <source>arXiv</source>, (<year>2018</year>).</mixed-citation></ref><ref id="r56"><label>56</label><mixed-citation publication-type="confproc">
<person-group person-group-type="author"><name><surname>Fan</surname><given-names>D.P.</given-names></name><name><surname>Cheng</surname><given-names>M.M.</given-names></name><name><surname>Liu</surname><given-names>Y.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>Structure-Measure: A New Way to Evaluate Foreground Maps</article-title>,&#x0201d; in <conf-name>Proceedings of the IEEE International Conference on Computer Vision (ICCV)</conf-name>, pp. <fpage>4548</fpage>&#x02013;<lpage>4557</lpage>, <year>2017</year>.</mixed-citation></ref><ref id="r57"><label>57</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Fan</surname><given-names>D. P.</given-names></name><name><surname>Ji</surname><given-names>G. P.</given-names></name><name><surname>Qin</surname><given-names>X.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>Cognitive vision inspired object segmentation metric and loss function</article-title>,&#x0201d; <source>Scientia Sinica Informationis</source>
<volume>6</volume>(<issue>6</issue>), <fpage>1</fpage> (<year>2021</year>).<pub-id pub-id-type="doi">10.1360/SSI-2020-0370</pub-id>
</mixed-citation></ref><ref id="r58"><label>58</label><mixed-citation publication-type="preprint">
<person-group person-group-type="author"><name><surname>Loshchilov</surname><given-names>I.</given-names></name><name><surname>Hutter</surname><given-names>F.</given-names></name></person-group>, &#x0201c;<article-title>Decoupled Weight Decay Regularization</article-title>,&#x0201d; <source>arXiv</source>, (<year>2017</year>).<pub-id pub-id-type="doi">10.48550/arXiv.1711.05101</pub-id>
</mixed-citation></ref><ref id="r59"><label>59</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Creswell</surname><given-names>A.</given-names></name><name><surname>White</surname><given-names>T.</given-names></name><name><surname>Dumoulin</surname><given-names>V.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>Generative Adversarial Networks: An overview</article-title>,&#x0201d; <source>IEEE Signal Proc. Mag.</source>
<volume>35</volume>(<issue>1</issue>), <fpage>53</fpage>&#x02013;<lpage>65</lpage> (<year>2018</year>).<pub-id pub-id-type="doi">10.1109/MSP.2017.2765202</pub-id>
</mixed-citation></ref><ref id="r60"><label>60</label><mixed-citation publication-type="confproc">
<person-group person-group-type="author"><name><surname>Kim</surname><given-names>J.</given-names></name><name><surname>Kong</surname><given-names>J.</given-names></name><name><surname>Son</surname><given-names>J.</given-names></name></person-group>, &#x0201c;<article-title>Conditional variational autoencoder with adversarial learning for end-to-end text-to-speech</article-title>,&#x0201d; in <conf-name>International Conference on Machine Learning (ICML)</conf-name>, pp. <fpage>5530</fpage>&#x02013;<lpage>5540</lpage>, <year>2021</year>.</mixed-citation></ref><ref id="r61"><label>61</label><mixed-citation publication-type="preprint">
<person-group person-group-type="author"><name><surname>Zhang</surname><given-names>Y.Z.</given-names></name><name><surname>Zhou</surname><given-names>T.</given-names></name><name><surname>Wang</surname><given-names>S.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>Input augmentation with SAM: boosting medical image segmentation with segmentation foundation model</article-title>,&#x0201d; <source>arXiv</source>, , (<year>2023</year>).<pub-id pub-id-type="doi">10.48550/arXiv.2304.11332</pub-id>
</mixed-citation></ref><ref id="r62"><label>62</label><mixed-citation publication-type="confproc">
<person-group person-group-type="author"><name><surname>Khalid</surname><given-names>S.</given-names></name><name><surname>Khalil</surname><given-names>T.</given-names></name><name><surname>Nasreen</surname><given-names>S.</given-names></name></person-group>, &#x0201c;<article-title>A survey of feature selection and feature extraction techniques in machine learning</article-title>,&#x0201d; in <conf-name>Science and Information Conference (SAI)</conf-name>, London, UK, pp. <fpage>372</fpage>&#x02013;<lpage>378</lpage>, <year>2014</year>.</mixed-citation></ref><ref id="r63"><label>63</label><mixed-citation publication-type="preprint">
<person-group person-group-type="author"><name><surname>Dinh</surname><given-names>B.D.</given-names></name><name><surname>Nguyen</surname><given-names>T.T.</given-names></name><name><surname>Tran</surname><given-names>T.T.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>1 M parameters are enough? A Lightweight CNN-based model for medical image segmentation</article-title>,&#x0201d; <source>arXiv</source>, , (<year>2023</year>).<pub-id pub-id-type="doi">10.48550/arXiv.2306.16103</pub-id>
</mixed-citation></ref><ref id="r64"><label>64</label><mixed-citation publication-type="other">
<person-group person-group-type="author"><name><surname>Silva</surname><given-names>J.</given-names></name><name><surname>Sanchez</surname><given-names>F. J.</given-names></name><name><surname>Fern&#x000e1;ndez-Esparrach</surname><given-names>G.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>WM-DOVA maps for accurate polyp highlighting in colonoscopy: Validation vs. saliency maps from physicians</article-title>,&#x0201d; <source>Computerized Medical Imaging and Graphics</source>, <year>2015</year>, <ext-link xlink:href="https://polyp.grand-challenge.org/CVCClinicDB/" ext-link-type="uri">https://polyp.grand-challenge.org/CVCClinicDB/</ext-link>
</mixed-citation></ref><ref id="r65"><label>65</label><mixed-citation publication-type="other">
<person-group person-group-type="author"><name><surname>Jha</surname><given-names>D.</given-names></name><name><surname>Sedsrid</surname><given-names>P. H.</given-names></name><name><surname>Riegler</surname><given-names>M. A.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>Kvasir-SEG: A Segmented Polyp Dataset</article-title>,&#x0201d; <source>Lecture Notes in Computer Science</source>, vol. 11962 <year>2020</year>, <pub-id pub-id-type="doi">10.1007/978-3-030-37734-2_37</pub-id>
</mixed-citation></ref><ref id="r66"><label>66</label><mixed-citation publication-type="other">
<person-group person-group-type="author"><name><surname>Li</surname><given-names>F.</given-names></name><name><surname>Huang</surname><given-names>Z. T.</given-names></name><name><surname>Zhou</surname><given-names>L.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>Source Code</article-title>,&#x0201d; <source>Github</source>, <year>2024</year>, <ext-link xlink:href="https://github.com/Zachary-Hwang/Dua-PSNet" ext-link-type="uri">https://github.com/Zachary-Hwang/Dua-PSNet</ext-link>
</mixed-citation></ref></ref-list></back></article>