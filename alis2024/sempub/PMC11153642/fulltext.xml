<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd"> 
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><?properties open_access?><?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?><?DTDIdentifier.IdentifierType public?><?SourceDTD.DTDName A++V2.4.dtd?><?SourceDTD.Version 2.4?><?ConverterInfo.XSLTName springer2nlmx2.xsl?><?ConverterInfo.Version 1?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sci Rep</journal-id><journal-id journal-id-type="iso-abbrev">Sci Rep</journal-id><journal-title-group><journal-title>Scientific Reports</journal-title></journal-title-group><issn pub-type="epub">2045-2322</issn><publisher><publisher-name>Nature Publishing Group UK</publisher-name><publisher-loc>London</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmcid">11153642</article-id><article-id pub-id-type="pmid">38839794</article-id>
<article-id pub-id-type="publisher-id">63279</article-id><article-id pub-id-type="doi">10.1038/s41598-024-63279-2</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>A novel model for relation prediction in knowledge graphs exploiting semantic and structural feature integration</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Yang</surname><given-names>Jianliang</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author"><name><surname>Lu</surname><given-names>Guoxuan</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author"><name><surname>He</surname><given-names>Siyuan</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author"><name><surname>Cao</surname><given-names>Qiuer</given-names></name><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Liu</surname><given-names>Yuenan</given-names></name><address><email>liuyuenan@ruc.edu.cn</email></address><xref ref-type="aff" rid="Aff1">1</xref></contrib><aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/041pakw92</institution-id><institution-id institution-id-type="GRID">grid.24539.39</institution-id><institution-id institution-id-type="ISNI">0000 0004 0368 8103</institution-id><institution>School of Information Resource Management, </institution><institution>Renmin University of China, </institution></institution-wrap>Beijing, China </aff><aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/041pakw92</institution-id><institution-id institution-id-type="GRID">grid.24539.39</institution-id><institution-id institution-id-type="ISNI">0000 0004 0368 8103</institution-id><institution>School of Journalism and Communication, </institution><institution>Renmin University of China, </institution></institution-wrap>Beijing, China </aff></contrib-group><pub-date pub-type="epub"><day>5</day><month>6</month><year>2024</year></pub-date><pub-date pub-type="pmc-release"><day>5</day><month>6</month><year>2024</year></pub-date><pub-date pub-type="collection"><year>2024</year></pub-date><volume>14</volume><elocation-id>12962</elocation-id><history><date date-type="received"><day>4</day><month>1</month><year>2024</year></date><date date-type="accepted"><day>27</day><month>5</month><year>2024</year></date></history><permissions><copyright-statement>&#x000a9; The Author(s) 2024</copyright-statement><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p></license></permissions><abstract id="Abs1"><p id="Par1">Relation prediction is a critical task in knowledge graph completion and associated downstream tasks that rely on knowledge representation. Previous studies indicate that both structural features and semantic information are meaningful for predicting missing relations in knowledge graphs. This has led to the development of two types of methods: structure-based methods and semantics-based methods. Since these two approaches represent two distinct learning paradigms, it is difficult to fully utilize both sets of features within a single learning model, especially deep features. As a result, existing studies usually focus on only one type of feature. This leads to an insufficient representation of knowledge in current methods and makes them prone to overlooking certain patterns when predicting missing relations. In this study, we introduce a novel model, RP-ISS, which combines deep semantic and structural features for relation prediction. The RP-ISS model utilizes a two-part architecture, with the first component being a RoBERTa module that is responsible for extracting semantic features from entity nodes. The second part of the system employs an edge-based relational message-passing network designed to capture and interpret structural information within the data. To alleviate the computational burden of the message-passing network on the RoBERTa module during the sampling process, RP-ISS introduces a node embedding memory bank, which updates asynchronously to circumvent excessive computation. The model was assessed on three publicly accessible datasets (WN18RR, WN18, and FB15k-237), and the results revealed that RP-ISS surpasses all baseline methods across all evaluation metrics. Moreover, RP-ISS showcases robust performance in graph inductive learning.</p></abstract><kwd-group xml:lang="en"><title>Keywords</title><kwd>Knowledge graph</kwd><kwd>Relation prediction</kwd><kwd>BERT</kwd><kwd>Relation message passing</kwd></kwd-group><kwd-group kwd-group-type="npg-subject"><title>Subject terms</title><kwd>Computational science</kwd><kwd>Computer science</kwd><kwd>Information technology</kwd><kwd>Scientific data</kwd></kwd-group><funding-group><award-group><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100012456</institution-id><institution>National Social Science Fund of China</institution></institution-wrap></funding-source><award-id>No. 21CTQ030</award-id><principal-award-recipient><name><surname>Yang</surname><given-names>Jianliang</given-names></name></principal-award-recipient></award-group></funding-group><funding-group><award-group><funding-source><institution>Research Fund of Renmin University of China</institution></funding-source><award-id>No.23XNF039</award-id><principal-award-recipient><name><surname>Yang</surname><given-names>Jianliang</given-names></name></principal-award-recipient></award-group></funding-group><custom-meta-group><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#x000a9; Springer Nature Limited 2024</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1"><title>Introduction</title><p id="Par2">Knowledge graphs are a critical advancement in the field of information science and computer science, embodying a structured representation of human knowledge. In an age marked by the rise of large language models such as GPT-4, knowledge graphs act as a complementary resource to these models, providing structured insights that enhance and deepen their semantic understanding<sup><xref ref-type="bibr" rid="CR1">1</xref>,<xref ref-type="bibr" rid="CR2">2</xref></sup>. These graphs find applications in diverse fields ranging from semantic search and question answering to intelligent decision-making<sup><xref ref-type="bibr" rid="CR3">3</xref>&#x02013;<xref ref-type="bibr" rid="CR5">5</xref></sup>. However, the construction of robust knowledge graphs has been fraught with challenges, primarily due to their often incomplete and fragmented nature. For instance, about 71% of the entities in Freebase lack data regarding their place of birth<sup><xref ref-type="bibr" rid="CR6">6</xref></sup>, and DBpedia lacks descriptions for over 66% of scientists<sup><xref ref-type="bibr" rid="CR7">7</xref></sup>. These substantial gaps in data necessitate a focused effort towards knowledge graph completion (KGC), which aims to enrich the graph by addressing missing data, thereby enhancing the comprehensiveness and robustness of the complex structure<sup><xref ref-type="bibr" rid="CR8">8</xref>,<xref ref-type="bibr" rid="CR9">9</xref></sup>.</p><p id="Par3">Both structural features and semantic features are meaningful for relation prediction. For example, based on structural information, we can infer a new &#x02019;Grandfather_of&#x02019; relation from two consecutive &#x02018;Father_of&#x02019; relations. From the perspective of semantic information, we can deduce a corresponding &#x02018;Child_of&#x02019; relation from a &#x02018;Mother_of&#x02019; relation. However, these two types of features represent two different learning paradigms: structure-based methods and semantics-based methods. Structure-based methods like TransE<sup><xref ref-type="bibr" rid="CR10">10</xref></sup>, TransR<sup><xref ref-type="bibr" rid="CR11">11</xref></sup> and R-GCN<sup><xref ref-type="bibr" rid="CR12">12</xref></sup>, treat entities and relations as nodes and edges, using structure patterns to learn representations of nodes and edges to predict missing relations. On the other hand, semantics-based methods like KG-BERT<sup><xref ref-type="bibr" rid="CR13">13</xref></sup>, treat entities and edges as text, transforming the problem of relation prediction into one of sentence classification. Since these two approaches are completely different learning paradigms, it is difficult to fully utilize both types of features within a single learning model, especially deep features<sup><xref ref-type="bibr" rid="CR14">14</xref></sup>. These issues prevent fully leveraging the combination of structural and semantic features, which is crucial for effective prediction. Thus, there's a need for methods that can more skillfully utilize both structural and semantic aspects.</p><p id="Par4">Our research addresses the challenge of effectively integrating structural and semantic information in knowledge graph completion, a problem inadequately solved by existing methods. This led to the creation of RP-ISS (Relation Predictor Integrating Semantic and Structural Features), a novel model designed for balanced and deep integration of both modalities. The RP-ISS was inspired by the Lin et al.<sup><xref ref-type="bibr" rid="CR15">15</xref></sup>. It first dynamically initializes the representation of each entity using a BERT-style model. Then, the representations of entities are iteratively updated through an edge-based message-passing network. At the same time, edge representations can be synchronously updated according to entity representations. During initialization, semantic features are utilized for learning entity representations. In the iterative updating process, graph structural patterns are used for learning entity and edge representations. In this way, edge representations contain not only deep semantic features but also deep structural features, effectively integrating text semantic encoding with structural information. This approach, which contrasts with the partial integrations of previous efforts, has proven successful in experiments using standard datasets like WN18RR, WN18, and FB15k-237. RP-ISS's performance confirms the effectiveness of our method, advancing knowledge graph completion and opening new research directions.</p><p id="Par5">The key contributions of this study are:<list list-type="bullet"><list-item><p id="Par6">A novel model is proposed to address the challenge of integrating semantic and structural features in relation prediction in knowledge graphs. The proposed model leverages text semantic information in the knowledge graph and extracts semantic features of entity nodes using RoBERTa.</p></list-item><list-item><p id="Par7">By incorporating semantic encoding, this study introduces structural information and presents a novel edge-based relational message-passing network. This network enhances the transmission of edge messages through entity nodes.</p></list-item><list-item><p id="Par8">This study proposes a node embedding memory bank to mitigate the substantial computational pressure that the edge-based relational message-passing network imposes on the RoBERTa during sampling. The asynchronous update of the memory bank enables the effective integration of structural and semantic features, resulting in improved prediction performance without a significant increase in computational complexity.</p></list-item></list></p></sec><sec id="Sec2"><title>Related work and background</title><sec id="Sec3"><title>KGC based on graph embedding</title><p id="Par9">Relation prediction in knowledge graphs is a challenging task that involves predicting missing relations among richly semantically annotated nodes and multiple relation types. Early research in relation prediction predominantly employed knowledge embedding based on translational distance, such as TransE<sup><xref ref-type="bibr" rid="CR10">10</xref></sup> and TransR<sup><xref ref-type="bibr" rid="CR11">11</xref></sup>. These methods utilize only a limited amount of structural information surrounding the given entities. Graph embeddings have gained popularity for their ability to analyze graph structural data. For representing knowledge graph nodes, Schlichtkrull et al.<sup><xref ref-type="bibr" rid="CR12">12</xref></sup> proposed relational graph convolution network (R-GCN). R-GCN is an encoder-decoder model for knowledge graph completion, in which different weight matrices are constructed based on diverse relations to aggregate various neighbor nodes. R-GCN is a classic method for knowledge graph completion using graph neural networks, with many subsequent approaches being proposed based on this model. Nathani et al.<sup><xref ref-type="bibr" rid="CR16">16</xref></sup> proposed KBGAT, which defines an attention layer and uses a hierarchical iteration to spread attention based on the concatenation of entity and relation embeddings. Lee et al.<sup><xref ref-type="bibr" rid="CR17">17</xref></sup> proposed a link prediction method based on relation paths. This method transforms the graph structure into path sequences using a path ranking algorithm and predicts missing entities and relations through sequence models. Peng et al.<sup><xref ref-type="bibr" rid="CR18">18</xref></sup> proposed KPE-PTransE, a relation prediction method for knowledge graph reasoning tasks based on relational paths and K-nearest neighbors. This method incorporates relational paths as important features, in addition to structural information. Ma et al.<sup><xref ref-type="bibr" rid="CR19">19</xref></sup> proposed a model based on graph attention faded mechanism, which enhances the representation ability of structural features by capturing multi-hop neighbor information.</p></sec><sec id="Sec4"><title>KGC based on pre-trained language model</title><p id="Par10">Although graph embedding approaches have achieved impressive performance in relation prediction by representing nodes based on structural features, they tend to overlook the rich and latent semantic features of entities. To utilize these semantic features, Yao et al.<sup><xref ref-type="bibr" rid="CR13">13</xref></sup> proposed KG-BERT, which employs BERT for knowledge graph completion. BERT, pretrained on a large-scale corpus, possesses a strong semantic understanding and abundant implicit knowledge of natural language, allowing it to represent entities based on semantic features. This method obtains entity representations through entity names and descriptions, adapting input to fit the form of knowledge graph triples by using description sentences to represent entities. However, this approach has the limitation of only learning semantic features and implicit knowledge while disregarding structural information. Building on KG-BERT, Wang et al.<sup><xref ref-type="bibr" rid="CR20">20</xref></sup> proposed StAR to address the problem of link prediction. This model optimizes the input of KG-BERT by incorporating head entities and relations as inputs and trains the model using negative sampling. Due to the inclusion of relation information, the model achieves improved results in link prediction tasks but remains limited to link prediction applications. Nassiri et al.<sup><xref ref-type="bibr" rid="CR21">21</xref></sup> introduced the GilBERT model, a sophisticated approach grounded in the Transformer architecture for triple networks. The model creates an embedding space that aggregates information related to entities or relations within the knowledge graph. GilBERT generates textual sequences from factual data and further fine-tunes the pre-trained Transformer-based language model specifically for the triple network. This approach further translates the problem of knowledge graph completion into a spatial semantic search process, providing a novel perspective for knowledge graph completion based on textual semantic information. Nadkarni et al.<sup><xref ref-type="bibr" rid="CR22">22</xref></sup> conducted a systematic investigation into the integration of graph embedding with pre-trained language models for the purpose of knowledge graph completion. They introduced a series of fusion strategies, exemplified by KGE-BERT, which have been applied specifically to the task of medical knowledge graph completion. Fundamentally, this approach amalgamates shallow structural features with profound textual characteristics, thereby achieving a more nuanced and comprehensive completion of the knowledge graph. The synthesis of these distinct facets offers a robust and innovative methodology, underscoring the potential of multi-modal integration within the realm of knowledge representation. Shen et al.<sup><xref ref-type="bibr" rid="CR23">23</xref></sup> designed a novel method LASS that fuses semantic and structural features. To obtain semantic features, they adopted the KG-BERT approach, using pretrained language models to extract the semantic feature of entities. Instead of directly extracting structural features for structural information, their method employs a TransE-based scoring function to compute the probability scores of triples.</p></sec></sec><sec id="Sec5"><title>Model design</title><sec id="Sec6"><title>Problem definition</title><p id="Par11">In a knowledge graph <inline-formula id="IEq1"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathcal{G}=\left(\mathcal{V},\mathcal{E}\right)$$\end{document}</tex-math><mml:math id="M2"><mml:mrow><mml:mi mathvariant="script">G</mml:mi><mml:mo>=</mml:mo><mml:mfenced close=")" open="("><mml:mi mathvariant="script">V</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="script">E</mml:mi></mml:mfenced></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_63279_Article_IEq1.gif"/></alternatives></inline-formula>, where <inline-formula id="IEq2"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathcal{V}$$\end{document}</tex-math><mml:math id="M4"><mml:mi mathvariant="script">V</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_63279_Article_IEq2.gif"/></alternatives></inline-formula> is a set of nodes and <inline-formula id="IEq3"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathcal{E}$$\end{document}</tex-math><mml:math id="M6"><mml:mi mathvariant="script">E</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_63279_Article_IEq3.gif"/></alternatives></inline-formula> is a set of edges, the graph is composed of multiple triples <inline-formula id="IEq4"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\left\{h,r,t\right\}$$\end{document}</tex-math><mml:math id="M8"><mml:mfenced close="}" open="{"><mml:mi>h</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mfenced></mml:math><inline-graphic xlink:href="41598_2024_63279_Article_IEq4.gif"/></alternatives></inline-formula>, where <inline-formula id="IEq5"><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$h$$\end{document}</tex-math><mml:math id="M10"><mml:mi>h</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_63279_Article_IEq5.gif"/></alternatives></inline-formula> and <inline-formula id="IEq6"><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$t$$\end{document}</tex-math><mml:math id="M12"><mml:mi>t</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_63279_Article_IEq6.gif"/></alternatives></inline-formula> denote the head and tail nodes, and <inline-formula id="IEq7"><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$r$$\end{document}</tex-math><mml:math id="M14"><mml:mi>r</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_63279_Article_IEq7.gif"/></alternatives></inline-formula> represents the relation. A representative task within the domain of knowledge graph completion is Link Prediction, which aims to predict the missing components of a triplet <inline-formula id="IEq8"><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\left\{h,r,t\right\}$$\end{document}</tex-math><mml:math id="M16"><mml:mfenced close="}" open="{"><mml:mi>h</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mfenced></mml:math><inline-graphic xlink:href="41598_2024_63279_Article_IEq8.gif"/></alternatives></inline-formula>. Some studies further subdivide Link Prediction into several subtasks<sup><xref ref-type="bibr" rid="CR24">24</xref>,<xref ref-type="bibr" rid="CR25">25</xref></sup>. In our work, the relation prediction problem that we have conducted can be defined as: predicting the missing relation based on the head and tail entities, symbolized as predicting the missing relation in <inline-formula id="IEq9"><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\left\{h,?,t\right\}$$\end{document}</tex-math><mml:math id="M18"><mml:mfenced close="}" open="{"><mml:mi>h</mml:mi><mml:mo>,</mml:mo><mml:mo>?</mml:mo><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mfenced></mml:math><inline-graphic xlink:href="41598_2024_63279_Article_IEq9.gif"/></alternatives></inline-formula><sup><xref ref-type="bibr" rid="CR26">26</xref>,<xref ref-type="bibr" rid="CR27">27</xref></sup>.</p><p id="Par12">Generally speaking, the fundamental concept of knowledge graph completion is to establish a scoring function <inline-formula id="IEq10"><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$S\left(\cdot \right)$$\end{document}</tex-math><mml:math id="M20"><mml:mrow><mml:mi>S</mml:mi><mml:mfenced close=")" open="("><mml:mo>&#x000b7;</mml:mo></mml:mfenced></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_63279_Article_IEq10.gif"/></alternatives></inline-formula>, which assigns a score to any given triplet <inline-formula id="IEq11"><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\left\{h,?,t\right\}$$\end{document}</tex-math><mml:math id="M22"><mml:mfenced close="}" open="{"><mml:mi>h</mml:mi><mml:mo>,</mml:mo><mml:mo>?</mml:mo><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mfenced></mml:math><inline-graphic xlink:href="41598_2024_63279_Article_IEq11.gif"/></alternatives></inline-formula>. If this triplet is present within the knowledge graph, it receives a high score, and vice versa. Building upon this idea, for the task of relation prediction, we can transform the scoring function into:<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$S\left(r\right)=\Phi (r;h,t;\mathcal{G})$$\end{document}</tex-math><mml:math id="M24" display="block"><mml:mrow><mml:mi>S</mml:mi><mml:mfenced close=")" open="("><mml:mi>r</mml:mi></mml:mfenced><mml:mo>=</mml:mo><mml:mi mathvariant="normal">&#x003a6;</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo>&#x0037e;</mml:mo><mml:mi>h</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>&#x0037e;</mml:mo><mml:mi mathvariant="script">G</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math><graphic xlink:href="41598_2024_63279_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq12"><alternatives><tex-math id="M25">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$r$$\end{document}</tex-math><mml:math id="M26"><mml:mi>r</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_63279_Article_IEq12.gif"/></alternatives></inline-formula>, <inline-formula id="IEq13"><alternatives><tex-math id="M27">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$h$$\end{document}</tex-math><mml:math id="M28"><mml:mi>h</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_63279_Article_IEq13.gif"/></alternatives></inline-formula> and <inline-formula id="IEq14"><alternatives><tex-math id="M29">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$t$$\end{document}</tex-math><mml:math id="M30"><mml:mi>t</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_63279_Article_IEq14.gif"/></alternatives></inline-formula> respectively represent the relation, head entity, and tail entity. <inline-formula id="IEq15"><alternatives><tex-math id="M31">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathcal{G}$$\end{document}</tex-math><mml:math id="M32"><mml:mi mathvariant="script">G</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_63279_Article_IEq15.gif"/></alternatives></inline-formula> denotes the knowledge graph where the triplets reside. <inline-formula id="IEq16"><alternatives><tex-math id="M33">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\Phi $$\end{document}</tex-math><mml:math id="M34"><mml:mi mathvariant="normal">&#x003a6;</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_63279_Article_IEq16.gif"/></alternatives></inline-formula> represents the method for calculating relation scores, used to compute the likelihood of the existence of relation r between h and t.</p></sec><sec id="Sec7"><title>Model architecture</title><p id="Par13">To improve the prediction of missing relations by integrating textual semantic features and graph structural features, we propose the RP-ISS model. The model utilizes the tacit knowledge and semantic feature extraction ability of RoBERTa. Due to some advanced models for knowledge graph completion, such as KG-BERT and GilBERT, being end-to-end prediction models that cannot be used as PLMs, we primarily utilized RoBERTa in RP-ISS. To enable the model to process graph structural information, we design and apply a Relation Message Passing Network (as described in Section "<xref rid="Sec8" ref-type="sec">Relational message passing</xref>"). The structure of RP-ISS is shown in Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>.<fig id="Fig1"><label>Figure 1</label><caption><p>The architecture of our model RP-ISS.</p></caption><graphic xlink:href="41598_2024_63279_Fig1_HTML" id="MO1"/></fig></p><p id="Par14">To extract semantic feature of entities, we transform the information of an entity node into a set of tokens to serve as input for RoBERTa. For an entity, we transform its name and description into a token sequence in the form of [<italic>CLS</italic>][<italic>Entity Name</italic>][<italic>SEP</italic>][<italic>Entity Description</italic>]. Here, [<italic>Entity Name</italic>] represents the tokens of the entity's name. [<italic>Entity Description</italic>] represents the tokens of the entity's description. [<italic>CLS</italic>] is a special classification token. [<italic>SEP</italic>] signifies the separation of sentences. This set of tokens consists of two sentences, <inline-formula id="IEq17"><alternatives><tex-math id="M35">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${S}_{e}$$\end{document}</tex-math><mml:math id="M36"><mml:msub><mml:mi>S</mml:mi><mml:mi>e</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_63279_Article_IEq17.gif"/></alternatives></inline-formula> and <inline-formula id="IEq18"><alternatives><tex-math id="M37">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${S}_{d}$$\end{document}</tex-math><mml:math id="M38"><mml:msub><mml:mi>S</mml:mi><mml:mi>d</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_63279_Article_IEq18.gif"/></alternatives></inline-formula>. Sentence <inline-formula id="IEq19"><alternatives><tex-math id="M39">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${S}_{e}$$\end{document}</tex-math><mml:math id="M40"><mml:msub><mml:mi>S</mml:mi><mml:mi>e</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_63279_Article_IEq19.gif"/></alternatives></inline-formula> contains the tokens of the entity to be encoded, and sentence <inline-formula id="IEq20"><alternatives><tex-math id="M41">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${S}_{d}$$\end{document}</tex-math><mml:math id="M42"><mml:msub><mml:mi>S</mml:mi><mml:mi>d</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_63279_Article_IEq20.gif"/></alternatives></inline-formula> contains the tokens of its description. To predict the relations between the two nodes, we construct the tokens of head entity <inline-formula id="IEq21"><alternatives><tex-math id="M43">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$h$$\end{document}</tex-math><mml:math id="M44"><mml:mi>h</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_63279_Article_IEq21.gif"/></alternatives></inline-formula> and tail entity <inline-formula id="IEq22"><alternatives><tex-math id="M45">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$t$$\end{document}</tex-math><mml:math id="M46"><mml:mi>t</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_63279_Article_IEq22.gif"/></alternatives></inline-formula> separately, and input them into the same RoBERTa for encoding. The encoded output of <inline-formula id="IEq23"><alternatives><tex-math id="M47">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$h$$\end{document}</tex-math><mml:math id="M48"><mml:mi>h</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_63279_Article_IEq23.gif"/></alternatives></inline-formula> and <inline-formula id="IEq24"><alternatives><tex-math id="M49">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$t$$\end{document}</tex-math><mml:math id="M50"><mml:mi>t</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_63279_Article_IEq24.gif"/></alternatives></inline-formula> from the RoBERTa are represented as <inline-formula id="IEq25"><alternatives><tex-math id="M51">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$u$$\end{document}</tex-math><mml:math id="M52"><mml:mi>u</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_63279_Article_IEq25.gif"/></alternatives></inline-formula> and <inline-formula id="IEq26"><alternatives><tex-math id="M53">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$v$$\end{document}</tex-math><mml:math id="M54"><mml:mi>v</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_63279_Article_IEq26.gif"/></alternatives></inline-formula> respectively.</p><p id="Par15">To extract the structural feature of the graph, we put the encoded vectors <inline-formula id="IEq27"><alternatives><tex-math id="M55">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$u$$\end{document}</tex-math><mml:math id="M56"><mml:mi>u</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_63279_Article_IEq27.gif"/></alternatives></inline-formula> and <inline-formula id="IEq28"><alternatives><tex-math id="M57">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$v$$\end{document}</tex-math><mml:math id="M58"><mml:mi>v</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_63279_Article_IEq28.gif"/></alternatives></inline-formula> from RoBERTa through a Relation Message Passing Network (as described in Section "<xref rid="Sec8" ref-type="sec">Relational message passing</xref>"). In the Relation Message Passing Network, we use subgraph sampling to recursively sample and aggregate the information of neighboring edges related to the current edge, to obtain the representation of the current edge. The edge representation is calculated based on the entity embeddings with a fully connected layer. During each sampling hop, multiple nodes are sampled and edge representations are calculated. The vectors of these sampled nodes are from the previous round of RoBERTa encoding stored in the Node Embedding Memory Bank and do not participate in the current backpropagation computation. The encoding process can be expressed as follows:<disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M59">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${r}_{sema}=\sigma \left(\left[u,v\right]\cdot W+b\right)$$\end{document}</tex-math><mml:math id="M60" display="block"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi mathvariant="italic">sema</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>&#x003c3;</mml:mi><mml:mfenced close=")" open="("><mml:mfenced close="]" open="["><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi></mml:mfenced><mml:mo>&#x000b7;</mml:mo><mml:mi>W</mml:mi><mml:mo>+</mml:mo><mml:mi>b</mml:mi></mml:mfenced></mml:mrow></mml:math><graphic xlink:href="41598_2024_63279_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="M61">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${r}_{strc}^{l}=\sigma \left(\left[{r}_{h,t},{s}_{{e}_{h}}^{l-1},{s}_{{e}_{t}}^{l-1}\right]\cdot {W}^{l}+{b}^{l}\right) ,{e}_{h}\in E\left(h\right),{e}_{t}\in E\left(t\right)$$\end{document}</tex-math><mml:math id="M62" display="block"><mml:mrow><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mi mathvariant="italic">strc</mml:mi></mml:mrow><mml:mi>l</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mi>&#x003c3;</mml:mi><mml:mfenced close=")" open="("><mml:mfenced close="]" open="["><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mi>s</mml:mi><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mi>h</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>s</mml:mi><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mfenced><mml:mo>&#x000b7;</mml:mo><mml:msup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mi>l</mml:mi></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mi>l</mml:mi></mml:msup></mml:mfenced><mml:mo>,</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mi>h</mml:mi></mml:msub><mml:mo>&#x02208;</mml:mo><mml:mi>E</mml:mi><mml:mfenced close=")" open="("><mml:mi>h</mml:mi></mml:mfenced><mml:mo>,</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>&#x02208;</mml:mo><mml:mi>E</mml:mi><mml:mfenced close=")" open="("><mml:mi>t</mml:mi></mml:mfenced></mml:mrow></mml:math><graphic xlink:href="41598_2024_63279_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq29"><alternatives><tex-math id="M63">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${r}_{sema}$$\end{document}</tex-math><mml:math id="M64"><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi mathvariant="italic">sema</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_63279_Article_IEq29.gif"/></alternatives></inline-formula> represents the semantic representation of the edge between an entity pair and while <inline-formula id="IEq30"><alternatives><tex-math id="M65">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${r}_{strc}^{l}$$\end{document}</tex-math><mml:math id="M66"><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mi mathvariant="italic">strc</mml:mi></mml:mrow><mml:mi>l</mml:mi></mml:msubsup></mml:math><inline-graphic xlink:href="41598_2024_63279_Article_IEq30.gif"/></alternatives></inline-formula> refers to the structural representation of the edge between the entity pair. <inline-formula id="IEq31"><alternatives><tex-math id="M67">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${r}_{h,t}$$\end{document}</tex-math><mml:math id="M68"><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_63279_Article_IEq31.gif"/></alternatives></inline-formula> denotes the initial representation of the edge between head entity <inline-formula id="IEq32"><alternatives><tex-math id="M69">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$h$$\end{document}</tex-math><mml:math id="M70"><mml:mi>h</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_63279_Article_IEq32.gif"/></alternatives></inline-formula> and tail entity <inline-formula id="IEq33"><alternatives><tex-math id="M71">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$t$$\end{document}</tex-math><mml:math id="M72"><mml:mi>t</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_63279_Article_IEq33.gif"/></alternatives></inline-formula>. <inline-formula id="IEq34"><alternatives><tex-math id="M73">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${s}_{e}^{l-1}$$\end{document}</tex-math><mml:math id="M74"><mml:msubsup><mml:mi>s</mml:mi><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="41598_2024_63279_Article_IEq34.gif"/></alternatives></inline-formula> represents the result of sampling a subgraph and aggregating structural information, which aggregates information from the edges connected to the head node <inline-formula id="IEq35"><alternatives><tex-math id="M75">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${e}_{h}$$\end{document}</tex-math><mml:math id="M76"><mml:msub><mml:mi>e</mml:mi><mml:mi>h</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_63279_Article_IEq35.gif"/></alternatives></inline-formula> and the edges connected to the tail node separately <inline-formula id="IEq36"><alternatives><tex-math id="M77">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${e}_{t}$$\end{document}</tex-math><mml:math id="M78"><mml:msub><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_63279_Article_IEq36.gif"/></alternatives></inline-formula>. <inline-formula id="IEq37"><alternatives><tex-math id="M79">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$l$$\end{document}</tex-math><mml:math id="M80"><mml:mi>l</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_63279_Article_IEq37.gif"/></alternatives></inline-formula> represents the number of hops of the sampled subgraph, also known as sampling depth.</p><p id="Par16">Our goal is to predict the type of relation that connecting <inline-formula id="IEq38"><alternatives><tex-math id="M81">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$h$$\end{document}</tex-math><mml:math id="M82"><mml:mi>h</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_63279_Article_IEq38.gif"/></alternatives></inline-formula> and <inline-formula id="IEq39"><alternatives><tex-math id="M83">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$t$$\end{document}</tex-math><mml:math id="M84"><mml:mi>t</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_63279_Article_IEq39.gif"/></alternatives></inline-formula>, that is, to find the most appropriate <inline-formula id="IEq40"><alternatives><tex-math id="M85">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$r$$\end{document}</tex-math><mml:math id="M86"><mml:mi>r</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_63279_Article_IEq40.gif"/></alternatives></inline-formula> for the incomplete triple <inline-formula id="IEq41"><alternatives><tex-math id="M87">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\left\{h,?,t\right\}$$\end{document}</tex-math><mml:math id="M88"><mml:mfenced close="}" open="{"><mml:mi>h</mml:mi><mml:mo>,</mml:mo><mml:mo>?</mml:mo><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mfenced></mml:math><inline-graphic xlink:href="41598_2024_63279_Article_IEq41.gif"/></alternatives></inline-formula>. Once we calculated the semantic representation vector <inline-formula id="IEq42"><alternatives><tex-math id="M89">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${r}_{sema}$$\end{document}</tex-math><mml:math id="M90"><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi mathvariant="italic">sema</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_63279_Article_IEq42.gif"/></alternatives></inline-formula> and the structural feature representation vector <inline-formula id="IEq43"><alternatives><tex-math id="M91">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${r}_{strc}^{l}$$\end{document}</tex-math><mml:math id="M92"><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mi mathvariant="italic">strc</mml:mi></mml:mrow><mml:mi>l</mml:mi></mml:msubsup></mml:math><inline-graphic xlink:href="41598_2024_63279_Article_IEq43.gif"/></alternatives></inline-formula>, we concatenate <inline-formula id="IEq44"><alternatives><tex-math id="M93">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${r}_{sema}$$\end{document}</tex-math><mml:math id="M94"><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi mathvariant="italic">sema</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_63279_Article_IEq44.gif"/></alternatives></inline-formula> and <inline-formula id="IEq45"><alternatives><tex-math id="M95">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${r}_{strc}^{l}$$\end{document}</tex-math><mml:math id="M96"><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mi mathvariant="italic">strc</mml:mi></mml:mrow><mml:mi>l</mml:mi></mml:msubsup></mml:math><inline-graphic xlink:href="41598_2024_63279_Article_IEq45.gif"/></alternatives></inline-formula> into a comprehensive representation <inline-formula id="IEq46"><alternatives><tex-math id="M97">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${e}_{r}$$\end{document}</tex-math><mml:math id="M98"><mml:msub><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_63279_Article_IEq46.gif"/></alternatives></inline-formula> Then we pass <inline-formula id="IEq47"><alternatives><tex-math id="M99">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${e}_{r}$$\end{document}</tex-math><mml:math id="M100"><mml:msub><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_63279_Article_IEq47.gif"/></alternatives></inline-formula> through a fully connected layer to adjust the dimensions and output a probability distribution representing the relation type of <inline-formula id="IEq48"><alternatives><tex-math id="M101">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${e}_{r}$$\end{document}</tex-math><mml:math id="M102"><mml:msub><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_63279_Article_IEq48.gif"/></alternatives></inline-formula>. The softmax function for classification activates the final output.</p></sec><sec id="Sec8"><title>Relational message passing</title><p id="Par17">Our study explores integrating RoBERTa's linguistic capabilities with GNN's structural insights to improve relation prediction. A simple stacking of RoBERTa and GNN creates excessive computational load due to GNN's need to aggregate features from surrounding nodes. We propose a novel method combining RoBERTa's strengths with knowledge graph structures through an edge-based relational message-passing mechanism and a node embedding memory bank. This allows seamless integration of RoBERTa's encoded entity output into the message-passing network, enhancing performance without the computational burden of a stacked model.</p><sec id="Sec9"><title>Edge-based relational message passing</title><p id="Par18">The goal of the relation prediction task is to predict the type of the relation between two nodes. Therefore, we need to focus on the representation of the edges, rather than the representation of the nodes, which is the focus of most GNN models. Thus, we propose an edge-based relational message passing method. As shown in Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref>, we provide an example to illustrate how the edge-based relational message passing network helps us predict missing relations. Suppose there is a missing relation between the entities Alice and Bob, with the relationship type being Wife_of. This means that Alice is Bob's wife. There exists another entity Rick, whose mother and father are Alice and Bob respectively. When predicting this relation, the Edge-based relational message passing network can encode the information of the relation between Alice and Rick as well as between Bob and Rick, and pass it onto the current edge awaiting prediction. Compared to the general node-based message passing method, this method treats edges as nodes for message passing and information aggregation. The information aggregated on an edge is derived from the edges connected to the two entities to which the edge is linked. Nonetheless, considering the potentially large number of edges present in a graph, in our method, we do not directly store the representations of edges, but calculate them through the nodes. The message passing process of edges can be expressed as:<disp-formula id="Equ4"><label>4</label><alternatives><tex-math id="M103">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${e}_{h,t}=\sigma \left(\left[{n}_{h},{n}_{t}\right]\cdot W+b\right)$$\end{document}</tex-math><mml:math id="M104" display="block"><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>&#x003c3;</mml:mi><mml:mfenced close=")" open="("><mml:mfenced close="]" open="["><mml:msub><mml:mi>n</mml:mi><mml:mi>h</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mfenced><mml:mo>&#x000b7;</mml:mo><mml:mi>W</mml:mi><mml:mo>+</mml:mo><mml:mi>b</mml:mi></mml:mfenced></mml:mrow></mml:math><graphic xlink:href="41598_2024_63279_Article_Equ4.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ5"><label>5</label><alternatives><tex-math id="M105">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${m}_{h}=aggr\left(\left\{{e}_{h,\text{i}}\right\}\right),i\in N\left(h\right),i\ne t$$\end{document}</tex-math><mml:math id="M106" display="block"><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mi>h</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:mi>g</mml:mi><mml:mi>g</mml:mi><mml:mi>r</mml:mi><mml:mfenced close=")" open="("><mml:mfenced close="}" open="{"><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mo>,</mml:mo><mml:mtext>i</mml:mtext></mml:mrow></mml:msub></mml:mfenced></mml:mfenced><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi>N</mml:mi><mml:mfenced close=")" open="("><mml:mi>h</mml:mi></mml:mfenced><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>&#x02260;</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:math><graphic xlink:href="41598_2024_63279_Article_Equ5.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ6"><label>6</label><alternatives><tex-math id="M107">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${m}_{t}=aggr\left(\left\{{e}_{j,\text{t}}\right\}\right),j\in N\left(t\right),j\ne h$$\end{document}</tex-math><mml:math id="M108" display="block"><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:mi>g</mml:mi><mml:mi>g</mml:mi><mml:mi>r</mml:mi><mml:mfenced close=")" open="("><mml:mfenced close="}" open="{"><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mtext>t</mml:mtext></mml:mrow></mml:msub></mml:mfenced></mml:mfenced><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi>N</mml:mi><mml:mfenced close=")" open="("><mml:mi>t</mml:mi></mml:mfenced><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>&#x02260;</mml:mo><mml:mi>h</mml:mi></mml:mrow></mml:math><graphic xlink:href="41598_2024_63279_Article_Equ6.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ7"><label>7</label><alternatives><tex-math id="M109">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${e}_{h,t}{\prime}= U\left(\left[{e}_{h,t},{m}_{h},{m}_{t}\right]\right)$$\end{document}</tex-math><mml:math id="M110" display="block"><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02032;</mml:mo><mml:mo>=</mml:mo><mml:mi>U</mml:mi><mml:mfenced close=")" open="("><mml:mfenced close="]" open="["><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mi>h</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mfenced></mml:mfenced></mml:mrow></mml:math><graphic xlink:href="41598_2024_63279_Article_Equ7.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq49"><alternatives><tex-math id="M111">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${e}_{h,t}$$\end{document}</tex-math><mml:math id="M112"><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_63279_Article_IEq49.gif"/></alternatives></inline-formula> is the representation of the edge between an entity pair, which is calculated based on the head <inline-formula id="IEq50"><alternatives><tex-math id="M113">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${n}_{h}$$\end{document}</tex-math><mml:math id="M114"><mml:msub><mml:mi>n</mml:mi><mml:mi>h</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_63279_Article_IEq50.gif"/></alternatives></inline-formula> and tail <inline-formula id="IEq51"><alternatives><tex-math id="M115">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${n}_{t}$$\end{document}</tex-math><mml:math id="M116"><mml:msub><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_63279_Article_IEq51.gif"/></alternatives></inline-formula>. We use a fully connected layer to compute the representation of the edge <inline-formula id="IEq52"><alternatives><tex-math id="M117">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${e}_{h,t}$$\end{document}</tex-math><mml:math id="M118"><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_63279_Article_IEq52.gif"/></alternatives></inline-formula>. <inline-formula id="IEq53"><alternatives><tex-math id="M119">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${m}_{h}$$\end{document}</tex-math><mml:math id="M120"><mml:msub><mml:mi>m</mml:mi><mml:mi>h</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_63279_Article_IEq53.gif"/></alternatives></inline-formula> is the message from all edges connected to the head entity except for the edge <inline-formula id="IEq54"><alternatives><tex-math id="M121">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${e}_{h,t}$$\end{document}</tex-math><mml:math id="M122"><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_63279_Article_IEq54.gif"/></alternatives></inline-formula>. <inline-formula id="IEq55"><alternatives><tex-math id="M123">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${m}_{t}$$\end{document}</tex-math><mml:math id="M124"><mml:msub><mml:mi>m</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_63279_Article_IEq55.gif"/></alternatives></inline-formula> is the message from representation of all edges connected to the tail entity except for the edge <inline-formula id="IEq56"><alternatives><tex-math id="M125">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${e}_{h,t}$$\end{document}</tex-math><mml:math id="M126"><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_63279_Article_IEq56.gif"/></alternatives></inline-formula>. <inline-formula id="IEq57"><alternatives><tex-math id="M127">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$N\left(h\right)$$\end{document}</tex-math><mml:math id="M128"><mml:mrow><mml:mi>N</mml:mi><mml:mfenced close=")" open="("><mml:mi>h</mml:mi></mml:mfenced></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_63279_Article_IEq57.gif"/></alternatives></inline-formula> and <inline-formula id="IEq58"><alternatives><tex-math id="M129">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$N\left(t\right)$$\end{document}</tex-math><mml:math id="M130"><mml:mrow><mml:mi>N</mml:mi><mml:mfenced close=")" open="("><mml:mi>t</mml:mi></mml:mfenced></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_63279_Article_IEq58.gif"/></alternatives></inline-formula> are the neighbor nodes of entity <inline-formula id="IEq59"><alternatives><tex-math id="M131">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$h$$\end{document}</tex-math><mml:math id="M132"><mml:mi>h</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_63279_Article_IEq59.gif"/></alternatives></inline-formula> and <inline-formula id="IEq60"><alternatives><tex-math id="M133">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$t$$\end{document}</tex-math><mml:math id="M134"><mml:mi>t</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_63279_Article_IEq60.gif"/></alternatives></inline-formula>. <inline-formula id="IEq61"><alternatives><tex-math id="M135">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${m}_{h}$$\end{document}</tex-math><mml:math id="M136"><mml:msub><mml:mi>m</mml:mi><mml:mi>h</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_63279_Article_IEq61.gif"/></alternatives></inline-formula> and <inline-formula id="IEq62"><alternatives><tex-math id="M137">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${m}_{t}$$\end{document}</tex-math><mml:math id="M138"><mml:msub><mml:mi>m</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_63279_Article_IEq62.gif"/></alternatives></inline-formula> are calculated through an aggregate function. <inline-formula id="IEq63"><alternatives><tex-math id="M139">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${e}_{h,t}{\prime}$$\end{document}</tex-math><mml:math id="M140"><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02032;</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_63279_Article_IEq63.gif"/></alternatives></inline-formula> is the updated representation of <inline-formula id="IEq64"><alternatives><tex-math id="M141">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${e}_{h,t}$$\end{document}</tex-math><mml:math id="M142"><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_63279_Article_IEq64.gif"/></alternatives></inline-formula>, which is calculated by the original representation <inline-formula id="IEq65"><alternatives><tex-math id="M143">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${e}_{h,t}$$\end{document}</tex-math><mml:math id="M144"><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_63279_Article_IEq65.gif"/></alternatives></inline-formula>, the messages from neighbor edges <inline-formula id="IEq66"><alternatives><tex-math id="M145">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${m}_{h}$$\end{document}</tex-math><mml:math id="M146"><mml:msub><mml:mi>m</mml:mi><mml:mi>h</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_63279_Article_IEq66.gif"/></alternatives></inline-formula> and <inline-formula id="IEq67"><alternatives><tex-math id="M147">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${m}_{t}$$\end{document}</tex-math><mml:math id="M148"><mml:msub><mml:mi>m</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_63279_Article_IEq67.gif"/></alternatives></inline-formula>.<inline-formula id="IEq68"><alternatives><tex-math id="M149">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$U$$\end{document}</tex-math><mml:math id="M150"><mml:mi>U</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_63279_Article_IEq68.gif"/></alternatives></inline-formula> represents the updating function for edge representations. Within <inline-formula id="IEq69"><alternatives><tex-math id="M151">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$U$$\end{document}</tex-math><mml:math id="M152"><mml:mi>U</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_63279_Article_IEq69.gif"/></alternatives></inline-formula>, <inline-formula id="IEq70"><alternatives><tex-math id="M153">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${e}_{h,t}$$\end{document}</tex-math><mml:math id="M154"><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_63279_Article_IEq70.gif"/></alternatives></inline-formula>, <inline-formula id="IEq71"><alternatives><tex-math id="M155">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${m}_{h}$$\end{document}</tex-math><mml:math id="M156"><mml:msub><mml:mi>m</mml:mi><mml:mi>h</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_63279_Article_IEq71.gif"/></alternatives></inline-formula> and <inline-formula id="IEq72"><alternatives><tex-math id="M157">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${m}_{t}$$\end{document}</tex-math><mml:math id="M158"><mml:msub><mml:mi>m</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_63279_Article_IEq72.gif"/></alternatives></inline-formula> are concatenated and passed through a residual connection targeting <inline-formula id="IEq73"><alternatives><tex-math id="M159">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${e}_{h,t}$$\end{document}</tex-math><mml:math id="M160"><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_63279_Article_IEq73.gif"/></alternatives></inline-formula> to update its representation.<fig id="Fig2"><label>Figure 2</label><caption><p>Illustration of Edge-based relational message passing.</p></caption><graphic xlink:href="41598_2024_63279_Fig2_HTML" id="MO2"/></fig></p></sec><sec id="Sec10"><title>Node embedding memory bank</title><p id="Par19">Directly stacking RoBERTa with edge-based relation message passing networks incurs high computational costs due to the need for RoBERTa to encode each sampled neighboring node, exponentially increasing with sampling depth. To reduce this, especially during backward propagation in training, we implemented a node embedding memory bank (illustrated in Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3</xref>). This bank stores RoBERTa-encoded nodes from previous training, updating all node pair embeddings after each training epoch. While sampling, it uses previously stored encodings, updating the memory bank with new encodings and propagating only the current node pair's gradient information to RoBERTa. However, this creates an inconsistency between the memory bank's node embeddings and RoBERTa's encodings, which we address with a weighted updating strategy for encoding results. This process can be represented as:<disp-formula id="Equ8"><label>8</label><alternatives><tex-math id="M161">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${n}_{i}{\prime}=\lambda {s}_{i}+(1-\lambda ){n}_{i}$$\end{document}</tex-math><mml:math id="M162" display="block"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x02032;</mml:mo><mml:mo>=</mml:mo><mml:mi>&#x003bb;</mml:mi><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>&#x003bb;</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math><graphic xlink:href="41598_2024_63279_Article_Equ8.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq74"><alternatives><tex-math id="M163">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${n}_{i}{\prime}$$\end{document}</tex-math><mml:math id="M164"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x02032;</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_63279_Article_IEq74.gif"/></alternatives></inline-formula> is the updated embedding of node <inline-formula id="IEq75"><alternatives><tex-math id="M165">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$i$$\end{document}</tex-math><mml:math id="M166"><mml:mi>i</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_63279_Article_IEq75.gif"/></alternatives></inline-formula>, <inline-formula id="IEq76"><alternatives><tex-math id="M167">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${s}_{i}$$\end{document}</tex-math><mml:math id="M168"><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_63279_Article_IEq76.gif"/></alternatives></inline-formula> represents the embedding of node <inline-formula id="IEq77"><alternatives><tex-math id="M169">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$i$$\end{document}</tex-math><mml:math id="M170"><mml:mi>i</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_63279_Article_IEq77.gif"/></alternatives></inline-formula> encoded by BERT, <inline-formula id="IEq78"><alternatives><tex-math id="M171">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${n}_{i}$$\end{document}</tex-math><mml:math id="M172"><mml:msub><mml:mi>n</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_63279_Article_IEq78.gif"/></alternatives></inline-formula> refers to the original node embedding in the memory bank, and <inline-formula id="IEq79"><alternatives><tex-math id="M173">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\lambda $$\end{document}</tex-math><mml:math id="M174"><mml:mi>&#x003bb;</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_63279_Article_IEq79.gif"/></alternatives></inline-formula> is a weighting factor employed to regulate the ratio of the new encoding to the original node embedding. In our model, we set the value of the weighting factor <inline-formula id="IEq80"><alternatives><tex-math id="M175">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\lambda $$\end{document}</tex-math><mml:math id="M176"><mml:mi>&#x003bb;</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_63279_Article_IEq80.gif"/></alternatives></inline-formula> to 0.5.<fig id="Fig3"><label>Figure 3</label><caption><p>Illustration of node embedding memory bank.</p></caption><graphic xlink:href="41598_2024_63279_Fig3_HTML" id="MO3"/></fig></p></sec></sec></sec><sec id="Sec11"><title>Experiments</title><sec id="Sec12"><title>Datasets and baselines</title><p id="Par20">We carried out experiments on three representative and publicly accessible datasets: WN18, WN18RR, and FB15k-237. WN18 is a knowledge graph that contains concepts and semantic relations of English words, based on WordNet. WN18RR is a subset of WN18 that has removed the reverse relations. FB15k-237, on the other hand, is a knowledge graph based on a large-scale general knowledge from Freebase with reverse relations also removed. The statistics of these three datasets are presented in Table <xref rid="Tab1" ref-type="table">1</xref>.<table-wrap id="Tab1"><label>Table 1</label><caption><p>The statistics for the datasets.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"/><th align="left">Train set</th><th align="left">Validation set</th><th align="left">Test set</th><th align="left">Entities</th><th align="left">Relations</th><th align="left">Average degree</th></tr></thead><tbody><tr><td align="left">WN18RR</td><td align="left">86,835</td><td align="left">3034</td><td align="left">3134</td><td align="left">40,943</td><td align="left">11</td><td char="." align="char">4.2</td></tr><tr><td align="left">WN18</td><td align="left">141,442</td><td align="left">5000</td><td align="left">5000</td><td align="left">40,943</td><td align="left">18</td><td char="." align="char">6.9</td></tr><tr><td align="left">FB15k-237</td><td align="left">272,115</td><td align="left">17,535</td><td align="left">20,466</td><td align="left">14,541</td><td align="left">237</td><td char="." align="char">37.4</td></tr></tbody></table></table-wrap></p><p id="Par21">To evaluate the performance of our proposed model, we compare it with several baseline methods in order to gain insights into its relative effectiveness: including TransE<sup><xref ref-type="bibr" rid="CR10">10</xref></sup>, DistMult<sup><xref ref-type="bibr" rid="CR28">28</xref></sup>, RotatE<sup><xref ref-type="bibr" rid="CR29">29</xref></sup>, SimplE<sup><xref ref-type="bibr" rid="CR30">30</xref></sup>, ComplEx<sup><xref ref-type="bibr" rid="CR31">31</xref></sup>, R-GCN<sup><xref ref-type="bibr" rid="CR12">12</xref></sup>, KG-BERT<sup><xref ref-type="bibr" rid="CR32">32</xref></sup>,KPE-PTransE<sup><xref ref-type="bibr" rid="CR18">18</xref></sup>, KGE-BERT(with routers)<sup><xref ref-type="bibr" rid="CR22">22</xref></sup>, GilBERT<sup><xref ref-type="bibr" rid="CR21">21</xref></sup>, LASS<sup><xref ref-type="bibr" rid="CR23">23</xref></sup>. TransE, DistMult, RotatE, SimplE, ComplEx, and KPE-PTransE are very classic methods of knowledge representation, primarily expressing the structural features of knowledge graphs. R-GCN stands as a highly representative model for relation prediction, utilizing graph structure effectively. Conversely, KG-BERT, KGE-BERT and GilBERT emerges as an equally prominent model in this domain, leveraging pre-trained language models for relation prediction. LASS is one of the state-of-the-art models for knowledge graph completion based on semantic and structural features. We have used it to carry out relation prediction tasks, and it serves as one of our baselines.</p><p id="Par22">We used the DGL-KE toolkit (see <ext-link ext-link-type="uri" xlink:href="https://github.com/awslabs/dgl-ke/">https://github.com/awslabs/dgl-ke/</ext-link>) to reproduce the results of the TransE, DistMult, RotatE, and ComplEx methods on the experimental dataset, and the results of SimplE, R-GCN, KG-BERT, KGE-BERT, GilBERT and LASS were reproduced using the accompanying source code from the original paper. The results of KPE-PTransE is based on the original paper.</p></sec><sec id="Sec13"><title>Experiments setting</title><p id="Par23">Relation prediction involves predicting the missing relation between head and tail nodes, i.e., predicting the missing part in <inline-formula id="IEq81"><alternatives><tex-math id="M177">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\left\{h,?,t\right\}$$\end{document}</tex-math><mml:math id="M178"><mml:mfenced close="}" open="{"><mml:mi>h</mml:mi><mml:mo>,</mml:mo><mml:mo>?</mml:mo><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mfenced></mml:math><inline-graphic xlink:href="41598_2024_63279_Article_IEq81.gif"/></alternatives></inline-formula>. We use a multi-class classification model to predict the missing relation. Analogous to the evaluation method employed in link prediction, we utilize the probability distribution generated by the model to compute MRR, Hits@1, and Hits@3. These metrics serve as indicators of the model's performance.</p><p id="Par24">Our model was implemented in PyTorch, using RoBERTa for node sequence encoding and the Adam optimizer for training. We determined the optimal learning rates to be 5e&#x02212;5 for RoBERTa and 3e&#x02212;4 for the Relation Message Passing Network through grid search. A warmup mechanism was applied, with warmup steps set at 10% of total training steps, and training capped at 30 epochs to avoid fast convergence of RoBERTa's final layers.To save training time and prevent overfitting, we employ early stopping. Batch sizes were set at 1000 for WN18 and WN18RR, and 500 for FB15k-237, with a maximum input sequence length of 50. We used mixed precision to save GPU memory and expedite training. The models were trained and tested on 4 Nvidia A5000 RTX GPUs.</p></sec></sec><sec id="Sec14"><title>Results and discussion</title><sec id="Sec15"><title>Main results</title><p id="Par25">We recorded and compared the experimental outcomes of our proposed model and the established baseline models. The main results are shown in Table <xref rid="Tab2" ref-type="table">2</xref>. RP-ISS-Mean and RP-ISS-Attn are two different methods for aggregating information from neighboring edges in a graph-based relation message passing network. RP-ISS-Mean aggregates information from neighboring edges through mean pooling, while RP-ISS-Attn uses a multi-head attention mechanism to aggregate information from neighboring edges.<table-wrap id="Tab2"><label>Table 2</label><caption><p>The main results on WN18RR, WN18 and FB15K-237 in relation prediction task.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"/><th align="left" colspan="3">WN18RR<break/>MRR Hits@1<break/>Hits@3</th><th align="left" colspan="3">WN18<break/>MRR Hits@1<break/>Hits@3</th><th align="left" colspan="3">FB15K-237<break/>MRR Hits@1<break/>Hits@3</th></tr></thead><tbody><tr><td align="left">TransE</td><td char="." align="char">78.26</td><td char="." align="char">67.13</td><td char="." align="char">87.47</td><td char="." align="char">97.11</td><td char="." align="char">95.41</td><td char="." align="char">97.93</td><td char="." align="char">97.02</td><td char="." align="char">94.86</td><td char="." align="char">98.16</td></tr><tr><td align="left">DistMult</td><td char="." align="char">84.83</td><td char="." align="char">78.82</td><td char="." align="char">88.85</td><td char="." align="char">78.77</td><td char="." align="char">58.51</td><td char="." align="char">98.98</td><td char="." align="char">87.22</td><td char="." align="char">80.49</td><td char="." align="char">93.40</td></tr><tr><td align="left">RotatE</td><td char="." align="char">80.01</td><td char="." align="char">73.27</td><td char="." align="char">81.94</td><td char="." align="char">97.99</td><td char="." align="char">98.13</td><td char="." align="char">99.04</td><td char="." align="char">97.17</td><td char="." align="char">94.65</td><td char="." align="char">97.56</td></tr><tr><td align="left">SimplE</td><td char="." align="char">73.46</td><td char="." align="char">66.22</td><td char="." align="char">75.60</td><td char="." align="char">97.47</td><td char="." align="char">96.75</td><td char="." align="char">97.53</td><td char="." align="char">96.70</td><td char="." align="char">95.65</td><td char="." align="char">98.46</td></tr><tr><td align="left">ComplEx</td><td char="." align="char">82.48</td><td char="." align="char">76.92</td><td char="." align="char">85.16</td><td char="." align="char">98.09</td><td char="." align="char">97.15</td><td char="." align="char">98.17</td><td char="." align="char">92.41</td><td char="." align="char">87.80</td><td char="." align="char">96.50</td></tr><tr><td align="left">R-GCN</td><td char="." align="char">82.19</td><td char="." align="char">74.95</td><td char="." align="char">84.79</td><td char="." align="char">90.90</td><td char="." align="char">82.38</td><td char="." align="char">97.90</td><td char="." align="char">93.14</td><td char="." align="char">90.31</td><td char="." align="char">94.19</td></tr><tr><td align="left">KG-BERT</td><td char="." align="char">94.23</td><td char="." align="char">91.72</td><td char="." align="char">96.77</td><td char="." align="char">96.67</td><td char="." align="char">94.62</td><td char="." align="char">98.16</td><td char="." align="char">97.30</td><td char="." align="char">95.21</td><td char="." align="char">98.61</td></tr><tr><td align="left">KPE-PTransE</td><td char="." align="char">85.30</td><td char="." align="char">87.50</td><td char="." align="char">94.70</td><td char="." align="char">98.70</td><td char="." align="char">98.20</td><td char="." align="char">98.90</td><td char="." align="char">96.20</td><td char="." align="char"><bold>96.00</bold></td><td char="." align="char">98.50</td></tr><tr><td align="left">GilBERT</td><td char="." align="char">94.15</td><td char="." align="char">91.65</td><td char="." align="char">96.72</td><td char="." align="char">96.54</td><td char="." align="char">94.71</td><td char="." align="char">98.06</td><td char="." align="char">95.11</td><td char="." align="char">91.40</td><td char="." align="char">96.98</td></tr><tr><td align="left">KGE-BERT</td><td char="." align="char">94.79</td><td char="." align="char">92.41</td><td char="." align="char">96.12</td><td char="." align="char">96.45</td><td char="." align="char">95.68</td><td char="." align="char">97.93</td><td char="." align="char">97.22</td><td char="." align="char">94.28</td><td char="." align="char">98.96</td></tr><tr><td align="left">LASS</td><td char="." align="char">95.89</td><td char="." align="char">94.23</td><td char="." align="char">96.42</td><td char="." align="char">96.67</td><td char="." align="char">95.89</td><td char="." align="char">97.97</td><td char="." align="char">97.35</td><td char="." align="char">95.49</td><td char="." align="char">98.78</td></tr><tr><td align="left">RP-ISS-Mean</td><td char="." align="char">97.80</td><td char="." align="char">95.83</td><td char="." align="char">99.21</td><td char="." align="char">98.40</td><td char="." align="char">97.20</td><td char="." align="char">99.47</td><td char="." align="char">97.40</td><td char="." align="char">94.71</td><td char="." align="char">99.31</td></tr><tr><td align="left">RP-ISS-Attn</td><td char="." align="char"><bold>98.91</bold></td><td char="." align="char"><bold>97.93</bold></td><td char="." align="char"><bold>99.97</bold></td><td char="." align="char"><bold>99.33</bold></td><td char="." align="char"><bold>98.82</bold></td><td char="." align="char"><bold>99.90</bold></td><td char="." align="char"><bold>97.78</bold></td><td char="." align="char">95.98</td><td char="." align="char"><bold>99.61</bold></td></tr></tbody></table><table-wrap-foot><p>*The bold font indicates the best result among all outcomes.</p></table-wrap-foot></table-wrap></p><p id="Par26">As shown in Table <xref rid="Tab2" ref-type="table">2</xref>, the RP-ISS-Attn model exhibits the best performance and outperforms RP-ISS-Mean. Additionally, the RP-ISS-Attn model significantly outperforms all the baseline models. Compared to the structure-based models (TransE, DistMult, RotatE, SimplE, ComplEx, KPE-PTransE, R-GCN), our model shows an average improvement of 5.7%, 4.2%, and 2.5% in MRR, Hits@1, and Hits@3, respectively, compared to the best performance of these models. Compared to the semantic-based models (KG-BERT, GilBERT and KGE-BERT), our model shows an average improvement of 2.7%, 4.0%, and 2.0% in MRR, Hits@1, and Hits@3, respectively, compared to the best performance of these models.</p><p id="Par27">As depicted in Fig.&#x000a0;<xref rid="Fig4" ref-type="fig">4</xref>, our proposed RP-ISS model, adept at synthesizing both structural and semantic features, demonstrates notable advancements over established baseline models within the experimental datasets. We benchmarked against three salient models: KPE-TransE, grounded in graph embedding; GilBERT, which predominantly relies on textual semantic information; and LASS, which adeptly amalgamates both semantic and structural attributes. Each of these models epitomizes cutting-edge techniques in their respective domains. Upon analyzing the outcomes across the datasets, the RP-ISS model consistently outshines its counterparts. Specifically, it exceeds the KPE-TransE's performance by averages of 6.1% in MRR, 4.2% in Hits@1, and 2.6% in Hits@3. Against the GilBERT model, RP-ISS manifests superiority with average margins of 3.6% in MRR, 5.4% in Hits@1, and 2.6% in Hits@3. Furthermore, when juxtaposed with the LASS model, RP-ISS achieves a lead of 2.1% in MRR, 2.5% in Hits@1, and 2.2% in Hits@3.<fig id="Fig4"><label>Figure 4</label><caption><p>Comparison of RP-ISS with representative SOTA models.</p></caption><graphic xlink:href="41598_2024_63279_Fig4_HTML" id="MO4"/></fig></p><p id="Par28">The reason why RP-ISS can demonstrate a significant improvement over the baseline models is that it fully leverages structural and semantic features to predict missing relations. For those relations predicted through structural information, RP-ISS's edge-based message passing network module can effectively learn and represent these structural patterns, making accurate predictions for these relations. Therefore, compared to models primarily based on semantic features such as KG-BERT, GilBERT, KGE-BERT, RP-ISS shows a notable performance boost due to its utilization of structural features. For those relations that require inference from semantic information, RP-ISS's RoBERTa module can effectively learn and represent these semantic features and make correct predictions about these relations. Hence, compared to models mainly based on structural features like R-GCN, ComplEx, KPE-PTransE, RP-ISS also exhibits significant improvements. Since RP-ISS uses deep features relative to advanced models like LASS which also utilize both semantic and structural characteristics; it achieves a clear improvement as well.</p></sec><sec id="Sec16"><title>Ablation study</title><p id="Par29">We conducted an ablation study to further understand the role of semantic feature and structural feature. As shown in Table <xref rid="Tab4" ref-type="table">4</xref>. The results show that the performance of both models is significantly weaker than that of the RP-ISS model. Based on experiments on three datasets, the model without semantic feature fusion (RP-STRC) experienced a 2.87% decrease in MRR. The model without structural feature fusion (RP-SEM) saw a 2.94% MRR decrease.</p><p id="Par30">On WN18RR, RP-STRC and RP-SEM had MRR reductions of 6.25% and 5.25%, Hits@1 reductions of 10.63% and 9.04%, and Hits@3 reductions of 2.17% and 1.75%, respectively. On WN18, RP-STRC and RP-SEM experienced MRR decreases of 1.21% and 2.80%, Hits@1 decreases of 2.04% and 5.42%, and Hits@3 decreases of 0.54% and 0.14%, respectively. On FB15K-237, RP-STRC and RP-SEM had MRR reductions of 1.16% and 0.79%, Hits@1 reductions of 1.73% and 1.67%, and Hits@3 reductions of 0.68% and 0.67%, respectively.</p><p id="Par31">RP-STRC exclusively utilizes the edge-based message passing network from our research, effectively capturing structural features. RP-STRC adopts randomly initialized embeddings. Specifically, we initialize the node embeddings in RP-STRC with a uniform distribution ranging from [&#x02212;&#x000a0;1, 1]. During the learning process of RP-STRC, the node embeddings are iteratively updated. Comparisons with established structure-based approaches, as shown in Fig.&#x000a0;<xref rid="Fig5" ref-type="fig">5</xref>, demonstrate RP-STRC's competitive performance in relation prediction tasks. Notably, RP-STRC, focused on structural features, marginally outperforms the semantic feature-based RP-SEM model (see Table <xref rid="Tab3" ref-type="table">3</xref>), highlighting the importance of structural information in knowledge graph relation predictions.<fig id="Fig5"><label>Figure 5</label><caption><p>Comparison of RP-STRC with other structure-based methods.</p></caption><graphic xlink:href="41598_2024_63279_Fig5_HTML" id="MO5"/></fig><table-wrap id="Tab3"><label>Table 3</label><caption><p>Results of ablation study on test set.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="2"/><th align="left" colspan="3">WN18RR</th><th align="left" colspan="3">WN18</th><th align="left" colspan="3">FB15K-237</th></tr><tr><th align="left">MRR</th><th align="left">Hits@1</th><th align="left">Hits@3</th><th align="left">MRR</th><th align="left">Hits@1</th><th align="left">Hits@3</th><th align="left">MRR</th><th align="left">Hits@1</th><th align="left">Hits@3</th></tr></thead><tbody><tr><td align="left">RP-ISS</td><td char="." align="char">98.91</td><td char="." align="char">97.93</td><td char="." align="char">99.97</td><td char="." align="char">99.33</td><td char="." align="char">98.82</td><td char="." align="char">99.90</td><td char="." align="char">97.78</td><td char="." align="char">95.98</td><td char="." align="char">99.61</td></tr><tr><td align="left" rowspan="2">RP-STRC (w/o semantic feat.)</td><td char="." align="char">92.73</td><td char="." align="char">87.52</td><td char="." align="char">97.80</td><td char="." align="char">98.13</td><td char="." align="char">96.80</td><td char="." align="char">99.36</td><td char="." align="char">96.65</td><td char="." align="char">94.32</td><td char="." align="char">98.93</td></tr><tr><td char="." align="char">&#x02212;&#x000a0;6.25%</td><td char="." align="char">&#x02212;&#x000a0;10.63%</td><td char="." align="char">&#x02212;&#x000a0;2.17%</td><td char="." align="char">&#x02212;&#x000a0;1.21%</td><td char="." align="char">&#x02212;&#x000a0;2.04%</td><td char="." align="char">&#x02212;&#x000a0;0.54%</td><td char="." align="char">&#x02212;&#x000a0;1.16%</td><td char="." align="char">&#x02212;&#x000a0;1.73%</td><td char="." align="char">&#x02212;&#x000a0;0.68%</td></tr><tr><td align="left" rowspan="2">RP-SEM (w/o structural feat.)</td><td char="." align="char">93.72</td><td char="." align="char">89.08</td><td char="." align="char">98.22</td><td char="." align="char">96.55</td><td char="." align="char">93.46</td><td char="." align="char">99.76</td><td char="." align="char">97.01</td><td char="." align="char">94.38</td><td char="." align="char">98.94</td></tr><tr><td char="." align="char">&#x02212;&#x000a0;5.25%</td><td char="." align="char">&#x02212;&#x000a0;9.04%</td><td char="." align="char">&#x02212;&#x000a0;1.75%</td><td char="." align="char">&#x02212;&#x000a0;2.80%</td><td char="." align="char">&#x02212;&#x000a0;5.42%</td><td char="." align="char">&#x02212;&#x000a0;0.14%</td><td char="." align="char">&#x02212;&#x000a0;0.79%</td><td char="." align="char">&#x02212;&#x000a0;1.67%</td><td char="." align="char">&#x02212;&#x000a0;0.67%</td></tr></tbody></table></table-wrap></p><p id="Par32">The RP-ISS model uses a hyperparameter &#x003bb; to balance the weight of structural embeddings and semantic embeddings in the Node Embedding Memory Bank. To better study the impact of different settings of the hyperparameter &#x003bb; on the model, we conducted experiments with various values of &#x003bb;. As shown in Table <xref rid="Tab4" ref-type="table">4</xref>, we found that setting the hyperparameter &#x003bb; to 0.5 yields the best results. Looking at the distribution of results, it is evident that increasing &#x003bb; significantly reduces model performance. A larger value for hyperparameter &#x003bb; means fewer structural features and more semantic features are considered. This is consistent with conclusions drawn from ablation studies, which suggest that structural features are more important for relation prediction.<table-wrap id="Tab4"><label>Table 4</label><caption><p>Performance of the Model on the WN18RR with Different Hyperparameter &#x003bb; Settings.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"><inline-formula id="IEq82"><alternatives><tex-math id="M179">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\lambda $$\end{document}</tex-math><mml:math id="M180"><mml:mi>&#x003bb;</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_63279_Article_IEq82.gif"/></alternatives></inline-formula></th><th align="left"><inline-formula id="IEq83"><alternatives><tex-math id="M181">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\lambda $$\end{document}</tex-math><mml:math id="M182"><mml:mi>&#x003bb;</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_63279_Article_IEq83.gif"/></alternatives></inline-formula>=0.1</th><th align="left"><inline-formula id="IEq84"><alternatives><tex-math id="M183">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\lambda $$\end{document}</tex-math><mml:math id="M184"><mml:mi>&#x003bb;</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_63279_Article_IEq84.gif"/></alternatives></inline-formula>=0.3</th><th align="left"><inline-formula id="IEq85"><alternatives><tex-math id="M185">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\lambda $$\end{document}</tex-math><mml:math id="M186"><mml:mi>&#x003bb;</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_63279_Article_IEq85.gif"/></alternatives></inline-formula>=0.5</th><th align="left"><inline-formula id="IEq86"><alternatives><tex-math id="M187">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\lambda $$\end{document}</tex-math><mml:math id="M188"><mml:mi>&#x003bb;</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_63279_Article_IEq86.gif"/></alternatives></inline-formula>=0.7</th><th align="left"><inline-formula id="IEq87"><alternatives><tex-math id="M189">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\lambda $$\end{document}</tex-math><mml:math id="M190"><mml:mi>&#x003bb;</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_63279_Article_IEq87.gif"/></alternatives></inline-formula>=0.9</th></tr></thead><tbody><tr><td align="left">MRR</td><td char="." align="char">98.83</td><td char="." align="char">98.90</td><td char="." align="char">98.91</td><td char="." align="char">98.82</td><td char="." align="char">96.70</td></tr></tbody></table></table-wrap></p></sec><sec id="Sec17"><title>Inductive prediction</title><p id="Par33">We conducted an inductive graph learning experiment to evaluate our model's performance in inductive prediction, crucial in knowledge graph research. In real-world applications, domain knowledge graphs often expand over time, requiring models to handle different graph structures during training and prediction (inductive tasks)<sup><xref ref-type="bibr" rid="CR33">33</xref></sup>. Our approach uses BERT to convert node information into token sequences, aiding inductive learning. To assess this, we performed experiments on the WN18RR dataset by simulating an inductive task, varying the number of nodes in the training set and training models with different reduction ratios.</p><p id="Par34">In Fig.&#x000a0;<xref rid="Fig6" ref-type="fig">6</xref>, we compared four models' performance in inductive tasks, finding that our model and KG-BERT were effective. However, our model's performance slightly falls behind KG-BERT when more than 40% of nodes are reduced. This is likely due to differences in handling textual semantics: KG-BERT uses sequential encoding of entities for relation prediction without needing structural data, while RP-ISS uses individual entity encoding combined with structural information. KG-BERT's performance declines slowly in inductive prediction as it relies less on structural data. In contrast, RP-ISS, although also based on textual semantics, depends more on the graph's structural integrity, leading to faster performance drops as structural data decreases. We plan to further explore RP-ISS-based techniques for better handling inductive prediction challenges.<fig id="Fig6"><label>Figure 6</label><caption><p>Results of RP-ISS for inductive relation prediction on WN18RR.</p></caption><graphic xlink:href="41598_2024_63279_Fig6_HTML" id="MO6"/></fig></p></sec></sec><sec id="Sec18"><title>Conclusions</title><p id="Par35">Our study introduces RP-ISS, a new model for relation prediction in knowledge graphs, integrating both semantic and structural features for enhanced accuracy. RP-ISS uses BERT for semantic encoding and an edge-based relational message passing network for structural representation, with a node embedding memory bank to reduce computational costs. Tested on three classic datasets, RP-ISS showed significant improvements over models using only semantic or structural features. An ablation study confirmed the importance of both feature types. While RP-ISS surpassed GNN-based methods in inductive learning ability, it slightly lagged behind KG-BERT. Large language models like GPT-4 have introduced novel approaches to tasks related to knowledge graph completion. Pan et al.<sup><xref ref-type="bibr" rid="CR34">34</xref></sup> summarized some valuable ideas on leveraging LLMs to support the completion of knowledge graphs. In future research, we will further explore how to integrate the capabilities of large language models with the model proposed in this study, aiming for improved performance in completing knowledge graphs.</p></sec></body><back><fn-group><fn><p><bold>Publisher's note</bold></p><p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></fn></fn-group><ack><title>Acknowledgements</title><p>This work was supported by the National Social Science Fund of China under No. 21CTQ030 and the Research Fund of Renmin University of China the under No.23XNF039. The views expressed in this work represent the opinions of the authors and may not necessarily be shared or endorsed by their respective employers and/or sponsors.</p></ack><notes notes-type="author-contribution"><title>Author contributions</title><p>Jianliang Yang: Conceptualization, Methodology, Visualization, Data processing, Writing&#x02014;original draft. Guoxuan Lu: Writing&#x02014;review &#x00026; editing. Siyuan He: Data processing, Writing&#x02014;review &#x00026; editing. Qiuer Cao: Writing&#x02014;original draft. Yuenan Liu: Conceptualization, Writing&#x02014;review &#x00026; editing.</p></notes><notes notes-type="data-availability"><title>Data availability</title><p>The datasets analyzed and the code generated during the current study available from the corresponding author on reasonable request.</p></notes><notes id="FPar1" notes-type="COI-statement"><title>Competing interests</title><p id="Par36">The authors declare no competing interests.</p></notes><ref-list id="Bib1"><title>References</title><ref id="CR1"><label>1.</label><mixed-citation publication-type="other">Omar, R., Mangukiya, O., Kalnis, P. &#x00026; Mansour, E. <italic>ChatGPT versus Traditional Question Answering for Knowledge Graphs: Current Status and Future Directions Towards Knowledge Graph Chatbots</italic>. 10.48550/arXiv.2302.06466 (2023).</mixed-citation></ref><ref id="CR2"><label>2.</label><mixed-citation publication-type="other">Pan, S. <italic>et al.</italic><italic>Unifying Large Language Models and Knowledge Graphs: A Roadmap</italic>. 10.48550/arXiv.2306.08302 (2023).</mixed-citation></ref><ref id="CR3"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gao</surname><given-names>J</given-names></name><name><surname>Peng</surname><given-names>P</given-names></name><name><surname>Lu</surname><given-names>F</given-names></name><name><surname>Claramunt</surname><given-names>C</given-names></name><name><surname>Xu</surname><given-names>Y</given-names></name></person-group><article-title>Towards travel recommendation interpretability: Disentangling tourist decision-making process via knowledge graph</article-title><source>Inf. Process. Manage.</source><year>2023</year><volume>60</volume><fpage>103369</fpage><pub-id pub-id-type="doi">10.1016/j.ipm.2023.103369</pub-id></element-citation></ref><ref id="CR4"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guo</surname><given-names>Q</given-names></name><etal/></person-group><article-title>A survey on knowledge graph-based recommender systems</article-title><source>IEEE Trans. Knowl. Data Eng.</source><year>2022</year><volume>34</volume><fpage>3549</fpage><lpage>3568</lpage><pub-id pub-id-type="doi">10.1109/TKDE.2020.3028705</pub-id></element-citation></ref><ref id="CR5"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hogan</surname><given-names>A</given-names></name><etal/></person-group><article-title>Knowledge graphs</article-title><source>ACM Comput. Surv.</source><year>2021</year><volume>54</volume><fpage>711</fpage><lpage>737</lpage></element-citation></ref><ref id="CR6"><label>6.</label><mixed-citation publication-type="other">Dong, X. <italic>et al.</italic> Knowledge vault: A web-scale approach to probabilistic knowledge fusion. In <italic>Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</italic> 601&#x02013;610 (Association for Computing Machinery, 2014). 10.1145/2623330.2623623.</mixed-citation></ref><ref id="CR7"><label>7.</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Auer</surname><given-names>S</given-names></name><etal/><etal/></person-group><person-group person-group-type="editor"><name><surname>Aberer</surname><given-names>K</given-names></name><etal/><etal/></person-group><article-title>DBpedia: A nucleus for a web of open data</article-title><source>The Semantic Web</source><year>2007</year><publisher-name>Springer</publisher-name><fpage>722</fpage><lpage>735</lpage></element-citation></ref><ref id="CR8"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rossi</surname><given-names>A</given-names></name><name><surname>Barbosa</surname><given-names>D</given-names></name><name><surname>Firmani</surname><given-names>D</given-names></name><name><surname>Matinata</surname><given-names>A</given-names></name><name><surname>Merialdo</surname><given-names>P</given-names></name></person-group><article-title>Knowledge graph embedding for link prediction: A comparative analysis</article-title><source>ACM Trans. Knowl. Discov. Data</source><year>2021</year><volume>15</volume><fpage>1</fpage><lpage>49</lpage></element-citation></ref><ref id="CR9"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shi</surname><given-names>B</given-names></name><name><surname>Weninger</surname><given-names>T</given-names></name></person-group><article-title>Open-world knowledge graph completion</article-title><source>AAAI</source><year>2018</year><volume>32</volume><fpage>11535</fpage><pub-id pub-id-type="doi">10.1609/aaai.v32i1.11535</pub-id></element-citation></ref><ref id="CR10"><label>10.</label><mixed-citation publication-type="other">Bordes, A., Usunier, N., Garcia-Dur&#x000e1;n, A., Weston, J. &#x00026; Yakhnenko, O. Translating embeddings for modeling multi-relational data. In <italic>Proceedings of the 26th International Conference on Neural Information Processing Systems: Volume 2</italic> 2787&#x02013;2795 (Curran Associates Inc., 2013).</mixed-citation></ref><ref id="CR11"><label>11.</label><mixed-citation publication-type="other">Lin, Y., Liu, Z., Sun, M., Liu, Y. &#x00026; Zhu, X. Learning entity and relation embeddings for knowledge graph completion. <italic>Proceedings of the AAAI Conference on Artificial Intelligence</italic><bold>29</bold> (2015).</mixed-citation></ref><ref id="CR12"><label>12.</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Schlichtkrull</surname><given-names>M</given-names></name><etal/><etal/></person-group><person-group person-group-type="editor"><name><surname>Gangemi</surname><given-names>A</given-names></name><etal/><etal/></person-group><article-title>Modeling relational data with graph convolutional networks</article-title><source>The Semantic Web</source><year>2018</year><publisher-name>Springer International Publishing</publisher-name><fpage>593</fpage><lpage>607</lpage></element-citation></ref><ref id="CR13"><label>13.</label><mixed-citation publication-type="other">Yao, L., Mao, C. &#x00026; Luo, Y. KG-BERT: BERT for knowledge graph completion. <italic>CoRR</italic> (2019).</mixed-citation></ref><ref id="CR14"><label>14.</label><mixed-citation publication-type="other">Safavi, T., Downey, D. &#x00026; Hope, T. CascadER: Cross-Modal Cascading for Knowledge Graph Link Prediction. 10.48550/arXiv.2205.08012 (2022).</mixed-citation></ref><ref id="CR15"><label>15.</label><mixed-citation publication-type="other">Lin, Y. <italic>et al.</italic> BertGCN: Transductive text classification by combining GNN and BERT. In <italic>Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</italic> 1456&#x02013;1462 (Association for Computational Linguistics, 2021). 10.18653/v1/2021.findings-acl.126.</mixed-citation></ref><ref id="CR16"><label>16.</label><mixed-citation publication-type="other">Nathani, D., Chauhan, J., Sharma, C. &#x00026; Kaul, M. Learning attention-based embeddings for relation prediction in knowledge graphs. In <italic>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</italic> 4710&#x02013;4723 (Association for Computational Linguistics, 2019). 10.18653/v1/P19-1466.</mixed-citation></ref><ref id="CR17"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>W-K</given-names></name><etal/></person-group><article-title>A path-based relation networks model for knowledge graph completion</article-title><source>Expert Syst. Appl.</source><year>2021</year><volume>182</volume><fpage>115273</fpage><pub-id pub-id-type="doi">10.1016/j.eswa.2021.115273</pub-id></element-citation></ref><ref id="CR18"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peng</surname><given-names>Z</given-names></name><name><surname>Yu</surname><given-names>H</given-names></name><name><surname>Jia</surname><given-names>X</given-names></name></person-group><article-title>Path-based reasoning with K-nearest neighbor and position embedding for knowledge graph completion</article-title><source>J. Intell. Inf. Syst.</source><year>2022</year><volume>58</volume><fpage>513</fpage><lpage>533</lpage><pub-id pub-id-type="doi">10.1007/s10844-021-00671-8</pub-id></element-citation></ref><ref id="CR19"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ma</surname><given-names>J</given-names></name><etal/></person-group><article-title>GAFM: A knowledge graph completion method based on graph attention faded mechanism</article-title><source>Inf. Process. Manag.</source><year>2022</year><volume>59</volume><fpage>103004</fpage><pub-id pub-id-type="doi">10.1016/j.ipm.2022.103004</pub-id></element-citation></ref><ref id="CR20"><label>20.</label><mixed-citation publication-type="other">Wang, B. <italic>et al.</italic> Structure-augmented text representation learning for efficient knowledge graph completion. In <italic>Proceedings of the Web Conference 2021</italic> 1737&#x02013;1748 (Association for Computing Machinery, 2021). 10.1145/3442381.3450043.</mixed-citation></ref><ref id="CR21"><label>21.</label><mixed-citation publication-type="other">Nassiri, A. K., Pernelle, N., Sais, F. &#x00026; Quercini, G. <italic>Knowledge graph refinement based on Triplet BERT-networks</italic> (Hersonissos, 2022). 10.48550/arXiv.2211.10460</mixed-citation></ref><ref id="CR22"><label>22.</label><mixed-citation publication-type="other">Nadkarni, R. <italic>et al.</italic><italic>Scientific language models for biomedical knowledge base completion: An empirical study</italic> (2021).</mixed-citation></ref><ref id="CR23"><label>23.</label><mixed-citation publication-type="other">Shen, J., Wang, C., Gong, L. &#x00026; Song, D. Joint Language Semantic and Structure Embedding for Knowledge Graph Completion. In <italic>Proceedings of the 29th International Conference on Computational Linguistics</italic> 1965&#x02013;1978 (International Committee on Computational Linguistics, 2022).</mixed-citation></ref><ref id="CR24"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>M</given-names></name><name><surname>Qiu</surname><given-names>L</given-names></name><name><surname>Wang</surname><given-names>X</given-names></name></person-group><article-title>A survey on knowledge graph embeddings for link prediction</article-title><source>Symmetry</source><year>2021</year><volume>13</volume><fpage>485</fpage><pub-id pub-id-type="doi">10.3390/sym13030485</pub-id></element-citation></ref><ref id="CR25"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>Q</given-names></name><name><surname>Wang</surname><given-names>R</given-names></name><name><surname>Yang</surname><given-names>J</given-names></name><name><surname>Xue</surname><given-names>L</given-names></name></person-group><article-title>Knowledge graph embedding by translating in time domain space for link prediction</article-title><source>Knowl.-Based Syst.</source><year>2021</year><volume>212</volume><fpage>106564</fpage><pub-id pub-id-type="doi">10.1016/j.knosys.2020.106564</pub-id></element-citation></ref><ref id="CR26"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shen</surname><given-names>T</given-names></name><name><surname>Zhang</surname><given-names>F</given-names></name><name><surname>Cheng</surname><given-names>J</given-names></name></person-group><article-title>A comprehensive overview of knowledge graph completion</article-title><source>Knowl.-Based Syst.</source><year>2022</year><volume>255</volume><fpage>109597</fpage><pub-id pub-id-type="doi">10.1016/j.knosys.2022.109597</pub-id></element-citation></ref><ref id="CR27"><label>27.</label><mixed-citation publication-type="other">Wang, H., Ren, H. &#x00026; Leskovec, J. Relational message passing for knowledge graph completion. In <italic>Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &#x00026; Data Mining</italic> 1697&#x02013;1707 (2021).</mixed-citation></ref><ref id="CR28"><label>28.</label><mixed-citation publication-type="other">Yang, B., Yih, W., He, X., Gao, J. &#x00026; Deng, L. Embedding Entities and Relations for Learning and Inference in Knowledge Bases. In <italic>3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7&#x02013;9, 2015, Conference Track Proceedings</italic> (eds. Bengio, Y. &#x00026; LeCun, Y.) (2015).</mixed-citation></ref><ref id="CR29"><label>29.</label><mixed-citation publication-type="other">Sun, Z., Deng, Z.-H., Nie, J.-Y. &#x00026; Tang, J. RotatE: Knowledge graph embedding by relational rotation in complex space. In <italic>7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6&#x02013;9, 2019</italic> (OpenReview.net, 2019). 10.48550/arXiv.1902.10197.</mixed-citation></ref><ref id="CR30"><label>30.</label><mixed-citation publication-type="other">Kazemi, S. M. &#x00026; Poole, D. SimplE embedding for link prediction in knowledge graphs. In <italic>Proceedings of the 32nd International Conference on Neural Information Processing Systems</italic> 4289&#x02013;4300 (Curran Associates Inc., Red Hook, NY, USA, 2018).</mixed-citation></ref><ref id="CR31"><label>31.</label><mixed-citation publication-type="other">Trouillon, T., Welbl, J., Riedel, S., Gaussier, &#x000c9;. &#x00026; Bouchard, G. Complex embeddings for simple link prediction. In <italic>Proceedings of the 33rd International Conference on International Conference on Machine Learning: Volume 48</italic> 2071&#x02013;2080 (JMLR.org, 2016).</mixed-citation></ref><ref id="CR32"><label>32.</label><mixed-citation publication-type="other">Yao, L., Mao, C. &#x00026; Luo, Y. <italic>KG-BERT: BERT for Knowledge Graph Completion</italic>. <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1909.03193">http://arxiv.org/abs/1909.03193</ext-link> (2019).</mixed-citation></ref><ref id="CR33"><label>33.</label><mixed-citation publication-type="other">Zeng, H., Zhou, H., Srivastava, A., Kannan, R. &#x00026; Prasanna, V. K. GraphSAINT: Graph sampling based inductive learning method. In <italic>8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26&#x02013;30, 2020</italic> (OpenReview.net, 2020).</mixed-citation></ref><ref id="CR34"><label>34.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pan</surname><given-names>S</given-names></name><etal/></person-group><article-title>Unifying large language models and knowledge graphs: A roadmap</article-title><source>IEEE Trans. Knowl. Data Eng.</source><year>2024</year><pub-id pub-id-type="doi">10.1109/TKDE.2024.3352100</pub-id></element-citation></ref></ref-list></back></article>