<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd"> 
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><?properties open_access?><?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?><?DTDIdentifier.IdentifierType public?><?SourceDTD.DTDName A++V2.4.dtd?><?SourceDTD.Version 2.4?><?ConverterInfo.XSLTName springer2nlmx2.xsl?><?ConverterInfo.Version 1?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sci Rep</journal-id><journal-id journal-id-type="iso-abbrev">Sci Rep</journal-id><journal-title-group><journal-title>Scientific Reports</journal-title></journal-title-group><issn pub-type="epub">2045-2322</issn><publisher><publisher-name>Nature Publishing Group UK</publisher-name><publisher-loc>London</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmcid">10948387</article-id><article-id pub-id-type="publisher-id">56956</article-id><article-id pub-id-type="doi">10.1038/s41598-024-56956-9</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Fully automated landmarking and facial segmentation on 3D photographs</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><name><surname>Berends</surname><given-names>Bo</given-names></name><address><email>Bo.Berends@radboudumc.nl</email></address><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author"><name><surname>Bielevelt</surname><given-names>Freek</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author"><name><surname>Schreurs</surname><given-names>Ruud</given-names></name><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author"><name><surname>Vinayahalingam</surname><given-names>Shankeeth</given-names></name><xref ref-type="aff" rid="Aff3">3</xref></contrib><contrib contrib-type="author"><name><surname>Maal</surname><given-names>Thomas</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author"><name><surname>de Jong</surname><given-names>Guido</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/05wg1m734</institution-id><institution-id institution-id-type="GRID">grid.10417.33</institution-id><institution-id institution-id-type="ISNI">0000 0004 0444 9382</institution-id><institution>3D Lab Radboudumc, </institution><institution>Radboud University Medical Center, </institution></institution-wrap>Geert Grooteplein Zuid 10, 6525 GA Nijmegen, The Netherlands </aff><aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="GRID">grid.509540.d</institution-id><institution-id institution-id-type="ISNI">0000 0004 6880 3010</institution-id><institution>Department of Oral and Maxillofacial Surgery, </institution><institution>Amsterdam University Medical Center (UMC), AMC, Academic Center for Dentistry Amsterdam (ACTA), </institution></institution-wrap>Meibergdreef 9, 1105 AZ Amsterdam, The Netherlands </aff><aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/05wg1m734</institution-id><institution-id institution-id-type="GRID">grid.10417.33</institution-id><institution-id institution-id-type="ISNI">0000 0004 0444 9382</institution-id><institution>Department of Oral and Maxillofacial Surgery, </institution><institution>Radboud University Medical Center Nijmegen, </institution></institution-wrap>Geert Grooteplein Zuid 10, 6525 GA Nijmegen, The Netherlands </aff></contrib-group><pub-date pub-type="epub"><day>18</day><month>3</month><year>2024</year></pub-date><pub-date pub-type="pmc-release"><day>18</day><month>3</month><year>2024</year></pub-date><pub-date pub-type="collection"><year>2024</year></pub-date><volume>14</volume><elocation-id>6463</elocation-id><history><date date-type="received"><day>17</day><month>11</month><year>2023</year></date><date date-type="accepted"><day>13</day><month>3</month><year>2024</year></date></history><permissions><copyright-statement>&#x000a9; The Author(s) 2024</copyright-statement><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p></license></permissions><abstract id="Abs1"><p id="Par1">Three-dimensional facial stereophotogrammetry provides a detailed representation of craniofacial soft tissue without the use of ionizing radiation. While manual annotation of landmarks serves as the current gold standard for cephalometric analysis, it is a time-consuming process and is prone to human error. The aim in this study was to develop and evaluate an automated cephalometric annotation method using a deep learning-based approach. Ten landmarks were manually annotated on 2897 3D facial photographs. The automated landmarking workflow involved two successive DiffusionNet models. The dataset was randomly divided into a training and test dataset. The precision of the workflow was evaluated by calculating the Euclidean distances between the automated and manual landmarks and compared to the intra-observer and inter-observer variability of manual annotation and a semi-automated landmarking method. The workflow was successful in 98.6% of all test cases. The deep learning-based landmarking method achieved precise and consistent landmark annotation. The mean precision of 1.69&#x02009;&#x000b1;&#x02009;1.15 mm was comparable to the inter-observer variability (1.31&#x02009;&#x000b1;&#x02009;0.91 mm) of manual annotation. Automated landmark annotation on 3D photographs was achieved with the DiffusionNet-based approach. The proposed method allows quantitative analysis of large datasets and may be used in diagnosis, follow-up, and virtual surgical planning.</p></abstract><kwd-group xml:lang="en"><title>Keywords</title><kwd>Deep learning</kwd><kwd>DiffusionNet</kwd><kwd>Cephalometry</kwd><kwd>Landmarks</kwd><kwd>3D photogrammetry</kwd><kwd>3D meshes</kwd></kwd-group><kwd-group kwd-group-type="npg-subject"><title>Subject terms</title><kwd>Computational science</kwd><kwd>Genetics research</kwd><kwd>Dentistry</kwd><kwd>Three-dimensional imaging</kwd></kwd-group><custom-meta-group><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#x000a9; Springer Nature Limited 2024</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1"><title>Introduction</title><p id="Par2">The fields of genetics, orthodontics, craniomaxillofacial surgery, and plastic surgery have greatly benefitted from advances in imaging technology, particularly in three-dimensional (3D) imaging. Within these fields, cone-beam computed tomography (CBCT) has become a well-established imaging technique as an alternative for computed tomography (CT) imaging, the conventional imaging method for depicting hard tissues<sup><xref ref-type="bibr" rid="CR1">1</xref></sup>. CBCT provides a high-resolution multiplanar reconstruction of the craniofacial skeleton and facial soft tissue. However, a drawback of CT and CBCT is the use of ionizing radiation. In contrast, 3D stereophotogrammetry can capture a detailed and accurate representation of craniofacial soft tissue without the use of ionizing radiation and, therefore, has gained popularity in soft-tissue analysis<sup><xref ref-type="bibr" rid="CR2">2</xref>&#x02013;<xref ref-type="bibr" rid="CR4">4</xref></sup>.</p><p id="Par3">Cephalometric analysis can be performed on 3D stereophotographs to extract information about the position of individual landmarks or distances and angles between several landmarks, with the purpose of objectifying clinical observations<sup><xref ref-type="bibr" rid="CR5">5</xref></sup>. Despite being a commonly used diagnostic tool in the craniofacial region, landmarking often remains a manual task that is time-consuming, prone to observer variability, and affected by observer fatigue and skill level<sup><xref ref-type="bibr" rid="CR6">6</xref>,<xref ref-type="bibr" rid="CR7">7</xref></sup>. Therefore, there has been a growing interest in using artificial intelligence (AI), such as deep learning and machine learning algorithms to automate the landmark identification process.</p><p id="Par4">Several studies have described the use of deep learning algorithms for the automation of hard-tissue landmark extraction for cephalometric analysis<sup><xref ref-type="bibr" rid="CR5">5</xref>,<xref ref-type="bibr" rid="CR8">8</xref></sup>. Studies that include soft-tissue landmarks utilize (projective) 2D imaging, are pose dependent, or require manual input<sup><xref ref-type="bibr" rid="CR9">9</xref>,<xref ref-type="bibr" rid="CR10">10</xref></sup>. Only a limited number of studies were performed on the automated extraction of facial soft-tissue landmarks from 3D photographs<sup><xref ref-type="bibr" rid="CR11">11</xref>,<xref ref-type="bibr" rid="CR12">12</xref></sup>. Nevertheless, since the studies in question did not integrate deep learning algorithms into their methodologies, and considering that deep learning has demonstrated enhanced accuracy in hard tissue landmarking, the effectiveness of soft tissue landmarking could potentially see improvements through the integration of deep learning techniques<sup><xref ref-type="bibr" rid="CR5">5</xref></sup>. Therefore, this study aimed to develop and validate an automated approach for the extraction of soft-tissue facial landmarks from 3D photographs using deep learning.</p></sec><sec id="Sec2"><title>Material and methods</title><sec id="Sec3"><title>Data acquisition</title><p id="Par5">In total, 3188 3D facial photographs were collected from two databases: the Headspace database (n&#x02009;=&#x02009;1519) and the Radboudumc&#x02019;s database (n&#x02009;=&#x02009;1669)<sup><xref ref-type="bibr" rid="CR13">13</xref>,<xref ref-type="bibr" rid="CR14">14</xref></sup>. The Radboudumc&#x02019;s data consisted of healthy volunteers (n&#x02009;=&#x02009;1153) and Oral and Maxillofacial Surgery patients (n&#x02009;=&#x02009;516). The Radboudumc dataset was collected in accordance with the World Medical Association Declaration of Helsinki on medical research ethics. The following ethical approvals and waivers were used: Commissie Mensgebonden Onderzoek (CMO) Arnhem-Nijmegen (Nijmegen, the Netherlands) #2007/163; ARB NL 17934.091.07; CMO Arnhem-Nijmegen (Nijmegen, the Netherlands) #2019-5793. All data were captured using 3dMD&#x02019;s 5-pod 3dMDhead systems (3dMDCranial, 3dMD, Atlanta, Georgia USA). Exclusion criteria were large gaps within the mesh, stitching errors, excessive facial hair interfering with the facial landmarks, meshes that lacked texture (color information), and mesh-texture mismatches. An overview of the data is presented in Table <xref rid="Tab1" ref-type="table">1</xref>.
<table-wrap id="Tab1"><label>Table 1</label><caption><p>Dataset distribution with the percentage of the total dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="2">Dataset</th><th align="left" colspan="2">Train</th><th align="left" colspan="2">Test</th><th align="left" colspan="2">Total</th></tr><tr><th align="left">n</th><th align="left">%</th><th align="left">n</th><th align="left">%</th><th align="left">n</th><th align="left">%</th></tr></thead><tbody><tr><td align="left">Headspace</td><td char="." align="char">1053</td><td align="left">36.3</td><td char="." align="char">192</td><td align="left">6.6</td><td char="." align="char">1245</td><td align="left">43.0</td></tr><tr><td align="left">Radboudumc control</td><td char="." align="char">974</td><td align="left">33.6</td><td char="." align="char">165</td><td align="left">5.7</td><td char="." align="char">1139</td><td align="left">39.3</td></tr><tr><td align="left">Radboudumc patient</td><td char="." align="char">436</td><td align="left">15.1</td><td char="." align="char">77</td><td align="left">2.7</td><td char="." align="char">513</td><td align="left">17.7</td></tr><tr><td align="left">Total</td><td char="." align="char">2463</td><td align="left">85</td><td char="." align="char">434</td><td align="left">15</td><td char="." align="char">2897</td><td align="left">100</td></tr></tbody></table></table-wrap></p></sec><sec id="Sec4"><title>Data annotation</title><p id="Par6">The 3D photographs were manually annotated by a single observer using the 3DMedX<sup>&#x000ae;</sup> software (v1.2.29.0, 3D Lab Radboudumc, Nijmegen, The Netherlands; details can be found at <ext-link ext-link-type="uri" xlink:href="https://3dmedx.nl">https://3dmedx.nl</ext-link>). The following ten cephalometric facial landmarks were annotated: exocanthions, endocanthions, nasion, nose tip, alares, and cheilions. The 3D landmarks were annotated on the mesh surfaces (faces) independent of vertex coordinates. The texture of the 3D photographs was used in the annotation process as a visual cue. Manual annotation was repeated on 50 randomly selected 3D photos by the first observer, a second observer, and a third observer to assess the intra-observer and inter-observer variability.</p></sec><sec id="Sec5"><title>Automated landmarking workflow</title><p id="Par7">The automated landmarking workflow was developed for the same cephalometric landmarks (exocanthions, endocanthions, nasion, nose tip, alares, and cheilions). It consisted of four main steps: (1) rough prediction of landmarks using an initial DiffusionNet on the original meshes; (2) realignment of the meshes based on the roughly predicted landmarks; (3) segmentation of the facial region through fitting of a template facial mesh using a morphable model; (4) refined landmark prediction on the segmented meshes using a final DiffusionNet<sup><xref ref-type="bibr" rid="CR15">15</xref></sup>. The DiffusionNet models used spatial features only and did not use texture information for the automated landmarking task. An overview of the workflow can be seen in Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>.<fig id="Fig1"><label>Figure 1</label><caption><p>Automated landmarking workflow. Step 1: First semantic segmentation task for rough landmark prediction. Step 2: Realignment of the meshes using the roughly predicted landmarks. Step 3: Facial region segmentation (white) using MeshMonk (blue wireframe). Step 4: Second semantic segmentation task for refined landmark prediction.</p></caption><graphic xlink:href="41598_2024_56956_Fig1_HTML" id="MO1"/></fig></p></sec><sec id="Sec6"><title>DiffusionNet</title><p id="Par8">DiffusionNet is a deep learning algorithm able to learn upon non-uniform geometric data, like 3D curved surfaces. It is designed to address challenges associated with the representation and analysis of surfaces in the field of geometric deep learning. DiffusionNet was used for this study as it is a highly accurate method for performing segmentation tasks, also when compared to other state-of-the-art mesh-based deep-learning algorithms. A key feature is the relative mesh sampling/resolution robustness of the algorithm as well as the invariance to input orientations. The DiffusionNet consists of three main components: a multilayer perceptron (MLP) for feature transformation, a diffusion layer responsible for information propagation, and a spatial gradient enabling the network to understand and learn. The architecture of the DiffusionNet depends on the configuration of several adjustable parameters. These include C-width, which corresponds to the size of the DiffusionNet and N-blocks, corresponding to the number of DiffusionNet blocks. In addition, the size of the (per-vertex) MLP can be altered by adjustment of its layer size<sup><xref ref-type="bibr" rid="CR15">15</xref></sup>.</p></sec><sec id="Sec7"><title>Training</title><p id="Par9">The data were randomly divided into two sets, 85% for training and 15% for testing of the DiffusionNet models. As a data augmentation step, the 3D meshes from the training dataset were mirrored over the YZ plane to double the number of scans available for training. No validation set was used during training.</p></sec><sec id="Sec8"><title>Step 1: Rough prediction of landmarks</title><p id="Par10">A DiffusionNet was utilized for initial prediction of the exocanthions, endocanthions, nasion, nose tip, alares, and cheilions as visualized in Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref><sup><xref ref-type="bibr" rid="CR15">15</xref></sup>.<fig id="Fig2"><label>Figure 2</label><caption><p>First semantic segmentation task. The manually annotated landmarks (spheres) and corresponding masks are visualized in green. The green areas represent the vertices within 5 mm of the manually annotated landmark. The roughly predicted landmarks are visualized in yellow. The yellow area represents the positively predicted vertices out of which the rough landmarks will be calculated.</p></caption><graphic xlink:href="41598_2024_56956_Fig2_HTML" id="MO2"/></fig></p><sec id="Sec9"><title>Preprocessing</title><p id="Par11">To speed up the training process for rough landmark prediction, each mesh was downsampled to a maximum of 25.000 vertices<sup><xref ref-type="bibr" rid="CR16">16</xref></sup>. After downsampling, the 3D meshes had a mean inter-vertex distance of 2.035&#x02009;&#x000b1;&#x02009;0.356 mm. Subsequently, a mask was applied, assigning a value of 1 to all vertices located within 5 mm Euclidean distance to the manually annotated landmarks and a value of 0 to the remaining vertices.</p></sec><sec id="Sec10"><title>DiffusionNet configuration for the first semantic segmentation task</title><p id="Par12">Six output channels were configured for the first semantic segmentation task. The two midsagittal landmarks (nasion and nose tip) were assigned an individual channel. The four bilateral landmark pairs were assigned to the four remaining channels. The DiffusionNet model was configured with a C-width of 256, an MLP of 256 and 256 channels, and an N-block of 12. The network used an Adam optimizer with a Cosine Annealing learning rate of 2&#x02009;&#x000d7;&#x02009;10<sup>&#x02212;5</sup> and a T<sub>max</sub> of 50 epochs, representing the number of epochs at which the learning rate is annealed to its minimum and starts increasing again. Furthermore, a binary cross-entropy loss and a dropout rate of 0.10 were applied. Since the orientation and position of the included 3D meshes was not fixed, the network was trained with Heat Kernel Signature (HKS) Features of the 3D meshes. The final output layer was linear. The model was implemented in PyTorch on a 24 GB NVIDIA RTX A5000 GPU and trained for 200 epochs.</p></sec><sec id="Sec11"><title>Post-processing</title><p id="Par13">After the semantic segmentation, the model was used to predict which vertices belonged to each of the configured channels. For the symmetrical landmarks, a 3D clustering algorithm was utilized to distinguish the predicted vertex clusters from each other. Subsequently, a weighted combination of the output values (activations), as well as the locations of each of the vertices that received a non-zero activation value, were used to determine the landmark positions using Eq.&#x000a0;<xref rid="Equ1" ref-type="disp-formula">1</xref>.<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$Predicted\;landmark\;location = \frac{{\sum \left( {10^{{Activations_{1,2, \ldots ,i} }} *coordinate_{1,2, \ldots ,i} } \right)}}{{\sum 10^{{Activations_{1,2, \ldots ,i} }} }}$$\end{document}</tex-math><mml:math id="M2" display="block"><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mspace width="0.277778em"/><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>k</mml:mi><mml:mspace width="0.277778em"/><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo>&#x02211;</mml:mo><mml:mfenced close=")" open="("><mml:mrow><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mi>A</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup><mml:mrow/><mml:mo>&#x02217;</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mo>&#x02211;</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mi>A</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:math><graphic xlink:href="41598_2024_56956_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula></p><p id="Par14">A plane was formed by connecting the predicted nasion, nose tip, and cheilion midpoint to establish if the bilateral landmarks were on the left or right side of the face using the plane equation.</p></sec></sec><sec id="Sec12"><title>Step 2: Realignment</title><p id="Par15">Based on the rough prediction of the exocanthions, nasion, and cheilions, the 3D meshes were positioned in a reference frame. The nasion was defined as the origin, with the x-axis running parallel to the line connecting both exocanthions and the z-axis parallel to the nasion-cheilion midpoint line (Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>).</p></sec><sec id="Sec13"><title>Step 3: Facial region segmentation</title><p id="Par16">Facial segmentation was used to standardize the realigned 3D meshes for refined landmark prediction and reduce processing time without the need for downsampling. The MeshMonk algorithm, which utilizes a combination of rigid and non-rigid template matching, was utilized for segmentation (implemented in C++)<sup><xref ref-type="bibr" rid="CR10">10</xref></sup>. The default face template of MeshMonk was used. The exocanthions, nose tip, and cheilions (step 2) were used for the initial registration of the facial template mesh to the 3D meshes. The configuration of the MeshMonk fitting algorithm is given in the Appendix. An additional margin of 15 mm around the fitted MeshMonk template was included to supply the final DiffusionNet with more context while still excluding hair regions (Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>). After facial segmentation, the mean inter-vertex distance of the 3D meshes was 1.39&#x02009;&#x000b1;&#x02009;0.16 mm.</p><p id="Par17">Besides facial segmentation, the MeshMonk algorithm can also be utilized for landmark annotation. The ten landmarks were collected using this semi-automatic approach to serve as a reference for the precision of the automated annotation approach. In contrast to the automated approach, the manually annotated landmarks were used for template fitting in the semi-automated approach to comply with the MeshMonk workflow<sup><xref ref-type="bibr" rid="CR10">10</xref></sup>.</p></sec><sec id="Sec14"><title>Step 4: Refined landmark prediction</title><p id="Par18">A second semantic segmentation task, using DiffusionNet, was used to predict the landmarks on the realigned and segmented 3D meshes (Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3</xref>).<fig id="Fig3"><label>Figure 3</label><caption><p>Second semantic segmentation task. The manually annotated landmarks and corresponding masks that were used for training are visualized in green. The green areas represent the vertices within 3.5 mm of the manually annotated landmark. The positively predicted vertices and the calculated refined predicted landmarks are visualized in red.</p></caption><graphic xlink:href="41598_2024_56956_Fig3_HTML" id="MO3"/></fig></p><sec id="Sec15"><title>Preprocessing</title><p id="Par19">In contrast to step 1, the meshes were not downsampled as it is made unnecessary by the facial region segmentation step. A mask was created in which the vertices within 3.5 mm of the manually annotated landmarks were assigned value 1 and the other vertices were assigned value 0. Default mesh normalization and scaling were applied as provided by the DiffusionNet package<sup><xref ref-type="bibr" rid="CR15">15</xref></sup>.</p></sec><sec id="Sec16"><title>DiffusionNet configuration for the second semantic segmentation task</title><p id="Par20">For the second semantic segmentation task, ten output channels were configured: each individual landmark was assigned to an individual channel. The DiffusionNet was configured with a C-width of 384, an MLP of 768, and an N-blocks of 12. Compared to the first network, the same optimizer, loss, and drop-out were used. However, this second network was trained with XYZ settings instead of HKS settings as the rotation invariance was no longer present after step 3 (Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>). The model was implemented in PyTorch on a 24 GB NVIDIA RTX A5000 GPU and trained for 200 epochs. The final output layer was linear.</p></sec><sec id="Sec17"><title>Post-processing</title><p id="Par21">A weighted combination of the activations, supplemented by the locations of each of the vertices, was again used to determine the final landmark positions.</p></sec></sec><sec id="Sec18"><title>Statistical analysis</title><p id="Par22">Statistical analyses were performed on available patient characteristics to assess differences between the source databases and between the training and test data. To assess the intra-observer and inter-observer variability of the manual annotation method, the Euclidean distances between the landmarks annotated by the different observers were calculated. Descriptive statistics were used to summarize the results. The Euclidean distances between the predicted and the manually annotated landmarks were calculated for every test set to evaluate the performance of automated landmarking; descriptive statistics were used for summarizing the results. This was done for both the rough (initial DiffusionNet) and the refined (final DiffusionNet) predictions. The performance of the automated landmarking workflow was compared to the intra-observer and inter-observer variability of the manual annotation method.</p><p id="Par23">The Euclidean distances between the manually annotated landmarks and the predictions by the semi-automated MeshMonk method were calculated and compared to the precision of the refined predictions using a one-way repeated measures ANOVA test. A <italic>p</italic> value&#x02009;&#x0003c;&#x02009;0.05 was used as a cut-off value for statistical significance.</p></sec><sec id="Sec19"><title>Ethical approval and Informed consent</title><p id="Par24">CMO Arnhem-Nijmegen (Nijmegen, the Netherlands) #2007/163; ARB NL 17934.091.07; CMO Arnhem-Nijmegen (Nijmegen, the Netherlands) #2019-5793. Images of a 3D photo of the first author, who gave his informed consent to publish images in this online open-access publication, were used in Figs.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>, <xref rid="Fig2" ref-type="fig">2</xref> and <xref rid="Fig3" ref-type="fig">3</xref>.</p></sec></sec><sec id="Sec20"><title>Results</title><p id="Par25">Based on the stated exclusion criteria, 291 3D photographs were excluded, yielding a total of 2897 3D photographs that were used for training and testing of the developed workflow (Table <xref rid="Tab1" ref-type="table">1</xref>). Most of the exclusions were due to the lack of texture information (n&#x02009;=&#x02009;271). The age and gender characteristics are given in Table <xref rid="Tab2" ref-type="table">2</xref>. A statistically significant difference was found for age and gender between the source databases (<italic>p</italic>&#x02009;&#x0003c;&#x02009;0.001 and <italic>p</italic>&#x02009;&#x0003c;&#x02009;0.001, respectively). However, there were no statistically significant differences between ages and genders of the training and test splits (<italic>p</italic>&#x02009;=&#x02009;0.323 and <italic>p</italic>&#x02009;=&#x02009;0.479, respectively). There were no unknown genders or ages in the test dataset. The training dataset held one transgender case and had five unknown ages.
<table-wrap id="Tab2"><label>Table 2</label><caption><p>Population characteristics per dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="2">Dataset</th><th align="left" colspan="4">Age (years)</th><th align="left" colspan="3">Gender</th></tr><tr><th align="left">Mean</th><th align="left">Std</th><th align="left">Min</th><th align="left">Max</th><th align="left">Male</th><th align="left">Female</th><th align="left">Transgender</th></tr></thead><tbody><tr><td align="left">Headspace</td><td char="." align="char">35.9</td><td char="." align="char">17.6</td><td char="." align="char">2</td><td align="left">90</td><td align="left">631 (50.7%)</td><td align="left">613 (49.2%)</td><td align="left">1 (0.1%)</td></tr><tr><td align="left">Controls</td><td char="." align="char">42.1</td><td char="." align="char">19.4</td><td char="." align="char">0</td><td align="left">90</td><td align="left">492 (43.2%)</td><td align="left">647 (56.8%)</td><td align="left"/></tr><tr><td align="left">Patients</td><td char="." align="char">27.8</td><td char="." align="char">10.9</td><td char="." align="char">13</td><td align="left">69</td><td align="left">190 (37.0%)</td><td align="left">323 (63.0%)</td><td align="left"/></tr></tbody></table></table-wrap></p><p id="Par26">The intra-observer and interobserver differences of the manual annotation method are summarized in Tables <xref rid="Tab3" ref-type="table">3</xref> and <xref rid="Tab4" ref-type="table">4</xref>, respectively. The overall mean intra-observer variability for manual annotation of the ten landmarks was 0.94&#x02009;&#x000b1;&#x02009;0.71&#x000a0;mm; the overall mean interobserver variability was 1.31&#x02009;&#x000b1;&#x02009;0.91&#x000a0;mm.
<table-wrap id="Tab3"><label>Table 3</label><caption><p>The intra-observer variability for all ten landmarks is stated in millimeters&#x02009;&#x000b1;&#x02009;standard deviation.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" colspan="2">Exocanthion</th><th align="left" colspan="2">Endocanthion</th><th align="left">Nasion</th><th align="left">Nose tip</th><th align="left" colspan="2">Alare</th><th align="left" colspan="2">Cheilion</th></tr><tr><th align="left"><italic>Right</italic></th><th align="left"><italic>Left</italic></th><th align="left"><italic>Right</italic></th><th align="left"><italic>Left</italic></th><th align="left"/><th align="left"/><th align="left"><italic>Right</italic></th><th align="left"><italic>Left</italic></th><th align="left"><italic>Right</italic></th><th align="left"><italic>Left</italic></th></tr></thead><tbody><tr><td align="left">0.85&#x02009;&#x000b1;&#x02009;0.65</td><td char="." align="char">0.87&#x02009;&#x000b1;&#x02009;0.55</td><td char="." align="char">0.79&#x02009;&#x000b1;&#x02009;0.88</td><td char="." align="char">0.79&#x02009;&#x000b1;&#x02009;0.72</td><td char="." align="char">1.32&#x02009;&#x000b1;&#x02009;1.04</td><td char="." align="char">0.88&#x02009;&#x000b1;&#x02009;0.43</td><td char="." align="char">1.04&#x02009;&#x000b1;&#x02009;0.53</td><td char="." align="char">0.99&#x02009;&#x000b1;&#x02009;0.58</td><td char="." align="char">0.99&#x02009;&#x000b1;&#x02009;0.79</td><td char="." align="char">0.85&#x02009;&#x000b1;&#x02009;0.64</td></tr></tbody></table><table-wrap-foot><p>The intra-observer variability was determined by computing the Euclidean distance between annotations performed in twofold by a single observer.</p></table-wrap-foot></table-wrap><table-wrap id="Tab4"><label>Table 4</label><caption><p>The interobserver variability is computed by comparing the Euclidean distance between annotations made by three different observers.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="2"/><th align="left" colspan="2">Exocanthion</th><th align="left" colspan="2">Endocanthion</th><th align="left">Nasion</th><th align="left">Nose tip</th><th align="left" colspan="2">Alare</th><th align="left" colspan="2">Cheilion</th></tr><tr><th align="left"><italic>Right</italic></th><th align="left"><italic>Left</italic></th><th align="left"><italic>Right</italic></th><th align="left"><italic>Left</italic></th><th align="left"/><th align="left"/><th align="left"><italic>Right</italic></th><th align="left"><italic>Left</italic></th><th align="left"><italic>Right</italic></th><th align="left"><italic>Left</italic></th></tr></thead><tbody><tr><td align="left">Observer 1 versus 2</td><td char="." align="char">1.16&#x02009;&#x000b1;&#x02009;0.65</td><td char="." align="char">1.14&#x02009;&#x000b1;&#x02009;0.67</td><td char="." align="char">1.08&#x02009;&#x000b1;&#x02009;0.69</td><td char="." align="char">0.87&#x02009;&#x000b1;&#x02009;0.58</td><td char="." align="char">1.64&#x02009;&#x000b1;&#x02009;0.91</td><td char="." align="char">1.16&#x02009;&#x000b1;&#x02009;0.59</td><td char="." align="char">1.34&#x02009;&#x000b1;&#x02009;0.93</td><td char="." align="char">1.27&#x02009;&#x000b1;&#x02009;0.79</td><td char="." align="char">0.93&#x02009;&#x000b1;&#x02009;0.55</td><td char="." align="char">0.97&#x02009;&#x000b1;&#x02009;0.66</td></tr><tr><td align="left">Observer 1 versus 3</td><td char="." align="char">1.02&#x02009;&#x000b1;&#x02009;0.85</td><td char="." align="char">0.95&#x02009;&#x000b1;&#x02009;0.63</td><td char="." align="char">1.03&#x02009;&#x000b1;&#x02009;0.80</td><td char="." align="char">1.08&#x02009;&#x000b1;&#x02009;0.73</td><td char="." align="char">1.80&#x02009;&#x000b1;&#x02009;1.20</td><td char="." align="char">1.77&#x02009;&#x000b1;&#x02009;0.86</td><td char="." align="char">1.68&#x02009;&#x000b1;&#x02009;1.19</td><td char="." align="char">1.35&#x02009;&#x000b1;&#x02009;0.80</td><td char="." align="char">1.65&#x02009;&#x000b1;&#x02009;1.04</td><td char="." align="char">1.43&#x02009;&#x000b1;&#x02009;1.05</td></tr><tr><td align="left">Observer 2 versus 3</td><td char="." align="char">1.31&#x02009;&#x000b1;&#x02009;0.88</td><td char="." align="char">1.03&#x02009;&#x000b1;&#x02009;0.57</td><td char="." align="char">0.97&#x02009;&#x000b1;&#x02009;0.73</td><td char="." align="char">1.05&#x02009;&#x000b1;&#x02009;0.64</td><td char="." align="char">2.21&#x02009;&#x000b1;&#x02009;1.30</td><td char="." align="char">2.20&#x02009;&#x000b1;&#x02009;1.07</td><td char="." align="char">1.33&#x02009;&#x000b1;&#x02009;0.98</td><td char="." align="char">1.35&#x02009;&#x000b1;&#x02009;0.82</td><td char="." align="char">1.27&#x02009;&#x000b1;&#x02009;0.83</td><td char="." align="char">1.25&#x02009;&#x000b1;&#x02009;0.84</td></tr><tr><td align="left">Average</td><td char="." align="char">1.16&#x02009;&#x000b1;&#x02009;0.80</td><td char="." align="char">1.04&#x02009;&#x000b1;&#x02009;0.63</td><td char="." align="char">1.03&#x02009;&#x000b1;&#x02009;0.74</td><td char="." align="char">1.00&#x02009;&#x000b1;&#x02009;0.66</td><td char="." align="char">1.88&#x02009;&#x000b1;&#x02009;1.17</td><td char="." align="char">1.71&#x02009;&#x000b1;&#x02009;0.96</td><td char="." align="char">1.45&#x02009;&#x000b1;&#x02009;1.05</td><td char="." align="char">1.32&#x02009;&#x000b1;&#x02009;0.80</td><td char="." align="char">1.28&#x02009;&#x000b1;&#x02009;0.88</td><td char="." align="char">1.22&#x02009;&#x000b1;&#x02009;0.88</td></tr></tbody></table><table-wrap-foot><p>The Euclidean distances are stated in millimeters&#x02009;&#x000b1;&#x02009;standard deviation.</p></table-wrap-foot></table-wrap></p><p id="Par27">The initial DiffusionNet showed an average precision of 2.66&#x02009;&#x000b1;&#x02009;2.37&#x000a0;mm, and the complete workflow achieved a precision of 1.69&#x02009;&#x000b1;&#x02009;1.15&#x000a0;mm. The performance of both models is summarized in Table <xref rid="Tab5" ref-type="table">5</xref>. The workflow could be completed for 98.6% of the test data; for six 3D photos (1.4%), one of the rough landmarks required for the consecutive steps could not be predicted by the first DiffusionNet. Upon visual inspection, the six excluded 3D photos contained large gaps and/or substantial amounts of information outside the region of interest, such as clothing or hair. Since the workflow could not be completed, these data sets were excluded from the results.
<table-wrap id="Tab5"><label>Table 5</label><caption><p>The precision of the rough (first DiffusionNet) and refined (second DiffusionNet) is determined by computing the Euclidean distance between the DiffusionNet-predicted and manually annotated landmarks and is stated in millimeters&#x02009;&#x000b1;&#x02009;standard deviation.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="2"/><th align="left" colspan="2">Exocanthion</th><th align="left" colspan="2">Endocanthion</th><th align="left">Nasion</th><th align="left">Nose tip</th><th align="left" colspan="2">Alare</th><th align="left" colspan="2">Cheilion</th></tr><tr><th align="left"><italic>Right</italic></th><th align="left"><italic>Left</italic></th><th align="left"><italic>Right</italic></th><th align="left"><italic>Left</italic></th><th align="left"/><th align="left"/><th align="left"><italic>Right</italic></th><th align="left"><italic>Left</italic></th><th align="left"><italic>Right</italic></th><th align="left"><italic>Left</italic></th></tr></thead><tbody><tr><td align="left">Rough predictions</td><td char="." align="char">2.94&#x02009;&#x000b1;&#x02009;2.38</td><td char="." align="char">2.86&#x02009;&#x000b1;&#x02009;1.81</td><td char="." align="char">2.76&#x02009;&#x000b1;&#x02009;2.40</td><td char="." align="char">2.83&#x02009;&#x000b1;&#x02009;2.56</td><td char="." align="char">1.69&#x02009;&#x000b1;&#x02009;1.05</td><td char="." align="char">1.58&#x02009;&#x000b1;&#x02009;0.89</td><td char="." align="char">2.41&#x02009;&#x000b1;&#x02009;1.98</td><td char="." align="char">2.52&#x02009;&#x000b1;&#x02009;1.92</td><td char="." align="char">3.48&#x02009;&#x000b1;&#x02009;3.67</td><td char="." align="char">3.51&#x02009;&#x000b1;&#x02009;2.89</td></tr><tr><td align="left">Refined predictions</td><td char="." align="char">2.25&#x02009;&#x000b1;&#x02009;1.23</td><td char="." align="char">2.03&#x02009;&#x000b1;&#x02009;1.27</td><td char="." align="char">1.37&#x02009;&#x000b1;&#x02009;0.86</td><td char="." align="char">1.48&#x02009;&#x000b1;&#x02009;1.00</td><td char="." align="char">1.48&#x02009;&#x000b1;&#x02009;1.02</td><td char="." align="char">1.14&#x02009;&#x000b1;&#x02009;0.73</td><td char="." align="char">1.79&#x02009;&#x000b1;&#x02009;1.07</td><td char="." align="char">1.75&#x02009;&#x000b1;&#x02009;1.11</td><td char="." align="char">1.71&#x02009;&#x000b1;&#x02009;1.26</td><td char="." align="char">1.88&#x02009;&#x000b1;&#x02009;1.34</td></tr></tbody></table></table-wrap></p><p id="Par28">The precision was within 2 mm for 69% of the refined predicted landmarks, within 3 mm for 89% of the landmarks, and within 4 mm for 96% of the landmarks. Table <xref rid="Tab6" ref-type="table">6</xref> details the precision within these boundaries for the individual landmarks. The exocanthions and alares were found to perform the worst. The precision of the semi-automated MeshMonk method was on average 1.97&#x02009;&#x000b1;&#x02009;1.34&#x000a0;mm for the ten landmarks (Fig.&#x000a0;<xref rid="Fig4" ref-type="fig">4</xref>). Compared to this semi-automatic method, the DiffusionNet-based method was found to have significantly better precision for the left exocanthion, endocanthions, nose tip, and cheilions and worse precision for the alares; no significant differences were found for nasion and right exocanthion.
<table-wrap id="Tab6"><label>Table 6</label><caption><p>Overview of the accuracy distribution of each landmark as predicted by the complete workflow.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="2"/><th align="left" colspan="4">Percentage of landmarks predicted with a precision within range</th></tr><tr><th align="left">&#x02009;&#x0003c;&#x02009;2 mm (%)</th><th align="left">&#x02009;&#x0003c;&#x02009;3 mm (%)</th><th align="left">&#x02009;&#x0003c;&#x02009;4 mm (%)</th><th align="left">&#x02009;&#x0003c;&#x02009;5 mm (%)</th></tr></thead><tbody><tr><td align="left">Exocanthion right</td><td align="left">47</td><td align="left">77</td><td align="left">90</td><td char="." align="char">97</td></tr><tr><td align="left">Exocanthion left</td><td align="left">56</td><td align="left">80</td><td align="left">82</td><td char="." align="char">97</td></tr><tr><td align="left">Endocanthion right</td><td align="left">80</td><td align="left">96</td><td align="left">99</td><td char="." align="char">100</td></tr><tr><td align="left">Endocanthion left</td><td align="left">76</td><td align="left">92</td><td align="left">98</td><td char="." align="char">99</td></tr><tr><td align="left">Nasion</td><td align="left">77</td><td align="left">94</td><td align="left">97</td><td char="." align="char">99</td></tr><tr><td align="left">Nose tip</td><td align="left">88</td><td align="left">98</td><td align="left">99</td><td char="." align="char">100</td></tr><tr><td align="left">Alare right</td><td align="left">62</td><td align="left">88</td><td align="left">96</td><td char="." align="char">99</td></tr><tr><td align="left">Alare left</td><td align="left">67</td><td align="left">86</td><td align="left">96</td><td char="." align="char">98</td></tr><tr><td align="left">Cheilion right</td><td align="left">71</td><td align="left">89</td><td align="left">95</td><td char="." align="char">97</td></tr><tr><td align="left">Cheilion left</td><td align="left">65</td><td align="left">87</td><td align="left">94</td><td char="." align="char">97</td></tr><tr><td align="left">All Landmarks</td><td align="left">69</td><td align="left">89</td><td align="left">96</td><td char="." align="char">98</td></tr></tbody></table></table-wrap><fig id="Fig4"><label>Figure 4</label><caption><p>Precision of the landmarking methods. The precision of the prediction of the rough landmarks (first DiffusionNet), the refined landmarks (second DiffusionNet), and the semi-automated MeshMonk method are visualized for the right exocanthion (Exo R), left exocanthion (Exo L), right endocanthion (Endo R), left endocanthion (Endo L), nasion, nose tip, right alare (Alare R), left alare (Alare L), right cheilion (Cheilion R), and left cheilion (Cheilion L).</p></caption><graphic xlink:href="41598_2024_56956_Fig4_HTML" id="MO4"/></fig></p></sec><sec id="Sec21"><title>Discussion</title><p id="Par29">Soft-tissue cephalometric analysis can be used to objectify the clinical observations on 3D photographs, but manual annotation, the current gold standard, is time-consuming and tedious. Therefore, this study developed a deep learning-based approach for automated landmark extraction from randomly oriented 3D photographs. The performance was assessed for ten cephalometric landmarks: the results showed that the deep-learning-based landmarking method was precise and consistent, with a precision that approximated the inter-observer variability of the manual annotation method. A precision&#x02009;&#x0003c;&#x02009;2 mm, which may be considered a cut-off value for clinical relevance, was seen for 69% of the predicted landmarks<sup><xref ref-type="bibr" rid="CR17">17</xref>,<xref ref-type="bibr" rid="CR18">18</xref></sup>.</p><p id="Par30">In the field of craniofacial surgery, different studies have applied deep-learning models for automated cephalometric landmarking, mainly focusing on 2D and 3D radiographs. Dot et al. used a SpatialConfiguration-Net for the automated annotation of 33 different 3D hard-tissue landmarks from CT images and achieved a precision of 1.0&#x02009;&#x000b1;&#x02009;1.3&#x000a0;mm<sup><xref ref-type="bibr" rid="CR19">19</xref></sup>. An automated landmarking method, based on multi-stage deep reinforcement learning and volume-rendered imaging, was proposed by Kang et al. and yielded a precision of 1.96&#x02009;&#x000b1;&#x02009;0.78&#x000a0;mm<sup><xref ref-type="bibr" rid="CR20">20</xref></sup>. A systematic review by Serafin et al. found a mean precision of 2.44&#x000a0;mm for the prediction of 3D hard-tissue landmarks from CT and CBCT images<sup><xref ref-type="bibr" rid="CR5">5</xref></sup>.</p><p id="Par31">Some studies did describe automated algorithms for 3D soft tissue landmarking on 3D photographs, but these algorithms did not include deep learning models. Baksi et al. described an automated method, involving morphing of a template mesh, for the landmarking of 22 soft-tissue landmarks from 3D photographs that achieved a precision of 3.2&#x02009;&#x000b1;&#x02009;1.6&#x000a0;mm<sup><xref ref-type="bibr" rid="CR11">11</xref></sup>. An automated principal component analysis-based method, described by Guo et al., achieved an average root mean square error of 1.7&#x000a0;mm for the landmarking 17 soft-tissue landmarks from 3D photographs<sup><xref ref-type="bibr" rid="CR12">12</xref></sup>.</p><p id="Par32">Studies that did apply deep learning for landmark detection on 3D meshes mostly focused on landmarking on intra-oral scans. Wu et al. utilized a two-stage deep learning framework for the prediction of 44 dental landmarks and achieved a mean absolute error of 0.623&#x02009;&#x000b1;&#x02009;0.718 mm<sup><xref ref-type="bibr" rid="CR21">21</xref></sup>. DentalPointNet, consisting of two sub-networks; a region proposal network and a refinement network, as described by Lang et al.<sup><xref ref-type="bibr" rid="CR22">22</xref></sup>, achieved an average localization error of 0.24 mm for the detection of 68 landmarks. Even though the precision achieved in these studies is better than was achieved in this study, a direct comparison is infeasible to make due to the difference in landmarks, datasets, object size, and imaging modalities used. Therefore, future research should investigate the efficacy of these methods for cephalometric soft-tissue landmarking on 3D photographs. However, as the precision achieved by this method (1.69&#x02009;&#x000b1;&#x02009;1.15 mm) closely aligns with the inter-observer variability (1.31&#x02009;&#x000b1;&#x02009;0.91 mm) observed for the ten landmarks utilized, it is improbable that employing an alternative machine learning-based method trained and evaluated on the same dataset would yield large improvements in precision. Especially beyond the inter-observer variability, as the evaluation of the developed method is confined to the precision level dictated by the inter-observer variability of the manual annotation method.</p><p id="Par33">The effect of landmark choice on established precision is underlined by the MeshMonk results found in this study. In the original publication by White et al., an average error of 1.26 mm for 19 soft-tissue landmarks was seen<sup><xref ref-type="bibr" rid="CR10">10</xref></sup>. The same methodology was used to establish the precision for the ten landmarks used in this study, and an overall precision of 1.97&#x02009;&#x000b1;&#x02009;1.34 mm was found. This finding highlights the difficulty in comparing landmarking precision from literature. Compared to the semi-automatic method, the fully-automated workflow yielded significantly improved precision for six landmarks, emphasizing the feasibility of fully-automatically annotating soft-tissue landmarks from 3D photos using deep learning.</p><p id="Par34">The proposed workflow uses two successive networks and additional algorithms for alignment and facial segmentation. Advantages of this approach include that the DiffusionNet assures robustness against sampling densities and the HKS settings inherently account for rotational, positional, and scale invariance that may arise between different 3D photography systems. Input resolution can be considered a limiting factor for the maximum landmark detection performance. This can partially explain the differences as seen in the examples mentioned earlier (e.g. intra-oral scans have a much higher mesh resolution as compared to e.g. 3D photos or CT-scans). Furthermore, as DiffusionNet natively gives a per vertex prediction, the precision of the landmarking method is dependent on the vertex density of the 3D meshes. Therefore, the original high resolution meshes were used for the refined landmark prediction step. Since the achieved precision exceeded half the inter-vertex distance, it is expected that the impact of the inter-vertex distance on the precision was negligible. Moreover, employing the weighted mean vertex location for transforming the predicted vertices into landmark coordinates ensures that the predicted landmarks are not constrained solely to the vertex coordinates of the mesh.</p><p id="Par35">A limitation of the current study is that the workflow was only applied to 3D photographs captured using one 3D photography system. Despite the robust nature of DiffusionNet/HKS, the performance of the workflow might be affected when applied to 3D photographs captured with different hardware. Furthermore, the DiffusionNet models were only trained on spatial features, whereas in the manual annotation process texture information was used. Even though this has the advantage of making the DiffusionNet models insensitive to variations in skin tone or color, landmarks such as the exocanthions, endocanthions, and cheilions could presumably be located more precisely using manual annotation. This would not apply to the landmarks lacking color transitions, such as the nasion and nose tip. Based on these presumptions, the DiffusionNet-based approach might achieve a better precision if texture data of the 3D photographs would be available to the networks.</p><p id="Par36">Another limitation of the proposed workflow arises from the utilization of HKS settings in the initial DiffusionNet, leading to occasional issues with random left&#x02013;right flipping in the predictions of symmetrical landmarks (e.g., exocanthions). To overcome this challenge, a solution was devised that involved detecting symmetrical landmarks within a single channel. Subsequently, both landmarks were distinguished from each other using a clustering algorithm, followed by a left&#x02013;right classification based on the midsagittal plane. Although a success rate of 98.6% was achieved using this solution, the workflow failed when the initial DiffusionNet was unable to predict one of the landmarks in the midsagittal plane (nasion, nose tip, or cheilion midpoint). Since this was mainly due to suboptimal quality of the 3D photo, it might be prevented by optimizing image acquisition. For optimal performance of the workflow, it is important to minimize gaps and restrict the depicted area in 3D photos to the face.</p><p id="Par37">Within this workflow, ten clinically relevant yet rather easily identifiable and reproducible landmarks, were incorporated to provide a solid gold standard. However, the constraint of utilizing only ten landmarks may impede the adaptability of the current method to a broader anatomical region or specific research objectives. The effectiveness of the developed workflow for the detection of more challenging landmarks should be investigated in future research. However, adapting the current workflow to accommodate additional landmarks or different anatomical regions may necessitate modifications.</p><p id="Par38">Due to its high precision and consistency, the developed automated landmarking method has the potential to be applied in various fields. Possible applications include objective follow-up and analysis of soft-tissue facial deformities, growth evaluation, facial asymmetry assessment, and integration in virtual planning software for 3D backward planning<sup><xref ref-type="bibr" rid="CR23">23</xref>,<xref ref-type="bibr" rid="CR24">24</xref></sup>. Considering that the proposed DiffusionNet-based approach only uses spatial features, it could be applied on 3D meshes of facial soft-tissue that are derived from imaging modalities lacking texture, such as CT, CBCT, or MRI. Nevertheless, further research is necessary to ascertain the applicability of this workflow to these imaging modalities. The fully-automated nature of the workflow also enables cephalometric analysis on large-scale datasets, presenting significant value for research purposes. The position-independency of the workflow might make it suitable for automated landmarking in 4D stereophotogrammetry and give rise to real-time cephalometric movement analysis for diagnostic purposes<sup><xref ref-type="bibr" rid="CR25">25</xref>,<xref ref-type="bibr" rid="CR26">26</xref></sup>.</p></sec><sec id="Sec22"><title>Conclusion</title><p id="Par39">In conclusion, a deep learning-based approach for automated landmark extraction from 3D facial photographs was developed and its precision was evaluated. The results showed high precision and consistency in landmark annotation, comparable to manual and semi-automatic annotation methods. Automated landmarking methods offer potential for analyzing large datasets, with applications in orthodontics, genetics, and craniofacial surgery and in emerging new imaging techniques like 4D stereophotogrammetry.</p></sec><sec sec-type="supplementary-material"><sec id="Sec23"><title>Supplementary Information</title><p>
<supplementary-material content-type="local-data" id="MOESM1"><media xlink:href="41598_2024_56956_MOESM1_ESM.docx"><caption><p>Supplementary Information.</p></caption></media></supplementary-material></p></sec></sec></body><back><fn-group><fn><p><bold>Publisher's note</bold></p><p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></fn></fn-group><sec><title>Supplementary Information</title><p>The online version contains supplementary material available at 10.1038/s41598-024-56956-9.</p></sec><ack><title>Acknowledgements</title><p>The authors wish to thank Lisette Masselink for performing the annotations and Luca Carotenuto for exploring the different approaches and network architectures that led to the conceptualization of the research.</p></ack><notes notes-type="author-contribution"><title>Author contributions</title><p>BB: Conceptualization, Methodology, Software, Validation, Writing&#x02014;Original Draft. FB: Conceptualization, Methodology, Software, Validation, Writing&#x02014;Original Draft. RS: Conceptualization, Writing&#x02014;Review and Editing. SV: Writing&#x02014;Review and Editing. TM: Supervision. GdJ: Conceptualization, Software, Formal Analysis, Supervision.</p></notes><notes notes-type="funding-information"><title>Funding</title><p>This research did not receive any specific Grant from funding agencies in the public, commercial, or not-for-profit sectors.</p></notes><notes notes-type="data-availability"><title>Data availability</title><p>Coded scripts are available within the following GitHub repository: <ext-link ext-link-type="uri" xlink:href="https://github.com/rumc3dlab/3dlandmarkdetection/">https://github.com/rumc3dlab/3dlandmarkdetection/</ext-link>.</p></notes><notes id="FPar1" notes-type="COI-statement"><title>Competing interests</title><p id="Par40">The authors declare no competing interests.</p></notes><ref-list id="Bib1"><title>References</title><ref id="CR1"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ludlow</surname><given-names>JB</given-names></name><name><surname>Ivanovic</surname><given-names>M</given-names></name></person-group><article-title>Comparative dosimetry of dental CBCT devices and 64-slice CT for oral and maxillofacial radiology</article-title><source>Oral Surg. Oral Med. Oral Pathol. Oral Radiol. Endod.</source><year>2008</year><volume>106</volume><fpage>106</fpage><lpage>114</lpage><pub-id pub-id-type="doi">10.1016/j.tripleo.2008.03.018</pub-id><?supplied-pmid 18504152?><pub-id pub-id-type="pmid">18504152</pub-id>
</element-citation></ref><ref id="CR2"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dindaro&#x0011f;lu</surname><given-names>F</given-names></name><name><surname>Kutlu</surname><given-names>P</given-names></name><name><surname>Duran</surname><given-names>GS</given-names></name><name><surname>G&#x000f6;rg&#x000fc;l&#x000fc;</surname><given-names>S</given-names></name></person-group><article-title>Accuracy and reliability of 3D stereophotogrammetry: A comparison to direct anthropometry and 2D photogrammetry</article-title><source>Angle Orthod.</source><year>2016</year><volume>86</volume><fpage>487</fpage><lpage>494</lpage><pub-id pub-id-type="doi">10.2319/041415-244.1</pub-id><?supplied-pmid 26267357?><pub-id pub-id-type="pmid">26267357</pub-id>
</element-citation></ref><ref id="CR3"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heike</surname><given-names>CL</given-names></name><name><surname>Upson</surname><given-names>K</given-names></name><name><surname>Stuhaug</surname><given-names>E</given-names></name><name><surname>Weinberg</surname><given-names>SM</given-names></name></person-group><article-title>3D digital stereophotogrammetry: A practical guide to facial image acquisition</article-title><source>Head Face Med.</source><year>2010</year><volume>6</volume><fpage>18</fpage><pub-id pub-id-type="doi">10.1186/1746-160X-6-18</pub-id><?supplied-pmid 20667081?><pub-id pub-id-type="pmid">20667081</pub-id>
</element-citation></ref><ref id="CR4"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>J</given-names></name><etal/></person-group><article-title>Reliability of stereophotogrammetry for area measurement in the periocular region</article-title><source>Aesthet. Plast. Surg.</source><year>2021</year><volume>45</volume><fpage>1601</fpage><lpage>1610</lpage><pub-id pub-id-type="doi">10.1007/s00266-020-02091-5</pub-id></element-citation></ref><ref id="CR5"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Serafin</surname><given-names>M</given-names></name><etal/></person-group><article-title>Accuracy of automated 3D cephalometric landmarks by deep learning algorithms: Systematic review and meta-analysis</article-title><source>La Radiol. Med.</source><year>2023</year><volume>128</volume><fpage>544</fpage><lpage>555</lpage><pub-id pub-id-type="doi">10.1007/s11547-023-01629-2</pub-id></element-citation></ref><ref id="CR6"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Park</surname><given-names>J-H</given-names></name><etal/></person-group><article-title>Automated identification of cephalometric landmarks: Part 1&#x02014;Comparisons between the latest deep-learning methods YOLOV3 and SSD</article-title><source>Angle Orthod.</source><year>2019</year><volume>89</volume><fpage>903</fpage><lpage>909</lpage><pub-id pub-id-type="doi">10.2319/022019-127.1</pub-id><?supplied-pmid 31282738?><pub-id pub-id-type="pmid">31282738</pub-id>
</element-citation></ref><ref id="CR7"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stewart</surname><given-names>RF</given-names></name><name><surname>Edgar</surname><given-names>H</given-names></name><name><surname>Tatlock</surname><given-names>C</given-names></name><name><surname>Kroth</surname><given-names>PJ</given-names></name></person-group><article-title>Developing a standardized cephalometric vocabulary: Choices and possible strategies</article-title><source>J. Dent. Educ.</source><year>2008</year><volume>72</volume><fpage>989</fpage><lpage>997</lpage><pub-id pub-id-type="doi">10.1002/j.0022-0337.2008.72.9.tb04573.x</pub-id><?supplied-pmid 18768441?><pub-id pub-id-type="pmid">18768441</pub-id>
</element-citation></ref><ref id="CR8"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guo</surname><given-names>Y</given-names></name><etal/></person-group><article-title>Deep learning for 3D point clouds: A survey</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2020</year><pub-id pub-id-type="doi">10.1109/TPAMI.2020.3005434</pub-id><?supplied-pmid 31442969?><pub-id pub-id-type="pmid">31442969</pub-id>
</element-citation></ref><ref id="CR9"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Manal</surname><given-names>ER</given-names></name><name><surname>Arsalane</surname><given-names>Z</given-names></name><name><surname>Aicha</surname><given-names>M</given-names></name></person-group><article-title>Survey on the approaches based geometric information for 3D face landmarks detection</article-title><source>IET Image Process.</source><year>2019</year><volume>13</volume><fpage>1225</fpage><lpage>1231</lpage><pub-id pub-id-type="doi">10.1049/iet-ipr.2018.6117</pub-id></element-citation></ref><ref id="CR10"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>White</surname><given-names>JD</given-names></name><etal/></person-group><article-title>MeshMonk: Open-source large-scale intensive 3D phenotyping</article-title><source>Sci. Rep.</source><year>2019</year><volume>9</volume><fpage>6085</fpage><pub-id pub-id-type="doi">10.1038/s41598-019-42533-y</pub-id><?supplied-pmid 30988365?><pub-id pub-id-type="pmid">30988365</pub-id>
</element-citation></ref><ref id="CR11"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baksi</surname><given-names>S</given-names></name><name><surname>Freezer</surname><given-names>S</given-names></name><name><surname>Matsumoto</surname><given-names>T</given-names></name><name><surname>Dreyer</surname><given-names>C</given-names></name></person-group><article-title>Accuracy of an automated method of 3D soft tissue landmark detection</article-title><source>Eur. J. Orthod.</source><year>2021</year><volume>43</volume><fpage>622</fpage><lpage>630</lpage><pub-id pub-id-type="doi">10.1093/ejo/cjaa069</pub-id><?supplied-pmid 33377968?><pub-id pub-id-type="pmid">33377968</pub-id>
</element-citation></ref><ref id="CR12"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guo</surname><given-names>J</given-names></name><name><surname>Mei</surname><given-names>X</given-names></name><name><surname>Tang</surname><given-names>K</given-names></name></person-group><article-title>Automatic landmark annotation and dense correspondence registration for 3D human facial images</article-title><source>BMC Bioinform.</source><year>2013</year><volume>14</volume><fpage>232</fpage><pub-id pub-id-type="doi">10.1186/1471-2105-14-232</pub-id></element-citation></ref><ref id="CR13"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dai</surname><given-names>H</given-names></name><name><surname>Pears</surname><given-names>N</given-names></name><name><surname>Smith</surname><given-names>W</given-names></name><name><surname>Duncan</surname><given-names>C</given-names></name></person-group><article-title>Statistical modeling of craniofacial shape and texture</article-title><source>Int. J. Comput. Vis.</source><year>2020</year><volume>128</volume><fpage>547</fpage><lpage>571</lpage><pub-id pub-id-type="doi">10.1007/s11263-019-01260-7</pub-id></element-citation></ref><ref id="CR14"><label>14.</label><mixed-citation publication-type="other">Pears, N. E., Duncan, C., Smith, W. A. P. &#x00026; Dai, H. The Headspace dataset (2018).</mixed-citation></ref><ref id="CR15"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sharp</surname><given-names>N</given-names></name><name><surname>Attaiki</surname><given-names>S</given-names></name><name><surname>Crane</surname><given-names>K</given-names></name><name><surname>Ovsjanikov</surname><given-names>M</given-names></name></person-group><article-title>DiffusionNet: Discretization agnostic learning on surfaces</article-title><source>ACM Trans. Graph.</source><year>2022</year><pub-id pub-id-type="doi">10.1145/3507905</pub-id></element-citation></ref><ref id="CR16"><label>16.</label><mixed-citation publication-type="other">Garland, M. &#x00026; Heckbert, P. S. in <italic>Proceedings of the 24th Annual Conference on Computer Graphics and Interactive Techniques,</italic> 209&#x02013;216 (ACM Press, 1997).</mixed-citation></ref><ref id="CR17"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hsu</surname><given-names>SS</given-names></name><etal/></person-group><article-title>Accuracy of a computer-aided surgical simulation protocol for orthognathic surgery: A prospective multicenter study</article-title><source>J. Oral Maxillofac. Surg.</source><year>2013</year><volume>71</volume><fpage>128</fpage><lpage>142</lpage><pub-id pub-id-type="doi">10.1016/j.joms.2012.03.027</pub-id><?supplied-pmid 22695016?><pub-id pub-id-type="pmid">22695016</pub-id>
</element-citation></ref><ref id="CR18"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schouman</surname><given-names>T</given-names></name><etal/></person-group><article-title>Accuracy evaluation of CAD/CAM generated splints in orthognathic surgery: A cadaveric study</article-title><source>Head Face Med.</source><year>2015</year><pub-id pub-id-type="doi">10.1186/s13005-015-0082-9</pub-id><?supplied-pmid 26209339?><pub-id pub-id-type="pmid">26209339</pub-id>
</element-citation></ref><ref id="CR19"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dot</surname><given-names>G</given-names></name><etal/></person-group><article-title>Automatic 3-dimensional cephalometric landmarking via deep learning</article-title><source>J. Dent. Res.</source><year>2022</year><volume>101</volume><fpage>1380</fpage><lpage>1387</lpage><pub-id pub-id-type="doi">10.1177/00220345221112333</pub-id><?supplied-pmid 35982646?><pub-id pub-id-type="pmid">35982646</pub-id>
</element-citation></ref><ref id="CR20"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kang</surname><given-names>SH</given-names></name><name><surname>Jeon</surname><given-names>K</given-names></name><name><surname>Kang</surname><given-names>S-H</given-names></name><name><surname>Lee</surname><given-names>S-H</given-names></name></person-group><article-title>3D cephalometric landmark detection by multiple stage deep reinforcement learning</article-title><source>Sci. Rep.</source><year>2021</year><volume>11</volume><fpage>17509</fpage><pub-id pub-id-type="doi">10.1038/s41598-021-97116-7</pub-id><?supplied-pmid 34471202?><pub-id pub-id-type="pmid">34471202</pub-id>
</element-citation></ref><ref id="CR21"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wu</surname><given-names>T-H</given-names></name><etal/></person-group><article-title>Two-stage mesh deep learning for automated tooth segmentation and landmark localization on 3D intraoral scans</article-title><source>IEEE Trans. Med. Imaging</source><year>2022</year><volume>41</volume><fpage>3158</fpage><lpage>3166</lpage><pub-id pub-id-type="doi">10.1109/TMI.2022.3180343</pub-id><?supplied-pmid 35666796?><pub-id pub-id-type="pmid">35666796</pub-id>
</element-citation></ref><ref id="CR22"><label>22.</label><mixed-citation publication-type="other">Lang, Y. <italic>et al. Medical Image Computing and Computer Assisted Intervention&#x02014;MICCAI 2022</italic>, 444&#x02013;452 (Springer, 2022).</mixed-citation></ref><ref id="CR23"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Memon</surname><given-names>AR</given-names></name><name><surname>Li</surname><given-names>J</given-names></name><name><surname>Egger</surname><given-names>J</given-names></name><name><surname>Chen</surname><given-names>X</given-names></name></person-group><article-title>A review on patient-specific facial and cranial implant design using Artificial Intelligence (AI) techniques</article-title><source>Expert Rev. Med. Devices</source><year>2021</year><volume>18</volume><fpage>985</fpage><lpage>994</lpage><pub-id pub-id-type="doi">10.1080/17434440.2021.1969914</pub-id><?supplied-pmid 34404280?><pub-id pub-id-type="pmid">34404280</pub-id>
</element-citation></ref><ref id="CR24"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tel</surname><given-names>A</given-names></name><etal/></person-group><article-title>Systematic review of the software used for virtual surgical planning in craniomaxillofacial surgery over the last decade</article-title><source>Int. J. Oral Maxillofac. Surg.</source><year>2023</year><volume>52</volume><fpage>775</fpage><lpage>786</lpage><pub-id pub-id-type="doi">10.1016/j.ijom.2022.11.011</pub-id><?supplied-pmid 36481124?><pub-id pub-id-type="pmid">36481124</pub-id>
</element-citation></ref><ref id="CR25"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harkel</surname><given-names>TCT</given-names></name><etal/></person-group><article-title>Reliability and agreement of 3D anthropometric measurements in facial palsy patients using a low-cost 4D imaging system</article-title><source>IEEE Trans. Neural Syst. Rehabil. Eng.</source><year>2020</year><volume>28</volume><fpage>1817</fpage><lpage>1824</lpage><pub-id pub-id-type="doi">10.1109/TNSRE.2020.3007532</pub-id><?supplied-pmid 32746313?><pub-id pub-id-type="pmid">32746313</pub-id>
</element-citation></ref><ref id="CR26"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shujaat</surname><given-names>S</given-names></name><etal/></person-group><article-title>The clinical application of three-dimensional motion capture (4D): A novel approach to quantify the dynamics of facial animations</article-title><source>Int. J. Oral Maxillofac. Surg.</source><year>2014</year><volume>43</volume><fpage>907</fpage><lpage>916</lpage><pub-id pub-id-type="doi">10.1016/j.ijom.2014.01.010</pub-id><?supplied-pmid 24583138?><pub-id pub-id-type="pmid">24583138</pub-id>
</element-citation></ref></ref-list></back></article>