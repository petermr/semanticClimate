<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd"> 
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><?properties open_access?><?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?><?DTDIdentifier.IdentifierType public?><?SourceDTD.DTDName A++V2.4.dtd?><?SourceDTD.Version 2.4?><?ConverterInfo.XSLTName springer2nlmx2.xsl?><?ConverterInfo.Version 1?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sci Rep</journal-id><journal-id journal-id-type="iso-abbrev">Sci Rep</journal-id><journal-title-group><journal-title>Scientific Reports</journal-title></journal-title-group><issn pub-type="epub">2045-2322</issn><publisher><publisher-name>Nature Publishing Group UK</publisher-name><publisher-loc>London</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmcid">10958035</article-id><article-id pub-id-type="publisher-id">55612</article-id><article-id pub-id-type="doi">10.1038/s41598-024-55612-6</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>BSI-MVS: multi-view stereo network with bidirectional semantic information</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Jia</surname><given-names>Ruiming</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author"><name><surname>Yu</surname><given-names>Jun</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author"><name><surname>Hu</surname><given-names>Zhenghui</given-names></name><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Yuan</surname><given-names>Fei</given-names></name><address><email>yuanfei@iie.ac.cn</email></address><xref ref-type="aff" rid="Aff3">3</xref></contrib><aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/01nky7652</institution-id><institution-id institution-id-type="GRID">grid.440852.f</institution-id><institution-id institution-id-type="ISNI">0000 0004 1789 9542</institution-id><institution>School of Information Science and Technology, </institution><institution>North China University of Technology, </institution></institution-wrap>Beijing, 100144 China </aff><aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/00wk2mp56</institution-id><institution-id institution-id-type="GRID">grid.64939.31</institution-id><institution-id institution-id-type="ISNI">0000 0000 9999 1211</institution-id><institution>Hangzhou Innovation Institute, </institution><institution>Beihang University, </institution></institution-wrap>Hangzhou, 310051 China </aff><aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="GRID">grid.9227.e</institution-id><institution-id institution-id-type="ISNI">0000000119573309</institution-id><institution>Institute of Information Engineering, </institution><institution>Chinese Academy of Sciences, </institution></institution-wrap>Beijing, 10085 China </aff></contrib-group><pub-date pub-type="epub"><day>21</day><month>3</month><year>2024</year></pub-date><pub-date pub-type="pmc-release"><day>21</day><month>3</month><year>2024</year></pub-date><pub-date pub-type="collection"><year>2024</year></pub-date><volume>14</volume><elocation-id>6766</elocation-id><history><date date-type="received"><day>25</day><month>7</month><year>2023</year></date><date date-type="accepted"><day>26</day><month>2</month><year>2024</year></date></history><permissions><copyright-statement>&#x000a9; The Author(s) 2024</copyright-statement><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p></license></permissions><abstract id="Abs1"><p id="Par1">The basic principle of multi-view stereo (MVS) is to perform 3D reconstruction by extracting depth information from multiple views. Most current SOTA MVS networks are based on Vision Transformer, which usually means expensive computational complexity. To reduce computational complexity and improve depth map accuracy, we propose a MVS network with Bidirectional Semantic Information (BSI-MVS). Firstly, we design a Multi-Level Spatial Pyramid module to generate multiple layers of feature map for extracting multi-scale information. Then we propose a 2D Bidirectional-LSTM module to capture bidirectional semantic information at different time steps in the horizontal and vertical directions, which contains abundant depth information. Finally, cost volumes are built based on various levels of feature maps to optimize the final depth map. We experiment on the DTU and BlendedMVS datasets. The result shows that our network, in terms of overall metrics, surpasses TransMVSNet, CasMVSNet, CVP-MVSNet, and AACVP-MVSNet respectively by 17.84%, 36.42%, 14.96%, and 4.86%, which also shows a noticeable performance enhancement in objective metrics and visualizations.</p></abstract><kwd-group xml:lang="en"><title>Keywords</title><kwd>Multi-view stereo</kwd><kwd>Bidirectional-LSTM</kwd><kwd>3D reconstruction</kwd><kwd>Transformer</kwd></kwd-group><kwd-group kwd-group-type="npg-subject"><title>Subject terms</title><kwd>Electrical and electronic engineering</kwd><kwd>Computer science</kwd><kwd>Information technology</kwd></kwd-group><funding-group><award-group><funding-source><institution>Zhenghui Hu</institution></funding-source><award-id>2020-Y3-A-014</award-id><principal-award-recipient><name><surname>Hu</surname><given-names>Zhenghui</given-names></name></principal-award-recipient></award-group></funding-group><funding-group><award-group><funding-source><institution>Anhua Zheng</institution></funding-source><award-id>2020YFC0832503</award-id><principal-award-recipient><name><surname>Yuan</surname><given-names>Fei</given-names></name></principal-award-recipient></award-group></funding-group><custom-meta-group><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#x000a9; Springer Nature Limited 2024</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1"><title>Introduction</title><p id="Par2">MVS technology facilitates a profound interaction between the digital and real worlds through 3D reconstruction. Traditional methods for 3D reconstruction<sup><xref ref-type="bibr" rid="CR1">1</xref>,<xref ref-type="bibr" rid="CR2">2</xref></sup> based on geometric shapes can be categorized into various approaches depending on the inclusion of prior conditions. These approaches include contour-based methods, focused area methods, motion-based methods, and others. Traditional 3D reconstruction methods based on visual geometry primarily utilize the 3D geometric information present in 2D images as prior knowledge. This technique significantly restores the 3D scene without additional conditions. By leveraging the inherent geometric cues captured in the 2D images, these methods can deduce or derive the intrinsic 3D structure of the scene. However, traditional MVS methods, based on an ideal Lambertian scene and strict geometric relationships, may encounter challenges in reconstructing complex geometries or texture-free areas, leading to holes and texture blending issues.</p><p id="Par3">With increasing computing power and the continuous advancement of deep learning, there has been a surge in research utilizing deep learning techniques for MVS tasks. MVS essentially uses prior geometric knowledge to recover spatial 3D shapes. Deep learning based methods do not discard this principle; instead, they employ neural networks to facilitate the process of geometric reconstruction. These methods learn the matching relationships between images from different viewpoints, enabling more robust 3D reconstruction. MVSNet<sup><xref ref-type="bibr" rid="CR3">3</xref></sup>, as a learning-based method, introduces a breakthrough approach by transforming the 3D reconstruction issue into a deep map inference issue. This innovative methodology can be roughly divided into four key steps: image feature extraction, cost volume construction, cost volume regularization, and depth estimation<sup><xref ref-type="bibr" rid="CR4">4</xref></sup>. By adopting this framework, MVSNet pioneers the use of deep learning techniques to achieve multi-view 3D reconstruction, paving the way for significant advancements in the field.</p><p id="Par4">For the past few years, a new approach to 3D reconstruction incorporating the Transformer<sup><xref ref-type="bibr" rid="CR5">5</xref></sup> model, originally used for natural language processing, has emerged on the basis of deep learning. This approach employs the Transformer model for the feature extraction phase of 3D reconstruction, introducing a new perspective and potential improvements to the field. Thanks to the attention mechanism and the contextual aggregation of location encoding, the Transformer model has the ability to capture global information and semantic details of relevant locations. However, integrating the Transformer model into the 3D reconstruction field presents a significant challenge for the underlying hardware infrastructure. The Vision Transformer<sup><xref ref-type="bibr" rid="CR6">6</xref></sup> divides the input image into a series of patches and performs operations on them, resulting in a substantial increase in computational complexity. This can make it difficult to handle high-resolution images and slow down the convergence of the entire network.</p><p id="Par5">Additionally, there is a limitation on the sequence length that the Vision Transformer can effectively handle. Longer sequences may result in information loss during the processing stage. This constraint needs to be considered when applying the Vision Transformer to 3D reconstruction tasks to ensure that important details and contextual information are adequately preserved in the reconstructed output.</p><p id="Par6">The global attention mechanism in the Vision Transformer model can make the network more sensitive to noise in the input sequence during the 3D reconstruction process. This sensitivity to noise can potentially impact the quality and accuracy of the reconstructed output. Recently, there has been a resurgence in convolutional neural networks (CNNs) for sequence modeling. A notable approach involves utilizing BiLSTM<sup><xref ref-type="bibr" rid="CR7">7</xref></sup>, like the Sequencer<sup><xref ref-type="bibr" rid="CR8">8</xref></sup> model, to address the task. Unlike the attention mechanism employed in Transformers, Sequencer utilizes BiLSTMs to process time sequences in width and height directions. Additionally, feature fusion is performed using convolutions, enabling better attention to temporal information. This alternative approach offers an effective solution for modeling sequences, considering both the spatial and temporal aspects, and has shown promising results in various applications.</p><p id="Par7">We propose a MVS network with Bidirectional Semantic Information for 3D reconstruction called BSI-MVS. Our network utilizes a BiLSTM approach to spatially and temporally combine information for improved model generalization and enhanced 3D reconstruction accuracy. In BSI-MVS, we adopt a spatiotemporally combined approach incorporating a Multi-Level Spatial Pyramid module. This module enables the construction of a spatial pyramid at multiple scales, enabling the entire network to capture and encompass a diverse set of spatial semantic information. BSI-MVS can capture details and contextual information across different spatial resolutions by integrating information from multi-scales. This spatial pyramid construction enhances the network's ability to handle variations in spatial structures and improves the overall performance and accuracy of the MVS network. The coarse-resolution feature maps obtained from the Multi-Level Spatial Pyramid are individually processed in the horizontal and vertical directions using BiLSTM to address the problem of long-term dependency.</p><p id="Par8">The following are the essential contributions of this paper:<list list-type="bullet"><list-item><p id="Par9">We introduce an MVS network with Bidirectional Semantic Information for reconstructing low-resolution images in the MVS task to solve the calculation accuracy problem.</p></list-item><list-item><p id="Par10">We propose a Multi-Level Spatial Pyramid module (MLSP) and a Bidirectional-LSTM module (BiLSTM) for the feature extraction stage. For low-resolution images, The MLSP can enhance the robustness of BSI-MVS by constructing multi-scale information. Then, using the BiLSTM module at each level of the feature pyramid allows for enhancing the understanding of contextual semantic information.</p></list-item><list-item><p id="Par11">We have benchmarked our network against pre-existing methods on a DTU dataset<sup><xref ref-type="bibr" rid="CR9">9</xref></sup> widely used by the MVS task. In the end, our proposed network achieved superior results. Furthermore, we evaluated the network's visualization capabilities on the BlendedMVS dataset<sup><xref ref-type="bibr" rid="CR10">10</xref></sup>, which showed improved visualization results compared to other approaches.</p></list-item></list></p></sec><sec id="Sec2"><title>Related work</title><sec id="Sec3"><title>Vision transformer-based MVS</title><p id="Par12">Inspired by human visual perception, Vision Transformer mechanisms enable efficient image scanning to extract relevant information about the target of interest. As the Vision Transformer becomes more widely used in computer vision, the Vision Transformer is also used in 3D reconstruction for better feature extraction. TransMVSNet<sup><xref ref-type="bibr" rid="CR11">11</xref></sup> is the first network to use the Vision Transformer for the MVS task, which uses the Vision Transformer for global contextual perception within and between images. AACVP-MVSNet<sup><xref ref-type="bibr" rid="CR12">12</xref></sup> introduces an attention layer to improve feature extraction and uses a similarity metric to aggregate cost volume. Liao, Jinli, et al.<sup><xref ref-type="bibr" rid="CR13">13</xref></sup> proposed to use an improved window attention mechanism for the global feature aggregation and local feature matching phases of 3D reconstruction with the aim of reducing redundancy and increasing smoothness. SENet (Squeeze-and-Excitation Network) is a novel approach that leverages convolutional neural networks (CNNs) and attention mechanisms to model channel relationships. The attention mechanism in SENet is designed to learn channel dependencies in order to highlight valuable information within each channel while suppressing irrelevant or redundant features. By effectively capturing and recalibrating channel-wise feature responses, SENet enhances the network's ability to focus on informative features, leading to improved performance and better utilization of channel information within CNNs.</p><p id="Par13">While the Vision Transformer-based MVSNet has shown improvements in the quality of 3D reconstruction, it is crucial to consider the following limitations. The global self-attentiveness mechanism employed in the Vision Transformer introduces challenges such as increased network parameters and computational complexity. This network can result in higher resource requirements and longer inference times. Moreover, the inherently global nature of self-attention may make the network more sensitive to environmental disturbances and variations, potentially impacting the overall stability of the reconstruction process. These trade-offs between improved reconstruction quality and increased computational burden need to be carefully considered when applying Vision Transformer-based approaches in the context of MVS.</p></sec><sec id="Sec4"><title>CNN-based MVS</title><p id="Par14">MVSNet, as a CNN-based method<sup><xref ref-type="bibr" rid="CR14">14</xref></sup>, introduced the concept of transforming the 3D reconstruction problem into a depth map inference problem. This novel approach paved the way for leveraging deep learning techniques in multi-view 3D reconstruction. Cas-MVSNet<sup><xref ref-type="bibr" rid="CR15">15</xref></sup> utilizes a cascading architecture with multiple sub-networks for 3D reconstruction. CVP-MVSNet<sup><xref ref-type="bibr" rid="CR16">16</xref></sup> is a system that utilizes a compact and lightweight network to construct a pyramid of cost volumes. This approach allows for achieving enhanced resolution in 3D reconstruction. By leveraging this technique, CVP-MVSNet can generate more detailed and accurate reconstructions. GeoMVSNet<sup><xref ref-type="bibr" rid="CR17">17</xref></sup> uses the geometric prior to guide the fusion process for better feature fusion. In the MVS task, it is common to apply cost volume regularization<sup><xref ref-type="bibr" rid="CR18">18</xref></sup> to smooth the features. However, this regularization technique alone cannot completely address the issue of ambiguous feature matching caused by reflections or texture-free regions with unreliable 2D image features. These challenges can still cause imprecision in the reconstructed 3D models. Hence, it is crucial to focus on learning influential and representative characteristics during the feature extraction stage to enhance the generalizability of MVS systems. By obtaining high-quality features, the MVS algorithm can better handle challenging scenarios, such as reflections<sup><xref ref-type="bibr" rid="CR19">19</xref></sup> and texture-free regions, leading to more reliable and accurate 3D reconstructions.</p><p id="Par15">Experimental results show that a well-designed CNN can achieve results beyond the Vision Transformer. ConvNext<sup><xref ref-type="bibr" rid="CR20">20</xref></sup> is a novel architecture that builds upon the SwimTransformer<sup><xref ref-type="bibr" rid="CR21">21</xref></sup> model. It incorporates convolutional layers to achieve an attention-like mechanism, surpassing the performance of the original SwimTransformer model. By leveraging convolutional operations, ConvNext enhances the model's ability to capture relevant features and improve its overall performance in various tasks, while FLOPS are significantly lower than SwimTransformer. InceptionNext<sup><xref ref-type="bibr" rid="CR22">22</xref></sup> achieves superior performance using separable convolution compared to SwimTransformer with significant reductions in both the number of parameters and FLOPS. Sequencer utilizes LSTM instead of an attention mechanism for natural language processing. The network leverages bidirectional long-short-distance memory to perform classification tasks by serializing feature maps. And BH-RMVSNet<sup><xref ref-type="bibr" rid="CR23">23</xref></sup> uses bidirectional hybrid LSTM for the cost volume regularisation in MVSNet to improve memory efficiency. Hence, replacing the Vision Transformer for feature extraction in 3D reconstruction with convolutional neural networks can improve network performance while reducing the number of parameters.</p></sec></sec><sec id="Sec5"><title>Method</title><p id="Par16">To optimize the utilization of visual data and enhance the fidelity of the reconstructed 3D models, we have proposed an innovative method called BSI-MVS. Figure&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref> depicts the detailed process of our proposed method, which is divided into three main stages: spatiotemporal feature extraction, cost volume regularization, and depth estimation. The input to our network is a reference image <inline-formula id="IEq1"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$I_{0} \in {\mathbb{R}}^{H \times W}$$\end{document}</tex-math><mml:math id="M2"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_55612_Article_IEq1.gif"/></alternatives></inline-formula>, where H and W are the height and width of the image, N source images <inline-formula id="IEq2"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\left\{ {I_{{{\text{i}} = 1}}^{N} } \right\}$$\end{document}</tex-math><mml:math id="M4"><mml:mfenced close="}" open="{"><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mrow><mml:mtext>i</mml:mtext><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow><mml:mi>N</mml:mi></mml:msubsup></mml:mfenced></mml:math><inline-graphic xlink:href="41598_2024_55612_Article_IEq2.gif"/></alternatives></inline-formula>, and the camera's internal and external reference matrices for the corresponding viewpoint <inline-formula id="IEq3"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\left\{ {K_{i} ,R_{i} ,{\text{t}}_{i} } \right\}_{i = 0}^{N}$$\end{document}</tex-math><mml:math id="M6"><mml:msubsup><mml:mfenced close="}" open="{"><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mtext>t</mml:mtext><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup></mml:math><inline-graphic xlink:href="41598_2024_55612_Article_IEq3.gif"/></alternatives></inline-formula>. In our proposed network, the initial stage involves passing all input images through a Multi-level Pyramid module with weight sharing. In the second step of our proposed network, the low-resolution feature maps obtained from the previous step are inputted into the BiLSTM module. By leveraging the capabilities of the BiLSTM module, the network can better understand and encode relevant contextual information in the feature maps. Finally, similar to standard MVS networks, our approach performs cost volume construction, regularization, and in-depth reasoning.<fig id="Fig1"><label>Figure 1</label><caption><p>The network structure of BSI-MVS.</p></caption><graphic xlink:href="41598_2024_55612_Fig1_HTML" id="MO1"/></fig></p><sec id="Sec6"><title>Multi-level spatial pyramid (MLSP)</title><p id="Par17">Our proposed MLSP module facilitates the interaction of data across diverse spatial levels, enabling the fusion of information at various resolutions. It achieves this by downsampling the input image and fusing the resulting feature maps from various layers. It allows the merging of different details from different levels of spatial detail, enhancing the network's ability to capture comprehensive and contextually rich representations of the input data. By enabling information exchange at multiple spatial scales, the MLSP module contributes to the network's capacity for more robust and effective feature extraction. The MLSP module utilizes distinct branches to aggregate features from various layers, capturing different perceptual fields and information at multiple scales (Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref>). In the first step of our proposed MLSP module, we generate two images of different resolutions by employing bilinear interpolation on the input image. These images are then processed by a feature fusion layer with shared weights, extracting features and generating a feature map with a channel dimension of 16 at various scales. This process allows for the construction of a spatial pyramid, incorporating information from different resolutions into the subsequent stages of the network. In the second step, feature maps from various scales in the spatial pyramid are concatenated to achieve multi-scale information fusion. Finally, the feature maps in the constructed multi-level pyramid are enhanced and aggregated using the Mixer Layer, which optimally utilizes them within the BiLSTM module.<fig id="Fig2"><label>Figure 2</label><caption><p>The structure of MLSP Module.</p></caption><graphic xlink:href="41598_2024_55612_Fig2_HTML" id="MO2"/></fig></p></sec><sec id="Sec7"><title>BiLSTM for coarse feature fusion</title><p id="Par18">Integrating the Transformer into 3D reconstruction has led to the inclusion of a global attention mechanism in the feature extraction module of many MVS networks. This mechanism enables improved consideration of global information. However, it also results in increased computational complexity and heightened sensitivity to noise, which can adversely affect the quality of 3D reconstruction. As a solution, we suggest the utilization of BiLSTM in the feature fusion module instead of the Transformer. This approach enables us to prioritize long-range semantic features, increase resistance to interference, improve the capacity for generalization, and ultimately elevate the quality of 3D reconstruction. In the BiLSTM module, we implemented long-range dependencies using LSTM in both vertical and horizontal directions, similar to the Vision Transformer, while also reducing the number of parameters.</p><sec id="Sec8"><title>BiLSTM module</title><p id="Par19">We propose to apply BiLSTM to the low-resolution feature map fusion module. The BiLSTM module is mainly composed of the Layer-norm<sup><xref ref-type="bibr" rid="CR24">24</xref></sup> block, BiLSTM block, Depth Fusion block, and Residual connection, as shown in Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3</xref>. For the sake of future reference, we define the input low-resolution feature map as <inline-formula id="IEq4"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$p_{i} \in {\mathbb{R}}^{C \times H \times W}$$\end{document}</tex-math><mml:math id="M8"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>H</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_55612_Article_IEq4.gif"/></alternatives></inline-formula>. To align with the Layer-norm block used in the Transformer, the input to the BiLSTM block requires restructuring, denoted as <inline-formula id="IEq5"><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\left\{ {H \times W \times C} \right\}$$\end{document}</tex-math><mml:math id="M10"><mml:mfenced close="}" open="{"><mml:mrow><mml:mi>H</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>W</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:mfenced></mml:math><inline-graphic xlink:href="41598_2024_55612_Article_IEq5.gif"/></alternatives></inline-formula>. Initially, the permute function is employed to modify the channel structure of the input feature map, resulting in the restructured representation denoted as <inline-formula id="IEq6"><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$P \in {\mathbb{R}}^{H \times W \times C}$$\end{document}</tex-math><mml:math id="M12"><mml:mrow><mml:mi>P</mml:mi><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>W</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_55612_Article_IEq6.gif"/></alternatives></inline-formula>. Subsequently, the modified feature maps were passed into the BiLSTM block, where two residual joins were executed. Finally, the output is processed by the Layer-norm block, and then the permute function is used to restore it to its original channel structure, which is denoted as <inline-formula id="IEq7"><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\left\{ {C \times H \times W} \right\}$$\end{document}</tex-math><mml:math id="M14"><mml:mfenced close="}" open="{"><mml:mrow><mml:mi>C</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>H</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:mfenced></mml:math><inline-graphic xlink:href="41598_2024_55612_Article_IEq7.gif"/></alternatives></inline-formula>.<fig id="Fig3"><label>Figure 3</label><caption><p>The structure of BiLSTM Module.</p></caption><graphic xlink:href="41598_2024_55612_Fig3_HTML" id="MO3"/></fig></p></sec><sec id="Sec9"><title>BiLSTM block</title><p id="Par20">The BiLSTM block consists of two separate BiLSTM layers that process feature sequences in the row and column directions, as illustrated in Fig.&#x000a0;<xref rid="Fig4" ref-type="fig">4</xref> below. The combination of the two individual LSTMs<sup><xref ref-type="bibr" rid="CR25">25</xref></sup> forms a BiLSTM layer. LSTMs belong to a distinct category of recurrent neural networks (RNNs) that exhibit proficiency in capturing long-range relationships and mitigating the challenge of vanishing or exploding gradients related to distant connections.<fig id="Fig4"><label>Figure 4</label><caption><p>The structure of BiLSTM Block.</p></caption><graphic xlink:href="41598_2024_55612_Fig4_HTML" id="MO4"/></fig></p><p id="Par21">Taking the BiLSTM layer in the vertical direction as an example, if the input sequence of the BiLSTM is defined as <inline-formula id="IEq8"><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\overrightarrow {H}$$\end{document}</tex-math><mml:math id="M16"><mml:mover accent="true"><mml:mi>H</mml:mi><mml:mo stretchy="false">&#x02192;</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="41598_2024_55612_Article_IEq8.gif"/></alternatives></inline-formula>, then <inline-formula id="IEq9"><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\overleftarrow {H}$$\end{document}</tex-math><mml:math id="M18"><mml:mover accent="true"><mml:mi>H</mml:mi><mml:mo stretchy="false">&#x02190;</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="41598_2024_55612_Article_IEq9.gif"/></alternatives></inline-formula> is an inverted rearrangement sequence of <inline-formula id="IEq10"><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\overrightarrow {H}$$\end{document}</tex-math><mml:math id="M20"><mml:mover accent="true"><mml:mi>H</mml:mi><mml:mo stretchy="false">&#x02192;</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="41598_2024_55612_Article_IEq10.gif"/></alternatives></inline-formula>. The <inline-formula id="IEq11"><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\overrightarrow {H}$$\end{document}</tex-math><mml:math id="M22"><mml:mover accent="true"><mml:mi>H</mml:mi><mml:mo stretchy="false">&#x02192;</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="41598_2024_55612_Article_IEq11.gif"/></alternatives></inline-formula> sequence is input into one of the ordinary LSTMs, and the corresponding LSTM is named Forward LSTM. The sequence, <inline-formula id="IEq12"><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\overleftarrow {H}$$\end{document}</tex-math><mml:math id="M24"><mml:mover accent="true"><mml:mi>H</mml:mi><mml:mo stretchy="false">&#x02190;</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="41598_2024_55612_Article_IEq12.gif"/></alternatives></inline-formula>, is input into another standard LSTM, which is commonly known as the Backward LSTM. The Backward LSTM handles the input sequence in reverse order. The resulting output, represented as <inline-formula id="IEq13"><alternatives><tex-math id="M25">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\overrightarrow {{Y_{for} }}$$\end{document}</tex-math><mml:math id="M26"><mml:mover accent="true"><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi mathvariant="italic">for</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">&#x02192;</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="41598_2024_55612_Article_IEq13.gif"/></alternatives></inline-formula>, from the Forward LSTM, and the output, denoted as <inline-formula id="IEq14"><alternatives><tex-math id="M27">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\overleftarrow {{Y_{back} }}$$\end{document}</tex-math><mml:math id="M28"><mml:mover accent="true"><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi mathvariant="italic">back</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">&#x02190;</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="41598_2024_55612_Article_IEq14.gif"/></alternatives></inline-formula>, from the Backward LSTM, are subsequently concatenated together. Both <inline-formula id="IEq15"><alternatives><tex-math id="M29">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\overrightarrow {{Y_{for} }}$$\end{document}</tex-math><mml:math id="M30"><mml:mover accent="true"><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi mathvariant="italic">for</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">&#x02192;</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="41598_2024_55612_Article_IEq15.gif"/></alternatives></inline-formula> and <inline-formula id="IEq16"><alternatives><tex-math id="M31">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\overleftarrow {{Y_{back} }}$$\end{document}</tex-math><mml:math id="M32"><mml:mover accent="true"><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi mathvariant="italic">back</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">&#x02190;</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="41598_2024_55612_Article_IEq16.gif"/></alternatives></inline-formula> have a channel dimension of D. The final concatenated output, <inline-formula id="IEq17"><alternatives><tex-math id="M33">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$Y$$\end{document}</tex-math><mml:math id="M34"><mml:mi>Y</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_55612_Article_IEq17.gif"/></alternatives></inline-formula>, has a channel dimension of 2D. This splicing operation combines the information from both the forward and backward directions, enabling the model to capture bidirectional dependencies and leverage them for improved performance. The resulting spliced output, <inline-formula id="IEq18"><alternatives><tex-math id="M35">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$Y$$\end{document}</tex-math><mml:math id="M36"><mml:mi>Y</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_55612_Article_IEq18.gif"/></alternatives></inline-formula>, with an increased channel dimension, provides richer and more comprehensive information for subsequent stages of the model.<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M37">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \overrightarrow {{Y_{for} }} = LSTM_{forward} (\overrightarrow {H} ) $$\end{document}</tex-math><mml:math id="M38" display="block"><mml:mrow><mml:mover accent="true"><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi mathvariant="italic">for</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">&#x02192;</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mi>L</mml:mi><mml:mi>S</mml:mi><mml:mi>T</mml:mi><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi mathvariant="italic">forward</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi>H</mml:mi><mml:mo stretchy="false">&#x02192;</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math><graphic xlink:href="41598_2024_55612_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M39">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \overleftarrow {{Y_{back} }} = LSTM_{backward} (\overleftarrow {H} ) $$\end{document}</tex-math><mml:math id="M40" display="block"><mml:mrow><mml:mover accent="true"><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi mathvariant="italic">back</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">&#x02190;</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mi>L</mml:mi><mml:mi>S</mml:mi><mml:mi>T</mml:mi><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi mathvariant="italic">backward</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi>H</mml:mi><mml:mo stretchy="false">&#x02190;</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math><graphic xlink:href="41598_2024_55612_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="M41">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ Y = concatenate(\overrightarrow {{Y_{for} }} ,\overleftarrow {{Y_{back} )}} $$\end{document}</tex-math><mml:math id="M42" display="block"><mml:mrow><mml:mi>Y</mml:mi><mml:mo>=</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi mathvariant="italic">for</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">&#x02192;</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi mathvariant="italic">back</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">&#x02190;</mml:mo></mml:mover></mml:mrow></mml:math><graphic xlink:href="41598_2024_55612_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula></p><p id="Par22">In the preliminary step, the input characteristic map is specified as <inline-formula id="IEq19"><alternatives><tex-math id="M43">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$P \in {\mathbb{R}}^{H \times W \times C}$$\end{document}</tex-math><mml:math id="M44"><mml:mrow><mml:mi>P</mml:mi><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>W</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_55612_Article_IEq19.gif"/></alternatives></inline-formula>. In utilizing the BiLSTM network, the characteristic map must be serialized. Serialization involves converting the multidimensional characteristic map into a sequential representation. As shown in Fig.&#x000a0;<xref rid="Fig4" ref-type="fig">4</xref> below for the Vertical BiLSTM module and the Horizontal BiLSTM module, the number of tokens in the height and width directions are W and H, respectively.</p><p id="Par23">During the subsequent step, the feature map sequence is separately fed into the BiLSTM block along the width and height directions. Where <inline-formula id="IEq20"><alternatives><tex-math id="M45">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\left\{ {P_{:,w,:} \in {\mathbf{\mathbb{R}}}^{H \times C} } \right\}_{w = 1}^{W}$$\end{document}</tex-math><mml:math id="M46"><mml:msubsup><mml:mfenced close="}" open="{"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mi>w</mml:mi><mml:mo>,</mml:mo><mml:mo>:</mml:mo></mml:mrow></mml:msub><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfenced><mml:mrow><mml:mi>w</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>W</mml:mi></mml:msubsup></mml:math><inline-graphic xlink:href="41598_2024_55612_Article_IEq20.gif"/></alternatives></inline-formula> represents a group of sequences entered horizontally, W represents the aggregate count of sequences entered horizontally, and C represents the number of channels. All input sequences <inline-formula id="IEq21"><alternatives><tex-math id="M47">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$P_{:,w,:}$$\end{document}</tex-math><mml:math id="M48"><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mi>w</mml:mi><mml:mo>,</mml:mo><mml:mo>:</mml:mo></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_55612_Article_IEq21.gif"/></alternatives></inline-formula> are passed through the weight-sharing Vertical BiLSTM block:<disp-formula id="Equ4"><label>4</label><alternatives><tex-math id="M49">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ Y_{:,w,:}^{ver} = BiLSTM\left( {P_{:,w,:} } \right) $$\end{document}</tex-math><mml:math id="M50" display="block"><mml:mrow><mml:msubsup><mml:mi>Y</mml:mi><mml:mrow><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mi>w</mml:mi><mml:mo>,</mml:mo><mml:mo>:</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="italic">ver</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>B</mml:mi><mml:mi>i</mml:mi><mml:mi>L</mml:mi><mml:mi>S</mml:mi><mml:mi>T</mml:mi><mml:mi>M</mml:mi><mml:mfenced close=")" open="("><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mi>w</mml:mi><mml:mo>,</mml:mo><mml:mo>:</mml:mo></mml:mrow></mml:msub></mml:mfenced></mml:mrow></mml:math><graphic xlink:href="41598_2024_55612_Article_Equ4.gif" position="anchor"/></alternatives></disp-formula></p><p id="Par24">Similarly, <inline-formula id="IEq22"><alternatives><tex-math id="M51">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\left\{ {P_{h,:,:} \in {\mathbf{\mathbb{R}}}^{W \times C} } \right\}_{h = 1}^{H}$$\end{document}</tex-math><mml:math id="M52"><mml:msubsup><mml:mfenced close="}" open="{"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mo>,</mml:mo><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mo>:</mml:mo></mml:mrow></mml:msub><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>W</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfenced><mml:mrow><mml:mi>h</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>H</mml:mi></mml:msubsup></mml:math><inline-graphic xlink:href="41598_2024_55612_Article_IEq22.gif"/></alternatives></inline-formula> represents the set of vertically-oriented input sequences, H represents the total count of vertically-oriented input sequences, and C represents the number of channels. These inputs are all fed into the weight-sharing Horizontal BiLSTM block:<disp-formula id="Equ5"><label>5</label><alternatives><tex-math id="M53">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ Y_{h,:,:}^{hor} = BiLSTM\left( {P_{h,:,:} } \right) $$\end{document}</tex-math><mml:math id="M54" display="block"><mml:mrow><mml:msubsup><mml:mi>Y</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mo>,</mml:mo><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mo>:</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="italic">hor</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>B</mml:mi><mml:mi>i</mml:mi><mml:mi>L</mml:mi><mml:mi>S</mml:mi><mml:mi>T</mml:mi><mml:mi>M</mml:mi><mml:mfenced close=")" open="("><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mo>,</mml:mo><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mo>:</mml:mo></mml:mrow></mml:msub></mml:mfenced></mml:mrow></mml:math><graphic xlink:href="41598_2024_55612_Article_Equ5.gif" position="anchor"/></alternatives></disp-formula></p><p id="Par25">In the third step, <inline-formula id="IEq23"><alternatives><tex-math id="M55">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$Y^{ver} \in {\mathbf{\mathbb{R}}}^{H \times W \times 4D}$$\end{document}</tex-math><mml:math id="M56"><mml:mrow><mml:msup><mml:mi>Y</mml:mi><mml:mrow><mml:mi mathvariant="italic">ver</mml:mi></mml:mrow></mml:msup><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>W</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mn>4</mml:mn><mml:mi>D</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_55612_Article_IEq23.gif"/></alternatives></inline-formula> and <inline-formula id="IEq24"><alternatives><tex-math id="M57">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$Y^{hor} \in {\mathbf{\mathbb{R}}}^{H \times W \times 4D}$$\end{document}</tex-math><mml:math id="M58"><mml:mrow><mml:msup><mml:mi>Y</mml:mi><mml:mrow><mml:mi mathvariant="italic">hor</mml:mi></mml:mrow></mml:msup><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>W</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mn>4</mml:mn><mml:mi>D</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_55612_Article_IEq24.gif"/></alternatives></inline-formula> are concatenated in the channel dimension, where D represents the hidden dimension of the BiLSTM block. This concatenated feature map is then propagated through a fully connected layer feedforward network. That is, <inline-formula id="IEq25"><alternatives><tex-math id="M59">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$FC( \bullet )$$\end{document}</tex-math><mml:math id="M60"><mml:mrow><mml:mi>F</mml:mi><mml:mi>C</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>&#x02219;</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_55612_Article_IEq25.gif"/></alternatives></inline-formula> in the following equation accomplishes channel fusion, yielding the ultimate output feature map <inline-formula id="IEq26"><alternatives><tex-math id="M61">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$Y \in {\mathbf{\mathbb{R}}}^{H \times W \times C}$$\end{document}</tex-math><mml:math id="M62"><mml:mrow><mml:mi>Y</mml:mi><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>W</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_55612_Article_IEq26.gif"/></alternatives></inline-formula>.<disp-formula id="Equ6"><label>6</label><alternatives><tex-math id="M63">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ Y_{hidden} = concatenate\left( {Y^{ver} ,Y^{hor} } \right) $$\end{document}</tex-math><mml:math id="M64" display="block"><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi mathvariant="italic">hidden</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msup><mml:mi>Y</mml:mi><mml:mrow><mml:mi mathvariant="italic">ver</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>Y</mml:mi><mml:mrow><mml:mi mathvariant="italic">hor</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow></mml:math><graphic xlink:href="41598_2024_55612_Article_Equ6.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ7"><label>7</label><alternatives><tex-math id="M65">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ Y = FC(Y_{hidden} ) $$\end{document}</tex-math><mml:math id="M66" display="block"><mml:mrow><mml:mi>Y</mml:mi><mml:mo>=</mml:mo><mml:mi>F</mml:mi><mml:mi>C</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi mathvariant="italic">hidden</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><graphic xlink:href="41598_2024_55612_Article_Equ7.gif" position="anchor"/></alternatives></disp-formula></p></sec><sec id="Sec10"><title>Depth Fusion</title><p id="Par26">In the BiLSTM block, the input feature map needs to be serialized in horizontal and vertical directions. The resulting sequences are then separately fed into the corresponding BiLSTM layer in each direction. Finally, the outputs from both directions are stitched together. The described process may limit the network's ability to effectively combine local and global features, leading to a potential lack of generalization. To enhance the feature representation further, we propose the utilization of a deep fusion module that combines convolutional operations with a Bottleneck-like approach to fuse the resulting feature maps. This fusion module aims to capture and integrate multi-scale information effectively, leading to improved feature representation capabilities.</p><p id="Par27">The Depth Fusion Block is visually represented in Fig.&#x000a0;<xref rid="Fig5" ref-type="fig">5</xref> below. In the initial phase, a 1&#x02009;&#x000d7;&#x02009;1 convolutional kernel is employed to expand the channel dimension of the feature map, doubling its original size while preserving the feature map scale. Subsequently, a convolutional kernel of size three performs the same mapping while preserving the channel dimension and the feature map scale unchanged. A convolution kernel of size one is employed to restore the original channel dimension. The GELU activation function is applied throughout the module.<fig id="Fig5"><label>Figure 5</label><caption><p>The structure of Depth Fusion Block.</p></caption><graphic xlink:href="41598_2024_55612_Fig5_HTML" id="MO5"/></fig></p></sec></sec><sec id="Sec11"><title>Depth inference for MVS</title><p id="Par28">Sampling occurs at the lowest resolution level, perpendicular to the direction normal to the reference viewpoint, assuming widely spaced depths, as set up in CVPMVSNet. To acquire the ultimate depth value of a point, the output probability volume is aggregated along the depth axis through averaging. The probabilities associated with pixel points corresponding to different depths undergo a weighting process based on their respective depth values, followed by aggregation.<disp-formula id="Equ8"><label>8</label><alternatives><tex-math id="M67">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ D^{i} \left( x \right) = \sum\limits_{m = 0}^{M - 1} {dX_{x}^{i} (d)} $$\end{document}</tex-math><mml:math id="M68" display="block"><mml:mrow><mml:msup><mml:mi>D</mml:mi><mml:mi>i</mml:mi></mml:msup><mml:mfenced close=")" open="("><mml:mi>x</mml:mi></mml:mfenced><mml:mo>=</mml:mo><mml:munderover><mml:mo movablelimits="false">&#x02211;</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:mrow><mml:mi>d</mml:mi><mml:msubsup><mml:mi>X</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math><graphic xlink:href="41598_2024_55612_Article_Equ8.gif" position="anchor"/></alternatives></disp-formula></p><p id="Par29">To improve the accuracy of the depth map, our network leverages the lowest resolution depth map as prior information and applies Bicubic interpolation to upsample the initial rough depth map. Building upon this approach, the depth hypothesis is progressively refined to construct a cost volume pyramid. This pyramid enables continuous optimization of the depth map while adding finer details.</p></sec><sec id="Sec12"><title>Loss function</title><p id="Par30">In line with the coarse-to-fine multi-stage MVS method, we employ the L1 loss as a supervised signal. This involves sampling the true depth map into the layer pyramid depth map and calculating the absolute distance between the true and predicted depths. The loss function is calculated as follows:<disp-formula id="Equ9"><label>9</label><alternatives><tex-math id="M69">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ Loss = \sum\limits_{i = 0}^{I - 1} {\sum\limits_{X \in \Omega } {\left\| {{\mathbf{D}}_{GT}^{i} \left( x \right) - {\mathbf{D}}^{i} (x)} \right\|_{i} } } $$\end{document}</tex-math><mml:math id="M70" display="block"><mml:mrow><mml:mi>L</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:munderover><mml:mo movablelimits="false">&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:mrow><mml:munder><mml:mo movablelimits="false">&#x02211;</mml:mo><mml:mrow><mml:mi>X</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi mathvariant="normal">&#x003a9;</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mfenced close="&#x02225;" open="&#x02225;"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">D</mml:mi><mml:mrow><mml:mi mathvariant="italic">GT</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msubsup><mml:mfenced close=")" open="("><mml:mi>x</mml:mi></mml:mfenced><mml:mo>-</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">D</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mfenced><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math><graphic xlink:href="41598_2024_55612_Article_Equ9.gif" position="anchor"/></alternatives></disp-formula></p><p id="Par31">In this context, the notation &#x003a9; represents the collection of valid pixel points. At the same time, i signifies the ith level of the pyramid, <inline-formula id="IEq27"><alternatives><tex-math id="M71">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathbf{D}}_{GT}^{l} \left( p \right)$$\end{document}</tex-math><mml:math id="M72"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">D</mml:mi><mml:mrow><mml:mi mathvariant="italic">GT</mml:mi></mml:mrow><mml:mi>l</mml:mi></mml:msubsup><mml:mfenced close=")" open="("><mml:mi>p</mml:mi></mml:mfenced></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_55612_Article_IEq27.gif"/></alternatives></inline-formula> is the actual depth value of pixel x at level i, and <inline-formula id="IEq28"><alternatives><tex-math id="M73">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathbf{D}}^{i} (x)$$\end{document}</tex-math><mml:math id="M74"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">D</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_55612_Article_IEq28.gif"/></alternatives></inline-formula> is the predicted depth value of pixel x at level i.</p></sec></sec><sec id="Sec13"><title>Experiments</title><sec id="Sec14"><title>Datasets</title><p id="Par32">Our network is trained and tested using the DTU dataset, which is a publicly available dataset. This dataset utilizes an industrial robot arm with adjustable luminance lights to capture photographs of objects from various viewpoints. Each viewpoint in the DTU dataset is precisely controlled, ensuring that a 3D point cloud is acquired using a structured light sensor. This allows for offline evaluation of the point cloud and facilitates easy monitoring of experimental results. The DTU dataset comprises 124 distinct scenes, each captured from either 49 or 64 viewpoints. These viewpoints cover a range of seven different lighting conditions, encompassing various geometries and texture structures in the scenes. As an illustration, Fig.&#x000a0;<xref rid="Fig6" ref-type="fig">6</xref> displays the scan96 scenes within the dataset. The images are arranged from left to right, showcasing the images captured at seven distinct luminance levels. Moreover, the images are vertically arranged in sequential order, signifying that they correspond to images taken from unique vantage points.<fig id="Fig6"><label>Figure 6</label><caption><p>DTU dataset partial visualization.</p></caption><graphic xlink:href="41598_2024_55612_Fig6_HTML" id="MO6"/></fig></p><p id="Par33">As depicted in Fig.&#x000a0;<xref rid="Fig7" ref-type="fig">7</xref>, presented underneath, the BlendedMVS dataset encompasses a diverse collection of 113 scenes with varying scales, and the number of views ranges from 20 to 1000. In contrast to the DTU dataset, which utilizes a fixed number of views, the BlendedMVS dataset employs multiple cameras to capture random shots. Additionally, depth sensors are used to accurately measure depth information. By incorporating randomness in viewpoint selection and incorporating accurate depth measurements, the BlendedMVS dataset aims to simulate real-world scenarios more effectively, enabling better performance and generalization of algorithms trained on it. However, this dataset does not provide a true value point cloud and does not allow for point cloud evaluation. Therefore, we solely utilize the BlendedMVS dataset for qualitative evaluation and visualization result presentation.<fig id="Fig7"><label>Figure 7</label><caption><p>BlendedMVS dataset partial visualization.</p></caption><graphic xlink:href="41598_2024_55612_Fig7_HTML" id="MO7"/></fig></p></sec><sec id="Sec15"><title>Metrics</title><p id="Par34">For evaluating the point cloud model generated by our network, three selected metrics are Accuracy (Acc), Completeness (Comp), and Overall score (Overall). These metrics are employed to evaluate the precision, comprehensiveness, and overall fidelity of the reconstructed point cloud in comparison to the ground truth reference. Each metric is measured in millimeters, and decreased values for these metrics correspond to improved algorithm effectiveness. This means that a smaller value for Accuracy (Acc), Completeness (Comp), and Overall score (Overall) indicates a better reconstruction quality of the point cloud generated by the algorithm. The objective is to minimize these metrics, indicating a stronger alignment of the reconstructed point cloud with the ground truth data.<disp-formula id="Equ10"><label>10</label><alternatives><tex-math id="M75">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ Acc = \frac{1}{\left| P \right|}\sum\limits_{x \in P} {\mathop {\min }\limits_{y \in G} } \left\| {x - y} \right\|^{2} $$\end{document}</tex-math><mml:math id="M76" display="block"><mml:mrow><mml:mi>A</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mfenced close="|" open="|"><mml:mi>P</mml:mi></mml:mfenced></mml:mfrac><mml:munder><mml:mo movablelimits="false">&#x02211;</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:munder><mml:munder><mml:mo movablelimits="false">min</mml:mo><mml:mrow><mml:mi>y</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi>G</mml:mi></mml:mrow></mml:munder><mml:msup><mml:mfenced close="&#x02225;" open="&#x02225;"><mml:mrow><mml:mi>x</mml:mi><mml:mo>-</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mfenced><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math><graphic xlink:href="41598_2024_55612_Article_Equ10.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ11"><label>11</label><alternatives><tex-math id="M77">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ Comp = \frac{1}{\left| G \right|}\sum\limits_{x \in G} {\mathop {\min }\limits_{y \in p} } \left\| {x - y} \right\|^{2} $$\end{document}</tex-math><mml:math id="M78" display="block"><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>m</mml:mi><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mfenced close="|" open="|"><mml:mi>G</mml:mi></mml:mfenced></mml:mfrac><mml:munder><mml:mo movablelimits="false">&#x02211;</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi>G</mml:mi></mml:mrow></mml:munder><mml:munder><mml:mo movablelimits="false">min</mml:mo><mml:mrow><mml:mi>y</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:munder><mml:msup><mml:mfenced close="&#x02225;" open="&#x02225;"><mml:mrow><mml:mi>x</mml:mi><mml:mo>-</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mfenced><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math><graphic xlink:href="41598_2024_55612_Article_Equ11.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ12"><label>12</label><alternatives><tex-math id="M79">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ Overall = \frac{Acc + Comp}{2} $$\end{document}</tex-math><mml:math id="M80" display="block"><mml:mrow><mml:mi>O</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>A</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi><mml:mo>+</mml:mo><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>m</mml:mi><mml:mi>p</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:math><graphic xlink:href="41598_2024_55612_Article_Equ12.gif" position="anchor"/></alternatives></disp-formula></p><p id="Par35">In this context, P represents the points in the predicted point cloud model, while G represents all the points in the real point cloud.</p></sec><sec id="Sec16"><title>Implementation details</title><p id="Par36">The experimental environment consisted of an AMD Ryzen 7 5800X 8-Core Processor as the CPU, 32 G.B. of memory, and an NVIDIA GeForce TITAN RTX GPU with 24 G.B. of video memory. The deep learning framework used was PyTorch, version 1.7.1, with CUDA version 10.1 for GPU acceleration. For training and testing, the image width is 160, height is 128 and the number of input views is 3. For model optimization, the Adam optimizer was employed with hyperparameters set to &#x003b2;1&#x02009;=&#x02009;0.9 and &#x003b2;2&#x02009;=&#x02009;0.999.</p></sec><sec id="Sec17"><title>Experimental performance</title><sec id="Sec18"><title>Results on DTU dataset</title><p id="Par37">Table <xref rid="Tab1" ref-type="table">1</xref> presents a comprehensive analysis of the algorithm discussed in our model and other learning-based MVS methods using the DTU dataset. The comparison is based on objective metrics, and it provides insights into the performance of our algorithm concerning other existing methods. In terms of runtime and parameter count, our model has achieved satisfactory results, demonstrating relatively low runtime and reasonable parameter count. This balance enhances the practical feasibility and efficiency of our method in real-world applications.<table-wrap id="Tab1"><label>Table 1</label><caption><p>Comparison of results on DTU Dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Method</th><th align="left">Acc. (mm)&#x02193;</th><th align="left">Comp. (mm)&#x02193;</th><th align="left">Overall (mm)&#x02193;</th><th align="left">Params (M)&#x02193;</th><th align="left">Runtime (s)&#x02193;</th></tr></thead><tbody><tr><td align="left">Colmap<sup><xref ref-type="bibr" rid="CR26">26</xref></sup></td><td char="." align="char">6.5778</td><td char="." align="char">10.1405</td><td char="." align="char">8.2930</td><td char="." align="char">-</td><td char="." align="char">-</td></tr><tr><td align="left">R-MVSNet<sup><xref ref-type="bibr" rid="CR27">27</xref></sup></td><td char="." align="char">1.0896</td><td char="." align="char">1.4115</td><td char="." align="char">1.2506</td><td char="." align="char">0.80</td><td char="." align="char">0.051</td></tr><tr><td align="left">TransMVSNet<sup><xref ref-type="bibr" rid="CR11">11</xref></sup></td><td char="." align="char">1.0248</td><td char="." align="char">1.3075</td><td char="." align="char">1.1662</td><td char="." align="char">1.15</td><td char="." align="char">0.097</td></tr><tr><td align="left">CasMVSNet<sup><xref ref-type="bibr" rid="CR15">15</xref></sup></td><td char="." align="char">1.4045</td><td char="." align="char">1.6096</td><td char="." align="char">1.5071</td><td char="." align="char">0.93</td><td char="." align="char"><bold>0.022</bold></td></tr><tr><td align="left">CVP-MVSNet<sup><xref ref-type="bibr" rid="CR16">16</xref></sup></td><td char="." align="char">1.1964</td><td char="." align="char">1.0569</td><td char="." align="char">1.1267</td><td char="." align="char">0.55</td><td char="." align="char">0.067</td></tr><tr><td align="left">AACVP-MVSNet<sup><xref ref-type="bibr" rid="CR12">12</xref></sup></td><td char="." align="char">1.1329</td><td char="." align="char"><bold>0.8874</bold></td><td char="." align="char">1.0071</td><td char="." align="char"><bold>0.54</bold></td><td char="." align="char">0.064</td></tr><tr><td align="left">GeoMVSNet<sup><xref ref-type="bibr" rid="CR17">17</xref></sup></td><td char="." align="char">1.1406</td><td char="." align="char">1.8218</td><td char="." align="char">1.4812</td><td char="." align="char">15.00</td><td char="." align="char">0.695</td></tr><tr><td align="left">Ours</td><td char="." align="char"><bold>0.9285</bold></td><td char="." align="char">0.9879</td><td char="." align="char"><bold>0.9582</bold></td><td char="." align="char">0.72</td><td char="." align="char">0.036</td></tr></tbody></table><table-wrap-foot><p>Significant values are in bold.</p></table-wrap-foot></table-wrap></p><p id="Par38">Colmap is a classical MVS algorithm that reconstructs 3D models by iteratively establishing correspondences between image pairs. This step-by-step approach allows Colmap to gradually build the 3D model by leveraging the detected correspondences. In the case of low-resolution images, the Colmap method may encounter challenges in the feature point matching phase, leading to sparse correspondences and, subsequently, poor reconstruction results. R-MVSNet<sup><xref ref-type="bibr" rid="CR27">27</xref></sup> Replaces 3DCNN with GRU for Recurrent Regularisation. TransMVS is a method that adopts a Transformer-based approach for MVS tasks. Cas-MVSNet, on the other hand, utilizes a cascaded multi-scale cost volume strategy combined with adaptive depth sampling. CVP-MVSNet, in contrast, employs a compact and lightweight network architecture to construct pyramids of cost volumes, enabling higher-resolution 3D reconstruction. AACVP-MVSNet, in contrast, incorporates an attention layer into the network architecture to enhance feature extraction. Due to the extensive computational resources required by GeoMVSNet<sup><xref ref-type="bibr" rid="CR17">17</xref></sup>, we did not retrain the model but conducted direct testing. Regarding accuracy and overall performance, as shown in Fig.&#x000a0;<xref rid="Fig8" ref-type="fig">8</xref>, the algorithm proposed in this paper showcases superior results compared to other methods.<fig id="Fig8"><label>Figure 8</label><caption><p>Comparison of results on DTU dataset.</p></caption><graphic xlink:href="41598_2024_55612_Fig8_HTML" id="MO8"/></fig></p></sec><sec id="Sec19"><title>Results on BlendedMVS dataset</title><p id="Par39">To comprehensively evaluate the generalization capability of our proposed model, we reconstruct the scene from the BlendedMVS dataset without conducting any fine-tuning on the network explicitly trained on the DTU dataset. The results are shown in Fig.&#x000a0;<xref rid="Fig9" ref-type="fig">9</xref> below, where the significantly overall reconstructed point cloud of this network shows greater density in the visualization output when compared to the other networks.<fig id="Fig9"><label>Figure 9</label><caption><p>Comparison of results on BlendedMVS dataset.</p></caption><graphic xlink:href="41598_2024_55612_Fig9_HTML" id="MO9"/></fig></p></sec><sec id="Sec20"><title>Ablation study</title><p id="Par40">
<list list-type="order"><list-item><p id="Par41">Effectiveness of Different Components</p></list-item></list></p><p id="Par42">Our proposed network incorporates the MLSP module, which facilitates the construction of a multi-level spatial pyramid. This design enables the network to interact with image information at different scales, capturing holistic and detailed information. By leveraging the multi-level spatial pyramid, our algorithm can effectively incorporate contextual information and capture the hierarchical structure of the input data. We integrate the BiLSTM module into our model to enable semantic information filtering. This block allows the model to selectively focus on the most relevant and helpful information for the task of 3D reconstruction. By incorporating the BiLSTM module, our model becomes more adept at capturing and utilizing meaningful semantic information, which enhances the quality and accuracy of the generated 3D reconstructions. The effectiveness of all our proposed modules is validated through ablation experiments, and the corresponding results are presented in Table <xref rid="Tab2" ref-type="table">2</xref>. The comprehensive model that incorporates all the proposed modules demonstrates superior performance across all metrics, attaining optimal results.<list list-type="simple"><list-item><label>2.</label><p id="Par43">Number of Different BiLSTM Modules</p></list-item></list><table-wrap id="Tab2"><label>Table 2</label><caption><p>Quantitative performance with different components.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">MLSP</th><th align="left">BiLSTM</th><th align="left">Acc. (mm)&#x02193;</th><th align="left">Comp. (mm)&#x02193;</th><th align="left">Overall (mm)&#x02193;</th></tr></thead><tbody><tr><td align="left"/><td align="left"/><td char="." align="char">1.1964</td><td char="." align="char">1.0569</td><td char="." align="char">1.1267</td></tr><tr><td align="left">&#x0221a;</td><td align="left"/><td char="." align="char">0.9635</td><td char="." align="char">1.0257</td><td char="." align="char">0.9946</td></tr><tr><td align="left">&#x0221a;</td><td align="left">&#x0221a;</td><td char="." align="char"><bold>0.9285</bold></td><td char="." align="char"><bold>0.9879</bold></td><td char="." align="char"><bold>0.9582</bold></td></tr></tbody></table><table-wrap-foot><p>Significant values are in bold.</p></table-wrap-foot></table-wrap></p><p id="Par44">To account for the varying requirements of the BiLSTM module, we investigated to assess the impact of selecting different numbers of layers on the results. Based on the observations outlined in Table <xref rid="Tab3" ref-type="table">3</xref>, a discernible pattern emerges whereby the metrics exhibit a decline to a certain degree as the number of layers in the BiLSTM module increases. However, beyond a specific range of layer configurations, the metrics start to increase. Consequently, the best results were achieved when utilizing four layers in the BiLSTM module. This indicates that four layers strike an optimal balance between model complexity and performance, resulting in the most favorable outcomes for the task at hand.<list list-type="simple"><list-item><label>3.</label><p id="Par45">Effectiveness of Depth Fusion</p></list-item></list><table-wrap id="Tab3"><label>Table 3</label><caption><p>Ablation study on the number of Different BiLSTM modules.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">BiLSTM</th><th align="left">Acc. (mm)&#x02193;</th><th align="left">Comp. (mm)&#x02193;</th><th align="left">Overall (mm)&#x02193;</th></tr></thead><tbody><tr><td align="left">2</td><td char="." align="char">0.9563</td><td char="." align="char">1.0163</td><td char="." align="char">0.9863</td></tr><tr><td align="left">4</td><td char="." align="char"><bold>0.9285</bold></td><td char="." align="char"><bold>0.9879</bold></td><td char="." align="char"><bold>0.9582</bold></td></tr><tr><td align="left">6</td><td char="." align="char">0.9421</td><td char="." align="char">0.9912</td><td char="." align="char">0.9666</td></tr></tbody></table><table-wrap-foot><p>Significant values are in bold.</p></table-wrap-foot></table-wrap></p><p id="Par46">As the BiLSTM Module utilizes BiLSTM to process feature sequences in the row and column directions, it is necessary to serialize the input feature. Our proposed BiLSTM Module integrates an additional feature fusion block. This block is positioned before the output and aims to fuse and enhance semantic information across different temporal and spatial dimensions. As shown in the data in Table <xref rid="Tab4" ref-type="table">4</xref>, Depth-CNN indicates the use of depth-separable convolution for feature fusion, while Linear indicates the use of linear layers for feature fusion. Based on the experimental results, it is evident that the proposed Depth Fusion block, which incorporates a BottleNeck-like structure, outperforms other methods in all metrics. The block indicates that the chosen architecture and design of the Depth Fusion block are highly effective in achieving superior performance across various evaluation metrics. The BottleNeck-like structure likely contributes to the optimal fusion of depth information, resulting in enhanced results for the task under consideration.<list list-type="simple"><list-item><label>4.</label><p id="Par47">Effectiveness of BiLSTM Module</p></list-item></list><table-wrap id="Tab4"><label>Table 4</label><caption><p>Ablation study on depth fusion.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Depth fusion</th><th align="left">Acc. (mm)&#x02193;</th><th align="left">Comp. (mm)&#x02193;</th><th align="left">Overall (mm)&#x02193;</th></tr></thead><tbody><tr><td align="left">Depth-CNN</td><td char="." align="char">0.9369</td><td char="." align="char">1.0096</td><td char="." align="char">0.9733</td></tr><tr><td align="left">Linear</td><td char="." align="char">0.9342</td><td char="." align="char">1.0078</td><td char="." align="char">0.9710</td></tr><tr><td align="left">Ours</td><td char="." align="char"><bold>0.9285</bold></td><td char="." align="char"><bold>0.9879</bold></td><td char="." align="char"><bold>0.9582</bold></td></tr></tbody></table><table-wrap-foot><p>Significant values are in bold.</p></table-wrap-foot></table-wrap></p><p id="Par48">In our proposed network, we incorporate the BiLSTM module for feature aggregation following the MLSP module. This design allows for effectively integrating and aggregating features from multiple spatial scales. The BiLSTM module plays a crucial role in leveraging contextual information and capturing long-range dependencies among the features, leading to improved performance in tasks that require a comprehensive understanding of the input data. By combining the MLSP and BiLSTM modules, our network benefits from multi-scale feature representation and contextual modeling, enhancing its overall capability for the given task. Table <xref rid="Tab5" ref-type="table">5</xref> primarily investigates the substitution of the BiLSTM module in our proposed network with either an attention mechanism or a convolutional neural network.<table-wrap id="Tab5"><label>Table 5</label><caption><p>Results on different modules.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Methods</th><th align="left">Acc. (mm)&#x02193;</th><th align="left">Comp. (mm)&#x02193;</th><th align="left">Overall (mm)&#x02193;</th></tr></thead><tbody><tr><td align="left">Vanilla-SA<sup><xref ref-type="bibr" rid="CR6">6</xref></sup></td><td char="." align="char">0.9269</td><td char="." align="char">1.0120</td><td char="." align="char">0.9708</td></tr><tr><td align="left">LSDA<sup><xref ref-type="bibr" rid="CR28">28</xref></sup></td><td char="." align="char">0.9732</td><td char="." align="char">1.0738</td><td char="." align="char">1.0235</td></tr><tr><td align="left">SSA<sup><xref ref-type="bibr" rid="CR29">29</xref></sup></td><td char="." align="char">0.9756</td><td char="." align="char">1.0400</td><td char="." align="char">1.0078</td></tr><tr><td align="left">SSA-IWSA<sup><xref ref-type="bibr" rid="CR13">13</xref>,<xref ref-type="bibr" rid="CR29">29</xref></sup></td><td char="." align="char"><bold>0.9266</bold></td><td char="." align="char">1.0244</td><td char="." align="char">0.9755</td></tr><tr><td align="left">Linear-SA-soft<sup><xref ref-type="bibr" rid="CR30">30</xref></sup></td><td char="." align="char">0.9295</td><td char="." align="char">1.0145</td><td char="." align="char">0.9720</td></tr><tr><td align="left">Control-SA<sup><xref ref-type="bibr" rid="CR31">31</xref></sup></td><td char="." align="char">0.9557</td><td char="." align="char">1.0387</td><td char="." align="char">0.9972</td></tr><tr><td align="left">ConvNext-Depth<sup><xref ref-type="bibr" rid="CR20">20</xref></sup></td><td char="." align="char">0.9486</td><td char="." align="char">1.0139</td><td char="." align="char">0.9813</td></tr><tr><td align="left">ConvNext-Block<sup><xref ref-type="bibr" rid="CR20">20</xref></sup></td><td char="." align="char">0.9608</td><td char="." align="char">1.0191</td><td char="." align="char">0.9899</td></tr><tr><td align="left">Incep-Block<sup><xref ref-type="bibr" rid="CR22">22</xref></sup></td><td char="." align="char">0.9784</td><td char="." align="char">1.0248</td><td char="." align="char">1.0016</td></tr><tr><td align="left">ConvNext-Incep<sup><xref ref-type="bibr" rid="CR20">20</xref></sup></td><td char="." align="char">0.9894</td><td char="." align="char">1.0163</td><td char="." align="char">1.0028</td></tr><tr><td align="left">Ours</td><td char="." align="char">0.9285</td><td char="." align="char"><bold>0.9879</bold></td><td char="." align="char"><bold>0.9582</bold></td></tr></tbody></table><table-wrap-foot><p>Significant values are in bold.</p></table-wrap-foot></table-wrap></p><p id="Par49">In Table <xref rid="Tab5" ref-type="table">5</xref>, Vanilla-SA, LSDA, and SSA represent initial self-attention, long-short distance attention<sup><xref ref-type="bibr" rid="CR28">28</xref></sup>, and scaling attention<sup><xref ref-type="bibr" rid="CR29">29</xref></sup>. In Table <xref rid="Tab5" ref-type="table">5</xref>, SSA-IWS<sup><xref ref-type="bibr" rid="CR13">13</xref></sup> refers to alternate scaling and interactive window attention, while Linear-SA-soft represents linear attention<sup><xref ref-type="bibr" rid="CR30">30</xref></sup>. Control-SA is to perform feature extraction using a network that freezes pre-trained weights along with self-attention and then fuses the extracted feature maps. ConvNext-Depth applies ConvNext architecture with four layers featuring hidden dimensions that progressively grow. On the other hand, ConvNext-Block denotes the usage of ConvNext with four layers of the exact dimensions. ConvNext is a convolutional implementation of the SwimTransformer and ResNet-like structure. It has performed superior to SwimTransformer and ResNet in various classification and image detection tasks. This indicates that ConvNext improves accuracy and effectiveness in capturing and representng features in the context of these tasks.</p><p id="Par50">Incep-Block denotes a structure that employs InceptionNext as the primary architecture. ConvNext-Incep indicates a structure that alternates between using ConvNext and InceptionNext within the network. The metric results (Fig.&#x000a0;<xref rid="Fig10" ref-type="fig">10</xref>) indicate that our proposed structure, the BiLSTM module, achieves optimal performance in completeness and objectivity metrics. This suggests that the BiLSTM module effectively captures and incorporates relevant information, resulting in comprehensive and accurate results for the given task.<list list-type="simple"><list-item><label>3.</label><p id="Par51">Explore Cost Volume Regularization</p></list-item></list><fig id="Fig10"><label>Figure 10</label><caption><p>Results on different Modules. (<bold>a</bold>) ours; (<bold>b</bold>) vanilla-SA; (<bold>c</bold>) LSDA; (<bold>d</bold>) SSA; (<bold>e</bold>) SSA-IWSA; (<bold>f</bold>) SSA-IWSA-soft; (<bold>g</bold>) Control-SA; (<bold>h</bold>) ConvNext-depth; (<bold>i</bold>) ConvNext-Block; (<bold>j</bold>) Incep-Block; (<bold>k</bold>) ConvNext-Incep.</p></caption><graphic xlink:href="41598_2024_55612_Fig10_HTML" id="MO10"/></fig></p><p id="Par52">We compared these networks' cost volume regularization modules to facilitate a more comprehensive comparison between convolutional neural networks and Vision Transformer-based neural networks. In Table <xref rid="Tab6" ref-type="table">6</xref>, 3Dvit refers to using 3DTransformer, while 3DVit-BN indicates using 3D Transformer with BatchNorm regularization. The experimental metrics demonstrate that convolutional networks (3DCNN) for cost volume regularization achieve optimal performance across all metrics. In Table <xref rid="Tab6" ref-type="table">6</xref>, 3Dvit refers to using 3DTransformer, while 3DVit-BN indicates using 3D Transformer with BatchNorm regularization. The experimental metrics demonstrate that convolutional networks (3DCNN) for cost volume regularization achieve optimal performance across all metrics.<table-wrap id="Tab6"><label>Table 6</label><caption><p>Results on different regularization modules.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Block</th><th align="left">Acc. (mm)&#x02193;</th><th align="left">Comp. (mm)&#x02193;</th><th align="left">Overall (mm)&#x02193;</th></tr></thead><tbody><tr><td align="left">3Dvit</td><td char="." align="char">0.9538</td><td char="." align="char">1.0192</td><td char="." align="char">0.9865</td></tr><tr><td align="left">3DVit-BN</td><td char="." align="char">1.0531</td><td char="." align="char">1.0411</td><td char="." align="char">1.0471</td></tr><tr><td align="left">Ours</td><td char="." align="char"><bold>0.9285</bold></td><td char="." align="char"><bold>0.9879</bold></td><td char="." align="char"><bold>0.9582</bold></td></tr></tbody></table><table-wrap-foot><p>Significant values are in bold.</p></table-wrap-foot></table-wrap></p></sec></sec></sec><sec id="Sec21"><title>Conclusion</title><p id="Par53">We introduce a MVS network with Bidirectional Semantic Information explicitly designed for 3D reconstruction on low-resolution images. We propose utilizing an MLSP module during the feature extraction phase to establish a spatial pyramid, enabling the interaction of various spatial information. Additionally, we utilize the BiLSTM module to enable the interaction of feature representations across diverse locations. By comparing the BiLSTM module with different attention mechanisms, it outperforms them in objective metrics and visual representation. This experimental verification of BSI-MVS showcased a significant enhancement in the precision of 3D reconstruction. But our network is primarily designed for low resolution. In the future, we will delve into the research of high-resolution 3D reconstruction networks and hope to further reduce the network parameters while improving the accuracy of 3D reconstruction.</p></sec></body><back><fn-group><fn><p><bold>Publisher's note</bold></p><p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></fn></fn-group><notes notes-type="author-contribution"><title>Author contributions</title><p>Conceptualization, R.J. and J.Y.; methodology, R.J. and J.Y.; software, J.Y.; validation, R.J.; formal analysis, R.J. and J.Y.; investigation, J.Y.; resources, R.J. and J.Y.; data curation, J.Y.; writing&#x02014;original draft preparation, J.Y.; writing&#x02014;review and editing, R.J. and F.Y.; visualization, J.Y. and R.J.; supervision, F.Y. and Z.H.; project administration, F.Y., Z.H. and R.J.; funding acquisition, F.Y. and Z.H. All authors have read and agreed to the published version of the manuscript.</p></notes><notes notes-type="funding-information"><title>Funding</title><p>This research was supported by National Key Research and Development Program Project (2020YFC0832503), Hangzhou Innovation Institute of Beihang University (No. 2020-Y3-A-014).</p></notes><notes notes-type="data-availability"><title>Data availability</title><p>Our network uses two publicly available datasets, BlendedMVS dataset and DTU dataset. And BlendedMVS dataset is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/YoYo000/BlendedMVS">https://github.com/YoYo000/BlendedMVS</ext-link>, DTU dataset is available at <ext-link ext-link-type="uri" xlink:href="https://roboimagedata.compute.dtu.dk/?page_id=36">https://roboimagedata.compute.dtu.dk/?page_id=36</ext-link>.</p></notes><notes id="FPar1" notes-type="COI-statement"><title>Competing interests</title><p id="Par54">The authors declare no competing interests.</p></notes><ref-list id="Bib1"><title>References</title><ref id="CR1"><label>1.</label><mixed-citation publication-type="other">Liu, J. <italic>et al.</italic> PlaneMVS: 3D Plane Reconstruction from Multi-View Stereo. in <italic>Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</italic> vols 2022-June 8655&#x02013;8665 (2022).</mixed-citation></ref><ref id="CR2"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hirschm&#x000fc;ller</surname><given-names>H</given-names></name></person-group><article-title>Stereo processing by semiglobal matching and mutual information</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2008</year><volume>30</volume><fpage>328</fpage><lpage>341</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2007.1166</pub-id><?supplied-pmid 18084062?><pub-id pub-id-type="pmid">18084062</pub-id>
</element-citation></ref><ref id="CR3"><label>3.</label><mixed-citation publication-type="other">Yao, Y., Luo, Z., Li, S., Fang, T. &#x00026; Quan, L. MVSNet: Depth inference for unstructured multi-view stereo. In <italic>Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)</italic> vol. 11212 LNCS 785&#x02013;801 (2018).</mixed-citation></ref><ref id="CR4"><label>4.</label><mixed-citation publication-type="other">Wei, Z., Zhu, Q., Min, C., Chen, Y. &#x00026; Wang, G. AA-RMVSNet: Adaptive aggregation recurrent multi-view stereo network. in <italic>Proceedings of the IEEE International Conference on Computer Vision</italic> 6167&#x02013;6176 (2021).</mixed-citation></ref><ref id="CR5"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vaswani</surname><given-names>A</given-names></name><etal/></person-group><article-title>Attention is all you need</article-title><source>Adv. Neural Inf. Process. Syst.</source><year>2017</year><volume>2017</volume><fpage>5999</fpage><lpage>6009</lpage></element-citation></ref><ref id="CR6"><label>6.</label><mixed-citation publication-type="other">Dosovitskiy, A. <italic>et al.</italic> an Image Is Worth 16X16 Words: Transformers for Image Recognition At Scale. <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2010.11929">arXiv:2010.11929</ext-link> (2020).</mixed-citation></ref><ref id="CR7"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schuster</surname><given-names>M</given-names></name><name><surname>Paliwal</surname><given-names>KK</given-names></name></person-group><article-title>Bidirectional recurrent neural networks</article-title><source>IEEE Trans. Signal Process.</source><year>1997</year><volume>45</volume><fpage>2673</fpage><lpage>2681</lpage><pub-id pub-id-type="doi">10.1109/78.650093</pub-id></element-citation></ref><ref id="CR8"><label>8.</label><mixed-citation publication-type="other">Tatsunami, Y. &#x00026; Taki, M. Sequencer: Deep LSTM for image classification. arXiv <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2205.01972">arXiv:2205.01972</ext-link> (2022).</mixed-citation></ref><ref id="CR9"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kar</surname><given-names>A</given-names></name><name><surname>H&#x000e4;ne</surname><given-names>C</given-names></name><name><surname>Malik</surname><given-names>J</given-names></name></person-group><article-title>Large-scale data for multiple-view stereopsis</article-title><source>Adv. Neural Inf. Process. Syst.</source><year>2017</year><volume>2017</volume><fpage>365</fpage><lpage>376</lpage></element-citation></ref><ref id="CR10"><label>10.</label><mixed-citation publication-type="other">Yao, Y. <italic>et al.</italic> BlendedMVS: A large-scale dataset for generalized multi-view stereo networks. In <italic>Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</italic> 1787&#x02013;1796 (2020).</mixed-citation></ref><ref id="CR11"><label>11.</label><mixed-citation publication-type="other">Ding, Y. <italic>et al.</italic> TransMVSNet: Global context-aware multi-view stereo network with transformers. In <italic>Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</italic> vols 2022-June 8575&#x02013;8584 (2022).</mixed-citation></ref><ref id="CR12"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yu</surname><given-names>A</given-names></name><etal/></person-group><article-title>Attention aware cost volume pyramid based multi-view stereo network for 3D reconstruction</article-title><source>ISPRS J. Photogramm. Remote Sens.</source><year>2021</year><volume>175</volume><fpage>448</fpage><lpage>460</lpage><pub-id pub-id-type="doi">10.1016/j.isprsjprs.2021.03.010</pub-id></element-citation></ref><ref id="CR13"><label>13.</label><mixed-citation publication-type="other">Liao, J. <italic>et al.</italic> WT-MVSNet: Window-based transformers for multi-view stereo. arXiv arXiv:2205.14319 (2022).</mixed-citation></ref><ref id="CR14"><label>14.</label><mixed-citation publication-type="other">Zhang, J., Yao, Y. &#x00026; Quan, L. Learning signed distance field for multi-view surface reconstruction. In <italic>Proceedings of the IEEE International Conference on Computer Vision</italic> 6505&#x02013;6514 (2021).</mixed-citation></ref><ref id="CR15"><label>15.</label><mixed-citation publication-type="other">Gu, X. <italic>et al.</italic> Cascade cost volume for high-resolution multi-view stereo and stereo matching. In <italic>Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</italic> 2492&#x02013;2501 (2020).</mixed-citation></ref><ref id="CR16"><label>16.</label><mixed-citation publication-type="other">Yang, J., Mao, W., Alvarez, J. M. &#x00026; Liu, M. Cost volume pyramid based depth inference for multi-view stereo. In <italic>Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</italic> 4876&#x02013;4885 (2020).</mixed-citation></ref><ref id="CR17"><label>17.</label><mixed-citation publication-type="other">Zhang, Z., Peng, R., Hu, Y. &#x00026; Wang, R. GeoMVSNet: Learning multi-view stereo with geometry perception. <italic>Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit.</italic><bold>2023</bold>-<bold>June</bold>, 21508&#x02013;21518 (2023).</mixed-citation></ref><ref id="CR18"><label>18.</label><mixed-citation publication-type="other">Lee, J. Y., DeGol, J., Zou, C. &#x00026; Hoiem, D. PatchMatch-RL: Deep MVS with pixelwise depth, normal, and visibility. In <italic>Proceedings of the IEEE International Conference on Computer Vision</italic> 6138&#x02013;6147 (2021).</mixed-citation></ref><ref id="CR19"><label>19.</label><mixed-citation publication-type="other">Cao, C., Ren, X. &#x00026; Fu, Y. MVSFormer: Multi-view stereo by learning robust image features and temperature-based depth. arXiv arXiv:2208.02541 (2022).</mixed-citation></ref><ref id="CR20"><label>20.</label><mixed-citation publication-type="other">Liu, Z. <italic>et al.</italic> A ConvNet for the 2020s. In <italic>Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</italic> vols 2022-June 11966&#x02013;11976 (2022).</mixed-citation></ref><ref id="CR21"><label>21.</label><mixed-citation publication-type="other">Liu, Z. <italic>et al.</italic> Swin Transformer: Hierarchical vision transformer using shifted windows. In <italic>Proceedings of the IEEE International Conference on Computer Vision</italic> 9992&#x02013;10002 (2021).</mixed-citation></ref><ref id="CR22"><label>22.</label><mixed-citation publication-type="other">Yu, W., Zhou, P., Yan, S. &#x00026; Wang, X. InceptionNeXt: When inception meets ConvNeXt. <italic>arXiv</italic> arXiv:2303.16900 (2023).</mixed-citation></ref><ref id="CR23"><label>23.</label><mixed-citation publication-type="other">Wei, Z., Zhu, Q., Min, C., Chen, Y. &#x00026; Wang, G. Bidirectional hybrid LSTM based recurrent neural network for multi-view stereo. <italic>IEEE Trans. Vis. Comput. Graph.</italic> 1 (2022).</mixed-citation></ref><ref id="CR24"><label>24.</label><mixed-citation publication-type="other">Ba, J. L., Kiros, J. R. &#x00026; Hinton, G. E. Layer Normalization. arXiv <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1607.06450">arXiv:1607.06450</ext-link> (2016).</mixed-citation></ref><ref id="CR25"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shi</surname><given-names>X</given-names></name><etal/></person-group><article-title>Convolutional LSTM network: A machine learning approach for precipitation nowcasting</article-title><source>Adv. Neural Inf. Process. Syst.</source><year>2015</year><volume>2015</volume><fpage>802</fpage><lpage>810</lpage></element-citation></ref><ref id="CR26"><label>26.</label><mixed-citation publication-type="other">Schonberger, J. L. &#x00026; Frahm, J. M. Structure-from-motion revisited. In <italic>Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</italic> vols 2016-Decem 4104&#x02013;4113 (2016).</mixed-citation></ref><ref id="CR27"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yao</surname><given-names>Y</given-names></name><etal/></person-group><article-title>Recurrent MVSnet for high-resolution multi-view stereo depth inference</article-title><source>Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit.</source><year>2019</year><volume>2019</volume><fpage>5520</fpage><lpage>5529</lpage></element-citation></ref><ref id="CR28"><label>28.</label><mixed-citation publication-type="other">Wang, W. <italic>et al.</italic> CrossFormer++: A versatile vision transformer hinging on cross-scale attention. arXiv arXiv:2303.06908 (2023).</mixed-citation></ref><ref id="CR29"><label>29.</label><mixed-citation publication-type="other">Yang, R. <italic>et al.</italic> ScalableViT: Rethinking the context-oriented generalization of vision transformer. In <italic>Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)</italic> vol. 13684 LNCS 480&#x02013;496 (2022).</mixed-citation></ref><ref id="CR30"><label>30.</label><mixed-citation publication-type="other">Katharopoulos, A., Vyas, A. &#x00026; Pappas, N. Transformers are RNNs: Fast autoregressive transformers with linear attention. In <italic>Proceedings of the 37th International Conference on Machine Learning</italic> (2020).</mixed-citation></ref><ref id="CR31"><label>31.</label><mixed-citation publication-type="other">Zhang, L. &#x00026; Agrawala, M. Adding conditional control to text-to-image diffusion models. arXiv <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2302.05543">arXiv:2302.05543</ext-link> (2023).</mixed-citation></ref></ref-list></back></article>