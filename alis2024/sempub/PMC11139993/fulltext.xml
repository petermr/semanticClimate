<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd"> 
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><?properties open_access?><?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?><?DTDIdentifier.IdentifierType public?><?SourceDTD.DTDName A++V2.4.dtd?><?SourceDTD.Version 2.4?><?ConverterInfo.XSLTName springer2nlmx2.xsl?><?ConverterInfo.Version 1?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sci Rep</journal-id><journal-id journal-id-type="iso-abbrev">Sci Rep</journal-id><journal-title-group><journal-title>Scientific Reports</journal-title></journal-title-group><issn pub-type="epub">2045-2322</issn><publisher><publisher-name>Nature Publishing Group UK</publisher-name><publisher-loc>London</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmcid">11139993</article-id><article-id pub-id-type="pmid">38816446</article-id>
<article-id pub-id-type="publisher-id">63195</article-id><article-id pub-id-type="doi">10.1038/s41598-024-63195-5</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Multiscale knowledge distillation with attention based fusion for robust human activity recognition</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><name><surname>Yuan</surname><given-names>Zhaohui</given-names></name><address><email>yuanzh@whu.edu.cn</email></address><xref ref-type="aff" rid="Aff1"/></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Yang</surname><given-names>Zhengzhe</given-names></name><address><email>yzzqwq@gmail.com</email></address><xref ref-type="aff" rid="Aff1"/></contrib><contrib contrib-type="author"><name><surname>Ning</surname><given-names>Hao</given-names></name><xref ref-type="aff" rid="Aff1"/></contrib><contrib contrib-type="author"><name><surname>Tang</surname><given-names>Xiangyang</given-names></name><xref ref-type="aff" rid="Aff1"/></contrib><aff id="Aff1"><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/05x2f1m38</institution-id><institution-id institution-id-type="GRID">grid.440711.7</institution-id><institution-id institution-id-type="ISNI">0000 0004 1793 3093</institution-id><institution>Department of Software Engineering,School of Software, </institution><institution>East China Jiaotong University, </institution></institution-wrap>No. 808 Shuanggang East Street, Nanchang, 330013 Jiangxi China </aff></contrib-group><pub-date pub-type="epub"><day>30</day><month>5</month><year>2024</year></pub-date><pub-date pub-type="pmc-release"><day>30</day><month>5</month><year>2024</year></pub-date><pub-date pub-type="collection"><year>2024</year></pub-date><volume>14</volume><elocation-id>12411</elocation-id><history><date date-type="received"><day>10</day><month>3</month><year>2024</year></date><date date-type="accepted"><day>27</day><month>5</month><year>2024</year></date></history><permissions><copyright-statement>&#x000a9; The Author(s) 2024</copyright-statement><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article&#x02019;s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article&#x02019;s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p></license></permissions><abstract id="Abs1"><p id="Par1">Knowledge distillation is an effective approach for training robust multi-modal machine learning models when synchronous multimodal data are unavailable. However, traditional knowledge distillation techniques have limitations in comprehensively transferring knowledge across modalities and models. This paper proposes a multiscale knowledge distillation framework to address these limitations. Specifically, we introduce a multiscale semantic graph mapping (SGM) loss function to enable more comprehensive knowledge transfer between teacher and student networks at multiple feature scales. We also design a fusion and tuning (FT) module to fully utilize correlations within and between different data types of the same modality when training teacher networks. Furthermore, we adopt transformer-based backbones to improve feature learning compared to traditional convolutional neural networks. We apply the proposed techniques to multimodal human activity recognition and compared with the baseline method, it improved by 2.31% and 0.29% on the MMAct and UTD-MHAD datasets. Ablation studies validate the necessity of each component.</p></abstract><kwd-group xml:lang="en"><title>Keywords</title><kwd>Human activity recognition</kwd><kwd>Knowledge distillation</kwd><kwd>Multi-modalities</kwd><kwd>Self-attention</kwd><kwd>Transfer learning</kwd></kwd-group><kwd-group kwd-group-type="npg-subject"><title>Subject terms</title><kwd>Information theory and computation</kwd><kwd>Computational science</kwd><kwd>Computer science</kwd></kwd-group><funding-group><award-group><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100004479</institution-id><institution>Natural Science Foundation of Jiangxi Province</institution></institution-wrap></funding-source><award-id>20224BAB202030</award-id></award-group></funding-group><custom-meta-group><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#x000a9; Springer Nature Limited 2024</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1"><title>Introduction</title><p id="Par2">To overcome the limitations of traditional machine learning with single-modal data, adopting multimodal data machine learning has become an important method for enhancing the performance of deep learning models. However, traditional multimodal data learning models require the synchronous input of various modal data for training and prediction, which is not always possible in many real-world application scenarios. For example, in conventional monitoring applications, it is often difficult to synchronize motion sensor data with the motion images of a monitored object in real time, leading to a significant decline in the performance of multimodal models, and sometimes the accuracy is even lower than that of models trained with single-modal data.</p><p id="Par3">Correspondingly, to improve the robustness of multimodal machine learning algorithms mentioned above, knowledge distillation technology<sup><xref ref-type="bibr" rid="CR1">1</xref></sup> offers an effective solution. Generally, knowledge distillation techniques train different models with data from different modalities, defining these models as student and teacher models, respectively. During training, the student acquires knowledge from the teacher, thereby fully utilizing the knowledge from multiple modalities. The advantage of this mechanism is that during the prediction or recognition phase, the student model can utilize both its own knowledge and that learned from the teacher model to achieve predictions that are close to or even better than those with multimodal data, even in the presence of only unimodal data. Although knowledge distillation technology has been successfully applied in many fields, traditional knowledge distillation frameworks still face the following problems:<list list-type="bullet"><list-item><p id="Par4">Traditional knowledge distillation structures usually only transfer knowledge at the final feature output layer, ignoring the extraction of knowledge at intermediate feature layers of different scales. This may lead to the student network not fully learning complementary knowledge from the teacher network, which is limited in its receptive field of data features, and thus reducing the efficiency of multimodal learning.</p></list-item><list-item><p id="Par5">In traditional knowledge distillation, the data of teacher modality modality generally considers only a single data type, neglecting the fusion representation of multiple different types of data. For example, in multimodal human activity recognition (HAR), when training a teacher network with wearable sensor sampling data, the data from wearable sensors will include various physically different data such as acceleration, gyroscope, orientation, and even magnetometer readings, requiring a specific fusion mechanism to fully utilize the knowledge of correlations within these data.</p></list-item><list-item><p id="Par6">In traditional knowledge distillation, both teacher and student networks typically use convolutional neural networks (CNNs) as their backbone, but convolutional neural networks lack globality in early feature extraction. This limits the model&#x02019;s ability to extract good features, thereby restricting the system&#x02019;s ultimate performance.</p></list-item></list>To address these obstacles, we propose a new multiscale knowledge distillation architecture that overcomes the shortcomings of traditional knowledge distillation techniques, and selects human activity recognition (HAR) as the target problem. Specifically, against the backdrop of teacher networks transferring knowledge to student networks at only a single scale layer, we propose multiscale semantic graph mapping (SGM) knowledge distillation loss at different network nodes. This module uses semantic embedding for joint representation of different types of data and combines the internal graph structure of final feature generation to generate semantic change rates at corresponding scales, ultimately generating focus areas on intermediate feature maps of different modalities. We calculate the knowledge distillation loss based on the multiscale differences in the focus areas of intermediate feature maps between teacher and student networks to complete the transfer of knowledge from the teacher to student networks at different intermediate layers. This facilitates more comprehensive learning of heterogeneous knowledge by student networks from intermediate layers of teacher networks.</p><p id="Par7">Furthermore, for the joint representation of different types of data within a modality, we designed an Fusion and Tuning (FT) module. This module inputs different types of data into multiple teacher networks and fuses multiple data features of teachers within the teacher network to calibrate the original features among various teachers&#x02019; data, ensuring similarity within and correlation between different types of data. This allows for the full utilization of complementary information from different types of data, and extracting knowledge is more beneficial for learning of student networks.</p><p id="Par8">Additionally, to address the issue of incomplete feature extraction by early convolutional neural networks, we introduce a self-attention mechanism and adopt the Swin Transformer<sup><xref ref-type="bibr" rid="CR2">2</xref></sup> as the backbone network for teacher networks while introducing Video Swin Transformer<sup><xref ref-type="bibr" rid="CR3">3</xref></sup> as the backbone for student networks. The self-attention mechanism enables global analysis of relationships between data at an early stage of feature extraction, aiding in better extraction of useful information and laying a solid foundation for subsequent knowledge transfer. Finally, we constructed our model based on features from the MMAct<sup><xref ref-type="bibr" rid="CR4">4</xref></sup> and UTD-MHAD<sup><xref ref-type="bibr" rid="CR5">5</xref></sup> HAR datasets and designed extensive experiments to validate the effectiveness of our proposed structure and methods. Our results demonstrate performance improvements over other traditional knowledge distillation models and, employ corresponding ablation studies to analyze the necessity of our proposed structures. The contributions of this paper are as follows:<list list-type="bullet"><list-item><p id="Par9">We designed a new multiscale knowledge distillation loss function, semantic graph mapping (SGM) loss, based on feature outputs at different scale stages to calculate multiscale knowledge distillation loss between teacher and student networks, achieving more comprehensive knowledge transfer from teacher to student networks.</p></list-item><list-item><p id="Par10">To fully utilize knowledge from different sensor sampling data within a modality, we designed a fusion and tuning (FT) module for fusing features of different types of data and correcting features across modules to maintain similarity within single modalities and complementarity between modalities.</p></list-item><list-item><p id="Par11">We selected the HAR problem, designed extensive experiments on different datasets, and compared our model with current research findings. Our proposed model outperforms the baseline algorithm on both the MMAct and UTD-MHAD datasets. These results validate the effectiveness of the proposed architecture.</p></list-item></list></p></sec><sec id="Sec2"><title>Related work</title><sec id="Sec3"><title>Unimodal activity recognition</title><p id="Par12">Human activity recognition is an active research field<sup><xref ref-type="bibr" rid="CR6">6</xref></sup>, in which traditional methods mainly rely on a single modality and can be broadly categorized into three types: (1) Handcrafted representation-based methods<sup><xref ref-type="bibr" rid="CR7">7</xref>&#x02013;<xref ref-type="bibr" rid="CR14">14</xref></sup>. (2) Graph-based learning methods<sup><xref ref-type="bibr" rid="CR15">15</xref>&#x02013;<xref ref-type="bibr" rid="CR22">22</xref></sup>. and (3) Deep learning-based methods<sup><xref ref-type="bibr" rid="CR23">23</xref>&#x02013;<xref ref-type="bibr" rid="CR33">33</xref></sup>. It is noteworthy that these methods largely rely on visual sensor modalities such as skeletal, infrared imagery, depth maps, or RGB videos. This indicates that image-based methods are generally more intuitive in recognizing human activities. Wang et al.<sup><xref ref-type="bibr" rid="CR34">34</xref></sup> proposed a method using segment sampling and aggregation modules to model remote temporal structures, implying that when processing video data, it is unnecessary to handle all frames, and only selecting key frames sufficient. Zhou et al.<sup><xref ref-type="bibr" rid="CR35">35</xref></sup> achieved higher accuracy in video classification by learning and inferring time dependencies across multiple time scales of video frames, emphasizing the crucial role of temporal correlations between video frames in video classification. Li et al.<sup><xref ref-type="bibr" rid="CR36">36</xref></sup> introduced a spatio-temporal manifold network (STMN), which utilizes data manifold structures to regularize deep action feature learning, investigating the influence of structural properties within the data on feature learning. Luo et al.<sup><xref ref-type="bibr" rid="CR37">37</xref></sup> proposed a semantic-assisted network for video action recognition, indicating that semantic information can bridge the conceptual gap between raw video data and human behaviors. Qiao et al.<sup><xref ref-type="bibr" rid="CR38">38</xref></sup> proposed a federated learning method to solve the problem of user customization. Ren et al.<sup><xref ref-type="bibr" rid="CR39">39</xref></sup> proposed GSKG to make data acquisition more secure. Although the aforementioned methods have achieved significant advancements in segment sampling, considering temporal relations between frames, data stream structures, and semantic assistance, they fail to address issues that are challenging for images to handle, such as cluttered backgrounds, occlusions, and varying camera perspectives. Therefore, alternative approaches are needed to address these challenges.</p><p id="Par13">With the widespread adoption of wearable sensors such as smartphones and smartwatches, action recognition based on wearable sensors has become a key research direction in human activity understanding<sup><xref ref-type="bibr" rid="CR40">40</xref></sup>. Many issues inherent in vision-based approaches can be effectively addressed through wearable sensor data. This is because wearable sensor data primarily focus on capturing detailed variations in motion, unaffected by surrounding visual environmental changes. Zebhi et al.<sup><xref ref-type="bibr" rid="CR41">41</xref></sup> transformed motion sensor data into images using two-dimensional fast fourier transform (2-D FFT) and wigner-ville transform (WVT) to achieve human activity recognition, enhancing the representation capability of sensor data by converting one-dimensional time-series signals into images. To improve accuracy, Alghamdi et al.<sup><xref ref-type="bibr" rid="CR42">42</xref></sup> applied a multi-head attention mechanism to realize two-level interaction based on inertial sensors. Shi et al.<sup><xref ref-type="bibr" rid="CR43">43</xref></sup> first learned features from different sensors and then used Transformer encoders to model dependencies between multisensor data and extract features. Both approaches indicate that sensor data between different modalities contain complementary information that can be leveraged. Wannenburg et al.<sup><xref ref-type="bibr" rid="CR44">44</xref></sup> utilized ten different classification algorithms to classify human behaviors using accelerometer signals captured by smartphones. Unlike visual sensor-based approaches, the data format of wearable sensor data is typically a one-dimensional time series. Compared to image data, wearable sensor data inherently lack color, texture, and other essential information. This makes it challenging to accurately identify certain specific actions that require visual resolution, such as making a fist and waving versus opening the palm and waving.</p><p id="Par14">Combining the characteristics of the above methods, to enhance the representation capability of wearable sensor data features, we adopted gramian angular field<sup><xref ref-type="bibr" rid="CR45">45</xref></sup> (GAF) technique to transform one-dimensional time-series signals into RGB images. To fully utilize the feature representation capability of different sensor modalities, we fused and corrected features from different sensors during the intermediate feature extraction process to maintain similarity between different modalities.</p></sec><sec id="Sec4"><title>Multimodal activity recognition</title><p id="Par15">While single-modality action recognition methods have achieved decent recognition accuracy, there are still many limitations, necessitating richer data to enhance model robustness. With the development of hardware devices such as deep learning device, cameras, and wearable devices, an increasing number of multimodal action recognition methods have emerged in the field of human motion recognition. Chen et al.<sup><xref ref-type="bibr" rid="CR46">46</xref></sup> directly input data from different modalities into a Transformer as sequences for action recognition. Zhong et al.<sup><xref ref-type="bibr" rid="CR47">47</xref></sup> utilized skeletal and depth data to design a motion co-occurrence spatial feature model, integrating information from various joints and proposing a quantification standard to measure the contribution of each joint movement to the recognition results. Bruce et al.<sup><xref ref-type="bibr" rid="CR48">48</xref></sup> devised a model-level multimodal fusion approach, employing spatiotemporal graph convolutional networks as the skeletal modality to learn how to transfer attention weights from the skeletal modality to the RGB modality network. Shaikh et al.<sup><xref ref-type="bibr" rid="CR49">49</xref></sup> proposed a meaningful representation extraction method for image and audio and fused it with video representation, demonstrating better performance in activity recognition compared to single-modality audio and video. The first two methods primarily rely on multihead self-attention mechanism-based multimodal recognition methods, enabling the global learning of relationships between different modalities. The latter two methods employ traditional convolutional neural network methods, with Bruce separately learning knowledge from different models for each modality and then combining it, while Shaikh transfers knowledge learned from audio to RGB videos using attention weights.</p><p id="Par16">Unlike the aforementioned multimodal methods, we adopt a knowledge distillation architecture to leverage multimodal data, allowing the teacher network to transfer knowledge to the student network. Ultimately, only inputting data from one modality can yield results approximating those from multimodal data.</p></sec><sec id="Sec5"><title>Knowledge distillation in HAR</title><p id="Par17">
<fig id="Fig1"><label>Figure 1</label><caption><p>The workflow of the proposed framework. Sensor data is transformed into virtual images through GAF and serves as input to the teacher network. The student network takes video frame sequences as input. Both the teacher and student networks consist of four stages, with knowledge distillation conducted between the feature maps outputted at each stage.</p></caption><graphic xlink:href="41598_2024_63195_Fig1_HTML" id="MO1"/></fig>
</p><p id="Par18">Knowledge distillation is a general technique used to supervise the training of student networks by capturing and transferring useful knowledge from well-trained teacher networks. Hinton et al.<sup><xref ref-type="bibr" rid="CR1">1</xref></sup> employed softened labels with temperature to transfer knowledge to smaller student networks. Ni et al.<sup><xref ref-type="bibr" rid="CR46">46</xref></sup> proposed a novel progressive Skeleton sensor knowledge distillation (PSKD) model, which combines acceleration with the corresponding skeleton positions for better representation. Xu et al.<sup><xref ref-type="bibr" rid="CR48">48</xref></sup> introduced a novel contrastive distillation with regularization knowledge (ConDRK) framework, which can significantly eliminate biases introduced by teachers. Deng et al.<sup><xref ref-type="bibr" rid="CR47">47</xref></sup> presented a lightweight human activity recognition (LHAR) knowledge distillation framework, allowing the model to balance performance and complexity through multiple depth-wise separable convolutions. Park et al.<sup><xref ref-type="bibr" rid="CR50">50</xref></sup> introduced distance and angle distillation losses to achieve relational knowledge transfer. Tung et al.<sup><xref ref-type="bibr" rid="CR51">51</xref></sup> constructed a knowledge distillation loss, the constraint of which is to generate similar (dissimilar) activations in the teacher network corresponding to inputs that should generate similar (dissimilar) activations in the student network. Crasto et al.<sup><xref ref-type="bibr" rid="CR52">52</xref></sup> introduced feature-based losses compared to flow, specifically a linear combination of feature-based loss and standard cross-entropy loss, to mimic motion flow and avoid flow computation during testing.</p><p id="Par19">Unlike traditional knowledge distillation, between the teacher and student networks, we designed a knowledge distillation loss based on semantic graph mapping, which allows the student network to learn heterogeneous knowledge from the teacher network, enabling single-video data features to learn multimodal data features and possess more feature representations, thereby improving the activity recognition performance of the student network.</p></sec></sec><sec id="Sec6"><title>Methods</title><sec id="Sec7"><title>Overview</title><p id="Par20">Overall, we propose a knowledge distillation architecture with multiple teacher networks and a single student network, as illustrated in Fig. <xref rid="Fig1" ref-type="fig">1</xref>. This architecture first learns modal knowledge from wearable sensors through pre-training the teacher networks. Subsequently, the acquired knowledge is transferred to the student network through knowledge distillation, guiding the training of the student network. Finally, the student network achieves a performance similar to that of inputting multimodal data when only video data is provided.</p><p id="Par21">In Fig. <xref rid="Fig1" ref-type="fig">1</xref>, we use virtual images generated from one-dimensional time-series data from wearable sensors as inputs to the teacher network. Multiple teacher networks correspond to different types of wearable sensor data. The input to the student network is a sequence of 32 frame images constituting a video frame sequence.</p><p id="Par22">Both the teacher and student networks consist of four stages. After each stage, both modal networks obtain corresponding feature maps. The teacher network transfers heterogeneous knowledge from intermediate layers to the student network through the semantic graph mapping (SGM) module and calculates semantic loss (SE Loss) and soft target loss (ST Loss) at the final stage for training the student network.<fig id="Fig2"><label>Figure 2</label><caption><p>In the teacher network, virtual images are processed using Swin Transformer blocks, while in the student network, video data is processed using Video Swin Transformer blocks. Within the teacher network, features from different types of sensor data are fused and adjusted using the fusion and tuning (FT) module. The teacher network transfers heterogeneous knowledge to the student network through the semantic graph mapping (SGM) module.</p></caption><graphic xlink:href="41598_2024_63195_Fig2_HTML" id="MO2"/></fig></p></sec><sec id="Sec8"><title>Details of stage</title><p id="Par23">The Fig. <xref rid="Fig2" ref-type="fig">2</xref> provides detailed specifics of each stage. For image recognition, most traditional methods rely on convolutional neural networks (CNNs). However, CNNs have the drawback of a relatively small receptive field during the early stages of feature extraction may lead to the inability of the model to effectively identify detailed features in images at the beginning. Therefore, in the teacher network, we adopt the feature extraction method of the Swin Transformer (Swin), which achieves global understanding through self-attention mechanisms and window-based attention mechanisms that CNNs lack.</p><p id="Par24">For feature extraction in videos, traditional recurrent neural networks (RNNs) or long short-term memory networks (LSTMs) suffer from problems such as forgetting long-term frame relationships, making it difficult to focus on long-term temporal relationships. In the student network, we introduce video swin transformer (Video Swin), which simultaneously considers both temporal and spatial dimensions. This helps the model systematically extract motion information from videos. Both approaches are based on Swin, providing inherent compatibility between them.</p><p id="Par25">Next, we briefly introduce the working principle of Swin Transformer. Given an image <inline-formula id="IEq1"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$I \in \mathbb {R}^{h\times w\times 3}$$\end{document}</tex-math><mml:math id="M2"><mml:mrow><mml:mi>I</mml:mi><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>w</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_63195_Article_IEq1.gif"/></alternatives></inline-formula>, we first divide it into patches of size <inline-formula id="IEq2"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$p$$\end{document}</tex-math><mml:math id="M4"><mml:mi>p</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_63195_Article_IEq2.gif"/></alternatives></inline-formula>, and then map each patch to dimensions recognizable by Swin Transformer, denoted as <inline-formula id="IEq3"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$x \in \mathbb {R}^{H \times W \times C}$$\end{document}</tex-math><mml:math id="M6"><mml:mrow><mml:mi>x</mml:mi><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>W</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_63195_Article_IEq3.gif"/></alternatives></inline-formula>, where <inline-formula id="IEq4"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$H=h/p$$\end{document}</tex-math><mml:math id="M8"><mml:mrow><mml:mi>H</mml:mi><mml:mo>=</mml:mo><mml:mi>h</mml:mi><mml:mo stretchy="false">/</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_63195_Article_IEq4.gif"/></alternatives></inline-formula>, <inline-formula id="IEq5"><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$W=w/p$$\end{document}</tex-math><mml:math id="M10"><mml:mrow><mml:mi>W</mml:mi><mml:mo>=</mml:mo><mml:mi>w</mml:mi><mml:mo stretchy="false">/</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_63195_Article_IEq5.gif"/></alternatives></inline-formula>, and <inline-formula id="IEq6"><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$C=C\times p^2$$\end{document}</tex-math><mml:math id="M12"><mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mi>C</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:msup><mml:mi>p</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_63195_Article_IEq6.gif"/></alternatives></inline-formula>. Except for the first stage, each stage begins with a patch merging operation to obtain the input for the next Stage Block, denoted as <inline-formula id="IEq7"><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$x^i \in \mathbb {R}^{H/2\times W/2\times 2C}$$\end{document}</tex-math><mml:math id="M14"><mml:mrow><mml:msup><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msup><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo stretchy="false">/</mml:mo><mml:mn>2</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mi>W</mml:mi><mml:mo stretchy="false">/</mml:mo><mml:mn>2</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>2</mml:mn><mml:mi>C</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_63195_Article_IEq7.gif"/></alternatives></inline-formula>, where <inline-formula id="IEq8"><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$i$$\end{document}</tex-math><mml:math id="M16"><mml:mi>i</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_63195_Article_IEq8.gif"/></alternatives></inline-formula> represents the <inline-formula id="IEq9"><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$i$$\end{document}</tex-math><mml:math id="M18"><mml:mi>i</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_63195_Article_IEq9.gif"/></alternatives></inline-formula>-th stage, <inline-formula id="IEq10"><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$i&#x0003e;1$$\end{document}</tex-math><mml:math id="M20"><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#x0003e;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_63195_Article_IEq10.gif"/></alternatives></inline-formula>, and <inline-formula id="IEq11"><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$H,W,C$$\end{document}</tex-math><mml:math id="M22"><mml:mrow><mml:mi>H</mml:mi><mml:mo>,</mml:mo><mml:mi>W</mml:mi><mml:mo>,</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_63195_Article_IEq11.gif"/></alternatives></inline-formula> are from the previous stage. After several stages, we obtain our final output.</p><p id="Par26">To leverage the complementarity and consistency among different types of sensor data, we design the Fusion and Tuning (FT) module in the first three stages of the teacher network. This module is used to fuse and adjust features from different types of sensors. First, in the teacher network, we map the virtual images from wearable sensors to feature embeddings of dimensionality through patch embedding. Then, we extract features from different types of data using Swin blocks. Subsequently, through the FT module, we fuse the features from different types of sensors and adjust the original features of different types of data. This process helps us maintain consistency within the data while incorporating complementary knowledge from different types of data.</p><p id="Par27">Since traditional knowledge distillation only transfers knowledge at the final feature output layer, neglecting the rich feature knowledge in intermediate layers, we propose the SGM module to transfer heterogeneous knowledge from the intermediate layer of the teacher network to the student network. In the student network, we first map the video frame sequence to feature embeddings of dimensionality through patch embedding. Then, we perform knowledge distillation between the corrected features from the teacher network and the features of the student network through the SGM module. Subsequently, the feature maps of each network undergo patch merging before entering the next block. Finally, SE loss and ST loss calculations are performed on the output features of the last stage to train the student network.</p><p id="Par28">We seamlessly integrate the FT module and the SGM module with the two Swin architectures and transfer heterogeneous knowledge through knowledge distillation. In the final implementation, we achieve both approximate multimodal effects and maintain high accuracy in the absence of data.</p></sec><sec id="Sec9"><title>Virtual image generation</title><p id="Par29">The raw sensor data typically only contains motion information, thus concealing time-related details. Relying solely on raw data for analysis overlooks the correlation of each time step, which is not conducive to the final prediction.To preserve local temporal relationships in one-dimensional time series data from wearable sensors and better convey their content to RGB video data, we employed gramian angular field (GAF) to convert the one-dimensional time series data into two-dimensional, three-channel color images.</p><p id="Par30">Since wearable sensor data contain time series signals from three axes (<italic>x</italic>,&#x000a0;<italic>y</italic>,&#x000a0;<italic>z</italic>), we represent the signal data from one axis as <inline-formula id="IEq12"><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$X=\{x_1, x_2,..., x_n\}$$\end{document}</tex-math><mml:math id="M24"><mml:mrow><mml:mi>X</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">{</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_63195_Article_IEq12.gif"/></alternatives></inline-formula>. Then, we normalize the original signal <italic>X</italic> using the min-max normalization method to scale it within the range of <inline-formula id="IEq13"><alternatives><tex-math id="M25">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$[-1, 1]$$\end{document}</tex-math><mml:math id="M26"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_63195_Article_IEq13.gif"/></alternatives></inline-formula>, obtaining the normalized signal <inline-formula id="IEq14"><alternatives><tex-math id="M27">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\hat{X}$$\end{document}</tex-math><mml:math id="M28"><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="41598_2024_63195_Article_IEq14.gif"/></alternatives></inline-formula>:<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M29">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} \hat{X}=\frac{(X-\text {max}(X))+(X-\text {min}(X))}{\text {max}(X)-\text {min}(X)} \end{aligned}$$\end{document}</tex-math><mml:math id="M30" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:mo>-</mml:mo><mml:mtext>max</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:mo>-</mml:mo><mml:mtext>min</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mtext>max</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>-</mml:mo><mml:mtext>min</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_63195_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula>Next, the normalized signal <inline-formula id="IEq15"><alternatives><tex-math id="M31">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\hat{X}$$\end{document}</tex-math><mml:math id="M32"><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="41598_2024_63195_Article_IEq15.gif"/></alternatives></inline-formula> is represented in polar coordinates using a transformation function <italic>f</italic>. Specifically, this involves using the timestamp as the radial coordinate <inline-formula id="IEq16"><alternatives><tex-math id="M33">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\rho$$\end{document}</tex-math><mml:math id="M34"><mml:mi>&#x003c1;</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_63195_Article_IEq16.gif"/></alternatives></inline-formula>, and expressing the normalized signal <inline-formula id="IEq17"><alternatives><tex-math id="M35">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\hat{X}$$\end{document}</tex-math><mml:math id="M36"><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="41598_2024_63195_Article_IEq17.gif"/></alternatives></inline-formula> as the polar angle <inline-formula id="IEq18"><alternatives><tex-math id="M37">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\theta$$\end{document}</tex-math><mml:math id="M38"><mml:mi>&#x003b8;</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_63195_Article_IEq18.gif"/></alternatives></inline-formula> in the range <inline-formula id="IEq19"><alternatives><tex-math id="M39">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$[0, \pi ]$$\end{document}</tex-math><mml:math id="M40"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>&#x003c0;</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_63195_Article_IEq19.gif"/></alternatives></inline-formula>. Ultimately, the one-dimensional normalized signal <inline-formula id="IEq20"><alternatives><tex-math id="M41">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\hat{X}$$\end{document}</tex-math><mml:math id="M42"><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="41598_2024_63195_Article_IEq20.gif"/></alternatives></inline-formula> is transformed into two-dimensional polar coordinates <inline-formula id="IEq21"><alternatives><tex-math id="M43">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$(\rho , \theta )$$\end{document}</tex-math><mml:math id="M44"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>&#x003c1;</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003b8;</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_63195_Article_IEq21.gif"/></alternatives></inline-formula>:<disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M45">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} f(x_i,t_i,N)= {\left\{ \begin{array}{ll} \theta _i= \arccos (\hat{x_i}), &#x00026;{}x_i\in \hat{X} \\ \rho _i = \frac{t_i}{N} \end{array}\right. } \end{aligned}$$\end{document}</tex-math><mml:math id="M46" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfenced open="{"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo>arccos</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mrow/><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x02208;</mml:mo><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mrow/><mml:msub><mml:mi>&#x003c1;</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mi>N</mml:mi></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_63195_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula>Where <inline-formula id="IEq22"><alternatives><tex-math id="M47">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\hat{x_i}$$\end{document}</tex-math><mml:math id="M48"><mml:mover accent="true"><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="41598_2024_63195_Article_IEq22.gif"/></alternatives></inline-formula> represents the normalized temporal signal, <inline-formula id="IEq23"><alternatives><tex-math id="M49">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$t_i$$\end{document}</tex-math><mml:math id="M50"><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_63195_Article_IEq23.gif"/></alternatives></inline-formula> denotes the temporal position of the signal, and <italic>N</italic> represents the number of data points in the temporal signal. After encoding the one-dimensional time series signal into polar coordinates, the calculation of the sum of triangles between points facilitates the easy derivation of the correlation coefficient between two points within local time. As correlation coefficients can be computed using the cosine of the angle between vectors, the correlation between time <italic>i</italic> and time <italic>j</italic> can be expressed as <inline-formula id="IEq24"><alternatives><tex-math id="M51">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\cos (\theta _i + \theta _j)$$\end{document}</tex-math><mml:math id="M52"><mml:mrow><mml:mo>cos</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_63195_Article_IEq24.gif"/></alternatives></inline-formula> . Consequently, we obtain the GAF correlation matrix <italic>G</italic>:<disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="M53">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} G=\begin{pmatrix} \cos (\theta _1 + \theta _2) &#x00026;{} \dots &#x00026;{} \cos (\theta _1 + \theta _n) \\ \vdots &#x00026;{} \ddots &#x00026;{} \vdots \\ \cos (\theta _n + \theta _1) &#x00026;{} \dots &#x00026;{} \cos (\theta _n + \theta _n) \end{pmatrix} \end{aligned}$$\end{document}</tex-math><mml:math id="M54" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>G</mml:mi><mml:mo>=</mml:mo><mml:mfenced close=")" open="("><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mo>cos</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mrow/><mml:mo>&#x022ef;</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mrow/><mml:mo>cos</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mrow/><mml:mo>&#x022ee;</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mrow/><mml:mo>&#x022f1;</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mrow/><mml:mo>&#x022ee;</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mrow/><mml:mo>cos</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mrow/><mml:mo>&#x022ef;</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mrow/><mml:mo>cos</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_63195_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula>Since <inline-formula id="IEq25"><alternatives><tex-math id="M55">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\cos (\theta )$$\end{document}</tex-math><mml:math id="M56"><mml:mrow><mml:mo>cos</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>&#x003b8;</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_63195_Article_IEq25.gif"/></alternatives></inline-formula> is uniquely determined within the interval <inline-formula id="IEq26"><alternatives><tex-math id="M57">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\theta \in [0, \pi ]$$\end{document}</tex-math><mml:math id="M58"><mml:mrow><mml:mi>&#x003b8;</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>&#x003c0;</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_63195_Article_IEq26.gif"/></alternatives></inline-formula>, this implies that the method can retain the characteristics of the original data. By computing the correlation matrix, it is possible to augment the local temporal relationships lacking in the original data.</p><p id="Par31">This approach can preserve local temporal relationships in the form of temporal correlations as the number of timestamps increases. The wearable sensor data is formatted as three-axis time series (x, y, and z axes), assuming a time length of <italic>n</italic> for each axis. Using the aforementioned method, we obtain a <inline-formula id="IEq27"><alternatives><tex-math id="M59">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$n \times n$$\end{document}</tex-math><mml:math id="M60"><mml:mrow><mml:mi>n</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_63195_Article_IEq27.gif"/></alternatives></inline-formula> GAF matrix for each axis. Subsequently, the GAF matrices of the three axes are concatenated along the channel dimension to form a three-channel image of size <inline-formula id="IEq28"><alternatives><tex-math id="M61">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$n \times n \times 3$$\end{document}</tex-math><mml:math id="M62"><mml:mrow><mml:mi>n</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>n</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_63195_Article_IEq28.gif"/></alternatives></inline-formula>, denoted as <inline-formula id="IEq29"><alternatives><tex-math id="M63">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$P = \{ G_x, G_y, G_z \}$$\end{document}</tex-math><mml:math id="M64"><mml:mrow><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">{</mml:mo><mml:msub><mml:mi>G</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>G</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>G</mml:mi><mml:mi>z</mml:mi></mml:msub><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_63195_Article_IEq29.gif"/></alternatives></inline-formula>.</p><p id="Par32">Through this method, we successfully convert one-dimensional time series data into a two-dimensional image format, followed by the stacking of the images of the three axes to obtain the final three-dimensional image format. This RGB image generated by encoding sensor data serves as the input to the teacher network.<fig id="Fig3"><label>Figure 3</label><caption><p>Fusion and tuning (FT) module in the teacher network. A and B represent the output features after the stage layer in the network. (<bold>a</bold>) Intra-modality global context modeling: constructing global models for feature maps to obtain global information of the feature maps. (<bold>b</bold>) Feature Fusion and Adaptive: fusion of different types of global feature information in multiple ways and using it to calibrate the original feature maps for subsequent feature extraction.</p></caption><graphic xlink:href="41598_2024_63195_Fig3_HTML" id="MO3"/></fig></p></sec><sec id="Sec10"><title>Fusion and tuning</title><p id="Par33">In the structure of the teacher network, we employ the Swin Transformer (Swin) as the feature extractor. Due to its utilization of global attention mechanisms, the Swin Transformer can better capture the global relationships within the data compared to traditional convolutional neural networks. However, for different types of sensor data, using the same backbone will result in different attention feature matrices. Existing attention mechanisms fail to adequately focus on the correlations between features from different types of sensors, often overlooking such correlations during multi-source feature fusion. To fully exploit the correlation information among different types of data, we integrate the similarity within features and the correlation knowledge between multiple sensor features between each Swin-Stage module. Subsequently, the fused activation signals are used to adjust the features of various sensor types. The similarity fusion module between two types of sensors is illustrated in Fig. <xref rid="Fig3" ref-type="fig">3</xref>.</p><sec id="Sec11"><title>Intra-source global context modeling</title><p id="Par34">Incorporating the FT module into each stage module of Swin for feature fusion of multi-source data.Suppose there are <italic>m</italic> types (e.g. different types of sensor data), and the backbone network consists of <italic>s</italic> Stage modules. Given an input batch size of <italic>B</italic>, the output feature maps <inline-formula id="IEq30"><alternatives><tex-math id="M65">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$F_m^s$$\end{document}</tex-math><mml:math id="M66"><mml:msubsup><mml:mi>F</mml:mi><mml:mi>m</mml:mi><mml:mi>s</mml:mi></mml:msubsup></mml:math><inline-graphic xlink:href="41598_2024_63195_Article_IEq30.gif"/></alternatives></inline-formula> of different types of data <italic>m</italic> at a specific Stage block <italic>s</italic> in the network are represented as <inline-formula id="IEq31"><alternatives><tex-math id="M67">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$F_m^s\in \mathbb {R}^{B\times L\times C}$$\end{document}</tex-math><mml:math id="M68"><mml:mrow><mml:msubsup><mml:mi>F</mml:mi><mml:mi>m</mml:mi><mml:mi>s</mml:mi></mml:msubsup><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>L</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_63195_Article_IEq31.gif"/></alternatives></inline-formula>, where <italic>B</italic> is the batch size, <italic>L</italic> is the number of patches, and <italic>C</italic> is the feature dimension per patch. Inspired by attention-based knowledge transfer methods<sup><xref ref-type="bibr" rid="CR51">51</xref>,<xref ref-type="bibr" rid="CR53">53</xref></sup>, this approach utilizes activation correlation for knowledge transfer. First, reshape the feature maps <inline-formula id="IEq32"><alternatives><tex-math id="M69">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$F_m^s$$\end{document}</tex-math><mml:math id="M70"><mml:msubsup><mml:mi>F</mml:mi><mml:mi>m</mml:mi><mml:mi>s</mml:mi></mml:msubsup></mml:math><inline-graphic xlink:href="41598_2024_63195_Article_IEq32.gif"/></alternatives></inline-formula> into <inline-formula id="IEq33"><alternatives><tex-math id="M71">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$R_m^s\in \mathbb {R}^{B\times L\cdot C}$$\end{document}</tex-math><mml:math id="M72"><mml:mrow><mml:msubsup><mml:mi>R</mml:mi><mml:mi>m</mml:mi><mml:mi>s</mml:mi></mml:msubsup><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>L</mml:mi><mml:mo>&#x000b7;</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_63195_Article_IEq33.gif"/></alternatives></inline-formula>, then perform matrix multiplication with its transpose and L2 normalization to obtain the similarity matrix within each type of data, denoted as <inline-formula id="IEq34"><alternatives><tex-math id="M73">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$G_m^s\in \mathbb {R}^{B\times B}$$\end{document}</tex-math><mml:math id="M74"><mml:mrow><mml:msubsup><mml:mi>G</mml:mi><mml:mi>m</mml:mi><mml:mi>s</mml:mi></mml:msubsup><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_63195_Article_IEq34.gif"/></alternatives></inline-formula>.<disp-formula id="Equ4"><label>4</label><alternatives><tex-math id="M75">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} \hat{R}_m^s= &#x00026; {} R_m^s \times R_m^{s\mathsf T} \end{aligned}$$\end{document}</tex-math><mml:math id="M76" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi>R</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>m</mml:mi><mml:mi>s</mml:mi></mml:msubsup><mml:mo>=</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mrow/><mml:msubsup><mml:mi>R</mml:mi><mml:mi>m</mml:mi><mml:mi>s</mml:mi></mml:msubsup><mml:mo>&#x000d7;</mml:mo><mml:msubsup><mml:mi>R</mml:mi><mml:mi>m</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi mathvariant="sans-serif">T</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_63195_Article_Equ4.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ5"><label>5</label><alternatives><tex-math id="M77">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} G_m^s= &#x00026; {} \frac{\hat{R}_m^s}{ \begin{Vmatrix} \hat{R}_m^s \end{Vmatrix}_2} \end{aligned}$$\end{document}</tex-math><mml:math id="M78" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msubsup><mml:mi>G</mml:mi><mml:mi>m</mml:mi><mml:mi>s</mml:mi></mml:msubsup><mml:mo>=</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mrow/><mml:mfrac><mml:msubsup><mml:mover accent="true"><mml:mi>R</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>m</mml:mi><mml:mi>s</mml:mi></mml:msubsup><mml:msub><mml:mfenced close="&#x02225;" open="&#x02225;"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:msubsup><mml:mover accent="true"><mml:mi>R</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>m</mml:mi><mml:mi>s</mml:mi></mml:msubsup></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced><mml:mn>2</mml:mn></mml:msub></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_63195_Article_Equ5.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq35"><alternatives><tex-math id="M79">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{Vmatrix} \cdot \end{Vmatrix}_2$$\end{document}</tex-math><mml:math id="M80"><mml:msub><mml:mfenced close="&#x02225;" open="&#x02225;"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mo>&#x000b7;</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced><mml:mn>2</mml:mn></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_63195_Article_IEq35.gif"/></alternatives></inline-formula> represents the L2 norm. After obtaining the similarity matrix within the data, we use it to guide the construction of global context within the data. First, we employ one-dimensional adaptive average pooling to compress the feature maps outputted by the Stage, obtaining the compressed feature vectors <inline-formula id="IEq36"><alternatives><tex-math id="M81">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$V_m^s\in \mathbb {R}^{B\times C}$$\end{document}</tex-math><mml:math id="M82"><mml:mrow><mml:msubsup><mml:mi>V</mml:mi><mml:mi>m</mml:mi><mml:mi>s</mml:mi></mml:msubsup><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_63195_Article_IEq36.gif"/></alternatives></inline-formula>. Then, we perform matrix multiplication between the similarity matrix <inline-formula id="IEq37"><alternatives><tex-math id="M83">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$G_m^s$$\end{document}</tex-math><mml:math id="M84"><mml:msubsup><mml:mi>G</mml:mi><mml:mi>m</mml:mi><mml:mi>s</mml:mi></mml:msubsup></mml:math><inline-graphic xlink:href="41598_2024_63195_Article_IEq37.gif"/></alternatives></inline-formula> and the compressed feature vectors <inline-formula id="IEq38"><alternatives><tex-math id="M85">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$V_m^s$$\end{document}</tex-math><mml:math id="M86"><mml:msubsup><mml:mi>V</mml:mi><mml:mi>m</mml:mi><mml:mi>s</mml:mi></mml:msubsup></mml:math><inline-graphic xlink:href="41598_2024_63195_Article_IEq38.gif"/></alternatives></inline-formula> to obtain the global context information within the modality <inline-formula id="IEq39"><alternatives><tex-math id="M87">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$S_m^s \in \mathbb {R}^{B\times C}$$\end{document}</tex-math><mml:math id="M88"><mml:mrow><mml:msubsup><mml:mi>S</mml:mi><mml:mi>m</mml:mi><mml:mi>s</mml:mi></mml:msubsup><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_63195_Article_IEq39.gif"/></alternatives></inline-formula>, with the calculation formula as follows:<disp-formula id="Equ6"><label>6</label><alternatives><tex-math id="M89">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} V_m^s(B,C)=\frac{1}{L}\sum _{i=1}^{L}F_m^s(B,i,C) \end{aligned}$$\end{document}</tex-math><mml:math id="M90" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msubsup><mml:mi>V</mml:mi><mml:mi>m</mml:mi><mml:mi>s</mml:mi></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>B</mml:mi><mml:mo>,</mml:mo><mml:mi>C</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>L</mml:mi></mml:mfrac><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>L</mml:mi></mml:munderover><mml:msubsup><mml:mi>F</mml:mi><mml:mi>m</mml:mi><mml:mi>s</mml:mi></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>B</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>C</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_63195_Article_Equ6.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ7"><label>7</label><alternatives><tex-math id="M91">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} S_m^s=G_m^s \times V_m^s \end{aligned}$$\end{document}</tex-math><mml:math id="M92" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msubsup><mml:mi>S</mml:mi><mml:mi>m</mml:mi><mml:mi>s</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>G</mml:mi><mml:mi>m</mml:mi><mml:mi>s</mml:mi></mml:msubsup><mml:mo>&#x000d7;</mml:mo><mml:msubsup><mml:mi>V</mml:mi><mml:mi>m</mml:mi><mml:mi>s</mml:mi></mml:msubsup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_63195_Article_Equ7.gif" position="anchor"/></alternatives></disp-formula></p></sec><sec id="Sec12"><title>Feature fusion and tuning</title><p id="Par35">The global context information <inline-formula id="IEq40"><alternatives><tex-math id="M93">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$S_m^s$$\end{document}</tex-math><mml:math id="M94"><mml:msubsup><mml:mi>S</mml:mi><mml:mi>m</mml:mi><mml:mi>s</mml:mi></mml:msubsup></mml:math><inline-graphic xlink:href="41598_2024_63195_Article_IEq40.gif"/></alternatives></inline-formula> include similarity features within the data, which can effectively reflect the characteristics of each type of data. To fully exploit the correlation and complementary information between different types of data, we employ three joint representation forms for the global context information <inline-formula id="IEq41"><alternatives><tex-math id="M95">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$S_m^s$$\end{document}</tex-math><mml:math id="M96"><mml:msubsup><mml:mi>S</mml:mi><mml:mi>m</mml:mi><mml:mi>s</mml:mi></mml:msubsup></mml:math><inline-graphic xlink:href="41598_2024_63195_Article_IEq41.gif"/></alternatives></inline-formula> across different types: concatenation, summation, and Hadamard product. These methods have been proven to be effective in previous studies<sup><xref ref-type="bibr" rid="CR54">54</xref></sup>. Below are the formulas for generating the three joint representation activation signals:<disp-formula id="Equ8"><label>8</label><alternatives><tex-math id="M97">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} E_{con}^s= &#x00026; {} W_{con}^s[S_1^s, \dots ,S_m^s]+b_{con}^s \end{aligned}$$\end{document}</tex-math><mml:math id="M98" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msubsup><mml:mi>E</mml:mi><mml:mrow><mml:mi mathvariant="italic">con</mml:mi></mml:mrow><mml:mi>s</mml:mi></mml:msubsup><mml:mo>=</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mrow/><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi mathvariant="italic">con</mml:mi></mml:mrow><mml:mi>s</mml:mi></mml:msubsup><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msubsup><mml:mi>S</mml:mi><mml:mn>1</mml:mn><mml:mi>s</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:mo>&#x022ef;</mml:mo><mml:mo>,</mml:mo><mml:msubsup><mml:mi>S</mml:mi><mml:mi>m</mml:mi><mml:mi>s</mml:mi></mml:msubsup><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msubsup><mml:mi>b</mml:mi><mml:mrow><mml:mi mathvariant="italic">con</mml:mi></mml:mrow><mml:mi>s</mml:mi></mml:msubsup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_63195_Article_Equ8.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ9"><label>9</label><alternatives><tex-math id="M99">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} E_{sum}^s= &#x00026; {} W_{add}^s \left( \sum _{i=1}^m S_i^s \right) +b_{sum}^s \end{aligned}$$\end{document}</tex-math><mml:math id="M100" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msubsup><mml:mi>E</mml:mi><mml:mrow><mml:mi mathvariant="italic">sum</mml:mi></mml:mrow><mml:mi>s</mml:mi></mml:msubsup><mml:mo>=</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mrow/><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi mathvariant="italic">add</mml:mi></mml:mrow><mml:mi>s</mml:mi></mml:msubsup><mml:mfenced close=")" open="("><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>m</mml:mi></mml:munderover><mml:msubsup><mml:mi>S</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi></mml:msubsup></mml:mfenced><mml:mo>+</mml:mo><mml:msubsup><mml:mi>b</mml:mi><mml:mrow><mml:mi mathvariant="italic">sum</mml:mi></mml:mrow><mml:mi>s</mml:mi></mml:msubsup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_63195_Article_Equ9.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ10"><label>10</label><alternatives><tex-math id="M101">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} E_{had}^s= &#x00026; {} W_{had}^s \left( \prod _{i=1}^m S_i^s \right) +b_{had}^s \end{aligned}$$\end{document}</tex-math><mml:math id="M102" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msubsup><mml:mi>E</mml:mi><mml:mrow><mml:mi mathvariant="italic">had</mml:mi></mml:mrow><mml:mi>s</mml:mi></mml:msubsup><mml:mo>=</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mrow/><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi mathvariant="italic">had</mml:mi></mml:mrow><mml:mi>s</mml:mi></mml:msubsup><mml:mfenced close=")" open="("><mml:munderover><mml:mo>&#x0220f;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>m</mml:mi></mml:munderover><mml:msubsup><mml:mi>S</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi></mml:msubsup></mml:mfenced><mml:mo>+</mml:mo><mml:msubsup><mml:mi>b</mml:mi><mml:mrow><mml:mi mathvariant="italic">had</mml:mi></mml:mrow><mml:mi>s</mml:mi></mml:msubsup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_63195_Article_Equ10.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq42"><alternatives><tex-math id="M103">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$[\cdot ,\cdot ]$$\end{document}</tex-math><mml:math id="M104"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mo>&#x000b7;</mml:mo><mml:mo>,</mml:mo><mml:mo>&#x000b7;</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_63195_Article_IEq42.gif"/></alternatives></inline-formula> represents concatenation operation, <inline-formula id="IEq43"><alternatives><tex-math id="M105">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\prod _{i=1}^m$$\end{document}</tex-math><mml:math id="M106"><mml:msubsup><mml:mo>&#x0220f;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>m</mml:mi></mml:msubsup></mml:math><inline-graphic xlink:href="41598_2024_63195_Article_IEq43.gif"/></alternatives></inline-formula> denotes element-wise multiplication (Hadamard product) operation on the global context information, <italic>W</italic> and <italic>b</italic> are learnable parameters, <inline-formula id="IEq44"><alternatives><tex-math id="M107">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$[W_{con}^s,b_{con}^s],[W_{sum}^s,b_{sum}^s],[W_{had}^s,b_{had}^s]$$\end{document}</tex-math><mml:math id="M108"><mml:mrow><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi mathvariant="italic">con</mml:mi></mml:mrow><mml:mi>s</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>b</mml:mi><mml:mrow><mml:mi mathvariant="italic">con</mml:mi></mml:mrow><mml:mi>s</mml:mi></mml:msubsup><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi mathvariant="italic">sum</mml:mi></mml:mrow><mml:mi>s</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>b</mml:mi><mml:mrow><mml:mi mathvariant="italic">sum</mml:mi></mml:mrow><mml:mi>s</mml:mi></mml:msubsup><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi mathvariant="italic">had</mml:mi></mml:mrow><mml:mi>s</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>b</mml:mi><mml:mrow><mml:mi mathvariant="italic">had</mml:mi></mml:mrow><mml:mi>s</mml:mi></mml:msubsup><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_63195_Article_IEq44.gif"/></alternatives></inline-formula> respectively represent the weights and biases of the concatenation, summation, and Hadamard product layers. These three joint representation layers scale the channel features of <inline-formula id="IEq45"><alternatives><tex-math id="M109">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$S_m^s$$\end{document}</tex-math><mml:math id="M110"><mml:msubsup><mml:mi>S</mml:mi><mml:mi>m</mml:mi><mml:mi>s</mml:mi></mml:msubsup></mml:math><inline-graphic xlink:href="41598_2024_63195_Article_IEq45.gif"/></alternatives></inline-formula> and apply the GELU activation function after the scaling operation to enhance the model&#x02019;s generalization capability.</p><p id="Par36">After obtaining the activation signals of the three aggregated global context information, we use them to correct the original feature maps to incorporate complementary information from different types. We use the GELU activation function to introduce non-linearity to the activation signals, then simply add them to obtain the fusion knowledge for correcting the original type features. The formulas for correcting different type features are as follows:<disp-formula id="Equ11"><label>11</label><alternatives><tex-math id="M111">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} \hat{F}^s_m=(GELU(E_{con}^s)+GELU(E_{sum}^s)+GELU(E_{had}^s)) \odot F_m^s \end{aligned}$$\end{document}</tex-math><mml:math id="M112" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi>F</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>m</mml:mi><mml:mi>s</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>G</mml:mi><mml:mi>E</mml:mi><mml:mi>L</mml:mi><mml:mi>U</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>E</mml:mi><mml:mrow><mml:mi mathvariant="italic">con</mml:mi></mml:mrow><mml:mi>s</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>G</mml:mi><mml:mi>E</mml:mi><mml:mi>L</mml:mi><mml:mi>U</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>E</mml:mi><mml:mrow><mml:mi mathvariant="italic">sum</mml:mi></mml:mrow><mml:mi>s</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>G</mml:mi><mml:mi>E</mml:mi><mml:mi>L</mml:mi><mml:mi>U</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>E</mml:mi><mml:mrow><mml:mi mathvariant="italic">had</mml:mi></mml:mrow><mml:mi>s</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>&#x02299;</mml:mo><mml:msubsup><mml:mi>F</mml:mi><mml:mi>m</mml:mi><mml:mi>s</mml:mi></mml:msubsup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_63195_Article_Equ11.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq46"><alternatives><tex-math id="M113">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\odot$$\end{document}</tex-math><mml:math id="M114"><mml:mo>&#x02299;</mml:mo></mml:math><inline-graphic xlink:href="41598_2024_63195_Article_IEq46.gif"/></alternatives></inline-formula> denotes element-wise multiplication operation in the <italic>C</italic> dimension. Through the FT module, we can achieve multi-source fusion of attention features and re-adjustment of features of different types, allowing different types of features to calibrate each other while maintaining the similarity within the data and the correlation between different types of data.<fig id="Fig4"><label>Figure 4</label><caption><p>The SGM module in the intermediate layers. (<bold>a</bold>) Concat Data: The combination of original video data with its ablated data is used as the input to the network. (<bold>b</bold>) semantic graph mapping: Based on the predicted semantic embeddings, the attention weight changes of the feature maps at the intermediate stage layers are obtained.</p></caption><graphic xlink:href="41598_2024_63195_Fig4_HTML" id="MO4"/></fig></p></sec></sec><sec id="Sec13"><title>Semantic graph mapping</title><p id="Par37">Traditional knowledge distillation methods typically transfer knowledge at the final feature output layer, overlooking the rich information in the intermediate layers. When dealing with heterogeneous data such as motion sensors and visual sensors, the complementary information in the intermediate layers is crucial for transferring heterogeneous knowledge. To better achieve heterogeneous knowledge transfer from the teacher network to the student network, we designed a semantics-based knowledge distillation module, which operates on the first s-1 stage modules to generate semantic-aware attention heatmaps for the intermediate layers of the teacher and student networks, aiming to train the student network accordingly. The objective of this module is to highlight important semantic regions in the output feature maps of each Stage and train the student network based on the differences between the teacher and student networks.</p><p id="Par38">Previous research<sup><xref ref-type="bibr" rid="CR55">55</xref></sup> has demonstrated that ablating certain units in a model can assess the importance of those units for specific category recognition. Therefore, this paper combines original images with their channel-zeroed ablated data to generate semantic ablation decreases, obtaining a good multimodal semantic visual interpretation through semantic ablation decreases.</p><p id="Par39">We take the student network as an example to explain the operation principle of the SGM module in Fig. <xref rid="Fig4" ref-type="fig">4</xref>. During the training of the student network, the student network takes a combination of the original data <inline-formula id="IEq47"><alternatives><tex-math id="M115">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$I \in \mathbb {R} ^{B\times C\times D\times H\times W}$$\end{document}</tex-math><mml:math id="M116"><mml:mrow><mml:mi>I</mml:mi><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>C</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>D</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>H</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_63195_Article_IEq47.gif"/></alternatives></inline-formula>and the ablated data <inline-formula id="IEq48"><alternatives><tex-math id="M117">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$I_a \in \mathbb {R}^{B\times C\times D\times H\times W}$$\end{document}</tex-math><mml:math id="M118"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>C</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>D</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>H</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_63195_Article_IEq48.gif"/></alternatives></inline-formula> with zeroed channel values as input, denoted as combined data <inline-formula id="IEq49"><alternatives><tex-math id="M119">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$[I;I_a]\in \mathbb {R}^{2B\times C\times D\times H\times W}$$\end{document}</tex-math><mml:math id="M120"><mml:mrow><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>I</mml:mi><mml:mo>&#x0037e;</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>B</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>C</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>D</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>H</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_63195_Article_IEq49.gif"/></alternatives></inline-formula>. Firstly, we obtain the network&#x02019;s classification prediction and find the corresponding semantics. Let <italic>y</italic> represent the predicted category of the original data, and <inline-formula id="IEq50"><alternatives><tex-math id="M121">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$y_a$$\end{document}</tex-math><mml:math id="M122"><mml:msub><mml:mi>y</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_63195_Article_IEq50.gif"/></alternatives></inline-formula> represent the predicted category of the ablated data. Then, we utilize Bert<sup><xref ref-type="bibr" rid="CR56">56</xref></sup> to compute the semantic embeddings for the original data<italic>B</italic> and the ablated data <inline-formula id="IEq51"><alternatives><tex-math id="M123">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$B_a$$\end{document}</tex-math><mml:math id="M124"><mml:msub><mml:mi>B</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_63195_Article_IEq51.gif"/></alternatives></inline-formula>:<disp-formula id="Equ12"><label>12</label><alternatives><tex-math id="M125">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} \begin{aligned} B&#x00026;= Bert(y) \in \mathbb {R}^{B\times 768}\\ B_a&#x00026;= Bert(y_a) \in \mathbb {R}^{B\times 768} \end{aligned} \end{aligned}$$\end{document}</tex-math><mml:math id="M126" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mi>B</mml:mi></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mi>B</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mn>768</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mrow/><mml:msub><mml:mi>B</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mi>B</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mn>768</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_63195_Article_Equ12.gif" position="anchor"/></alternatives></disp-formula>Next, leveraging manifold learning<sup><xref ref-type="bibr" rid="CR57">57</xref></sup> which can extract intrinsic structures from data, we constructed two graphs for the class score vectors of <italic>I</italic> and <inline-formula id="IEq52"><alternatives><tex-math id="M127">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$I_a$$\end{document}</tex-math><mml:math id="M128"><mml:msub><mml:mi>I</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_63195_Article_IEq52.gif"/></alternatives></inline-formula>. In these graphs, vertices represent feature vectors used for classification, while edges represent relationships between feature vectors, determined by Gaussian similarity. Assuming the input feature vectors are <inline-formula id="IEq53"><alternatives><tex-math id="M129">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$x_i$$\end{document}</tex-math><mml:math id="M130"><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_63195_Article_IEq53.gif"/></alternatives></inline-formula> and <inline-formula id="IEq54"><alternatives><tex-math id="M131">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$x_j$$\end{document}</tex-math><mml:math id="M132"><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_63195_Article_IEq54.gif"/></alternatives></inline-formula>, the weight of the edge between them is given by:<disp-formula id="Equ13"><label>13</label><alternatives><tex-math id="M133">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} W_{i,j}=exp\left( -\frac{ \begin{Vmatrix} x_i-x_j \end{Vmatrix}^2}{2}\right) \end{aligned}$$\end{document}</tex-math><mml:math id="M134" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mfenced close=")" open="("><mml:mo>-</mml:mo><mml:mfrac><mml:msup><mml:mfenced close="&#x02225;" open="&#x02225;"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced><mml:mn>2</mml:mn></mml:msup><mml:mn>2</mml:mn></mml:mfrac></mml:mfenced></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_63195_Article_Equ13.gif" position="anchor"/></alternatives></disp-formula>Once we obtained the graph structure <italic>W</italic>, we introduced normalized graph Laplacians<sup><xref ref-type="bibr" rid="CR58">58</xref></sup> and applied them to <italic>W</italic>, which helps further extract the internal partitioning features of <italic>W</italic>, i.e. <inline-formula id="IEq55"><alternatives><tex-math id="M135">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$L=D^{-\frac{1}{2}}WD^{-\frac{1}{2}}$$\end{document}</tex-math><mml:math id="M136"><mml:mrow><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mi>D</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:msup><mml:mi>W</mml:mi><mml:msup><mml:mi>D</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_63195_Article_IEq55.gif"/></alternatives></inline-formula>. Here, <italic>D</italic> is a diagonal matrix where the value on the diagonal of the <inline-formula id="IEq56"><alternatives><tex-math id="M137">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$i^{th}$$\end{document}</tex-math><mml:math id="M138"><mml:msup><mml:mi>i</mml:mi><mml:mrow><mml:mi mathvariant="italic">th</mml:mi></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="41598_2024_63195_Article_IEq56.gif"/></alternatives></inline-formula> row is equal to the sum of the <inline-formula id="IEq57"><alternatives><tex-math id="M139">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$i^{th}$$\end{document}</tex-math><mml:math id="M140"><mml:msup><mml:mi>i</mml:mi><mml:mrow><mml:mi mathvariant="italic">th</mml:mi></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="41598_2024_63195_Article_IEq57.gif"/></alternatives></inline-formula> row of <italic>W</italic>. This way, the manifold structure of the data can be effectively represented in the graph matrix <inline-formula id="IEq58"><alternatives><tex-math id="M141">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$L\in \mathbb {R}^{B\times B}$$\end{document}</tex-math><mml:math id="M142"><mml:mrow><mml:mi>L</mml:mi><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_63195_Article_IEq58.gif"/></alternatives></inline-formula>.</p><p id="Par40">To preserve the structure of the original features, we first multiply the semantic embeddings by the graph matrix, which is essentially a form of manifold regularization. Next, we define a slope metric <inline-formula id="IEq59"><alternatives><tex-math id="M143">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$k\in \mathbb {R}^{B\times 768}$$\end{document}</tex-math><mml:math id="M144"><mml:mrow><mml:mi>k</mml:mi><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mn>768</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_63195_Article_IEq59.gif"/></alternatives></inline-formula> to measure the rate of semantic change from the original data to the ablated data, and based on this metric, we allocate attention weights to the feature maps. The computation of <italic>k</italic> is given by Eq. (<xref rid="Equ14" ref-type="disp-formula">14</xref>):<disp-formula id="Equ14"><label>14</label><alternatives><tex-math id="M145">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} k = \frac{L\times B-L_a\times B_a}{L\times B} \end{aligned}$$\end{document}</tex-math><mml:math id="M146" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>L</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>B</mml:mi><mml:mo>-</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo>&#x000d7;</mml:mo><mml:msub><mml:mi>B</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_63195_Article_Equ14.gif" position="anchor"/></alternatives></disp-formula>where <italic>L</italic> and <inline-formula id="IEq60"><alternatives><tex-math id="M147">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$L_a$$\end{document}</tex-math><mml:math id="M148"><mml:msub><mml:mi>L</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_63195_Article_IEq60.gif"/></alternatives></inline-formula> are the normalized graph similarity matrices of the original data and the ablated data, respectively. By computing the rate of semantic change, we can represent the importance values of features in terms of the magnitude of the slope while preserving the essence of the data. Finally, we perform a Hadamard product operation between the semantic change rate and the original features to obtain the semantic graph mapping, which will be used for the loss calculation in knowledge distillation.<disp-formula id="Equ15"><label>15</label><alternatives><tex-math id="M149">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} M_m^s=LN(k\cdot F_m^s) \end{aligned}$$\end{document}</tex-math><mml:math id="M150" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msubsup><mml:mi>M</mml:mi><mml:mi>m</mml:mi><mml:mi>s</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mi>L</mml:mi><mml:mi>N</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo>&#x000b7;</mml:mo><mml:msubsup><mml:mi>F</mml:mi><mml:mi>m</mml:mi><mml:mi>s</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_63195_Article_Equ15.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq61"><alternatives><tex-math id="M151">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$LN(\cdot )$$\end{document}</tex-math><mml:math id="M152"><mml:mrow><mml:mi>L</mml:mi><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>&#x000b7;</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_63195_Article_IEq61.gif"/></alternatives></inline-formula> represents the LayerNorm function<sup><xref ref-type="bibr" rid="CR59">59</xref></sup>. We utilize the semantic graph mappings generated by the teacher network as soft targets, along with those generated by the student network, to construct the SGM knowledge distillation loss. Assuming we have <italic>m</italic> teacher modalities and one student modality with <italic>s</italic> Stage modules, we employ the mean squared error (MSE) loss of SGM to transfer cross-modal heterogeneous knowledge.<disp-formula id="Equ16"><label>16</label><alternatives><tex-math id="M153">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} L_{SGM}=\frac{\sum _{i=1}^{s-1}\sum _{j=1}^{m}MSE(M_S^i,M_j^i)}{m\times (s-1)} \end{aligned}$$\end{document}</tex-math><mml:math id="M154" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi mathvariant="italic">SGM</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>m</mml:mi></mml:msubsup><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:mi>E</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:mi>i</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>M</mml:mi><mml:mi>j</mml:mi><mml:mi>i</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_63195_Article_Equ16.gif" position="anchor"/></alternatives></disp-formula>where we only utilize the last <inline-formula id="IEq62"><alternatives><tex-math id="M155">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$s-1$$\end{document}</tex-math><mml:math id="M156"><mml:mrow><mml:mi>s</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_63195_Article_IEq62.gif"/></alternatives></inline-formula> stage modules to compute the SGM loss, and the last module is used for calculating the semantic loss; <inline-formula id="IEq63"><alternatives><tex-math id="M157">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$M_S^i$$\end{document}</tex-math><mml:math id="M158"><mml:msubsup><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:mi>i</mml:mi></mml:msubsup></mml:math><inline-graphic xlink:href="41598_2024_63195_Article_IEq63.gif"/></alternatives></inline-formula> represents the semantic graph mapping of the <italic>i</italic>-th Stage of the student network.</p></sec><sec id="Sec14"><title>Training</title><p id="Par41">Our framework is a multi-scale knowledge distillation architecture. We utilize a sensor image generation model to convert one-dimensional time-series sensor data into RGB image data recognizable by Swin through GAF, thereby preserving the original features while enhancing local temporal relationships for subsequent feature extraction. After the attention computation is completed in the Stage modules of the teacher network, the feature maps are input into the Fusion and Tuning (FT) module to obtain similarity features of different types of wearable sensor data and perform multi-type feature fusion. During the training of the student network, we use the Semantic Graph Mapping (SGM) module to map the semantic key parts of intermediate feature maps and calculate knowledge distillation loss using the semantic feature maps of the teacher network as soft targets. We seamlessly integrate these modules into the Swin knowledge distillation architecture to address the heterogeneous HAR problem between sensor data and visual data.</p><p id="Par42">We first independently train the teacher network, as shown in Fig. <xref rid="Fig2" ref-type="fig">2</xref>, using Swin as the backbone network to construct the teacher network, which primarily takes sensor image data as input and undergoes fine-tuning training on Swin with pre-trained weights. After the training of the teacher network is completed, it guides the training of the student network. Our ultimate goal is to use the student network, with video data as input, to generate predictions similar to those trained with multimodal data. The teacher network&#x02019;s fused features are used together with the features learned by the student network for computing the semantic graph mapping loss, thereby achieving knowledge transfer.</p><p id="Par43">To train the teacher network, we use the output features of the last stage as the semantic feature output of the entire network and compute the mean squared error (MSE) loss between it and the target true semantic embeddings, which we refer to as the semantic loss. Assuming we have m teacher modalities, the total loss of the teacher network, combined with semantic loss and predicted cross-entropy loss, is shown in Eq. (<xref rid="Equ17" ref-type="disp-formula">17</xref>):<disp-formula id="Equ17"><label>17</label><alternatives><tex-math id="M159">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} L_{SE}= &#x00026; {} \frac{1}{m}\sum _{i=1}^m MSE(F_i,B_i) \end{aligned}$$\end{document}</tex-math><mml:math id="M160" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi mathvariant="italic">SE</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mrow/><mml:mfrac><mml:mn>1</mml:mn><mml:mi>m</mml:mi></mml:mfrac><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>m</mml:mi></mml:munderover><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:mi>E</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>B</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_63195_Article_Equ17.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ18"><label>18</label><alternatives><tex-math id="M161">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} L_T= &#x00026; {} \frac{1}{2\times m}\left( \sum _{i=1}^m MSE(F_i,B_i)+\sum _{i=1}^m CE(P_i,T_i)\right) \end{aligned}$$\end{document}</tex-math><mml:math id="M162" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo>=</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mrow/><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:mfrac><mml:mfenced close=")" open="("><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>m</mml:mi></mml:munderover><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:mi>E</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>B</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>m</mml:mi></mml:munderover><mml:mi>C</mml:mi><mml:mi>E</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfenced></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_63195_Article_Equ18.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq64"><alternatives><tex-math id="M163">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$F_i$$\end{document}</tex-math><mml:math id="M164"><mml:msub><mml:mi>F</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_63195_Article_IEq64.gif"/></alternatives></inline-formula> represents the semantic features of modality <italic>i</italic> in the network, <inline-formula id="IEq65"><alternatives><tex-math id="M165">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$B_i$$\end{document}</tex-math><mml:math id="M166"><mml:msub><mml:mi>B</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_63195_Article_IEq65.gif"/></alternatives></inline-formula> represents the true semantic embeddings, <inline-formula id="IEq66"><alternatives><tex-math id="M167">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$P_i$$\end{document}</tex-math><mml:math id="M168"><mml:msub><mml:mi>P</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_63195_Article_IEq66.gif"/></alternatives></inline-formula> represents the model&#x02019;s predicted results, <inline-formula id="IEq67"><alternatives><tex-math id="M169">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$T_i$$\end{document}</tex-math><mml:math id="M170"><mml:msub><mml:mi>T</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_63195_Article_IEq67.gif"/></alternatives></inline-formula> represents the ground truth labels, <inline-formula id="IEq68"><alternatives><tex-math id="M171">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$MSE(\cdot )$$\end{document}</tex-math><mml:math id="M172"><mml:mrow><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:mi>E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>&#x000b7;</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_63195_Article_IEq68.gif"/></alternatives></inline-formula> denotes the mean squared error loss function, and <inline-formula id="IEq69"><alternatives><tex-math id="M173">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$CE(\cdot )$$\end{document}</tex-math><mml:math id="M174"><mml:mrow><mml:mi>C</mml:mi><mml:mi>E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>&#x000b7;</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_63195_Article_IEq69.gif"/></alternatives></inline-formula> denotes the cross-entropy loss function.</p><p id="Par44">After the training of the teacher network is completed, we utilize the knowledge of the teacher network to train the student network. The loss of the student network includes semantic loss, predicted cross-entropy loss, SGM knowledge distillation loss, and soft target knowledge distillation loss. Particularly, the soft target knowledge distillation loss focuses on the differences in similarity of classification score features and utilizes KL divergence for loss computation. The specific formula is as follows:<disp-formula id="Equ19"><label>19</label><alternatives><tex-math id="M175">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} L_{ST}^S= &#x00026; {} \frac{1}{m}\sum _{i=1}^m KL\left( \frac{P_i}{T},\frac{P_S}{T}\right) \end{aligned}$$\end{document}</tex-math><mml:math id="M176" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msubsup><mml:mi>L</mml:mi><mml:mrow><mml:mi mathvariant="italic">ST</mml:mi></mml:mrow><mml:mi>S</mml:mi></mml:msubsup><mml:mo>=</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mrow/><mml:mfrac><mml:mn>1</mml:mn><mml:mi>m</mml:mi></mml:mfrac><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>m</mml:mi></mml:munderover><mml:mi>K</mml:mi><mml:mi>L</mml:mi><mml:mfenced close=")" open="("><mml:mfrac><mml:msub><mml:mi>P</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mi>T</mml:mi></mml:mfrac><mml:mo>,</mml:mo><mml:mfrac><mml:msub><mml:mi>P</mml:mi><mml:mi>S</mml:mi></mml:msub><mml:mi>T</mml:mi></mml:mfrac></mml:mfenced></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_63195_Article_Equ19.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ20"><label>20</label><alternatives><tex-math id="M177">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} L_S= &#x00026; {} \alpha L_{ST}^S+\beta L_{SGM}^S+\gamma L_{SE}^S+L_{CE}^S \end{aligned}$$\end{document}</tex-math><mml:math id="M178" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>S</mml:mi></mml:msub><mml:mo>=</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mrow/><mml:mi>&#x003b1;</mml:mi><mml:msubsup><mml:mi>L</mml:mi><mml:mrow><mml:mi mathvariant="italic">ST</mml:mi></mml:mrow><mml:mi>S</mml:mi></mml:msubsup><mml:mo>+</mml:mo><mml:mi>&#x003b2;</mml:mi><mml:msubsup><mml:mi>L</mml:mi><mml:mrow><mml:mi mathvariant="italic">SGM</mml:mi></mml:mrow><mml:mi>S</mml:mi></mml:msubsup><mml:mo>+</mml:mo><mml:mi>&#x003b3;</mml:mi><mml:msubsup><mml:mi>L</mml:mi><mml:mrow><mml:mi mathvariant="italic">SE</mml:mi></mml:mrow><mml:mi>S</mml:mi></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>L</mml:mi><mml:mrow><mml:mi mathvariant="italic">CE</mml:mi></mml:mrow><mml:mi>S</mml:mi></mml:msubsup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_63195_Article_Equ20.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq70"><alternatives><tex-math id="M179">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\alpha , \beta , \gamma$$\end{document}</tex-math><mml:math id="M180"><mml:mrow><mml:mi>&#x003b1;</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003b2;</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003b3;</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_63195_Article_IEq70.gif"/></alternatives></inline-formula> are hyperparameters, we set <inline-formula id="IEq71"><alternatives><tex-math id="M181">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\alpha =0.1, \beta =1, \gamma =1$$\end{document}</tex-math><mml:math id="M182"><mml:mrow><mml:mi>&#x003b1;</mml:mi><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn><mml:mo>,</mml:mo><mml:mi>&#x003b2;</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>&#x003b3;</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_63195_Article_IEq71.gif"/></alternatives></inline-formula> and <italic>T</italic> is the temperature parameter in the KL divergence, <inline-formula id="IEq72"><alternatives><tex-math id="M183">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$L_{ST}^S$$\end{document}</tex-math><mml:math id="M184"><mml:msubsup><mml:mi>L</mml:mi><mml:mrow><mml:mi mathvariant="italic">ST</mml:mi></mml:mrow><mml:mi>S</mml:mi></mml:msubsup></mml:math><inline-graphic xlink:href="41598_2024_63195_Article_IEq72.gif"/></alternatives></inline-formula> is the soft target loss of the student network, <inline-formula id="IEq73"><alternatives><tex-math id="M185">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$L_{SGM}^S$$\end{document}</tex-math><mml:math id="M186"><mml:msubsup><mml:mi>L</mml:mi><mml:mrow><mml:mi mathvariant="italic">SGM</mml:mi></mml:mrow><mml:mi>S</mml:mi></mml:msubsup></mml:math><inline-graphic xlink:href="41598_2024_63195_Article_IEq73.gif"/></alternatives></inline-formula> is the semantic graph mapping loss of the student network, <inline-formula id="IEq74"><alternatives><tex-math id="M187">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$L_{SE}^S$$\end{document}</tex-math><mml:math id="M188"><mml:msubsup><mml:mi>L</mml:mi><mml:mrow><mml:mi mathvariant="italic">SE</mml:mi></mml:mrow><mml:mi>S</mml:mi></mml:msubsup></mml:math><inline-graphic xlink:href="41598_2024_63195_Article_IEq74.gif"/></alternatives></inline-formula> is the semantic loss of the student network, and <inline-formula id="IEq75"><alternatives><tex-math id="M189">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$L_{CE}^S$$\end{document}</tex-math><mml:math id="M190"><mml:msubsup><mml:mi>L</mml:mi><mml:mrow><mml:mi mathvariant="italic">CE</mml:mi></mml:mrow><mml:mi>S</mml:mi></mml:msubsup></mml:math><inline-graphic xlink:href="41598_2024_63195_Article_IEq75.gif"/></alternatives></inline-formula> is the predicted cross-entropy loss of the student network. We train the student network with the total loss of the student network.</p></sec></sec><sec id="Sec15"><title>Results and discussion</title><sec id="Sec16"><title>Experimental setup</title><sec id="Sec75"><title>Datasets</title><p id="Par45">We evaluated the performance of our model on two multimodal activity datasets: UTD-MHD and MMAct. UTD-MHAD and MMAct are both from public datasets and no living beings, humans, or animals were involved. MMAct is a large multimodal action dataset consisting of 35 different activities performed by 20 volunteers, totaling over 36,000 samples. These data include information from various modalities such as RGB videos, keypoints, depth images, and signals from wearable sensors. Each activity has 5 different executions and 4 different scenarios recorded from four different directions using visual sensors. In this study, we primarily utilized RGB videos and wearable sensor data (including smartphone accelerometers, smartwatch accelerometers, gyroscopes, and orientation information) from the MMAct dataset. The UTD-MHAD dataset comprises 27 activities performed by 8 volunteers. Each volunteer repeated each activity 4 times, and their wearable sensor data and video data were recorded. This dataset contains a total of 861 samples, each including RGB videos, depth images, skeleton information, and inertial sensor data. In our research, we mainly used RGB videos and inertial sensor data from the UTD-MHAD dataset, including accelerometer and gyroscope data.</p></sec><sec id="Sec78"><title>Implementation details</title><p id="Par46">For both the MMAct and UTD-MHAD datasets, we adopted the same model setup. We employed the Swin-Transformer Tiny (Swin-T) with pre-trained weights as the backbone network for the teacher network, while for the student network, we used the Video Swin-Transformer Tiny (Video Swin-T) with pre-trained weights as the backbone network. During the training process of both teacher and student networks, we did not use pre-trained weights for the classification heads but fine-tuned the original backbone network weights. When training the teacher network, we resized the sensor virtual images to <inline-formula id="IEq76"><alternatives><tex-math id="M191">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$224 \times 224$$\end{document}</tex-math><mml:math id="M192"><mml:mrow><mml:mn>224</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>224</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_63195_Article_IEq76.gif"/></alternatives></inline-formula>, set the learning rate to 1e-4, batch size to 32, and utilized the AdamW optimizer<sup><xref ref-type="bibr" rid="CR60">60</xref></sup>. When training the student network, we uniformly sampled 32 frames from the original video data, resized them to <inline-formula id="IEq77"><alternatives><tex-math id="M193">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$224 \times 224$$\end{document}</tex-math><mml:math id="M194"><mml:mrow><mml:mn>224</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>224</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_63195_Article_IEq77.gif"/></alternatives></inline-formula> resolution, set the learning rate to 5e-5, batch size to 16, and similarly used the AdamW optimizer for training. To obtain semantic representations across different modalities, we utilized a Bert model to obtain 768-dimensional semantic vectors corresponding to each action name. For the MMAct dataset, we followed the data split standard of MMAct for training and used F1-score as the evaluation metric. For the UTD-MHAD dataset, we used Accuracy as the evaluation metric. The training of the teacher network was completed on an NVIDIA RTX 3090 GPU, while the training of the student network was conducted on two NVIDIA A40 GPUs.<table-wrap id="Tab1"><label>Table 1</label><caption><p>Comparison with other models on MMAct and UTD-MHAD datasets.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" colspan="3">(a) MMAct (cross-session)</th></tr><tr><th align="left">Method</th><th align="left">Modality</th><th align="left">F1-score (cross-session)</th></tr></thead><tbody><tr><td align="left">TSN<sup><xref ref-type="bibr" rid="CR34">34</xref></sup></td><td align="left">RGB</td><td align="left">69.20</td></tr><tr><td align="left">TRN<sup><xref ref-type="bibr" rid="CR35">35</xref></sup></td><td align="left">RGB</td><td align="left">71.95</td></tr><tr><td align="left">MMD<sup><xref ref-type="bibr" rid="CR4">4</xref></sup></td><td align="left">A+G+O+RGB</td><td align="left">74.58</td></tr><tr><td align="left">MMAD<sup><xref ref-type="bibr" rid="CR4">4</xref></sup></td><td align="left">A+G+O+RGB</td><td align="left">78.82</td></tr><tr><td align="left">Keyless<sup><xref ref-type="bibr" rid="CR61">61</xref></sup></td><td align="left">A+G+O+RGB</td><td align="left">81.11</td></tr><tr><td align="left">SAKDN<sup><xref ref-type="bibr" rid="CR62">62</xref></sup></td><td align="left">A+G+O+RGB</td><td align="left">82.77</td></tr><tr><td align="left">HAMLET<sup><xref ref-type="bibr" rid="CR63">63</xref></sup></td><td align="left">A+G+O+RGB</td><td align="left">83.89</td></tr><tr><td align="left">MuMu<sup><xref ref-type="bibr" rid="CR64">64</xref></sup></td><td align="left">A+G+O+RGB</td><td align="left">87.50</td></tr><tr><td align="left"><bold>Ours</bold></td><td align="left"><bold>A+G+O+RGB</bold></td><td align="left"><bold>88.12</bold></td></tr></tbody></table><table frame="hsides" rules="groups"><thead><tr><th align="left" colspan="3">(b) MMAct (cross-subject)</th></tr><tr><th align="left">Method</th><th align="left">Modality</th><th align="left">F1-score (cross-subject)</th></tr></thead><tbody><tr><td align="left">TSN</td><td align="left">RGB</td><td align="left">59.5</td></tr><tr><td align="left">MMD</td><td align="left">RGB</td><td align="left">64.33</td></tr><tr><td align="left">MMAD</td><td align="left">A+G+O+RGB</td><td align="left">64.45</td></tr><tr><td align="left">TRN</td><td align="left">A+G+O+RGB</td><td align="left">66.56</td></tr><tr><td align="left">HAMLET</td><td align="left">A+G+O+RGB</td><td align="left">69.35</td></tr><tr><td align="left">Keyless</td><td align="left">A+G+O+RGB</td><td align="left">71.83</td></tr><tr><td align="left">MuMu</td><td align="left">A+G+O+RGB</td><td align="left">76.28</td></tr><tr><td align="left">SAKDN</td><td align="left">A+G+O+RGB</td><td align="left">77.23</td></tr><tr><td align="left"><bold>Ours</bold></td><td align="left"><bold>A+G+O+RGB</bold></td><td align="left"><bold>79.52</bold></td></tr></tbody></table><table frame="hsides" rules="groups"><thead><tr><th align="left" colspan="3">(c) MMAct (cross-scene)</th></tr><tr><th align="left">Method</th><th align="left">Modality</th><th align="left">F1-score (cross-scene)</th></tr></thead><tbody><tr><td align="left">TSN</td><td align="left">RGB</td><td align="left">51.21</td></tr><tr><td align="left">TRN</td><td align="left">RGB</td><td align="left">60.03</td></tr><tr><td align="left">MMD</td><td align="left">A+G+O+RGB</td><td align="left">62.23</td></tr><tr><td align="left">MMAD</td><td align="left">A+G+O+RGB</td><td align="left">64.12</td></tr><tr><td align="left">Keyless</td><td align="left">A+G+O+RGB</td><td align="left">70.77</td></tr><tr><td align="left">HAMLET</td><td align="left">A+G+O+RGB</td><td align="left">71.59</td></tr><tr><td align="left">SAKDN</td><td align="left">A+G+O+RGB</td><td align="left">73.48</td></tr><tr><td align="left">MuMu</td><td align="left">A+G+O+RGB</td><td align="left">74.57</td></tr><tr><td align="left"><bold>Ours</bold></td><td align="left"><bold>A+G+O+RGB</bold></td><td align="left"><bold>80.08</bold></td></tr></tbody></table><table frame="hsides" rules="groups"><thead><tr><th align="left" colspan="3">(d) UTD-MHAD</th></tr><tr><th align="left">Method</th><th align="left">Modality</th><th align="left">Accuracy</th></tr></thead><tbody><tr><td align="left">TSN</td><td align="left">RGB</td><td align="left">92.54</td></tr><tr><td align="left">Keyless</td><td align="left">A+G+O+RGB</td><td align="left">92.67</td></tr><tr><td align="left">MCRL<sup><xref ref-type="bibr" rid="CR65">65</xref></sup></td><td align="left">A+G+O+RGB</td><td align="left">93.02</td></tr><tr><td align="left">PoseMap<sup><xref ref-type="bibr" rid="CR66">66</xref></sup></td><td align="left">A+G+O+RGB</td><td align="left">94.51</td></tr><tr><td align="left">TRN</td><td align="left">RGB</td><td align="left">94.87</td></tr><tr><td align="left">HAMLET</td><td align="left">A+G+O+RGB</td><td align="left">95.12</td></tr><tr><td align="left">MuMu</td><td align="left">A+G+O+RGB</td><td align="left">97.60</td></tr><tr><td align="left">SAKDN</td><td align="left">A+G+O+RGB</td><td align="left">98.60</td></tr><tr><td align="left"><bold>Ours</bold></td><td align="left"><bold>A+G+O+RGB</bold></td><td align="left"><bold>98.89</bold></td></tr></tbody></table><table-wrap-foot><p>Significant values are in bold.</p></table-wrap-foot></table-wrap></p></sec></sec><sec id="Sec17"><title>Comparison</title><p id="Par47">We evaluate the performance of our model by comparing it with state-of-the-art HAR methods on two datasets. On the MMAct dataset, we evaluate according to MMAct&#x02019;s evaluation settings (cross-session, cross-subject, and cross-scene) using F1-Score. Cross-session denotes partitioning based on repeated actions, as shown in Table <xref rid="Tab1" ref-type="table">1</xref>a, where our model achieves a high recognition accuracy when observing all actions, outperforming the recently proposed MuMu model by 0.62%. MuMu is an attention-based collaborative learning model, which first classifies activities by grouping them and then further divides them, but it lacks fusion of intermediate feature similarities, which is why our model is superior. Cross-subject denotes partitioning based on volunteers, as shown in Table <xref rid="Tab1" ref-type="table">1</xref>b. Unlike cross-session partitioning, this partitioning mainly focuses on recognizing actions performed by different individuals, enhancing the model&#x02019;s robustness to unfamiliar actions. The results show that our model improves by 2.29% compared to SAKDN. The SAKDN model is a convolution-based knowledge distillation network that may not effectively focus on global information during feature extraction, which could lead to a localized representation of subsequent feature information. In contrast, our model extracts global feature information attentively, allowing for a better analysis of unfamiliar actions from a global perspective, thus enhancing action recognition robustness. Cross-scene denotes partitioning based on different scenes, as shown in Table <xref rid="Tab1" ref-type="table">1</xref>c. Surprisingly, we found a 5.51% improvement in our model&#x02019;s performance under environmental changes, further demonstrating the robustness of the global attention mechanism to complex environments.</p><p id="Par48">In addition to the MMAct dataset, we also evaluated our model&#x02019;s performance on the UTD-MHAD dataset as shown in Table <xref rid="Tab1" ref-type="table">1</xref>d. Due to the smaller size and simpler actions of the UTD-MHAD dataset, without complex scenes or occlusions, the activities performed by volunteers are simple but distinctive. Although the recognition accuracy of all models is satisfactory, our proposed model still achieves the highest accuracy, with a performance improvement of 0.29.</p><p id="Par49">We also compared with other attention-based methods such as Keyless and HAMLET. These attention-based methods do not consider how to better handle motion sensor data to represent motion information better. On the other hand, non-attention-based methods cannot effectively focus on global information during feature generation in intermediate layers, resulting in lower performance compared to models using a global attention mechanism. The performance drop in Table <xref rid="Tab1" ref-type="table">1</xref>b and c may be due to environmental changes requiring the model to reconsider, dispersing some attention, thus unable to focus solely on activity recognition.</p><p id="Par50">Based on the experimental results on the MMAct and UTD-MHAD datasets, we draw the following conclusions:<list list-type="bullet"><list-item><p id="Par51">Fusion of complementary information from different types of sensor data can better unify feature information across different types of data.</p></list-item><list-item><p id="Par52">Using a global attention mechanism can better acquire global information, thereby extracting better global features.</p></list-item><list-item><p id="Par53">Semantic embeddings can serve as cues between different modalities to increase their correlation.</p></list-item></list></p></sec><sec id="Sec18"><title>Ablation study</title><p id="Par54">To assess the contributions of the FT, SGM, SE, and ST modules, we conducted ablation experiments by excluding each of these modules individually. BaseLine: The student network (Video Swin-T) was trained using only RGB videos. W/O FT: Fusion and Tuning were not used in the teacher network. W/O SGM: Semantic Graph Mapping Loss calculation was not performed when transferring knowledge from the teacher network to the student network. W/O SE: Semantic embeddings were not included during the training of both the teacher and student networks. W/O ST: Soft target loss calculation was not performed during the training of the student network.</p><p id="Par55">From the results in Table <xref rid="Tab2" ref-type="table">2</xref>, it can be observed that compared to the ST module, the SGM module contributes more significantly to the overall model, indicating that the semantic graph mapping loss is very helpful for knowledge transfer between the two networks. This also demonstrates the benefits of using the classification score feature vectors from the teacher network as soft targets for knowledge transfer. The FT module also plays an important role throughout the network. Through the FT module, we can calibrate the features between different types of sensors, unifying the data features of different types of sensors. When transferring knowledge to the student network, the FT module enables the teacher network to better transfer knowledge from multiple different types of sensors to the student network. Additionally, semantic features contribute to the joint representation of two different modalities of data. We use the same semantic embeddings as a bridge to jointly represent data from different modalities. By calculating semantic losses between different modalities and within modalities, semantic coherence between different modalities can be unified, helping the model maintain similarity between different modalities. From the results in Table <xref rid="Tab2" ref-type="table">2</xref>, we can conclude that semantic embeddings are helpful for overall model training. It can be observed that compared to other modules, the traditional soft target loss has relatively lower importance for the overall model. This indicates that relying solely on soft targets for guidance cannot effectively transfer knowledge and requires the integration of knowledge from intermediate modules in the network for more comprehensive knowledge transfer.</p><p id="Par56">In our model, we successfully integrated the FT, SGM, and SE modules into the Swin Transformer knowledge distillation framework. During the training of the teacher network, the FT module combines features from different types of sensors to unify the similarity between features. When transferring knowledge to the student network, the SGM module obtains attention weights of semantics between different modalities, transferring heterogeneous knowledge between different modalities. During the training of both the student and teacher networks, the SE module is used to reduce the differences between different modalities of data. The integration of these modules mutually reinforces each other and contributes to the improvement of the model&#x02019;s performance.<table-wrap id="Tab2"><label>Table 2</label><caption><p>Ablation analysis of different modules of our proposed architecture under the cross-subject partitioning criterion on the MMAct dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Method</th><th align="left">Accuracy</th></tr></thead><tbody><tr><td align="left">Baseline (video swin-T)</td><td align="left">76.87</td></tr><tr><td align="left">W/O FT</td><td align="left">77.63</td></tr><tr><td align="left">W/O SGM</td><td align="left">77.06</td></tr><tr><td align="left">W/O ST</td><td align="left">78.75</td></tr><tr><td align="left">W/O SE</td><td align="left">78.23</td></tr><tr><td align="left"><bold>Ours</bold></td><td align="left"><bold>79.52</bold></td></tr></tbody></table><table-wrap-foot><p>&#x0201c;W/O&#x0201d; denotes without. Significant values are in bold.</p></table-wrap-foot></table-wrap></p><p id="Par57">
<fig id="Fig5"><label>Figure 5</label><caption><p>Visualization of the focal regions of interest in MMAct video data by our proposed model.</p></caption><graphic xlink:href="41598_2024_63195_Fig5_HTML" id="MO5"/></fig>
</p></sec><sec id="Sec19"><title>Visualization analysis</title><p id="Par58">In our study, to analyze whether the model correctly focuses attention on key locations, we conducted visual analysis of attention heatmaps. We extracted the feature maps from the last stage block output of the student network and its gradients during backpropagation, and used the Grad-CAM method<sup><xref ref-type="bibr" rid="CR67">67</xref></sup> to calculate the regions in the feature maps that had the most impact on the model&#x02019;s decision, generating corresponding heatmaps. Specifically, we computed the gradient changes based on the attention feature matrix of video data and averaged them over the time dimension to reflect the key frames in the video data. In Fig. <xref rid="Fig5" ref-type="fig">5</xref>, we present the attention heatmaps for 15 actions in the MMAct dataset. It can be observed that our model focuses attention on the activities rather than excessively on the human body or the surrounding environment, demonstrating the effectiveness and robustness of our model.<fig id="Fig6"><label>Figure 6</label><caption><p>Prediction matrix of our proposed model under the cross-subject partition criterion on the MMAct dataset.</p></caption><graphic xlink:href="41598_2024_63195_Fig6_HTML" id="MO6"/></fig></p><p id="Par59">The model prediction analysis based on the cross-subject partition results is illustrated in Fig. <xref rid="Fig6" ref-type="fig">6</xref>. The vertical axis represents the true labels, while the horizontal axis represents the predicted labels. We observed that our model performs well in identifying most activities. However, for certain activity categories, the recognition performance is poor, particularly for &#x0201c;carrying,&#x0201d; &#x0201c;jumping,&#x0201d; and &#x0201c;walking.&#x0201d; Specifically, the model struggles to differentiate between &#x0201c;carrying heavy&#x0201d; and &#x0201c;carrying light,&#x0201d; with most samples of &#x0201c;carrying light&#x0201d; being misclassified as the &#x0201c;carrying&#x0201d; category. Upon analysis, these activities exhibit high similarity in visual and motion sensor data features, posing challenges for the model in distinguishing them. Additionally, 22% of &#x0201c;jumping&#x0201d; activities are misclassified as &#x0201c;fall,&#x0201d; and 33% of &#x0201c;walking&#x0201d; activities are misclassified as &#x0201c;loitering&#x0201d; due to similarities in data features. However, for similar activities such as &#x0201c;setting down&#x0201d; and &#x0201c;picking up,&#x0201d; our model demonstrates effective identification, indicating its proficiency in recognizing most similar activities.
</p><p id="Par60">Through analysis of the model&#x02019;s attention heatmap visualization and prediction results, we found that although there are shortcomings in some challenging activity recognition aspects, overall, our model performs better than other models.</p></sec></sec><sec id="Sec20"><title>Conclusion</title><p id="Par61">In this paper, we propose a multimodal knowledge distillation architecture. First, we use GAF to convert wearable sensor data into images and use these images as training data input for a multi-type teacher network based on Swin-Transformer. In this network, we perform feature fusion and calibration between types through the FT module. Subsequently, a network of trained teachers is used to mentor the student network. During the training process of the student network, we use the SGM module to transfer the multi-modal knowledge learned by the teacher network to the student network. Experimental results show that our model effectively integrates complementary information between wearable sensors and visual sensors and exhibits good robustness in deployment in complex environments. However, the complexity of global attention calculation is still high, and we will study how to reduce the complexity in the future.</p></sec></body><back><fn-group><fn><p><bold>Publisher's note</bold></p><p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></fn></fn-group><ack><title>Acknowledgements</title><p>This work was partly supported by the Province Science Foundation of Jiangxi, No.20224BAB202030 and No.20202ACBL202009.</p></ack><notes notes-type="author-contribution"><title>Author contributions</title><p>X.T. and H.N. were responsible for collecting wearable sensor data and video data, respectively. Z. Yuan designed the main model framework. Z.Yang performs deep learning and statistical analysis of results. All authors approved the final version of the manuscript.</p></notes><notes notes-type="data-availability"><title>Data availibility</title><p>The original data sets UTD-MHAD<sup><xref ref-type="bibr" rid="CR5">5</xref></sup> and MMAct<sup><xref ref-type="bibr" rid="CR4">4</xref></sup> can be obtained according to the address given by the reference. Other data in the article are available upon request from the corresponding author.</p></notes><notes id="FPar1" notes-type="COI-statement"><title>Competing interests</title><p id="Par62">The authors declare no competing interests.</p></notes><ref-list id="Bib1"><title>References</title><ref id="CR1"><label>1.</label><mixed-citation publication-type="other">Hinton, G., Vinyals, O. &#x00026; Dean, J. Distilling the knowledge in a neural network. Preprint at http://arxiv.org/abs/1503.02531 (2015).</mixed-citation></ref><ref id="CR2"><label>2.</label><mixed-citation publication-type="other">Liu, Z. <italic>et&#x000a0;al.</italic> Swin transformer: Hierarchical vision transformer using shifted windows. In <italic>Proc. of the IEEE/CVF international conference on computer vision</italic>, 10012&#x02013;10022 (2021).</mixed-citation></ref><ref id="CR3"><label>3.</label><mixed-citation publication-type="other">Liu, Z. <italic>et&#x000a0;al.</italic> Video swin transformer. In <italic>Proc. of the IEEE/CVF conference on computer vision and pattern recognition</italic>, 3202&#x02013;3211 (2022).</mixed-citation></ref><ref id="CR4"><label>4.</label><mixed-citation publication-type="other">Kong, Q. <italic>et&#x000a0;al.</italic> Mmact: A large-scale dataset for cross modal human action understanding. In <italic>Proc. of the IEEE/CVF International Conference on Computer Vision</italic>, 8658&#x02013;8667 (2019).</mixed-citation></ref><ref id="CR5"><label>5.</label><mixed-citation publication-type="other">Chen, C., Jafari, R. &#x00026; Kehtarnavaz, N. Utd-mhad: A multimodal dataset for human action recognition utilizing a depth camera and a wearable inertial sensor. In <italic>2015 IEEE International conference on image processing (ICIP)</italic>, 168&#x02013;172 (IEEE, 2015).</mixed-citation></ref><ref id="CR6"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Qiu</surname><given-names>J</given-names></name><etal/></person-group><article-title>A survey on access control in the age of internet of things</article-title><source>IEEE Internet Things J.</source><year>2020</year><volume>7</volume><fpage>4682</fpage><lpage>4696</lpage><pub-id pub-id-type="doi">10.1109/JIOT.2020.2969326</pub-id></element-citation></ref><ref id="CR7"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chaquet</surname><given-names>JM</given-names></name><name><surname>Carmona</surname><given-names>EJ</given-names></name><name><surname>Fern&#x000e1;ndez-Caballero</surname><given-names>A</given-names></name></person-group><article-title>A survey of video datasets for human action and activity recognition</article-title><source>Comput. Vis. Image Underst.</source><year>2013</year><volume>117</volume><fpage>633</fpage><lpage>659</lpage><pub-id pub-id-type="doi">10.1016/j.cviu.2013.01.013</pub-id></element-citation></ref><ref id="CR8"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roshtkhari</surname><given-names>MJ</given-names></name><name><surname>Levine</surname><given-names>MD</given-names></name></person-group><article-title>Human activity recognition in videos using a single example</article-title><source>Image Vis. Comput.</source><year>2013</year><volume>31</volume><fpage>864</fpage><lpage>876</lpage><pub-id pub-id-type="doi">10.1016/j.imavis.2013.08.005</pub-id></element-citation></ref><ref id="CR9"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>X</given-names></name><name><surname>Qi</surname><given-names>C</given-names></name></person-group><article-title>Action recognition using edge trajectories and motion acceleration descriptor</article-title><source>Mach. Vis. Appl.</source><year>2016</year><volume>27</volume><fpage>861</fpage><lpage>875</lpage><pub-id pub-id-type="doi">10.1007/s00138-016-0746-x</pub-id></element-citation></ref><ref id="CR10"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>X</given-names></name><name><surname>Ji</surname><given-names>Q</given-names></name></person-group><article-title>Hierarchical context modeling for video event recognition</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2016</year><volume>39</volume><fpage>1770</fpage><lpage>1782</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2016.2616308</pub-id><?supplied-pmid 28113742?><pub-id pub-id-type="pmid">28113742</pub-id>
</element-citation></ref><ref id="CR11"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sargano</surname><given-names>AB</given-names></name><name><surname>Angelov</surname><given-names>P</given-names></name><name><surname>Habib</surname><given-names>Z</given-names></name></person-group><article-title>A comprehensive review on handcrafted and learning-based action representation approaches for human activity recognition</article-title><source>Appl. Sci.</source><year>2017</year><volume>7</volume><fpage>110</fpage><pub-id pub-id-type="doi">10.3390/app7010110</pub-id></element-citation></ref><ref id="CR12"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ma</surname><given-names>S</given-names></name><name><surname>Zhang</surname><given-names>J</given-names></name><name><surname>Sclaroff</surname><given-names>S</given-names></name><name><surname>Ikizler-Cinbis</surname><given-names>N</given-names></name><name><surname>Sigal</surname><given-names>L</given-names></name></person-group><article-title>Space-time tree ensemble for action recognition and localization</article-title><source>Int. J. Comput. Vis.</source><year>2018</year><volume>126</volume><fpage>314</fpage><lpage>332</lpage><pub-id pub-id-type="doi">10.1007/s11263-016-0980-8</pub-id></element-citation></ref><ref id="CR13"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Siddiqui</surname><given-names>S</given-names></name><etal/></person-group><article-title>Human action recognition: A construction of codebook by discriminative features selection approach</article-title><source>Int. J. Appl. Pattern Recogn.</source><year>2018</year><volume>5</volume><fpage>206</fpage><lpage>228</lpage><pub-id pub-id-type="doi">10.1504/IJAPR.2018.094815</pub-id></element-citation></ref><ref id="CR14"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sargano</surname><given-names>AB</given-names></name><name><surname>Gu</surname><given-names>X</given-names></name><name><surname>Angelov</surname><given-names>P</given-names></name><name><surname>Habib</surname><given-names>Z</given-names></name></person-group><article-title>Human action recognition using deep rule-based classifier</article-title><source>Multimed. Tools Appl.</source><year>2020</year><volume>79</volume><fpage>30653</fpage><lpage>30667</lpage><pub-id pub-id-type="doi">10.1007/s11042-020-09381-9</pub-id></element-citation></ref><ref id="CR15"><label>15.</label><mixed-citation publication-type="other">Wang, L. &#x00026; Sahbi, H. Directed acyclic graph kernels for action recognition. In <italic>Proc. of the IEEE International Conference on Computer Vision</italic>, 3168&#x02013;3175 (2013).</mixed-citation></ref><ref id="CR16"><label>16.</label><mixed-citation publication-type="other">Mazari, A. &#x00026; Sahbi, H. Mlgcn: Multi-laplacian graph convolutional networks for human action recognition. In <italic>The British Machine Vision Conference (BMVC)</italic> (2019).</mixed-citation></ref><ref id="CR17"><label>17.</label><mixed-citation publication-type="other">Zhang, P. <italic>et&#x000a0;al.</italic> Semantics-guided neural networks for efficient skeleton-based human action recognition. In <italic>Proc. of the IEEE/CVF conference on computer vision and pattern recognition</italic>, 1112&#x02013;1121 (2020).</mixed-citation></ref><ref id="CR18"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ahmad</surname><given-names>T</given-names></name><etal/></person-group><article-title>Graph convolutional neural network for human action recognition: A comprehensive survey</article-title><source>IEEE Trans. Artif. Intell.</source><year>2021</year><volume>2</volume><fpage>128</fpage><lpage>145</lpage><pub-id pub-id-type="doi">10.1109/TAI.2021.3076974</pub-id></element-citation></ref><ref id="CR19"><label>19.</label><mixed-citation publication-type="other">Zhou, J., Lin, K.-Y., Li, H. &#x00026; Zheng, W.-S. Graph-based high-order relation modeling for long-term action recognition. In <italic>Proc. of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</italic>, 8984&#x02013;8993 (2021).</mixed-citation></ref><ref id="CR20"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>K</given-names></name><name><surname>Liu</surname><given-names>C</given-names></name><name><surname>Yan</surname><given-names>G</given-names></name><name><surname>Lee</surname><given-names>VC</given-names></name><name><surname>Cao</surname><given-names>J</given-names></name></person-group><article-title>Accelerating DNN inference with reliability guarantee in vehicular edge computing</article-title><source>IEEE/ACM Trans. Netw.</source><year>2023</year><pub-id pub-id-type="doi">10.1109/TNET.2023.3279512</pub-id></element-citation></ref><ref id="CR21"><label>21.</label><mixed-citation publication-type="other">Yan, G., Liu, K. &#x00026; Liu, C., Zhang, J. A survey. IEEE Transactions on Consumer Electronics, Edge intelligence for internet of vehicles, 2024.</mixed-citation></ref><ref id="CR22"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>C</given-names></name><name><surname>Liu</surname><given-names>K</given-names></name></person-group><article-title>Toward reliable DNN-based task partitioning and offloading in vehicular edge computing</article-title><source>IEEE Trans. Consum. Electron.</source><year>2023</year><pub-id pub-id-type="doi">10.1109/tce.2023.3280484</pub-id></element-citation></ref><ref id="CR23"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ji</surname><given-names>S</given-names></name><name><surname>Xu</surname><given-names>W</given-names></name><name><surname>Yang</surname><given-names>M</given-names></name><name><surname>Yu</surname><given-names>K</given-names></name></person-group><article-title>3d convolutional neural networks for human action recognition</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2012</year><volume>35</volume><fpage>221</fpage><lpage>231</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2012.59</pub-id></element-citation></ref><ref id="CR24"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Simonyan</surname><given-names>K</given-names></name><name><surname>Zisserman</surname><given-names>A</given-names></name></person-group><article-title>Two-stream convolutional networks for action recognition in videos</article-title><source>Adv. Neural Inf. Process. Syst.</source><year>2014</year><volume>27</volume><fpage>1</fpage></element-citation></ref><ref id="CR25"><label>25.</label><mixed-citation publication-type="other">Tran, D., Bourdev, L., Fergus, R., Torresani, L. &#x00026; Paluri, M. Learning spatiotemporal features with 3d convolutional networks. In <italic>Proc. of the IEEE International Conference on Computer Vision</italic>, 4489&#x02013;4497 (2015).</mixed-citation></ref><ref id="CR26"><label>26.</label><mixed-citation publication-type="other">Wang, L. <italic>et&#x000a0;al.</italic> Temporal segment networks: Towards good practices for deep action recognition. In <italic>European Conference on Computer Vision</italic>, 20&#x02013;36 (Springer, 2016).</mixed-citation></ref><ref id="CR27"><label>27.</label><mixed-citation publication-type="other">Sargano, A. B., Wang, X., Angelov, P. &#x00026; Habib, Z. Human action recognition using transfer learning with deep representations. In <italic>2017 International Joint Conference on Neural Networks (IJCNN)</italic>, 463&#x02013;469 (IEEE, 2017).</mixed-citation></ref><ref id="CR28"><label>28.</label><mixed-citation publication-type="other">Lin, J., Gan, C. &#x00026; Han, S. Tsm: Temporal shift module for efficient video understanding. In <italic>Proc. of the IEEE/CVF International Conference on Computer Vision</italic>, 7083&#x02013;7093 (2019).</mixed-citation></ref><ref id="CR29"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dai</surname><given-names>C</given-names></name><name><surname>Liu</surname><given-names>X</given-names></name><name><surname>Lai</surname><given-names>J</given-names></name></person-group><article-title>Human action recognition using two-stream attention based lSTM networks</article-title><source>Appl. Soft Comput.</source><year>2020</year><volume>86</volume><fpage>105820</fpage><pub-id pub-id-type="doi">10.1016/j.asoc.2019.105820</pub-id></element-citation></ref><ref id="CR30"><label>30.</label><mixed-citation publication-type="other">Ulhaq, A., Akhtar, N., Pogrebna, G. &#x00026; Mian, A. Vision transformers for action recognition: A survey. Preprint at <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2209.05700">http://arxiv.org/abs/2209.05700</ext-link> (2022).</mixed-citation></ref><ref id="CR31"><label>31.</label><mixed-citation publication-type="other">Ahn, D., Kim, S., Hong, H. &#x00026; Ko, B. C. Star-transformer: a spatio-temporal cross attention transformer for human action recognition. In <italic>Proc. of the IEEE/CVF Winter Conference on Applications of Computer Vision</italic>, 3330&#x02013;3339 (2023).</mixed-citation></ref><ref id="CR32"><label>32.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Qiao</surname><given-names>C</given-names></name><name><surname>Brown</surname><given-names>KN</given-names></name><name><surname>Zhang</surname><given-names>F</given-names></name><name><surname>Tian</surname><given-names>Z</given-names></name></person-group><article-title>Adaptive asynchronous clustering algorithms for wireless mesh networks</article-title><source>IEEE Trans. Knowl. Data Eng.</source><year>2021</year><volume>35</volume><fpage>2610</fpage><lpage>2627</lpage></element-citation></ref><ref id="CR33"><label>33.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhou</surname><given-names>Y</given-names></name><etal/></person-group><article-title>Cdtier: A Chinese dataset of threat intelligence entity relationships</article-title><source>IEEE Trans. Sustain. Comput.</source><year>2023</year><pub-id pub-id-type="doi">10.1109/TSUSC.2023.3240411</pub-id></element-citation></ref><ref id="CR34"><label>34.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>L</given-names></name><etal/></person-group><article-title>Temporal segment networks for action recognition in videos</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2018</year><volume>41</volume><fpage>2740</fpage><lpage>2755</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2018.2868668</pub-id><?supplied-pmid 30183621?><pub-id pub-id-type="pmid">30183621</pub-id>
</element-citation></ref><ref id="CR35"><label>35.</label><mixed-citation publication-type="other">Zhou, B., Andonian, A., Oliva, A. &#x00026; Torralba, A. Temporal relational reasoning in videos. In <italic>Proc. of the European Conference on Computer Vision (ECCV)</italic>, 803&#x02013;818 (2018).</mixed-citation></ref><ref id="CR36"><label>36.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>C</given-names></name><etal/></person-group><article-title>Deep manifold structure transfer for action recognition</article-title><source>IEEE Trans. Image Process.</source><year>2019</year><volume>28</volume><fpage>4646</fpage><lpage>4658</lpage><pub-id pub-id-type="doi">10.1109/TIP.2019.2912357</pub-id></element-citation></ref><ref id="CR37"><label>37.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Luo</surname><given-names>H</given-names></name><etal/></person-group><article-title>Dense semantics-assisted networks for video action recognition</article-title><source>IEEE Trans. Circuits Syst. Video Technol.</source><year>2021</year><volume>32</volume><fpage>3073</fpage><lpage>3084</lpage><pub-id pub-id-type="doi">10.1109/TCSVT.2021.3100842</pub-id></element-citation></ref><ref id="CR38"><label>38.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Qiao</surname><given-names>C</given-names></name><etal/></person-group><article-title>Evaluation mechanism for decentralized collaborative pattern learning in heterogeneous vehicular networks</article-title><source>IEEE Trans. Intell. Transport. Syst.</source><year>2022</year><pub-id pub-id-type="doi">10.1109/TITS.2022.3186630</pub-id></element-citation></ref><ref id="CR39"><label>39.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ren</surname><given-names>Y</given-names></name><name><surname>Xiao</surname><given-names>Y</given-names></name><name><surname>Zhou</surname><given-names>Y</given-names></name><name><surname>Zhang</surname><given-names>Z</given-names></name><name><surname>Tian</surname><given-names>Z</given-names></name></person-group><article-title>Cskg4apt: A cybersecurity knowledge graph for advanced persistent threat organization attribution</article-title><source>IEEE Trans. Knowl. Data Eng.</source><year>2022</year><pub-id pub-id-type="doi">10.15680/IJIRSET.2024.1304287</pub-id><?supplied-pmid 37092026?><pub-id pub-id-type="pmid">37092026</pub-id>
</element-citation></ref><ref id="CR40"><label>40.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>J</given-names></name><name><surname>Chen</surname><given-names>Y</given-names></name><name><surname>Hao</surname><given-names>S</given-names></name><name><surname>Peng</surname><given-names>X</given-names></name><name><surname>Hu</surname><given-names>L</given-names></name></person-group><article-title>Deep learning for sensor-based activity recognition: A survey</article-title><source>Pattern Recogn. Lett.</source><year>2019</year><volume>119</volume><fpage>3</fpage><lpage>11</lpage><pub-id pub-id-type="doi">10.1016/j.patrec.2018.02.010</pub-id></element-citation></ref><ref id="CR41"><label>41.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zebhi</surname><given-names>S</given-names></name></person-group><article-title>Human activity recognition using wearable sensors based on image classification</article-title><source>IEEE Sens. J.</source><year>2022</year><volume>22</volume><fpage>12117</fpage><lpage>12126</lpage><pub-id pub-id-type="doi">10.1109/JSEN.2022.3174280</pub-id></element-citation></ref><ref id="CR42"><label>42.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alghamdi</surname><given-names>WY</given-names></name></person-group><article-title>A novel deep learning method for predicting athletes&#x02019; health using wearable sensors and recurrent neural networks</article-title><source>Decis. Anal. J.</source><year>2023</year><volume>7</volume><fpage>100213</fpage><pub-id pub-id-type="doi">10.1016/j.dajour.2023.100213</pub-id></element-citation></ref><ref id="CR43"><label>43.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shi</surname><given-names>H</given-names></name><name><surname>Hou</surname><given-names>Z</given-names></name><name><surname>Liang</surname><given-names>J</given-names></name><name><surname>Lin</surname><given-names>E</given-names></name><name><surname>Zhong</surname><given-names>Z</given-names></name></person-group><article-title>Dsfnet: A distributed sensors fusion network for action recognition</article-title><source>IEEE Sens. J.</source><year>2022</year><volume>23</volume><fpage>839</fpage><lpage>848</lpage><pub-id pub-id-type="doi">10.1109/JSEN.2022.3225031</pub-id></element-citation></ref><ref id="CR44"><label>44.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wannenburg</surname><given-names>J</given-names></name><name><surname>Malekian</surname><given-names>R</given-names></name></person-group><article-title>Physical activity recognition from smartphone accelerometer data for user context awareness sensing</article-title><source>IEEE Trans. Syst. Man Cybern. Syst.</source><year>2016</year><volume>47</volume><fpage>3142</fpage><lpage>3149</lpage><pub-id pub-id-type="doi">10.1109/TSMC.2016.2562509</pub-id></element-citation></ref><ref id="CR45"><label>45.</label><mixed-citation publication-type="other">Wang, Z. &#x00026; Oates, T. Imaging time-series to improve classification and imputation. Preprint at <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1506.00327">http://arxiv.org/abs/1506.00327</ext-link> (2015).</mixed-citation></ref><ref id="CR46"><label>46.</label><mixed-citation publication-type="other">Ni, J., Ngu, A. H. &#x00026; Yan, Y. Progressive cross-modal knowledge distillation for human action recognition. In <italic>Proc. of the 30th ACM International Conference on Multimedia</italic>, 5903&#x02013;5912 (2022).</mixed-citation></ref><ref id="CR47"><label>47.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Deng</surname><given-names>S</given-names></name><etal/></person-group><article-title>Lhar: Lightweight human activity recognition on knowledge distillation</article-title><source>IEEE J. Biomed. Health Inform.</source><year>2023</year><pub-id pub-id-type="doi">10.1109/JBHI.2023.3298932</pub-id><?supplied-pmid 37922162?><pub-id pub-id-type="pmid">37922162</pub-id>
</element-citation></ref><ref id="CR48"><label>48.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xu</surname><given-names>Q</given-names></name><name><surname>Wu</surname><given-names>M</given-names></name><name><surname>Li</surname><given-names>X</given-names></name><name><surname>Mao</surname><given-names>K</given-names></name><name><surname>Chen</surname><given-names>Z</given-names></name></person-group><article-title>Contrastive distillation with regularized knowledge for deep model compression on sensor-based human activity recognition</article-title><source>IEEE Trans. Ind. Cyber-Phys. Syst.</source><year>2023</year><pub-id pub-id-type="doi">10.1109/TICPS.2023.3320630</pub-id></element-citation></ref><ref id="CR49"><label>49.</label><mixed-citation publication-type="other">Shaikh, M. B., Chai, D., Islam, S. M.&#x000a0;S. &#x00026; Akhtar, N. Maivar: Multimodal audio-image and video action recognizer. In <italic>2022 IEEE International Conference on Visual Communications and Image Processing (VCIP)</italic>, 1&#x02013;5 (IEEE, 2022).</mixed-citation></ref><ref id="CR50"><label>50.</label><mixed-citation publication-type="other">Park, W., Kim, D., Lu, Y. &#x00026; Cho, M. Relational knowledge distillation. In <italic>Proc. of the IEEE/CVF conference on computer vision and pattern recognition</italic>, 3967&#x02013;3976 (2019).</mixed-citation></ref><ref id="CR51"><label>51.</label><mixed-citation publication-type="other">Tung, F. &#x00026; Mori, G. Similarity-preserving knowledge distillation. In <italic>Proc. of the IEEE/CVF international conference on computer vision</italic>, 1365&#x02013;1374 (2019).</mixed-citation></ref><ref id="CR52"><label>52.</label><mixed-citation publication-type="other">Crasto, N., Weinzaepfel, P., Alahari, K. &#x00026; Schmid, C. Mars: Motion-augmented rgb stream for action recognition. In <italic>Proc. of the IEEE/CVF conference on computer vision and pattern recognition</italic>, 7882&#x02013;7891 (2019).</mixed-citation></ref><ref id="CR53"><label>53.</label><mixed-citation publication-type="other">Zagoruyko, S. &#x00026; Komodakis, N. Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer. Preprint at <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1612.03928">http://arxiv.org/abs/1612.03928</ext-link> (2016).</mixed-citation></ref><ref id="CR54"><label>54.</label><mixed-citation publication-type="other">Zhao, H., Jia, J. &#x00026; Koltun, V. Exploring self-attention for image recognition. In <italic>Proc. of the IEEE/CVF conference on computer vision and pattern recognition</italic>, 10076&#x02013;10085 (2020).</mixed-citation></ref><ref id="CR55"><label>55.</label><mixed-citation publication-type="other">Morcos, A. S., Barrett, D. G., Rabinowitz, N. C. &#x00026; Botvinick, M. On the importance of single directions for generalization. Preprint at <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1803.06959">http://arxiv.org/abs/1803.06959</ext-link> (2018).</mixed-citation></ref><ref id="CR56"><label>56.</label><mixed-citation publication-type="other">Devlin, J., Chang, M.-W., Lee, K. &#x00026; Toutanova, K. Bert: Pre-training of deep bidirectional transformers for language understanding. Preprint at <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1810.04805">http://arxiv.org/abs/1810.04805</ext-link> (2018).</mixed-citation></ref><ref id="CR57"><label>57.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nie</surname><given-names>F</given-names></name><name><surname>Xu</surname><given-names>D</given-names></name><name><surname>Tsang</surname><given-names>IW-H</given-names></name><name><surname>Zhang</surname><given-names>C</given-names></name></person-group><article-title>Flexible manifold embedding: A framework for semi-supervised and unsupervised dimension reduction</article-title><source>IEEE Trans. Image Process.</source><year>2010</year><volume>19</volume><fpage>1921</fpage><lpage>1932</lpage><pub-id pub-id-type="doi">10.1109/TIP.2010.2044958</pub-id><?supplied-pmid 20215078?><pub-id pub-id-type="pmid">20215078</pub-id>
</element-citation></ref><ref id="CR58"><label>58.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Spielman</surname><given-names>D</given-names></name></person-group><article-title>Spectral graph theory</article-title><source>Combinatorial Sci. Comput.</source><year>2012</year><volume>18</volume><fpage>18</fpage></element-citation></ref><ref id="CR59"><label>59.</label><mixed-citation publication-type="other">Ba, J. L., Kiros, J. R. &#x00026; Hinton, G. E. Layer normalization. Preprint at <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1607.06450">http://arxiv.org/abs/1607.06450</ext-link> (2016).</mixed-citation></ref><ref id="CR60"><label>60.</label><mixed-citation publication-type="other">Loshchilov, I. &#x00026; Hutter, F. Decoupled weight decay regularization. Preprint at <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1711.05101">http://arxiv.org/abs/1711.05101</ext-link> (2017).</mixed-citation></ref><ref id="CR61"><label>61.</label><mixed-citation publication-type="other">Long, X. <italic>et&#x000a0;al.</italic> Multimodal keyless attention fusion for video classification. In <italic>Proc. of the aaai conference on artificial intelligence</italic>, Vol. 32 (2018).</mixed-citation></ref><ref id="CR62"><label>62.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>Y</given-names></name><name><surname>Wang</surname><given-names>K</given-names></name><name><surname>Li</surname><given-names>G</given-names></name><name><surname>Lin</surname><given-names>L</given-names></name></person-group><article-title>Semantics-aware adaptive knowledge distillation for sensor-to-vision action recognition</article-title><source>IEEE Trans. Image Process.</source><year>2021</year><volume>30</volume><fpage>5573</fpage><lpage>5588</lpage><pub-id pub-id-type="doi">10.1109/TIP.2021.3086590</pub-id><?supplied-pmid 34110991?><pub-id pub-id-type="pmid">34110991</pub-id>
</element-citation></ref><ref id="CR63"><label>63.</label><mixed-citation publication-type="other">Islam, M. M. &#x00026; Iqbal, T. Hamlet: A hierarchical multimodal attention-based human activity recognition algorithm. In <italic>2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</italic>, 10285&#x02013;10292 (IEEE, 2020).</mixed-citation></ref><ref id="CR64"><label>64.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Islam</surname><given-names>MM</given-names></name><name><surname>Iqbal</surname><given-names>T</given-names></name></person-group><article-title>Mumu: Cooperative multitask learning-based guided multimodal fusion</article-title><source>Proc. AAAI Conf. Artif. Intell.</source><year>2022</year><volume>36</volume><fpage>1043</fpage><lpage>1051</lpage></element-citation></ref><ref id="CR65"><label>65.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>T</given-names></name><name><surname>Kong</surname><given-names>J</given-names></name><name><surname>Jiang</surname><given-names>M</given-names></name></person-group><article-title>Rgb-d action recognition using multimodal correlative representation learning model</article-title><source>IEEE Sens. J.</source><year>2018</year><volume>19</volume><fpage>1862</fpage><lpage>1872</lpage><pub-id pub-id-type="doi">10.1109/JSEN.2018.2884443</pub-id></element-citation></ref><ref id="CR66"><label>66.</label><mixed-citation publication-type="other">Liu, M. &#x00026; Yuan, J. Recognizing human actions as the evolution of pose estimation maps. In <italic>Proc. of the IEEE Conference on Computer Vision and Pattern Recognition</italic>, 1159&#x02013;1168 (2018).</mixed-citation></ref><ref id="CR67"><label>67.</label><mixed-citation publication-type="other">Selvaraju, R. R. <italic>et&#x000a0;al.</italic> Grad-cam: Visual explanations from deep networks via gradient-based localization. In <italic>Proc. of the IEEE International Conference on Computer Vision</italic>, 618&#x02013;626 (2017).</mixed-citation></ref></ref-list></back></article>