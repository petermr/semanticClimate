<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd"> 
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><?properties open_access?><?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?><?DTDIdentifier.IdentifierType public?><?SourceDTD.DTDName A++V2.4.dtd?><?SourceDTD.Version 2.4?><?ConverterInfo.XSLTName springer2nlmx2.xsl?><?ConverterInfo.Version 1?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sci Rep</journal-id><journal-id journal-id-type="iso-abbrev">Sci Rep</journal-id><journal-title-group><journal-title>Scientific Reports</journal-title></journal-title-group><issn pub-type="epub">2045-2322</issn><publisher><publisher-name>Nature Publishing Group UK</publisher-name><publisher-loc>London</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmcid">10943221</article-id><article-id pub-id-type="publisher-id">56976</article-id><article-id pub-id-type="doi">10.1038/s41598-024-56976-5</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Application of the transformer model algorithm in chinese word sense disambiguation: a case study in chinese language</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Li</surname><given-names>Linlin</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Li</surname><given-names>Juxing</given-names></name><address><email>ljx2019@xjtu.edu.cn</email></address><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author"><name><surname>Wang</surname><given-names>Hongli</given-names></name><xref ref-type="aff" rid="Aff3">3</xref></contrib><contrib contrib-type="author"><name><surname>Nie</surname><given-names>Jianing</given-names></name><xref ref-type="aff" rid="Aff4">4</xref></contrib><aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/011ashp19</institution-id><institution-id institution-id-type="GRID">grid.13291.38</institution-id><institution-id institution-id-type="ISNI">0000 0001 0807 1581</institution-id><institution>The College of Literature and Journalism, </institution><institution>Sichuan University, </institution></institution-wrap>Chengdu, 610000 China </aff><aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/017zhmm22</institution-id><institution-id institution-id-type="GRID">grid.43169.39</institution-id><institution-id institution-id-type="ISNI">0000 0001 0599 1243</institution-id><institution>School of Journalism and New Media, </institution><institution>Xi&#x02019;an Jiaotong University, </institution></institution-wrap>Xi&#x02019;an, 710049 China </aff><aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="GRID">grid.410561.7</institution-id><institution-id institution-id-type="ISNI">0000 0001 0169 5113</institution-id><institution>School of Artificial Intelligence, </institution><institution>Tiangong University, </institution></institution-wrap>Tianjin, 300000 China </aff><aff id="Aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/02jgsf398</institution-id><institution-id institution-id-type="GRID">grid.413242.2</institution-id><institution-id institution-id-type="ISNI">0000 0004 1765 9039</institution-id><institution>School of Art, College of International Business and Economics, </institution><institution>Wuhan Textile University, </institution></institution-wrap>Wuhan, 430000 China </aff></contrib-group><pub-date pub-type="epub"><day>15</day><month>3</month><year>2024</year></pub-date><pub-date pub-type="pmc-release"><day>15</day><month>3</month><year>2024</year></pub-date><pub-date pub-type="collection"><year>2024</year></pub-date><volume>14</volume><elocation-id>6320</elocation-id><history><date date-type="received"><day>22</day><month>9</month><year>2023</year></date><date date-type="accepted"><day>13</day><month>3</month><year>2024</year></date></history><permissions><copyright-statement>&#x000a9; The Author(s) 2024</copyright-statement><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p></license></permissions><abstract id="Abs1"><p id="Par1">This study aims to explore the research methodology of applying the Transformer model algorithm to Chinese word sense disambiguation, seeking to resolve word sense ambiguity in the Chinese language. The study introduces deep learning and designs a Chinese word sense disambiguation model based on the fusion of the Transformer with the Bi-directional Long Short-Term Memory (BiLSTM) algorithm. By utilizing the self-attention mechanism of Transformer and the sequence modeling capability of BiLSTM, this model efficiently captures semantic information and context relationships in Chinese sentences, leading to accurate word sense disambiguation. The model&#x02019;s evaluation is conducted using the PKU Paraphrase Bank, a Chinese text paraphrase dataset. The results demonstrate that the model achieves a precision rate of 83.71% in Chinese word sense disambiguation, significantly outperforming the Long Short-Term Memory algorithm. Additionally, the root mean squared error of this algorithm is less than 17, with a loss function value remaining around 0.14. Thus, this study validates that the constructed Transformer-fused BiLSTM-based Chinese word sense disambiguation model algorithm exhibits both high accuracy and robustness in identifying word senses in the Chinese language. The findings of this study provide valuable insights for advancing the intelligent development of word senses in Chinese language applications.</p></abstract><kwd-group xml:lang="en"><title>Keywords</title><kwd>Transformer model algorithm</kwd><kwd>Chinese language</kwd><kwd>BiLSTM</kwd><kwd>Word sense disambiguation</kwd><kwd>Root mean squared error</kwd></kwd-group><kwd-group kwd-group-type="npg-subject"><title>Subject terms</title><kwd>Computer science</kwd><kwd>Information technology</kwd></kwd-group><custom-meta-group><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#x000a9; Springer Nature Limited 2024</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1"><title>Introduction</title><p id="Par2">Natural Language Processing (NLP), a vital branch of artificial intelligence, aims to facilitate computers in understanding, processing, and generating human language. Among the diverse tasks within NLP, word sense disambiguation is a challenging and fundamental problem<sup><xref ref-type="bibr" rid="CR1">1</xref>&#x02013;<xref ref-type="bibr" rid="CR3">3</xref></sup>. In complex languages like Chinese, word sense ambiguity is more pronounced due to the polysemy of words and the intricacies of context. For instance, a word may carry different meanings based on context, presenting significant challenges in text comprehension and semantic reasoning<sup><xref ref-type="bibr" rid="CR4">4</xref>,<xref ref-type="bibr" rid="CR5">5</xref></sup>. Consequently, it becomes essential to address word sense ambiguity in the Chinese language and achieve accurate word sense disambiguation to enhance the effectiveness of NLP applications.</p><p id="Par3">Traditional approaches in Chinese word sense disambiguation encounter certain limitations. While word vector representation methods like Word2Vec can capture semantic information to some extent, they struggle with handling long-range dependencies and complex sentence structures<sup><xref ref-type="bibr" rid="CR6">6</xref></sup>. Conversely, conventional sequence models such as Long Short-Term Memory (LSTM) can consider contextual information. Still, they may face challenges like vanishing or exploding gradients when dealing with lengthy sequences, leading to limited effectiveness<sup><xref ref-type="bibr" rid="CR7">7</xref></sup>. The Transformer model presents an innovative sequence modeling algorithm with a self-attention mechanism to overcome these limitations. This mechanism efficiently captures correlations between different positions in the input sequence, enabling parallel computation and capturing long-range dependencies<sup><xref ref-type="bibr" rid="CR8">8</xref>,<xref ref-type="bibr" rid="CR9">9</xref></sup>. Furthermore, the integration of Bi-directional Long Short-Term Memory (BiLSTM), which considers both forward and backward contexts, enhances the model&#x02019;s ability to comprehend sentence semantics<sup><xref ref-type="bibr" rid="CR10">10</xref>,<xref ref-type="bibr" rid="CR11">11</xref></sup>.</p><p id="Par4">In conclusion, this study presents a novel application of the Transformer model algorithm to address Chinese word sense ambiguity through word sense disambiguation. By combining the self-attention mechanism of Transformer with the sequence modeling capability of BiLSTM, an efficient and accurate Chinese word sense recognition model is constructed.</p><p id="Par5">The significance of this study can be summarized as follows: Firstly, it explores a practical approach to applying the Transformer model to Chinese word sense disambiguation tasks, contributing new perspectives to Chinese NLP. This study highlights the integration of the self-attention mechanism from the Transformer model and the sequence modeling capability of the BiLSTM algorithm. By combining these elements, the model excels in capturing semantic information and contextual relationships within Chinese sentences, leading to enhanced accuracy in word sense disambiguation. This improved accuracy is substantiated by experimental results from the PKU Paraphrase Bank, where the model achieved an accuracy of 83.71%, surpassing the performance of the LSTM algorithm. Secondly, it enhances the accuracy and robustness of Chinese word sense disambiguation, providing practical solutions to the challenge of word sense ambiguity. Lastly, this study lays the foundation for advancing Chinese semantic intelligence and offers substantial support for applications in intelligent dialogue, machine translation, text generation, and related fields. As a result, the findings of this study hold promise for future research and development in the field of Chinese language processing and semantic understanding. The innovation in this study manifests across multiple dimensions. Firstly, the research explores the application of the Transformer model in Chinese word sense disambiguation tasks, introducing novel ideas and methodologies to the realm of Chinese NLP. By integrating the Transformer model with the BiLSTM algorithm, the study comprehensively addresses the intricacies of the Chinese context in model construction, effectively tackling the challenge of Chinese word sense ambiguity. Linguistic disparities in structures, expressions, and grammar rules between Chinese and English directly impact approaches to word sense disambiguation tasks. To accommodate these distinctions, the study adapts the Transformer and BiLSTM to align more closely with the linguistic characteristics of Chinese. Through thorough adjustments and optimizations, the model ensures its capacity to accurately capture semantic information and contextual relationships in the Chinese language, thereby enhancing word sense disambiguation performance. Furthermore, by leveraging the self-attention mechanism of the Transformer and the sequence modeling capability of BiLSTM, the designed model adeptly captures semantic information and contextual relationships in Chinese sentences. This integrated design enhances the model&#x02019;s accuracy in Chinese word sense disambiguation, offering a fresh perspective for addressing natural language understanding challenges in Chinese contexts. The study meticulously selects a Chinese dataset (PKU Paraphrase Bank) that closely aligns with the Chinese context for model evaluation, avoiding merely adapting English datasets to the Chinese environment. This careful dataset selection, coupled with profound model optimizations for the Chinese context, underscores the innovation and distinctiveness of this study in addressing the intricacies of Chinese word sense disambiguation. The synergistic combination of dataset selection and innovative model structure design ensures the adaptability and high performance of the model for Chinese word sense disambiguation tasks. The study explores various facets, including methodology, model design, and dataset selection, presenting a novel approach for Chinese word sense disambiguation tasks. Experimental results showcase the model&#x02019;s high accuracy and robustness in the Chinese context, contributing new insights to the field of Chinese NLP and practical solutions for future research and applications in domains such as intelligent dialogue, machine translation, and text generation.</p></sec><sec id="Sec2"><title>Literature review</title><sec id="Sec3"><title>Research status of Chinese word sense disambiguation and word sense ambiguity</title><p id="Par6">As a complex and linguistically diverse language, Chinese has been a prominent research focus in the NLP field concerning word sense disambiguation and ambiguity. Word sense disambiguation involves comprehending the specific meaning of a word within varying contextual environments. Conversely, word sense ambiguity pertains to a word having multiple meanings that necessitate precise identification based on the context<sup><xref ref-type="bibr" rid="CR12">12</xref></sup>. Numerous scholars have made significant contributions to this area of research. Li et al<sup><xref ref-type="bibr" rid="CR13">13</xref></sup>. revealed the existence of universal and specific reading mechanisms under different writing systems. Their findings shed light on cross-linguistic differences in the reading process, offering valuable insights into the universality and specificity of the reading system. In the work of Wang et al<sup><xref ref-type="bibr" rid="CR14">14</xref></sup>., a robust pre-training language model was proposed for oral comprehension, exhibiting efficient performance on verbal comprehension tasks. S&#x000f8;gaard<sup><xref ref-type="bibr" rid="CR15">15</xref></sup> explored methods to establish word vector spaces from raw text to enhance the semantic meaning of words, demonstrating its effectiveness in handling semantic information. Chen and Chen<sup><xref ref-type="bibr" rid="CR16">16</xref></sup> uncovered that long-term semantic representations of known vocabulary undergo updates and integration with new information during second language learning, revealing the dynamic nature of semantic learning. Collectively, these studies contribute to the advancement of word sense understanding and disambiguation in the context of the Chinese language.</p></sec><sec id="Sec4"><title>Transformer model and its current applications in NLP</title><p id="Par7">The Transformer model, a deep learning architecture built upon attention mechanisms, has gained considerable prominence due to its robust modeling capabilities and advantages in parallel computing. Its performance has been remarkable across various NLP tasks, including language modeling, machine translation, and text classification. Numerous researchers in relevant fields have conducted in-depth investigations on this model. Von der Mosel et al<sup><xref ref-type="bibr" rid="CR17">17</xref></sup>. explored the efficacy of pre-trained Transformers in NLP for software engineering. The research substantiated the practicality of employing pre-trained Transformers in this context, providing valuable empirical evidence. Peer et al<sup><xref ref-type="bibr" rid="CR18">18</xref></sup>. proposed a greedy layer pruning method to accelerate Transformer models in NLP. This approach effectively compressed the model size, improving training and inference speeds. Yang et al<sup><xref ref-type="bibr" rid="CR19">19</xref></sup>. compared Transformers and traditional NLP methods in radiology report classification. The findings revealed that Transformer models necessitated fewer data to achieve automated report classification than conventional methods. Moreover, Remedios and Remedios<sup><xref ref-type="bibr" rid="CR20">20</xref></sup>. explored the potential of applying the Transformer model to large-scale language processing in clinical radiology, demonstrating its positive impact on advancing NLP applications in this domain.</p></sec><sec id="Sec5"><title>BiLSTM model and its advantages in sequence modeling</title><p id="Par8">The BiLSTM network stands as a widely employed model in the field of sequence modeling. This model exhibits the unique capability to consider both past and future information of the input sequence simultaneously, enabling it to effectively capture dependencies between contexts. Consequently, BiLSTM excels in tasks that demand word sense recognition. Extensive research has been conducted on the application of BiLSTM by various scholars. Xu<sup><xref ref-type="bibr" rid="CR21">21</xref></sup> explored the use of BiLSTM in intelligent human&#x02013;computer interaction (HCI) within the domain of deep learning. The findings underscored the potential of deep learning in achieving intelligent HCI. Sornlertlamvanich &#x00026; Yuenyong<sup><xref ref-type="bibr" rid="CR22">22</xref></sup> utilized the Bidirectional Long Short-Term Memory&#x02014;Convolutional Neural Network&#x02014;Conditional Random Field model in conjunction with the Thai character cluster method to implement Thai-named entity recognition. The results demonstrated the effectiveness of this approach in the Thai-named entity recognition task. Ma et al<sup><xref ref-type="bibr" rid="CR23">23</xref></sup>. proposed a graph-enhanced sequence-to-sequence model for neural question-answering generation, revealing its strong performance in the question-answering generation task. Additionally, Ye et al<sup><xref ref-type="bibr" rid="CR24">24</xref></sup>. employed the BiLSTM algorithm and Support Vector Machine Naive Bayes classifier for text sentiment recognition, with the method achieving commendable performance in sentiment recognition.</p></sec></sec><sec id="Sec6"><title>Summary</title><p id="Par9">In the field of Chinese lexical semantics disambiguation, numerous researchers and scholars have delved deeply into exploration and study. These methods can generally be categorized into two types: rule-based methods and machine learning-based methods. Rule-based methods typically rely on the expertise of linguists and a profound understanding of the characteristics of the Chinese language. These methods achieve the disambiguation of meanings by formulating a series of rules, such as rules for selecting meanings and context-matching rules. However, a major drawback of this approach is its difficulty in addressing complex language phenomena and the ever-changing nature of meanings. Additionally, rule design often depends on the personal experience and subjective judgment of linguists, lacking objectivity and universality. Machine learning-based methods, on the other hand, usually leverage large-scale corpora for training and automatically learn rules and patterns for lexical semantics disambiguation. Traditional machine learning algorithms such as support vector machines naive Bayes, as well as deep learning algorithms like convolutional neural networks, recurrent neural networks, etc., are widely applied to disambiguation tasks. However, the performance of these models is often influenced by the quality and quantity of the training data. If the training data is of low quality or insufficient, the model&#x02019;s performance may be constrained.</p><p id="Par10">Through the analysis of the research mentioned above, it is evident that, previous research on Chinese word sense disambiguation and ambiguity has been characterized by its scattered nature, lacking systematic integration and in-depth exploration.</p><p id="Par11">Although scholars have recognized differences in language structure and grammar rules, there has been relatively little exploration of the application of deep learning models to address Chinese word sense ambiguity tasks. Secondly, the current Transformer model has demonstrated outstanding performance in the field of NLP. However, previous research has primarily concentrated on its application in specific domains (such as software engineering and clinical radiology), with a paucity of systematic studies on the Transformer in Chinese word sense disambiguation tasks. This underscores the need for a comprehensive examination to validate the applicability and performance of the Transformer model in the Chinese context. Additionally, while BiLSTM has achieved significant success in the field of sequence modeling, previous research has predominantly centered on its application in various tasks, with limited integration with models like the Transformer, especially in the context of Chinese word sense disambiguation tasks. This motivates the endeavor to integrate Transformer and BiLSTM to leverage their respective strengths, with the aim of enhancing the accuracy of Chinese word sense disambiguation. Meanwhile, the application of deep learning models in various fields is common. For instance, BiLSTM is widely used in sequence modeling across different domains, while a Transformer is commonly employed in NLP tasks such as language modeling, machine translation, and text classification. However, applying Transformer to Chinese word sense recognition still presents significant challenges. Therefore, this study aims to fuse BiLSTM and Transformer to be used in Chinese word sense recognition, intending to contribute valuable insights and guidance for the accurate processing and credit of the Chinese language in future endeavors.</p></sec><sec id="Sec7"><title>Chinese word sense recognition method based on Transformer and BiLSTM</title><sec id="Sec8"><title>Chinese word sense dataset and preprocessing analysis</title><p id="Par12">In the field of NLP, Chinese word sense recognition plays a crucial role in multiple tasks and applications. Chinese word sense recognition refers to determining the specific meaning or semantics of a word in a given context. Due to the polysemy or multiple pronunciations of many Chinese words, a single word may have several distinct meanings<sup><xref ref-type="bibr" rid="CR25">25</xref>,<xref ref-type="bibr" rid="CR26">26</xref></sup>. Therefore, Chinese word sense recognition is essential for accurately understanding the semantic content of sentences or texts. Table <xref rid="Tab1" ref-type="table">1</xref> illustrates the polysemy of the same Chinese word in different contexts.<table-wrap id="Tab1"><label>Table 1</label><caption><p>List of examples of polysemy in Chinese.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"/><th align="left">Word</th><th align="left">Meaning 1</th><th align="left">Meaning 2</th></tr></thead><tbody><tr><td align="left" rowspan="4">Different meaning</td><td align="left">&#x04e70;</td><td align="left">Purchase items</td><td align="left">Win a competition</td></tr><tr><td align="left">&#x05b66;</td><td align="left">Acquire knowledge</td><td align="left">Go to school</td></tr><tr><td align="left">&#x07ea2;</td><td align="left">Color: Red</td><td align="left">State of affairs: loss, failure</td></tr><tr><td align="left">&#x05934;</td><td align="left">Body part: Head</td><td align="left">One end of an object: Pen tip, knife edge</td></tr><tr><td align="left" rowspan="4">Different sound</td><td align="left">&#x091cd;</td><td align="left">zh&#x000f2;ng (weight)</td><td align="left">ch&#x000f3;ng (repeat)</td></tr><tr><td align="left">&#x0884c;</td><td align="left">x&#x000ed;ng (to walk)</td><td align="left">h&#x000e1;ng (bank, company)</td></tr><tr><td align="left">&#x06253;</td><td align="left">d&#x001ce; (to make a phone call)</td><td align="left">d&#x000e1; (quantifier, a dozen eggs)</td></tr><tr><td align="left">&#x04e86;</td><td align="left">le (already)</td><td align="left">li&#x001ce;o (to understand)</td></tr></tbody></table></table-wrap></p><p id="Par13">Therefore, in tasks such as text analysis, information retrieval, and machine translation, word sense disambiguation plays a crucial role in ensuring the correct comprehension and expression of sentence meanings. In information retrieval, search engines rely on accurately identifying word senses to return relevant search results. In machine translation, precise recognition of word senses in the source language sentence aids the translation system in selecting more accurate translation results. Additionally, Chinese word sense disambiguation is crucial in accurately understanding the overall meaning and sentiment orientation of entire texts. Consequently, it is essential for tasks such as text comprehension, sentiment analysis, and question-answering systems.In this study, the task of word sense ambiguity is defined as addressing the challenge unique to the Chinese context, where a vocabulary item may possess multiple meanings (senses). The study specifically aims to develop an effective method capable of accurately identifying and distinguishing the specific meanings of the same word in different contexts. This task is grounded in the recognition that Chinese, as a language, exhibits rich grammatical structures and complex semantic expressions, resulting in the possibility that the same word may carry multiple meanings in different contexts. For example, many Chinese vocabulary items may share similar spelling or pronunciation in different contexts but convey significantly different meanings. Consequently, resolving the issue of word sense ambiguity is paramount for enhancing the accuracy and deep understanding of NLP tasks in Chinese. In the Chinese context, word sense ambiguity encounters a series of unique challenges. First, the expressive manner of Chinese is relatively flexible, and a word may convey multiple meanings through different combinations. Second, the common occurrence of homophones in Chinese increases the difficulty of word sense ambiguity, as a word with the same pronunciation may have entirely different meanings in different contexts. Additionally, the grammatical structure and expression style of Chinese differs significantly from languages like English, necessitating in-depth research on how to effectively capture semantic information in Chinese sentences to resolve ambiguity.</p><p id="Par14">This study selected the PKU-Paraphrase-Bank (<ext-link ext-link-type="uri" xlink:href="https://github.com/pkucoli/PKU-Paraphrase-Bank/">https://github.com/pkucoli/PKU-Paraphrase-Bank/</ext-link>), a Chinese text paraphrase dataset, as the dataset<sup><xref ref-type="bibr" rid="CR27">27</xref></sup>. Peking University developed the dataset to assist researchers in conducting studies related to text paraphrasing, such as text similarity, sentence generation, and semantic understanding. Each dataset entry consists of two columns representing two sentences with the same meaning, separated by &#x0201c;\t&#x0201d;. It contains a total of 509,832 pairs of sentences, with an average of 23.05 words per sentence<sup><xref ref-type="bibr" rid="CR28">28</xref></sup>. The specific preprocessing flow for applying this dataset to Chinese word sense disambiguation is illustrated in Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>.<fig id="Fig1"><label>Figure 1</label><caption><p>Data preprocessing flowchart.</p></caption><graphic xlink:href="41598_2024_56976_Fig1_HTML" id="MO1"/></fig></p><p id="Par15">As depicted in Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>, the preprocessing of the dataset begins with data selection and collection, involving many Chinese sentence pairs. These sentence pairs consist of paraphrased sentences with similar meanings. The subsequent step involves data cleaning to eliminate possible noise, irrelevant or redundant information, and duplicate data, thereby ensuring data quality and accuracy. Following data cleaning, text annotation is performed, wherein each text pair is labeled with a binary label indicating the relationship between the two sentences. Label 1 signifies that the two sentences are similar (paraphrase relationship), while label 0 demonstrates that the two sentences are dissimilar (non-paraphrase relationship). Subsequently, the data is split into training and testing sets in an 8:2 ratio. Lastly, text processing is executed, encompassing tokenization, stop word removal, and stemming, among other steps, to furnish cleaner and more concise inputs for the model.</p></sec><sec id="Sec9"><title>Transformer model and attention mechanism analysis</title><p id="Par16">The Transformer model consists of an encoder and a decoder. The encoder processes input text, while the decoder generates the output text. The model finds extensive application in tasks such as machine translation<sup><xref ref-type="bibr" rid="CR29">29</xref>,<xref ref-type="bibr" rid="CR30">30</xref></sup>. Figure&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref> illustrates the application of the Transformer model framework in NLP.<fig id="Fig2"><label>Figure 2</label><caption><p>Application of the Transformer model framework in NLP.</p></caption><graphic xlink:href="41598_2024_56976_Fig2_HTML" id="MO2"/></fig></p><p id="Par17">As depicted in Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref>, the Transformer model processes the corpus into word vectors, which are then input to the encoder. Each word vector <italic>x</italic><sub><italic>i</italic></sub> in the matrix <italic>X</italic> undergoes multiplication with three weight matrices, yielding the query vector (<italic>Q</italic>), key vector (<italic>K</italic>), and value vector (<italic>V</italic>). In the self-attention layer, attention is computed between each word of the Chinese ambiguous term and the sentence. Equation&#x000a0;(<xref rid="Equ1" ref-type="disp-formula">1</xref>) describes the calculation for attention, represented as <italic>Attention</italic>(<italic>Q</italic>, <italic>K</italic>, <italic>V</italic>).<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$Attention(Q,K,V) = soft\max \left( {\frac{{Q \cdot K^{T} }}{{\sqrt {d_{k} } }}} \right) \cdot V$$\end{document}</tex-math><mml:math id="M2" display="block"><mml:mrow><mml:mi>A</mml:mi><mml:mi>t</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>Q</mml:mi><mml:mo>,</mml:mo><mml:mi>K</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>s</mml:mi><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>t</mml:mi><mml:mo movablelimits="true">max</mml:mo><mml:mfenced close=")" open="("><mml:mfrac><mml:mrow><mml:mi>Q</mml:mi><mml:mo>&#x000b7;</mml:mo><mml:msup><mml:mi>K</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:mrow><mml:msqrt><mml:msub><mml:mi>d</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:msqrt></mml:mfrac></mml:mfenced><mml:mo>&#x000b7;</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:math><graphic xlink:href="41598_2024_56976_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula>where <italic>Q</italic> represents the matrix of &#x0201c;query&#x0201d; vectors, <italic>K</italic> denotes the matrix of &#x0201c;key&#x0201d; vectors, and <italic>V</italic> signifies the matrix of &#x0201c;value&#x0201d; vectors. The process of Chinese word sense disambiguation using the Transformer model involves data preprocessing, disambiguation feature extraction, and semantic classification. The pseudocode for this specific process is illustrated in Algorithm 1.</p><p id="Par18">
<fig position="anchor" id="Figa"><label>Algorithm 1</label><caption><p><bold>Pseudocode for applying the Transformer model to Chinese word sense disambiguation</bold></p></caption><graphic position="anchor" xlink:href="41598_2024_56976_Figa_HTML" id="MO3"/></fig></p><p id="Par19">In this process, the experiment leverages the multi-head attention mechanism to extract disambiguation features from multiple perspectives<sup><xref ref-type="bibr" rid="CR31">31</xref></sup>. The variation of the multi-head attention mechanism includes N linear transformations on matrices <italic>Q</italic>, <italic>K</italic>, and <italic>V</italic>, and its calculation is as follows:<disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$MultiHeadAtt = concat\left( {Att_{1} ,Att_{2} , \cdots ,Att_{N} } \right) \cdot W^{O}$$\end{document}</tex-math><mml:math id="M4" display="block"><mml:mrow><mml:mi>M</mml:mi><mml:mi>u</mml:mi><mml:mi>l</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>H</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mi>A</mml:mi><mml:mi>t</mml:mi><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:mi>A</mml:mi><mml:mi>t</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mi>t</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x022ef;</mml:mo><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mi>t</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mi>N</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:mo>&#x000b7;</mml:mo><mml:msup><mml:mi>W</mml:mi><mml:mi>O</mml:mi></mml:msup></mml:mrow></mml:math><graphic xlink:href="41598_2024_56976_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq1"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$W^{O}$$\end{document}</tex-math><mml:math id="M6"><mml:msup><mml:mi>W</mml:mi><mml:mi>O</mml:mi></mml:msup></mml:math><inline-graphic xlink:href="41598_2024_56976_Article_IEq1.gif"/></alternatives></inline-formula> refers to the linear mapping matrix that fuses the <italic>N</italic> representations. The formula for <inline-formula id="IEq2"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$Att_{i}$$\end{document}</tex-math><mml:math id="M8"><mml:mrow><mml:mi>A</mml:mi><mml:mi>t</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_56976_Article_IEq2.gif"/></alternatives></inline-formula> is given by Eq.&#x000a0;(<xref rid="Equ3" ref-type="disp-formula">3</xref>).<disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$Att_{i} = \frac{{\left( {\frac{{Q_{i} \cdot K_{{_{i} }}^{T} }}{{\sqrt {d_{k} } }}} \right)}}{{\sum\limits_{i} {\left( {\frac{{Q_{i} \cdot K_{{_{i} }}^{T} }}{{\sqrt {d_{k} } }}} \right)} }} \cdot V_{i}$$\end{document}</tex-math><mml:math id="M10" display="block"><mml:mrow><mml:mi>A</mml:mi><mml:mi>t</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mfenced close=")" open="("><mml:mfrac><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x000b7;</mml:mo><mml:msubsup><mml:mi>K</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mi>T</mml:mi></mml:msubsup></mml:mrow><mml:msqrt><mml:msub><mml:mi>d</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:msqrt></mml:mfrac></mml:mfenced><mml:mrow><mml:munder><mml:mo movablelimits="false">&#x02211;</mml:mo><mml:mi>i</mml:mi></mml:munder><mml:mfenced close=")" open="("><mml:mfrac><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x000b7;</mml:mo><mml:msubsup><mml:mi>K</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mi>T</mml:mi></mml:msubsup></mml:mrow><mml:msqrt><mml:msub><mml:mi>d</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:msqrt></mml:mfrac></mml:mfenced></mml:mrow></mml:mfrac><mml:mo>&#x000b7;</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math><graphic xlink:href="41598_2024_56976_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula></p><p id="Par20">In Eq.&#x000a0;(<xref rid="Equ3" ref-type="disp-formula">3</xref>), <italic>Q</italic><sub><italic>i</italic></sub>, <italic>K</italic><sub><italic>i</italic></sub>, and <italic>V</italic><sub><italic>i</italic></sub> denote the mapping matrices of the original input to the <italic>i</italic>-th subspace.</p></sec><sec id="Sec10"><title>BiLSTM model and its bidirectional sequence modeling</title><p id="Par21">BiLSTM, a variant of the Recurrent Neural Network, operates on the principle of bi-directionality. In the context of a given sentence, its tokens are arranged in a left-to-right order. The distinguishing feature of BiLSTM lies in its utilization of two LSTM networks that function concurrently on both the original sequence (left-to-right) and the reverse sequence (right-to-left). This bidirectional modeling allows the model to consider both the input sequence&#x02019;s context information and better capture the semantic relationships and mutual influences between word meanings<sup><xref ref-type="bibr" rid="CR32">32</xref>,<xref ref-type="bibr" rid="CR33">33</xref></sup>. The specific application of BiLSTM in NLP is illustrated in Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3</xref>.<fig id="Fig3"><label>Figure 3</label><caption><p>Flow of BiLSTM applied to NLP.</p></caption><graphic xlink:href="41598_2024_56976_Fig3_HTML" id="MO4"/></fig></p><p id="Par22">When applying the BiLSTM algorithm in NLP, the disambiguation features (<italic>x</italic><sub><italic>i</italic></sub> and<italic> h</italic><sub><italic>t</italic>-1</sub>) are fed into the input of the BiLSTM model. The input data passes through a sigmoid function to obtain the coefficients (<italic>f</italic><sub><italic>t</italic></sub> and <italic>i</italic><sub><italic>t</italic></sub>). Subsequently, the input is processed through an activation function to obtain the temporary cell variable (<inline-formula id="IEq3"><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\tilde{c}_{t}$$\end{document}</tex-math><mml:math id="M12"><mml:msub><mml:mover accent="true"><mml:mi>c</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mi>t</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_56976_Article_IEq3.gif"/></alternatives></inline-formula>). The computation process is illustrated by Eqs.&#x000a0;(<xref rid="Equ4" ref-type="disp-formula">4</xref>)&#x02013;(<xref rid="Equ5" ref-type="disp-formula">6</xref>).<disp-formula id="Equ4"><label>4</label><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$f_{t} = \delta \left( {\left[ {w_{f} \cdot \left[ {h_{t - 1} ,x_{t} } \right] + b_{f} ,w_{f}{\prime} \cdot \left[ {h_{t - 1}{\prime} ,x_{t} } \right] + b_{f}{\prime} } \right]} \right)$$\end{document}</tex-math><mml:math id="M14" display="block"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>&#x003b4;</mml:mi><mml:mfenced close=")" open="("><mml:mfenced close="]" open="["><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>f</mml:mi></mml:msub><mml:mo>&#x000b7;</mml:mo><mml:mfenced close="]" open="["><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>f</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>f</mml:mi></mml:msub><mml:mo>&#x02032;</mml:mo><mml:mo>&#x000b7;</mml:mo><mml:mfenced close="]" open="["><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>&#x02032;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>f</mml:mi></mml:msub><mml:mo>&#x02032;</mml:mo></mml:mrow></mml:mfenced></mml:mfenced></mml:mrow></mml:math><graphic xlink:href="41598_2024_56976_Article_Equ4.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ5"><label>5</label><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$i_{t} = \delta \left( {\left[ {w_{i} \cdot \left[ {h_{t - 1} ,x_{t} } \right] + b_{i} ,w_{i}{\prime} \cdot \left[ {h_{t - 1}{\prime} ,x_{t} } \right] + b_{i}{\prime} } \right]} \right)$$\end{document}</tex-math><mml:math id="M16" display="block"><mml:mrow><mml:msub><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>&#x003b4;</mml:mi><mml:mfenced close=")" open="("><mml:mfenced close="]" open="["><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x000b7;</mml:mo><mml:mfenced close="]" open="["><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x02032;</mml:mo><mml:mo>&#x000b7;</mml:mo><mml:mfenced close="]" open="["><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>&#x02032;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x02032;</mml:mo></mml:mrow></mml:mfenced></mml:mfenced></mml:mrow></mml:math><graphic xlink:href="41598_2024_56976_Article_Equ5.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ6"><label>6</label><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\tilde{c}_{t} = \tanh \left( {\left[ {w_{c} \cdot \left[ {h_{t - 1} ,x_{t} } \right] + b_{c} ,w_{c}{\prime} \cdot \left[ {h_{t - 1}{\prime} ,x_{t} } \right] + b_{c}{\prime} } \right]} \right)$$\end{document}</tex-math><mml:math id="M18" display="block"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>c</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo>tanh</mml:mo><mml:mfenced close=")" open="("><mml:mfenced close="]" open="["><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>&#x000b7;</mml:mo><mml:mfenced close="]" open="["><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>&#x02032;</mml:mo><mml:mo>&#x000b7;</mml:mo><mml:mfenced close="]" open="["><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>&#x02032;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>&#x02032;</mml:mo></mml:mrow></mml:mfenced></mml:mfenced></mml:mrow></mml:math><graphic xlink:href="41598_2024_56976_Article_Equ6.gif" position="anchor"/></alternatives></disp-formula>where <italic>w</italic> stands for the weight matrix, and <italic>b</italic> refers to the bias vector.</p><p id="Par23">Then, the hidden state <italic>h t</italic> at time <italic>t</italic> can be expressed as Eqs.&#x000a0;(<xref rid="Equ7" ref-type="disp-formula">7</xref>)&#x02013;(<xref rid="Equ8" ref-type="disp-formula">9</xref>).<disp-formula id="Equ7"><label>7</label><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\vec{h}_{t} = \overrightarrow {LSTM} \left( {\vec{w}_{t} ,\vec{h}_{t - 1} ,\vec{b}_{t} ,\vec{c}_{t - 1} } \right)$$\end{document}</tex-math><mml:math id="M20" display="block"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo stretchy="false">&#x02192;</mml:mo></mml:mover><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="italic">LSTM</mml:mi></mml:mrow><mml:mo stretchy="false">&#x02192;</mml:mo></mml:mover><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>w</mml:mi><mml:mo stretchy="false">&#x02192;</mml:mo></mml:mover><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo stretchy="false">&#x02192;</mml:mo></mml:mover><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>b</mml:mi><mml:mo stretchy="false">&#x02192;</mml:mo></mml:mover><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>c</mml:mi><mml:mo stretchy="false">&#x02192;</mml:mo></mml:mover><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:math><graphic xlink:href="41598_2024_56976_Article_Equ7.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ8"><label>8</label><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathop{h}\limits^{\leftarrow} _{t} = \overleftarrow {LSTM} \left( {\mathop{w}\limits^{\leftarrow} _{t} ,\mathop{h}\limits^{\leftarrow} _{t - 1} ,\mathop{b}\limits^{\leftarrow} _{t} ,\mathop{c}\limits^{\leftarrow} _{t + 1} } \right)$$\end{document}</tex-math><mml:math id="M22" display="block"><mml:mrow><mml:munderover><mml:mi>h</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy="false">&#x02190;</mml:mo></mml:munderover><mml:mo>=</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="italic">LSTM</mml:mi></mml:mrow><mml:mo stretchy="false">&#x02190;</mml:mo></mml:mover><mml:mfenced close=")" open="("><mml:mrow><mml:munderover><mml:mi>w</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy="false">&#x02190;</mml:mo></mml:munderover><mml:mo>,</mml:mo><mml:munderover><mml:mi>h</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">&#x02190;</mml:mo></mml:munderover><mml:mo>,</mml:mo><mml:munderover><mml:mi>b</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy="false">&#x02190;</mml:mo></mml:munderover><mml:mo>,</mml:mo><mml:munderover><mml:mi>c</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">&#x02190;</mml:mo></mml:munderover></mml:mrow></mml:mfenced></mml:mrow></mml:math><graphic xlink:href="41598_2024_56976_Article_Equ8.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ9"><label>9</label><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$h_{t} = \left[ {\vec{h}_{t} ,\mathop{h}\limits^{\leftarrow} _{t} } \right]$$\end{document}</tex-math><mml:math id="M24" display="block"><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfenced close="]" open="["><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo stretchy="false">&#x02192;</mml:mo></mml:mover><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:munderover><mml:mi>h</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy="false">&#x02190;</mml:mo></mml:munderover></mml:mrow></mml:mfenced></mml:mrow></mml:math><graphic xlink:href="41598_2024_56976_Article_Equ9.gif" position="anchor"/></alternatives></disp-formula></p><p id="Par24">where <italic>W</italic> and<italic> b</italic> represent the relevant weights of the gate unit and the memory cell; <italic>c</italic><sub><italic>t</italic></sub> and <italic>h</italic><sub><italic>t</italic></sub> denote the state of the memory cell and the value of the hidden state of the LSTM at <italic>t</italic>, respectively;&#x02009;&#x02192;&#x02009;and&#x02009;&#x02190;&#x02009;represent forward text and reverse text, respectively.</p><p id="Par25">Finally, the Softmax function is used to classify the Chinese word sense data information, as shown in Eq.&#x000a0;(<xref rid="Equ10" ref-type="disp-formula">10</xref>).<disp-formula id="Equ10"><label>10</label><alternatives><tex-math id="M25">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$Soft\max_{i} = \frac{{e^{{\left( {W^{\prime} \cdot flatten\left( {M*} \right) + b^{\prime}} \right)}} }}{{\sum\limits_{C} {e^{{\left( {W^{\prime} \cdot flatten\left( {M*} \right) + b^{\prime}} \right)}} } }}$$\end{document}</tex-math><mml:math id="M26" display="block"><mml:mrow><mml:mi>S</mml:mi><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>t</mml:mi><mml:munder><mml:mo movablelimits="true">max</mml:mo><mml:mi>i</mml:mi></mml:munder><mml:mo>=</mml:mo><mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msup><mml:mi>W</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup><mml:mo>&#x000b7;</mml:mo><mml:mi>f</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:mi>M</mml:mi><mml:mrow/><mml:mo>&#x02217;</mml:mo></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:msup><mml:mi>b</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup></mml:mrow></mml:mfenced></mml:msup><mml:mrow><mml:munder><mml:mo movablelimits="false">&#x02211;</mml:mo><mml:mi>C</mml:mi></mml:munder><mml:msup><mml:mi>e</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msup><mml:mi>W</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup><mml:mo>&#x000b7;</mml:mo><mml:mi>f</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:mi>M</mml:mi><mml:mrow/><mml:mo>&#x02217;</mml:mo></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:msup><mml:mi>b</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup></mml:mrow></mml:mfenced></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:math><graphic xlink:href="41598_2024_56976_Article_Equ10.gif" position="anchor"/></alternatives></disp-formula></p><p id="Par26">In Eq.&#x000a0;(<xref rid="Equ10" ref-type="disp-formula">10</xref>), <inline-formula id="IEq4"><alternatives><tex-math id="M27">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$W^{\prime}$$\end{document}</tex-math><mml:math id="M28"><mml:msup><mml:mi>W</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup></mml:math><inline-graphic xlink:href="41598_2024_56976_Article_IEq4.gif"/></alternatives></inline-formula> refers to the weight matrix of the fully connected layer; <inline-formula id="IEq5"><alternatives><tex-math id="M29">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$flatten\left( {M*} \right)$$\end{document}</tex-math><mml:math id="M30"><mml:mrow><mml:mi>f</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:mi>M</mml:mi><mml:mrow/><mml:mo>&#x02217;</mml:mo></mml:mrow></mml:mfenced></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_56976_Article_IEq5.gif"/></alternatives></inline-formula> represents stretching the<italic> M</italic>* matrix into a vector; <inline-formula id="IEq6"><alternatives><tex-math id="M31">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$b^{\prime}$$\end{document}</tex-math><mml:math id="M32"><mml:msup><mml:mi>b</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup></mml:math><inline-graphic xlink:href="41598_2024_56976_Article_IEq6.gif"/></alternatives></inline-formula> signifies the connection bias item of the fully connected layer; <italic>i</italic> indicates the index of the classification category; <italic>C</italic> stands for the total number of categories. The calculation result vector referred to <inline-formula id="IEq7"><alternatives><tex-math id="M33">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$Soft\max_{i}$$\end{document}</tex-math><mml:math id="M34"><mml:mrow><mml:mi>S</mml:mi><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>t</mml:mi><mml:msub><mml:mo movablelimits="true">max</mml:mo><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_56976_Article_IEq7.gif"/></alternatives></inline-formula> is the probability obtained for the <italic>i</italic>-th word sense category.</p></sec><sec id="Sec11"><title>Construction of Chinese word sense recognition model fusing transformer and BiLSTM algorithms</title><p id="Par27">The word sense disambiguation task involves a close semantic relationship between the context of ambiguous words and the target word, necessitating the disambiguation model to extract semantic information from the context. This study introduces the Transformer algorithm from deep learning and integrates it with the BiLSTM algorithm to address Chinese word sense disambiguation. By leveraging the self-attention mechanism of the Transformer and the sequence modeling capability of BiLSTM, the model efficiently captures semantic information and context relationships in Chinese sentences, leading to accurate word sense disambiguation. The Transformer-fused BiLSTM-based Chinese word sense disambiguation model is depicted in Fig.&#x000a0;<xref rid="Fig4" ref-type="fig">4</xref>.<fig id="Fig4"><label>Figure 4</label><caption><p>Framework of the Transformer-fused BiLSTM-based Chinese word sense disambiguation model.</p></caption><graphic xlink:href="41598_2024_56976_Fig4_HTML" id="MO5"/></fig></p><p id="Par28">As depicted in Fig.&#x000a0;<xref rid="Fig4" ref-type="fig">4</xref>, the system model begins with tokenizing and encoding the original Chinese sentence to obtain word vector representations, which serve as inputs to the model. The word vectors then undergo the Transformer encoder layer, comprising multiple attention heads (self-attention heads). These attention heads efficiently capture global semantic information based on word relationships. Subsequently, the BiLSTM layer is applied. Within the BiLSTM layer, bidirectional LSTM networks process the input sentence in both forward and backward directions, capturing contextual information from preceding and succeeding words. Next, the outputs of the Transformer encoder layer and the BiLSTM layer are fused through concatenation and weighted averaging. This fusion process combines the global relationships captured by the Transformer with the bidirectional contextual modeling capability of BiLSTM, resulting in a more comprehensive understanding of the semantic information within the sentence. Finally, the fused features are projected through a fully connected layer and passed through the Softmax function, generating the probability distribution of each word sense label.</p><p id="Par29">Assuming the Transformer encoder consists of N layers, each layer outputs an encoded representation. The output of the <italic>i</italic>-th layer is computed as shown in Eq.&#x000a0;(<xref rid="Equ11" ref-type="disp-formula">11</xref>):<disp-formula id="Equ11"><label>11</label><alternatives><tex-math id="M35">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\left[ {{\text{Transformer}}_{{{\text{encoder}},i}} \left( X \right){\text{ = Norm}}\left( {X{\text{ + Dropout}}\left( {{\text{F}}\left( {{\text{Norm}}\left( X \right)} \right)} \right)} \right)} \right]$$\end{document}</tex-math><mml:math id="M36" display="block"><mml:mfenced close="]" open="["><mml:mrow><mml:msub><mml:mtext>Transformer</mml:mtext><mml:mrow><mml:mtext>encoder</mml:mtext><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mfenced close=")" open="("><mml:mi>X</mml:mi></mml:mfenced><mml:mrow><mml:mspace width="0.333333em"/><mml:mtext>= Norm</mml:mtext></mml:mrow><mml:mfenced close=")" open="("><mml:mrow><mml:mi>X</mml:mi><mml:mrow><mml:mspace width="0.333333em"/><mml:mtext>+ Dropout</mml:mtext></mml:mrow><mml:mfenced close=")" open="("><mml:mrow><mml:mtext>F</mml:mtext><mml:mfenced close=")" open="("><mml:mrow><mml:mtext>Norm</mml:mtext><mml:mfenced close=")" open="("><mml:mi>X</mml:mi></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:math><graphic xlink:href="41598_2024_56976_Article_Equ11.gif" position="anchor"/></alternatives></disp-formula></p><p id="Par30">In Eq.&#x000a0;(<xref rid="Equ11" ref-type="disp-formula">11</xref>), (X) represents the data input to the Transformer encoder, (F) denotes the function of the Transformer encoder layer, which includes self-attention mechanisms and feedforward neural networks, (Norm) represents layer normalization, and (Dropout) is a random dropout operation used to prevent overfitting.</p><p id="Par31">The output of the final layer of the Transformer encoder is computed as shown in Eq.&#x000a0;(<xref rid="Equ12" ref-type="disp-formula">12</xref>):<disp-formula id="Equ12"><label>12</label><alternatives><tex-math id="M37">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\left[ {{\text{Transformer}}\,{\text{final}}\left( X \right){\text{ = Transformer}}\,{\text{encoder}},N\left( X \right)} \right]$$\end{document}</tex-math><mml:math id="M38" display="block"><mml:mfenced close="]" open="["><mml:mrow><mml:mtext>Transformer</mml:mtext><mml:mspace width="0.166667em"/><mml:mtext>final</mml:mtext><mml:mfenced close=")" open="("><mml:mi>X</mml:mi></mml:mfenced><mml:mrow><mml:mspace width="0.333333em"/><mml:mtext>= Transformer</mml:mtext></mml:mrow><mml:mspace width="0.166667em"/><mml:mtext>encoder</mml:mtext><mml:mo>,</mml:mo><mml:mi>N</mml:mi><mml:mfenced close=")" open="("><mml:mi>X</mml:mi></mml:mfenced></mml:mrow></mml:mfenced></mml:math><graphic xlink:href="41598_2024_56976_Article_Equ12.gif" position="anchor"/></alternatives></disp-formula></p><p id="Par32">For the BiLSTM layer, its output depends on the concatenation of the outputs from the forward LSTM and backward LSTM. Assuming the BiLSTM has M layers, the output of the j-th layer is given by Eq.&#x000a0;(<xref rid="Equ13" ref-type="disp-formula">13</xref>):<disp-formula id="Equ13"><label>13</label><alternatives><tex-math id="M39">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\left[ {{\text{BiLSTM}}_{j} \left( X \right){ = }\left[ {\overrightarrow {{{\text{LSTM}}_{j} }} \left( X \right);\overleftarrow {{{\text{LSTM}}_{j} }} \left( X \right)} \right]} \right]$$\end{document}</tex-math><mml:math id="M40" display="block"><mml:mfenced close="]" open="["><mml:mrow><mml:msub><mml:mtext>BiLSTM</mml:mtext><mml:mi>j</mml:mi></mml:msub><mml:mfenced close=")" open="("><mml:mi>X</mml:mi></mml:mfenced><mml:mo>=</mml:mo><mml:mfenced close="]" open="["><mml:mrow><mml:mover accent="true"><mml:msub><mml:mtext>LSTM</mml:mtext><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">&#x02192;</mml:mo></mml:mover><mml:mfenced close=")" open="("><mml:mi>X</mml:mi></mml:mfenced><mml:mo>&#x0037e;</mml:mo><mml:mover accent="true"><mml:msub><mml:mtext>LSTM</mml:mtext><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">&#x02190;</mml:mo></mml:mover><mml:mfenced close=")" open="("><mml:mi>X</mml:mi></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:math><graphic xlink:href="41598_2024_56976_Article_Equ13.gif" position="anchor"/></alternatives></disp-formula></p><p id="Par33">In Eq.&#x000a0;(<xref rid="Equ13" ref-type="disp-formula">13</xref>), <inline-formula id="IEq8"><alternatives><tex-math id="M41">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$(\overrightarrow {{{\text{LSTM}}_{j} }} (X))$$\end{document}</tex-math><mml:math id="M42"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:msub><mml:mtext>LSTM</mml:mtext><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">&#x02192;</mml:mo></mml:mover><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_56976_Article_IEq8.gif"/></alternatives></inline-formula> represents the output of the forward LSTM, and <inline-formula id="IEq9"><alternatives><tex-math id="M43">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$(\overleftarrow {{{\text{LSTM}}_{j} }} (X))$$\end{document}</tex-math><mml:math id="M44"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:msub><mml:mtext>LSTM</mml:mtext><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">&#x02190;</mml:mo></mml:mover><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_56976_Article_IEq9.gif"/></alternatives></inline-formula> represents the output of the backward LSTM. ([;]) denotes the concatenation operation.</p><p id="Par34">The output of the final layer of the BiLSTM is expressed in Eq.&#x000a0;(<xref rid="Equ14" ref-type="disp-formula">14</xref>):<disp-formula id="Equ14"><label>14</label><alternatives><tex-math id="M45">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\left[ {{\text{BiLSTM}}_{{{\text{final}}}} \left( X \right) = {\text{BiLSTM}}_{M\left( X \right)} } \right]$$\end{document}</tex-math><mml:math id="M46" display="block"><mml:mfenced close="]" open="["><mml:mrow><mml:msub><mml:mtext>BiLSTM</mml:mtext><mml:mtext>final</mml:mtext></mml:msub><mml:mfenced close=")" open="("><mml:mi>X</mml:mi></mml:mfenced><mml:mo>=</mml:mo><mml:msub><mml:mtext>BiLSTM</mml:mtext><mml:mrow><mml:mi>M</mml:mi><mml:mfenced close=")" open="("><mml:mi>X</mml:mi></mml:mfenced></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math><graphic xlink:href="41598_2024_56976_Article_Equ14.gif" position="anchor"/></alternatives></disp-formula></p><p id="Par35">The outputs of the Transformer encoder and BiLSTM are fused using a simple approach, such as concatenation or weighted averaging. The concatenation method is used as shown in Eq.&#x000a0;(<xref rid="Equ15" ref-type="disp-formula">15</xref>):<disp-formula id="Equ15"><label>15</label><alternatives><tex-math id="M47">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\left[ {{\text{Fusion}}\left( X \right) = \left[ {{\text{Transformer}}\,{\text{final}}\left( X \right);{\text{BiLSTM}}\,{\text{final}}\left( X \right)} \right]} \right]$$\end{document}</tex-math><mml:math id="M48" display="block"><mml:mfenced close="]" open="["><mml:mrow><mml:mtext>Fusion</mml:mtext><mml:mfenced close=")" open="("><mml:mi>X</mml:mi></mml:mfenced><mml:mo>=</mml:mo><mml:mfenced close="]" open="["><mml:mrow><mml:mtext>Transformer</mml:mtext><mml:mspace width="0.166667em"/><mml:mtext>final</mml:mtext><mml:mfenced close=")" open="("><mml:mi>X</mml:mi></mml:mfenced><mml:mo>&#x0037e;</mml:mo><mml:mtext>BiLSTM</mml:mtext><mml:mspace width="0.166667em"/><mml:mtext>final</mml:mtext><mml:mfenced close=")" open="("><mml:mi>X</mml:mi></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:math><graphic xlink:href="41598_2024_56976_Article_Equ15.gif" position="anchor"/></alternatives></disp-formula></p><p id="Par36">The fused output (Fusion(<italic>X</italic>)) can then be further passed to a fully connected layer for classification or other tasks.</p></sec><sec id="Sec12"><title>Simulation experiment evaluation</title><p id="Par37">The implementation was carried out using the TensorFlow framework software to evaluate the performance of the Transformer-fused BiLSTM-based Chinese word sense disambiguation model. The software was executed on a 64-bit Linux operating system and utilized the OpenCV 4.2.0 image processing library and Python version 3.7. The hardware setup comprised an Intel Core i7-8700&#x000a0;K CPU, 512&#x000a0;GB SSD storage, 16&#x000a0;GB RAM, and Nvidia GeForce GTX 1080 Ti GPU. The experimental data for the study was sourced from the PKU-Paraphrase-Bank (<ext-link ext-link-type="uri" xlink:href="https://github.com/pkucoli/PKU-Paraphrase-Bank/">https://github.com/pkucoli/PKU-Paraphrase-Bank/</ext-link>), and a total of 20,725 pairs of sentences were collected after applying data cleaning and preprocessing measures to handle noise, redundancy, and duplicate data. This study implemented an enhanced dataset split to evaluate model performance comprehensively and validate research findings. The adjustment included the introduction of a validation set and modifying the dataset split ratio to 70% for the training set, 20% for the validation set, and 10% for the test set. The rationale behind this modification is twofold. First, it facilitates the model training process by using the training set to fit the model, allowing it to learn the features and patterns of the data. Second, it enables the assessment of the model&#x02019;s performance on unseen data using the test set, thereby detecting potential issues such as overfitting and evaluating the model&#x02019;s generalization ability. During the model training process, particular attention was given to hyperparameter tuning to ensure optimal performance in various aspects. A grid search method explored and adjusted hyperparameters, including learning rate, batch size, and model structure. The final hyperparameter combination, determined by comparing performance on the validation set, consisted of a learning rate of 0.001 and a batch size of 64, with four layers for the Transformer and two layers for BiLSTM. This meticulous approach to dataset split and hyperparameter tuning enhances the reliability and robustness of the model, ensuring that it is well-adapted to diverse scenarios. The outcomes of hyperparameter tuning are summarized in Table <xref rid="Tab2" ref-type="table">2</xref>:<table-wrap id="Tab2"><label>Table 2</label><caption><p>Hyperparameter tuning results.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Learning rate</th><th align="left">Batch size</th><th align="left">Transformer layers</th><th align="left">BiLSTM layers</th><th align="left">Validation accuracy (%)</th><th align="left">Test accuracy (%)</th><th align="left">Final model performance</th></tr></thead><tbody><tr><td align="left">0.0005&#x02013;0.0015</td><td align="left">32&#x02013;96</td><td align="left">4</td><td align="left">2</td><td char="." align="char">88.50</td><td char="." align="char">86.20</td><td align="left">Optimal</td></tr><tr><td align="left">0.005&#x02013;0.02</td><td align="left">16&#x02013;64</td><td align="left">6</td><td align="left">3</td><td char="." align="char">85.30</td><td char="." align="char">82.70</td><td align="left">Moderate</td></tr><tr><td align="left">0.05&#x02013;0.2</td><td align="left">64&#x02013;256</td><td align="left">8</td><td align="left">4</td><td char="." align="char">79.80</td><td char="." align="char">76.50</td><td align="left">Poor</td></tr></tbody></table></table-wrap></p><p id="Par38">To assess the performance of the constructed model, a series of comparative experiments were conducted, including comparisons with BiLSTM<sup><xref ref-type="bibr" rid="CR34">34</xref></sup>, Transformer<sup><xref ref-type="bibr" rid="CR35">35</xref></sup>, and recent models proposed by Zheng et al<sup><xref ref-type="bibr" rid="CR36">36</xref></sup>. and Hou et al<sup><xref ref-type="bibr" rid="CR37">37</xref></sup>. The primary evaluation metric utilized in this study was accuracy, which quantifies the proportion of correct predictions made by the model for the word sense disambiguation task. Additionally, to comprehensively evaluate the methods&#x02019; performance and stability, other metrics considered in this study were root mean square error (RMSE) and loss value. The accuracy is calculated according to Eq.&#x000a0;(<xref rid="Equ16" ref-type="disp-formula">16</xref>).<disp-formula id="Equ16"><label>16</label><alternatives><tex-math id="M49">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$Acc = \frac{TP + TN}{{TP + FP + TN + FN}}$$\end{document}</tex-math><mml:math id="M50" display="block"><mml:mrow><mml:mi>A</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math><graphic xlink:href="41598_2024_56976_Article_Equ16.gif" position="anchor"/></alternatives></disp-formula></p><p id="Par39">In Eq.&#x000a0;(<xref rid="Equ16" ref-type="disp-formula">16</xref>), <italic>TP</italic> represents the number of positive samples predicted to be positive; <italic>FP</italic> denotes the number of negative samples expected to be positive; <italic>FN</italic> refers to the number of positive samples predicted to be negative; <italic>TN</italic> stands for the number of negative samples predicted to be negative.</p><p id="Par40">Equation&#x000a0;(<xref rid="Equ17" ref-type="disp-formula">17</xref>) describes RMSE.<disp-formula id="Equ17"><label>17</label><alternatives><tex-math id="M51">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$RMSE = \sqrt {\frac{1}{N}\sum\limits_{t = 1}^{N} {\left| {x_{i} - y_{i} } \right|^{2} } }$$\end{document}</tex-math><mml:math id="M52" display="block"><mml:mrow><mml:mi>R</mml:mi><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:mi>E</mml:mi><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:munderover><mml:mo movablelimits="false">&#x02211;</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:msup><mml:mfenced close="|" open="|"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msqrt></mml:mrow></mml:math><graphic xlink:href="41598_2024_56976_Article_Equ17.gif" position="anchor"/></alternatives></disp-formula></p><p id="Par41">In Eq.&#x000a0;(<xref rid="Equ17" ref-type="disp-formula">17</xref>), <italic>x</italic><sub><italic>i</italic></sub> denotes the actual Chinese text data at the <italic>i</italic>-th moment,<italic> y</italic><sub><italic>i</italic></sub> signifies the Chinese text prediction data output by the model at the <italic>i</italic>-th moment, and<italic> N</italic> refers to the length of the Chinese text data sequence to be evaluated.</p></sec></sec><sec id="Sec13"><title>Results and discussions</title><sec id="Sec14"><title>Performance comparison results of models under different algorithms</title><p id="Par42">Figure&#x000a0;<xref rid="Fig5" ref-type="fig">5</xref> presents the loss function results of the models under different algorithms.<fig id="Fig5"><label>Figure 5</label><caption><p>Loss function results.</p></caption><graphic xlink:href="41598_2024_56976_Fig5_HTML" id="MO6"/></fig></p><p id="Par43">Figure&#x000a0;<xref rid="Fig5" ref-type="fig">5</xref> illustrates the analysis of loss values for various algorithms. Evidently, the proposed Transformer-fused BiLSTM-based model achieves the minimum loss value and reaches a relatively stable state at the 15th iteration cycle, maintaining around 0.14. In contrast, the loss functions of other algorithms all exceed 0.19. As a result, all algorithms converge during the Chinese word sense disambiguation task, and the proposed Transformer-fused BiLSTM-based Chinese word sense disambiguation model demonstrates superior convergence performance with lower loss values. This outcome underscores the supremacy of the Transformer-enhanced BiLSTM model in encoding semantic information and contextual relationships. The Transformer&#x02019;s self-attention mechanism excels at capturing long-range dependencies, complemented by BiLSTM&#x02019;s proficiency in sequential modeling. This synergistic integration enables the model to adeptly capture semantic nuances in Chinese sentences, reducing the loss value. The combined strength of these two architectures contributes to the model&#x02019;s heightened performance in Chinese word sense disambiguation tasks.</p><p id="Par44">Furthermore, the recognition accuracy of different algorithms on the training and testing datasets is analyzed and depicted in Figs.&#x000a0;<xref rid="Fig6" ref-type="fig">6</xref> and <xref rid="Fig7" ref-type="fig">7</xref>.<fig id="Fig6"><label>Figure 6</label><caption><p>Accuracy results of each algorithm in the training set.</p></caption><graphic xlink:href="41598_2024_56976_Fig6_HTML" id="MO7"/></fig><fig id="Fig7"><label>Figure 7</label><caption><p>Accuracy results of each algorithm in the test set.</p></caption><graphic xlink:href="41598_2024_56976_Fig7_HTML" id="MO8"/></fig></p><p id="Par45">Figure&#x000a0;<xref rid="Fig6" ref-type="fig">6</xref> demonstrates that as the number of iterations increases, the accuracy of different algorithms on the training set rapidly improves and stabilizes. Notably, the proposed Transformer-fused BiLSTM-based model achieves a recognition accuracy of 81.91%, which is at least 3.5% higher than the accuracy achieved by other model algorithms. The recognition accuracy of the algorithms for Chinese word sense disambiguation ranks as follows, from highest to lowest: the proposed Transformer-fused BiLSTM-based model, the model algorithm by Hou et al., the model algorithm by Zheng et al., BiLSTM, and Transformer. Consequently, the Transformer-fused BiLSTM-based model exhibits superior accuracy in identifying Chinese word senses in a shorter period.</p><p id="Par46">The analysis of the recognition accuracy of different algorithms on the test set is depicted in Fig.&#x000a0;<xref rid="Fig7" ref-type="fig">7</xref>. As the number of iterations increases, the accuracy rapidly increases, followed by stabilization. Notably, the proposed Transformer-fused BiLSTM-based model achieves a recognition accuracy of 83.71% for Chinese word sense disambiguation on the test set, which surpasses the accuracy achieved by other model algorithms by at least 3.05%. Furthermore, the recognition accuracy of the algorithms for Chinese word senses, in descending order, is as follows: the proposed Transformer-fused BiLSTM-based model, the model algorithm by Hou et al., the model algorithm by Zheng et al., BiLSTM, and Transformer. Consequently, the Transformer-fused BiLSTM-based model exhibits superior recognition accuracy in Chinese word sense disambiguation. The observed superiority depicted in Figs.&#x000a0;<xref rid="Fig6" ref-type="fig">6</xref> and <xref rid="Fig7" ref-type="fig">7</xref> can be attributed to several pivotal factors. Firstly, as the number of iterations increases, the Transformer-enhanced BiLSTM model consistently exhibits heightened accuracy on both the training and testing sets. This performance can be primarily ascribed to the superior model structure, adeptly leveraging the self-attention mechanism of the Transformer and the sequential modeling advantages inherent in BiLSTM. This synergy enables the model to effectively capture semantic nuances and contextual relationships in Chinese sentences, thereby elevating recognition accuracy. Secondly, post-parameter tuning, the model showcases enhanced performance across the training and testing sets. The adjustment of hyperparameters to better align with the intricacies of the Chinese language context contributes to an overall performance boost. Lastly, the Transformer-enhanced BiLSTM model, amalgamating two distinct neural network structures, achieves a harmonious equilibrium between the demands of long-distance dependency and sequential modeling. This delicate balance markedly enhances the overall effectiveness of the algorithm.</p><p id="Par47">Additionally, a comparative analysis of the RMSE for different algorithms on both the training and test sets is presented in Figs.&#x000a0;<xref rid="Fig8" ref-type="fig">8</xref> and <xref rid="Fig9" ref-type="fig">9</xref>, respectively.<fig id="Fig8"><label>Figure 8</label><caption><p>RMSE results of each algorithm in the training set under different sentence pairs.</p></caption><graphic xlink:href="41598_2024_56976_Fig8_HTML" id="MO9"/></fig><fig id="Fig9"><label>Figure 9</label><caption><p>RMSE results of each algorithm in the test set under different sentence pairs.</p></caption><graphic xlink:href="41598_2024_56976_Fig9_HTML" id="MO10"/></fig></p><p id="Par48">In Figs.&#x000a0;<xref rid="Fig8" ref-type="fig">8</xref> and <xref rid="Fig9" ref-type="fig">9</xref>, the analysis of the RMSE results for different algorithms on the training and test sets indicates a slight upward trend in RMSE with the increase in sentence pairs, but the trend is not significant. In the training set, the proposed Transformer-fused BiLSTM-based model achieves the highest RMSE of 16.95, while the RMSE for other model algorithms is more elevated, exceeding 18.88. Similarly, in the test set, the highest RMSE for the proposed Transformer-fused BiLSTM-based model is 15.47, and the RMSE for other model algorithms is also higher than that of this paper&#x02019;s model algorithm. The RMSE results of each algorithm, sorted from the smallest to the largest, are as follows: the proposed Transformer-fused BiLSTM-based model, the model algorithm by Hou et al. (2020), the model algorithm by Zheng et al. (2021), BiLSTM, and Transformer. Consequently, the Transformer-fused BiLSTM-based model demonstrates lower classification and recognition errors for Chinese word sense disambiguation. The meticulous analysis of the trend in RMSE, as illustrated in Figs.&#x000a0;<xref rid="Fig8" ref-type="fig">8</xref> and <xref rid="Fig9" ref-type="fig">9</xref>, unveils several salient factors. Firstly, within the training set, the proposed Transformer-enhanced BiLSTM model showcases a notably low RMSE (16.95), underscoring its supremacy in comprehending Chinese sentence contexts. Its inherent sequential modeling prowess empowers it to adeptly capture contextual nuances, thereby mitigating classification and recognition errors. Comparative to alternative models, its diminished RMSE signifies superior performance on the training set. In the testing set, the proposed Transformer-enhanced BiLSTM model similarly demonstrates a low RMSE (15.47), affirming its superiority over alternative model algorithms. This advantage underscores the model&#x02019;s capability to sustain a low error level when confronted with previously unseen data, thereby affirming its commendable generalization ability. Secondly, the overall trend indicates a marginal increase in RMSE with a corresponding escalation in the number of sentence pairs; however, this trend is not pronounced. This increase attests to the model&#x02019;s relative robustness in the face of a growing corpus of contexts and sentence pairs. The model&#x02019;s flexible contextual modeling imparts resilience, ensuring consistent performance across varying data scales.</p></sec><sec id="Sec15"><title>Performance of the transformer-fused BiLSTM Chinese word sense disambiguation model on different standard benchmarks</title><p id="Par49">The performance evaluation of the proposed model on various standard benchmarks is detailed in Table <xref rid="Tab3" ref-type="table">3</xref>. The Transformer-fused BiLSTM Chinese Word Sense Disambiguation model effectively tackles word sense ambiguity in Chinese contexts across diverse benchmarks. The model attains commendable accuracy on the PKU Phrase Bank and Chinese Vocabulary Semantic Benchmark, achieving 83.71% and 84.47%, respectively. This result underscores the model&#x02019;s capacity for accurate prediction in disambiguating word senses within Chinese sentences. The synergistic integration of Transformer and BiLSTM models empowers the system to adeptly capture semantic intricacies and contextual relationships, thereby amplifying disambiguation effectiveness. On the SENSEVAL-3 Chinese Dataset, the model achieves an accuracy of 76.93%, marginally lower than other benchmarks. Several factors may contribute to this comparatively lower performance. The dataset encompasses texts with unique contexts and language structures, introducing heightened complexity and including contextual information that may pose challenges for the model. Additionally, specific domain differences in the dataset compared to other benchmarks might impact performance. Inadequate coverage of linguistic characteristics of this domain during training could lead to suboptimal performance. Enhancing domain adaptation becomes imperative for optimal performance on the SENSEVAL-3 dataset. The accuracy of dataset annotations plays a pivotal role in model performance. Inaccurate or ambiguous annotations in the SENSEVAL-3 Chinese dataset can detrimentally affect the model, resulting in diminished performance. Rigorous validation and cleaning of annotations are imperative to ensure label accuracy. Furthermore, if the training set lacks coverage of specific contexts or word senses present in the SENSEVAL-3 Chinese dataset, the model&#x02019;s generalization to these situations might be compromised. In such scenarios, augmenting the diversity and coverage of the training set becomes crucial for enhancing model performance on the SENSEVAL-3 dataset. Moreover, the model&#x02019;s architecture may exhibit varying performance on distinct types of contexts or tasks. If the model structure inadequately adapts to the characteristics of the SENSEVAL-3 Chinese dataset, its performance could be constrained. Exploring adjustments to the model structure or considering a model better suited to this dataset represents a potential avenue for improvement.<table-wrap id="Tab3"><label>Table 3</label><caption><p>Performance of the model on different standard benchmarks.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Standard benchmark</th><th align="left">Accuracy (%)</th><th align="left">Recall (%)</th><th align="left">Precision (%)</th><th align="left">F1 (%)</th><th align="left">RMSE</th><th align="left">Loss</th></tr></thead><tbody><tr><td align="left">PKU phrase bank</td><td char="." align="char">83.71</td><td char="." align="char">83.33</td><td char="." align="char">95.24</td><td char="." align="char">88.89</td><td char="." align="char">16.95</td><td char="." align="char">0.14</td></tr><tr><td align="left">Chinese vocabulary semantic Benchmark <sup><xref ref-type="bibr" rid="CR36">36</xref></sup></td><td char="." align="char">84.47</td><td char="." align="char">88.89</td><td char="." align="char">92.31</td><td char="." align="char">90.57</td><td char="." align="char">17.69</td><td char="." align="char">0.39</td></tr><tr><td align="left">SENSEVAL-3 Chinese dataset <sup><xref ref-type="bibr" rid="CR38">38</xref></sup></td><td char="." align="char">76.93</td><td char="." align="char">78.26</td><td char="." align="char">85.71</td><td char="." align="char">81.52</td><td char="." align="char">19.83</td><td char="." align="char">0.68</td></tr></tbody></table></table-wrap></p><p id="Par50">Nevertheless, the model demonstrates relatively high recall rates across all standard benchmarks, reaching 83.33%, 88.89%, and 78.26% for PKU Phrase Bank, Chinese Vocabulary Semantic Benchmark, and SENSEVAL-3 Chinese Dataset, respectively. These outcomes underscore its efficacy in capturing positive samples within the text. In terms of precision, the model achieves 95.24% and 92.31% on the PKU Phrase Bank and Chinese Vocabulary Semantic Benchmark, respectively, while attaining 85.71% on the SENSEVAL-3 Chinese Dataset. Striking a balance between recall and precision, the F1 values are 88.89% and 90.57% for the PKU Phrase Bank and Chinese Vocabulary Semantic Benchmark, respectively, and 81.52% for the SENSEVAL-3 Chinese Dataset. The model&#x02019;s performance is also further assessed through RMSE and loss function values across all benchmarks. Collectively, these results highlight the superiority of the Transformer-fused BiLSTM model in Chinese Word Sense Disambiguation tasks, providing robust support for semantic understanding in the Chinese context.</p></sec><sec id="Sec16"><title>Discussion</title><p id="Par51">This study designed and implemented a novel model algorithm that combines Transformer and BiLSTM for Chinese word sense disambiguation. The performance of this model algorithm was evaluated by comparing it with other existing algorithms, namely Hou et al., Zheng et al., BiLSTM, and Transformer. Uniform parameter settings were consistently applied throughout the comparison process to ensure a fair comparison. All models were configured with identical parameters, setting the learning rate to 0.001 and batch size to 64, among other specifications. This approach mitigates performance differences that may arise from disparate parameter configurations, thereby ensuring the fairness and validity of the comparison. The results indicate that the Transformer-fused BiLSTM-based model outperformed other algorithms, particularly in terms of loss values. It attained a stable state with a notably low loss value of approximately 0.14 by the 15th iteration cycle. In the 15 iteration cycles, the BiLSTM model exhibited a loss value of 0.30, while the Transformer model showed a loss value of 0.35. This signifies a notable improvement of over 0.05 compared to the other algorithms. These findings highlight that all algorithms can achieve convergence in the Chinese word sense disambiguation task and demonstrate relatively lower loss values, which aligns with the observations made by &#x000d6;z&#x000e7;ift et al<sup><xref ref-type="bibr" rid="CR39">39</xref></sup>.</p><p id="Par52">Further analysis of recognition accuracy revealed that the Transformer-fused BiLSTM model achieved outstanding recognition accuracies of 81.91% and 83.71% on the training set and testing set, respectively. In comparison, the BiLSTM model achieved recognition accuracies of 77.17% on the training set and 73.10% on the test set, while the RNN model recorded recognition accuracies of 62.70% on the training set and 65.00% on the test set. Notably, these accuracies surpassed those of other model algorithms, including Hou et al., Yang et al., Zheng et al., Von der Mosel et al., BiLSTM, and Transformer. Furthermore, in terms of RMSE, the Transformer-fused BiLSTM-based model exhibited lower values, recording 16.95 on the training set and 15.47 on the testing set. In comparison, the BiLSTM model showed RMSE values of 24.60 on the training set and 18.26 on the test set, while the Transformer model displayed RMSE values of 29.75 on the training set and 19.91 on the test set. Notably, all other model algorithms registered higher RMSE values. This finding indicates that the Transformer-fused BiLSTM-based model exhibits lower classification and recognition errors in Chinese word sense disambiguation. These observations align with the views presented by Ji et al<sup><xref ref-type="bibr" rid="CR40">40</xref></sup>. and Chen et al<sup><xref ref-type="bibr" rid="CR41">41</xref></sup>.</p><p id="Par53">Therefore, the Transformer-fused BiLSTM-based Chinese word sense disambiguation model demonstrated excellent performance in terms of loss values, accuracy, and RMSE. The results highlight the model&#x02019;s high accuracy and robustness in Chinese word sense understanding tasks, providing a strong reference for advancing the development of intelligent Chinese word sense disambiguation.</p></sec></sec><sec id="Sec17"><title>Conclusion</title><p id="Par54">This study introduces deep learning algorithms to efficiently and accurately identify word senses in the Chinese language. A Chinese word sense disambiguation model is constructed based on the fusion of Transformer and BiLSTM. The experimental analysis of the model&#x02019;s performance demonstrates that the proposed Transformer-fused BiLSTM-based model outperforms other methods in terms of loss values, accuracy, and RMSE, achieving a recognition accuracy of approximately 83%. This result indicates superior convergence and recognition accuracy, showcasing the model&#x02019;s significant advantages in Chinese word sense understanding tasks.</p><p id="Par55">However, there are some limitations in this study. This study has several limitations that warrant consideration. Firstly, the primary focus of this research is on Chinese word sense disambiguation, mainly through the integration of Transformer and BiLSTM. The effectiveness of the proposed methodology may not be as pronounced when applied to word sense disambiguation in languages other than Chinese due to variations in language structure, grammar rules, and linguistic contexts. Secondly, the methodology may exhibit suboptimal performance in handling specific scenarios. Additionally, when dealing with word sense disambiguation in specific domains, the methodology might demand more domain-specific knowledge, which may not be easily acquired from general datasets. Therefore, future research should consider utilizing more diversified Chinese word sense datasets that cover different domains and topics to validate the model&#x02019;s applicability in broader contexts.</p></sec><sec sec-type="supplementary-material"><sec id="Sec18"><title>Supplementary Information</title><p>
<supplementary-material content-type="local-data" id="MOESM1"><media xlink:href="41598_2024_56976_MOESM1_ESM.zip"><caption><p>Supplementary Information.</p></caption></media></supplementary-material></p></sec></sec></body><back><fn-group><fn><p><bold>Publisher's note</bold></p><p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></fn></fn-group><sec><title>Supplementary Information</title><p>The online version contains supplementary material available at 10.1038/s41598-024-56976-5.</p></sec><notes notes-type="author-contribution"><title>Author contributions</title><p>L.L., J.L., H.W., and J.N. contributed to conception and design of the study. L.L. organized the database. J.L. performed the statistical analysis. H.W. wrote the first draft of the manuscript. J.N. wrote sections of the manuscript. All authors contributed to manuscript revision, read, and approved the submitted version.</p></notes><notes notes-type="funding-information"><title>Funding</title><p>The National Postdoctoral Research Program (Level B) project &#x0201c;Research on Intelligent Communication of Strengthening the Awareness of the Chinese National Community&#x0201d; (GZB20230578); Special Funding Project of Shaanxi Postdoctoral Fund (2023BSHTBZZ10); Basic Research Business Fees for Central Universities (Humanities and Social Sciences) Project &#x0201c;Current Status of Science and Technology Development in the Five Central Asian Countries and Exploration of the &#x02018;China Central Asia Science and Technology Cooperation Community&#x02019; by Jiaotong University&#x0201d; (SK2023076).</p></notes><notes notes-type="data-availability"><title>Data availability</title><p>All data generated or analysed during this study are included in this published article [and its supplementary information files].&#x000a0; &#x000a0;&#x000a0;</p></notes><notes id="FPar1" notes-type="COI-statement"><title>Competing interests</title><p id="Par56">The authors declare no competing interests.</p></notes><ref-list id="Bib1"><title>References</title><ref id="CR1"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bharadiya</surname><given-names>J</given-names></name></person-group><article-title>A comprehensive survey of deep learning techniques natural language processing[J]</article-title><source>Eur. J. Technol.</source><year>2023</year><volume>7</volume><issue>1</issue><fpage>58</fpage><lpage>66</lpage><pub-id pub-id-type="doi">10.47672/ejt.1473</pub-id></element-citation></ref><ref id="CR2"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xu</surname><given-names>N</given-names></name><name><surname>Ma</surname><given-names>L</given-names></name><name><surname>Wang</surname><given-names>L</given-names></name><etal/></person-group><article-title>Extracting domain knowledge elements of construction safety management: Rule-based approach using Chinese natural language processing[J]</article-title><source>J. Manag. Eng.</source><year>2021</year><volume>37</volume><issue>2</issue><fpage>04021001</fpage><pub-id pub-id-type="doi">10.1061/(ASCE)ME.1943-5479.0000870</pub-id></element-citation></ref><ref id="CR3"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zheng</surname><given-names>Z</given-names></name><name><surname>Lu</surname><given-names>XZ</given-names></name><name><surname>Chen</surname><given-names>KY</given-names></name><etal/></person-group><article-title>Pretrained domain-specific language model for natural language processing tasks in the AEC domain[J]</article-title><source>Comput. Ind.</source><year>2022</year><volume>142</volume><fpage>103733</fpage><pub-id pub-id-type="doi">10.1016/j.compind.2022.103733</pub-id></element-citation></ref><ref id="CR4"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pan</surname><given-names>DJ</given-names></name><name><surname>Lin</surname><given-names>D</given-names></name></person-group><article-title>Cognitive&#x02013;linguistic skills explain Chinese reading comprehension within and beyond the simple view of reading in Hong Kong kindergarteners[J]</article-title><source>Lang. Learn.</source><year>2023</year><volume>73</volume><issue>1</issue><fpage>126</fpage><lpage>160</lpage><pub-id pub-id-type="doi">10.1111/lang.12515</pub-id></element-citation></ref><ref id="CR5"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guo</surname><given-names>H</given-names></name></person-group><article-title>Chinese primary school students&#x02019; translanguage in EFL classrooms: What is it and why is it needed? [J]</article-title><source>Asia-Pacific Educ. Res.</source><year>2023</year><volume>32</volume><issue>2</issue><fpage>211</fpage><lpage>226</lpage></element-citation></ref><ref id="CR6"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tian</surname><given-names>Z</given-names></name></person-group><article-title>Translanguage design in a third grade Chinese language arts class[J]</article-title><source>Appl Linguistics Rev.</source><year>2022</year><volume>13</volume><issue>3</issue><fpage>327</fpage><lpage>343</lpage><pub-id pub-id-type="doi">10.1515/applirev-2021-0024</pub-id></element-citation></ref><ref id="CR7"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Worth</surname><given-names>PJ</given-names></name></person-group><article-title>Word embeddings and semantic spaces in natural language processing[J]</article-title><source>Int. J. Intell. Sci.</source><year>2023</year><volume>13</volume><issue>1</issue><fpage>1</fpage><lpage>21</lpage></element-citation></ref><ref id="CR8"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chaichulee</surname><given-names>S</given-names></name><name><surname>Promchai</surname><given-names>C</given-names></name><name><surname>Kaewkomon</surname><given-names>T</given-names></name><etal/></person-group><article-title>Multi-label classification of symptom terms from free-text bilingual adverse drug reaction reports using natural language processing[J]</article-title><source>PLoS One</source><year>2022</year><volume>17</volume><issue>8</issue><fpage>e0270595</fpage><pub-id pub-id-type="doi">10.1371/journal.pone.0270595</pub-id><?supplied-pmid 35925971?><pub-id pub-id-type="pmid">35925971</pub-id>
</element-citation></ref><ref id="CR9"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haynes</surname><given-names>K</given-names></name><name><surname>Shirazi</surname><given-names>H</given-names></name><name><surname>Ray</surname><given-names>I</given-names></name></person-group><article-title>Lightweight URL-based phishing detection using natural language processing transformers for mobile devices[J]</article-title><source>Proc. Comput. Sci.</source><year>2021</year><volume>191</volume><fpage>127</fpage><lpage>134</lpage><pub-id pub-id-type="doi">10.1016/j.procs.2021.07.040</pub-id></element-citation></ref><ref id="CR10"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Benavides-Astudillo</surname><given-names>E</given-names></name><name><surname>Fuertes</surname><given-names>W</given-names></name><name><surname>Sanchez-Gordon</surname><given-names>S</given-names></name><etal/></person-group><article-title>A phishing-attack-detection model using natural language processing and deep learning[J]</article-title><source>Appl. Sci.</source><year>2023</year><volume>13</volume><issue>9</issue><fpage>5275</fpage><pub-id pub-id-type="doi">10.3390/app13095275</pub-id></element-citation></ref><ref id="CR11"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bao</surname><given-names>Y</given-names></name><name><surname>Sun</surname><given-names>Z</given-names></name><name><surname>Zhao</surname><given-names>Q</given-names></name><etal/></person-group><article-title>Hot news prediction method based on natural language processing technology and its application[J]</article-title><source>Autom. Control Comput. Sci.</source><year>2022</year><volume>56</volume><issue>1</issue><fpage>83</fpage><lpage>94</lpage><pub-id pub-id-type="doi">10.3103/S0146411622010023</pub-id></element-citation></ref><ref id="CR12"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhou</surname><given-names>J</given-names></name></person-group><article-title>Meaning patterns of the NP de VP construction in modern Chinese: Approaches of covarying collexeme analysis and hierarchical cluster analysis[J]</article-title><source>Hum. Soc. Sci. Commun.</source><year>2023</year><volume>10</volume><issue>1</issue><fpage>1</fpage><lpage>12</lpage></element-citation></ref><ref id="CR13"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>X</given-names></name><name><surname>Huang</surname><given-names>L</given-names></name><name><surname>Yao</surname><given-names>P</given-names></name><etal/></person-group><article-title>Universal and specific reading mechanisms across different writing systems[J]</article-title><source>Nat. Rev. Psychol.</source><year>2022</year><volume>1</volume><issue>3</issue><fpage>133</fpage><lpage>144</lpage><pub-id pub-id-type="doi">10.1038/s44159-022-00022-6</pub-id></element-citation></ref><ref id="CR14"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>C</given-names></name><name><surname>Dai</surname><given-names>S</given-names></name><name><surname>Wang</surname><given-names>Y</given-names></name><etal/></person-group><article-title>Arobert: An asr robust pre-trained language model for spoken language understanding[J]</article-title><source>IEEE/ACM Trans. Audio Speech Lang. Process.</source><year>2022</year><volume>30</volume><fpage>1207</fpage><lpage>1218</lpage><pub-id pub-id-type="doi">10.1109/TASLP.2022.3153268</pub-id></element-citation></ref><ref id="CR15"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>S&#x000f8;gaard</surname><given-names>A</given-names></name></person-group><article-title>Grounding the vector space of an octopus: Word meaning from raw text[J]</article-title><source>Minds Mach.</source><year>2023</year><volume>33</volume><issue>1</issue><fpage>33</fpage><lpage>54</lpage><pub-id pub-id-type="doi">10.1007/s11023-023-09622-4</pub-id></element-citation></ref><ref id="CR16"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>J</given-names></name><name><surname>Chen</surname><given-names>B</given-names></name></person-group><article-title>Learning new meanings for known L2 words: Long-term semantic representation is updated to integrate new information after consolidation[J]</article-title><source>Psychophysiology</source><year>2023</year><volume>60</volume><issue>5</issue><fpage>e14228</fpage><pub-id pub-id-type="doi">10.1111/psyp.14228</pub-id><?supplied-pmid 36416572?><pub-id pub-id-type="pmid">36416572</pub-id>
</element-citation></ref><ref id="CR17"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Von der Mosel</surname><given-names>J</given-names></name><name><surname>Trautsch</surname><given-names>A</given-names></name><name><surname>Herbold</surname><given-names>S</given-names></name></person-group><article-title>On the validity of pre-trained transformers for natural language processing in the software engineering domain[J]</article-title><source>IEEE Trans. Softw. Eng.</source><year>2022</year><volume>49</volume><issue>4</issue><fpage>1487</fpage><lpage>1507</lpage><pub-id pub-id-type="doi">10.1109/TSE.2022.3178469</pub-id></element-citation></ref><ref id="CR18"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peer</surname><given-names>D</given-names></name><name><surname>Stabinger</surname><given-names>S</given-names></name><name><surname>Engl</surname><given-names>S</given-names></name><etal/></person-group><article-title>Greedy-layer pruning: Speeding up transformer models for natural language processing[J]</article-title><source>Pattern Recogn. Lett.</source><year>2022</year><volume>157</volume><fpage>76</fpage><lpage>82</lpage><pub-id pub-id-type="doi">10.1016/j.patrec.2022.03.023</pub-id></element-citation></ref><ref id="CR19"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>E</given-names></name><name><surname>Li</surname><given-names>MD</given-names></name><name><surname>Raghavan</surname><given-names>S</given-names></name><etal/></person-group><article-title>Transformer versus traditional natural language processing: How much data is enough for automated radiation report classification ? [J]</article-title><source>Br. J. Radiol.</source><year>2023</year><volume>96</volume><fpage>20220769</fpage><pub-id pub-id-type="doi">10.1259/bjr.20220769</pub-id><?supplied-pmid 37162253?><pub-id pub-id-type="pmid">37162253</pub-id>
</element-citation></ref><ref id="CR20"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Remedios</surname><given-names>D</given-names></name><name><surname>Remedios</surname><given-names>A</given-names></name></person-group><article-title>Transformers, codes and labels: Large language modeling for natural language processing in clinical radiation[J]</article-title><source>Eur. Radiol.</source><year>2023</year><volume>33</volume><issue>6</issue><fpage>4226</fpage><lpage>4227</lpage><pub-id pub-id-type="doi">10.1007/s00330-023-09566-4</pub-id><?supplied-pmid 37014411?><pub-id pub-id-type="pmid">37014411</pub-id>
</element-citation></ref><ref id="CR21"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xu</surname><given-names>K</given-names></name><name><surname>Lv</surname><given-names>Z</given-names></name><name><surname>Poiesi</surname><given-names>F</given-names></name><name><surname>Dong</surname><given-names>Q</given-names></name><etal/></person-group><article-title>Deep learning for intelligent human&#x02013;computer interaction[J]</article-title><source>Appl. Sci.</source><year>2022</year><volume>12</volume><issue>22</issue><fpage>11457</fpage><pub-id pub-id-type="doi">10.3390/app122211457</pub-id></element-citation></ref><ref id="CR22"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sornlertlamvanich</surname><given-names>V</given-names></name><name><surname>Yuenyong</surname><given-names>S</given-names></name></person-group><article-title>Thai named entity recognition using BiLSTM-CNN-CRF enhanced by TCC[J]</article-title><source>IEEE Access</source><year>2022</year><volume>10</volume><fpage>53043</fpage><lpage>53052</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2022.3175201</pub-id></element-citation></ref><ref id="CR23"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ma</surname><given-names>H</given-names></name><name><surname>Wang</surname><given-names>J</given-names></name><name><surname>Lin</surname><given-names>H</given-names></name><etal/></person-group><article-title>Graph augmented sequence-to-sequence model for neural question generation[J]</article-title><source>Appl. Intell.</source><year>2023</year><volume>53</volume><issue>11</issue><fpage>14628</fpage><lpage>14644</lpage><pub-id pub-id-type="doi">10.1007/s10489-022-04260-2</pub-id></element-citation></ref><ref id="CR24"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ye</surname><given-names>Z</given-names></name><name><surname>Zuo</surname><given-names>T</given-names></name><name><surname>Chen</surname><given-names>W</given-names></name><etal/></person-group><article-title>Textual emotion recognition method based on ALBERT-BiLSTM model and SVM-NB classification[J]</article-title><source>Soft Comput.</source><year>2023</year><volume>27</volume><issue>8</issue><fpage>5063</fpage><lpage>5075</lpage><pub-id pub-id-type="doi">10.1007/s00500-023-07924-4</pub-id></element-citation></ref><ref id="CR25"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liao</surname><given-names>J</given-names></name><name><surname>Wang</surname><given-names>M</given-names></name><name><surname>Chen</surname><given-names>X</given-names></name><etal/></person-group><article-title>Dynamic commonsense knowledge fused method for Chinese implicit sentiment analysis[J]</article-title><source>Inf. Process. Manag.</source><year>2022</year><volume>59</volume><issue>3</issue><fpage>102934</fpage><pub-id pub-id-type="doi">10.1016/j.ipm.2022.102934</pub-id></element-citation></ref><ref id="CR26"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>S</given-names></name><name><surname>Cheng</surname><given-names>C</given-names></name><name><surname>Wu</surname><given-names>P</given-names></name><etal/></person-group><article-title>Phonological processing, visuospatial skills, and pattern understanding in chinese developmental dyscalculia[J]</article-title><source>J. Learn. Disabil.</source><year>2022</year><volume>55</volume><issue>6</issue><fpage>499</fpage><lpage>512</lpage><pub-id pub-id-type="doi">10.1177/00222194211063650</pub-id><?supplied-pmid 34905999?><pub-id pub-id-type="pmid">34905999</pub-id>
</element-citation></ref><ref id="CR27"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shi</surname><given-names>J</given-names></name><name><surname>Li</surname><given-names>H</given-names></name><name><surname>Wang</surname><given-names>C</given-names></name><etal/></person-group><article-title>Pseudo- siamese networks with lexicon for Chinese short text matching[J]</article-title><source>J. Intell. Fuzzy Syst.</source><year>2021</year><volume>41</volume><issue>6</issue><fpage>6097</fpage><lpage>6109</lpage><pub-id pub-id-type="doi">10.3233/JIFS-202592</pub-id></element-citation></ref><ref id="CR28"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hafeez</surname><given-names>H</given-names></name><name><surname>Muneer</surname><given-names>I</given-names></name><name><surname>Sharjeel</surname><given-names>M</given-names></name><etal/></person-group><article-title>Urdu short paraphrase detection at sentence level[J]</article-title><source>ACM Trans. Asian Low-Resour. Lang. Inf. Process.</source><year>2023</year><volume>22</volume><issue>4</issue><fpage>1</fpage><lpage>20</lpage><pub-id pub-id-type="doi">10.1145/3586009</pub-id></element-citation></ref><ref id="CR29"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jeon</surname><given-names>H</given-names></name><name><surname>Park</surname><given-names>S</given-names></name><name><surname>Kim</surname><given-names>JG</given-names></name><etal/></person-group><article-title>PET: Parameter-efficient knowledge distillation on transformer[J]</article-title><source>Plos One</source><year>2023</year><volume>18</volume><issue>7</issue><fpage>e0288060</fpage><pub-id pub-id-type="doi">10.1371/journal.pone.0288060</pub-id><?supplied-pmid 37410716?><pub-id pub-id-type="pmid">37410716</pub-id>
</element-citation></ref><ref id="CR30"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Badaro</surname><given-names>G</given-names></name><name><surname>Papotti</surname><given-names>P</given-names></name></person-group><article-title>Transformers for tabular data representation: A tutorial on models and applications[J]</article-title><source>Proc. VLDB Endow.</source><year>2022</year><volume>15</volume><issue>12</issue><fpage>3746</fpage><lpage>3749</lpage><pub-id pub-id-type="doi">10.14778/3554821.3554890</pub-id></element-citation></ref><ref id="CR31"><label>31.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Korngiebel</surname><given-names>DM</given-names></name><name><surname>Mooney</surname><given-names>SD</given-names></name></person-group><article-title>Considering the possibilities and pitfalls of generative pre-trained transformer 3 (GPT-3) in healthcare delivery[J]</article-title><source>NPJ Digital Med.</source><year>2021</year><volume>4</volume><issue>1</issue><fpage>93</fpage><pub-id pub-id-type="doi">10.1038/s41746-021-00464-x</pub-id></element-citation></ref><ref id="CR32"><label>32.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Anand</surname><given-names>M</given-names></name><name><surname>Sahay</surname><given-names>KB</given-names></name><name><surname>Ahmed</surname><given-names>MA</given-names></name><etal/></person-group><article-title>Deep learning and natural language processing in computation for offensive language detection in online social networks by feature selection and ensemble classification techniques[J]</article-title><source>Theor. Comput. Sci.</source><year>2023</year><volume>943</volume><fpage>203</fpage><lpage>218</lpage><pub-id pub-id-type="doi">10.1016/j.tcs.2022.06.020</pub-id></element-citation></ref><ref id="CR33"><label>33.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Murphy</surname><given-names>RM</given-names></name><name><surname>Klopotowska</surname><given-names>JE</given-names></name><name><surname>de Keizer</surname><given-names>NF</given-names></name><etal/></person-group><article-title>Adverse drug event detection using natural language processing: A scoping review of supervised learning methods[J]</article-title><source>Plos one</source><year>2023</year><volume>18</volume><issue>1</issue><fpage>e0279842</fpage><pub-id pub-id-type="doi">10.1371/journal.pone.0279842</pub-id><?supplied-pmid 36595517?><pub-id pub-id-type="pmid">36595517</pub-id>
</element-citation></ref><ref id="CR34"><label>34.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tao</surname><given-names>L</given-names></name><name><surname>Xie</surname><given-names>Z</given-names></name><name><surname>Xu</surname><given-names>D</given-names></name><etal/></person-group><article-title>Geographic named entity recognition by employing natural language processing and an improved BERT model[J]</article-title><source>ISPRS Int. J. Geo-Inf.</source><year>2022</year><volume>11</volume><issue>12</issue><fpage>598</fpage><pub-id pub-id-type="doi">10.3390/ijgi11120598</pub-id></element-citation></ref><ref id="CR35"><label>35.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Geng</surname><given-names>S</given-names></name><name><surname>Zhu</surname><given-names>Z</given-names></name><name><surname>Wang</surname><given-names>Z</given-names></name><etal/></person-group><article-title>LW- ViT: The lightweight vision transformer model applied in offline handwritten Chinese character recognition[J]</article-title><source>Electronics</source><year>2023</year><volume>12</volume><issue>7</issue><fpage>1693</fpage><pub-id pub-id-type="doi">10.3390/electronics12071693</pub-id></element-citation></ref><ref id="CR36"><label>36.</label><mixed-citation publication-type="other">Zheng, H., Li, L., Dai, D., et al. Leveraging word-formation knowledge for Chinese word sense disambiguation[C]. <italic>Proc. Findings of the Association for Computational Linguistics: EMNLP 2021</italic>. 918&#x02013;923 (2021).</mixed-citation></ref><ref id="CR37"><label>37.</label><mixed-citation publication-type="other">Hou, B., Qi, F., Zang, Y., et al. Try to substitute: An unsupervised chinese word sense disambiguation method based on hownet[C]. <italic>Proc. of the 28th International Conference on Computational Linguistics</italic>. 1752&#x02013;1757 (2020).</mixed-citation></ref><ref id="CR38"><label>38.</label><mixed-citation publication-type="other"><ext-link ext-link-type="uri" xlink:href="https://web.eecs.umich.edu/~mihalcea/senseval/senseval3/data.html">https://web.eecs.umich.edu/~mihalcea/senseval/senseval3/data.html</ext-link>.</mixed-citation></ref><ref id="CR39"><label>39.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>&#x000d6;z&#x000e7;ift</surname><given-names>A</given-names></name><name><surname>Akarsu</surname><given-names>K</given-names></name><name><surname>Yumuk</surname><given-names>F</given-names></name><etal/></person-group><article-title>Advancing natural language processing (NLP) applications of morphologically rich languages with bidirectional encoder representations from transformers (BERT): an empirical case study for Turkish[J]</article-title><source>Automatika &#x0010d;as opis za automatiku, mjerenje, elektroniku, ra&#x0010d;unarstvo i komunikacije</source><year>2021</year><volume>62</volume><issue>2</issue><fpage>226</fpage><lpage>238</lpage></element-citation></ref><ref id="CR40"><label>40.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ji</surname><given-names>M</given-names></name><name><surname>Zhou</surname><given-names>J</given-names></name><name><surname>Wei</surname><given-names>N</given-names></name></person-group><article-title>AFR-BERT: Attention-based mechanism feature relevance fusion multimodal sentiment analysis model[J]</article-title><source>Plos one</source><year>2022</year><volume>17</volume><issue>9</issue><fpage>e0273936</fpage><pub-id pub-id-type="doi">10.1371/journal.pone.0273936</pub-id><pub-id pub-id-type="pmid">36084041</pub-id>
</element-citation></ref><ref id="CR41"><label>41.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>D</given-names></name><name><surname>Yang</surname><given-names>W</given-names></name><name><surname>Wang</surname><given-names>L</given-names></name><etal/></person-group><article-title>PCAT-UNet: UNet -like network fused convolution and transformer for retinal vessel segmentation[J]</article-title><source>PloS one</source><year>2022</year><volume>17</volume><issue>1</issue><fpage>e0262689</fpage><pub-id pub-id-type="doi">10.1371/journal.pone.0262689</pub-id><?supplied-pmid 35073371?><pub-id pub-id-type="pmid">35073371</pub-id>
</element-citation></ref></ref-list></back></article>