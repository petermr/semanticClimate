<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd"> 
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><?properties open_access?><?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?><?DTDIdentifier.IdentifierType public?><?SourceDTD.DTDName A++V2.4.dtd?><?SourceDTD.Version 2.4?><?ConverterInfo.XSLTName springer2nlmx2.xsl?><?ConverterInfo.Version 1?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sci Rep</journal-id><journal-id journal-id-type="iso-abbrev">Sci Rep</journal-id><journal-title-group><journal-title>Scientific Reports</journal-title></journal-title-group><issn pub-type="epub">2045-2322</issn><publisher><publisher-name>Nature Publishing Group UK</publisher-name><publisher-loc>London</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmcid">11026521</article-id><article-id pub-id-type="pmid">38637613</article-id>
<article-id pub-id-type="publisher-id">59735</article-id><article-id pub-id-type="doi">10.1038/s41598-024-59735-8</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Computed tomography-based automated measurement of abdominal aortic aneurysm using semantic segmentation with active learning</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Kim</surname><given-names>Taehun</given-names></name><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author"><name><surname>On</surname><given-names>Sungchul</given-names></name><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff3">3</xref></contrib><contrib contrib-type="author" corresp="yes" equal-contrib="yes"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-1681-1014</contrib-id><name><surname>Gwon</surname><given-names>Jun Gyo</given-names></name><address><email>doctorgjg@gmail.com</email></address><xref ref-type="aff" rid="Aff4">4</xref></contrib><contrib contrib-type="author" corresp="yes" equal-contrib="yes"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-3438-2217</contrib-id><name><surname>Kim</surname><given-names>Namkug</given-names></name><address><email>namkugkim@gmail.com</email></address><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff5">5</xref></contrib><aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="GRID">grid.267370.7</institution-id><institution-id institution-id-type="ISNI">0000 0004 0533 4667</institution-id><institution>Department of Convergence Medicine, Asan Medical Institute of Convergence Science and Technology, Asan Medical Center, </institution><institution>University of Ulsan College of Medicine, </institution></institution-wrap>88, Olympic-ro 43-gil, Songpa-gu, Seoul, 05505 Republic of Korea </aff><aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/04qh86j58</institution-id><institution-id institution-id-type="GRID">grid.496416.8</institution-id><institution-id institution-id-type="ISNI">0000 0004 5934 6655</institution-id><institution>Artificial Intelligence and Robotics Institute, </institution><institution>Korea Institute of Science and Technology, </institution></institution-wrap>Seoul, Republic of Korea </aff><aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="GRID">grid.267370.7</institution-id><institution-id institution-id-type="ISNI">0000 0004 0533 4667</institution-id><institution>Department of Biomedical Engineering, Asan Medical Institute of Convergence Science and Technology, Asan Medical Center, </institution><institution>University of Ulsan College of Medicine, </institution></institution-wrap>Seoul, Republic of Korea </aff><aff id="Aff4"><label>4</label><institution-wrap><institution-id institution-id-type="GRID">grid.267370.7</institution-id><institution-id institution-id-type="ISNI">0000 0004 0533 4667</institution-id><institution>Division of Vascular Surgery, Department of Surgery, Asan Medical Center, </institution><institution>University of Ulsan College of Medicine, </institution></institution-wrap>88, Olympic-ro 43-gil, Songpa-gu, Seoul, 05505 Republic of Korea </aff><aff id="Aff5"><label>5</label><institution-wrap><institution-id institution-id-type="GRID">grid.267370.7</institution-id><institution-id institution-id-type="ISNI">0000 0004 0533 4667</institution-id><institution>Department of Radiology, Asan Medical Center, </institution><institution>University of Ulsan College of Medicine, </institution></institution-wrap>Seoul, Republic of Korea </aff></contrib-group><pub-date pub-type="epub"><day>18</day><month>4</month><year>2024</year></pub-date><pub-date pub-type="pmc-release"><day>18</day><month>4</month><year>2024</year></pub-date><pub-date pub-type="collection"><year>2024</year></pub-date><volume>14</volume><elocation-id>8924</elocation-id><history><date date-type="received"><day>30</day><month>11</month><year>2023</year></date><date date-type="accepted"><day>15</day><month>4</month><year>2024</year></date></history><permissions><copyright-statement>&#x000a9; The Author(s) 2024</copyright-statement><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p></license></permissions><abstract id="Abs1"><p id="Par1">Accurate measurement of abdominal aortic aneurysm is essential for selecting suitable stent-grafts to avoid complications of endovascular aneurysm repair. However, the conventional image-based measurements are inaccurate and time-consuming. We introduce the automated workflow including semantic segmentation with active learning (AL) and measurement using an application programming interface of computer-aided design. 300 patients underwent CT scans, and semantic segmentation for aorta, thrombus, calcification, and vessels was performed in 60&#x02013;300 cases with AL across five stages using UNETR, SwinUNETR, and nnU-Net consisted of 2D, 3D U-Net, 2D-3D U-Net ensemble, and cascaded 3D U-Net. 7 clinical landmarks were automatically measured for 96 patients. In AL stage 5, 3D U-Net achieved the highest dice similarity coefficient (DSC) with statistically significant differences (p&#x02009;&#x0003c;&#x02009;0.01) except from the 2D&#x02013;3D U-Net ensemble and cascade 3D U-Net. SwinUNETR excelled in 95% Hausdorff distance (HD95) with significant differences (p&#x02009;&#x0003c;&#x02009;0.01) except from UNETR and 3D U-Net. DSC of aorta and calcification were saturated at stage 1 and 4, whereas thrombus and vessels were continuously improved at stage 5. The segmentation time between the manual and AL-corrected segmentation using the best model (3D U-Net) was reduced to 9.51&#x02009;&#x000b1;&#x02009;1.02, 2.09&#x02009;&#x000b1;&#x02009;1.06, 1.07&#x02009;&#x000b1;&#x02009;1.10, and 1.07&#x02009;&#x000b1;&#x02009;0.97&#x000a0;min for the aorta, thrombus, calcification, and vessels, respectively (p&#x02009;&#x0003c;&#x02009;0.001). All measurement and tortuosity ratio measured&#x02009;&#x02212;&#x02009;1.71&#x02009;&#x000b1;&#x02009;6.53&#x000a0;mm and&#x02009;&#x02212;&#x02009;0.15&#x02009;&#x000b1;&#x02009;0.25. We developed an automated workflow with semantic segmentation and measurement, demonstrating its efficiency compared to conventional methods.</p></abstract><kwd-group xml:lang="en"><title>Keywords</title><kwd>Abdominal aortic aneurysm</kwd><kwd>Active learning</kwd><kwd>Application programming interface</kwd><kwd>Computer-aided design</kwd><kwd>Deep learning</kwd><kwd>Endovascular abdominal repair stent graft</kwd></kwd-group><kwd-group kwd-group-type="npg-subject"><title>Subject terms</title><kwd>Biomedical engineering</kwd><kwd>Computer science</kwd></kwd-group><funding-group><award-group><funding-source><institution>Ministry of Health &#x00026; Welfare</institution></funding-source><award-id>HI18C2383</award-id><principal-award-recipient><name><surname>Kim</surname><given-names>Namkug</given-names></name></principal-award-recipient></award-group></funding-group><custom-meta-group><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#x000a9; Springer Nature Limited 2024</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1"><title>Introduction</title><p id="Par2">Currently, endovascular aneurysm repair (EVAR) is the standard treatment for abdominal aortic aneurysm (AAA)<sup><xref ref-type="bibr" rid="CR1">1</xref></sup>. Post-EVAR complications, including endoleaks, vary depending on the anatomical shape, degree of calcification, and extent of the thrombus in the aorta<sup><xref ref-type="bibr" rid="CR2">2</xref></sup>. Particularly, when EVAR of AAA deviates from the instructions for use (IFU), clinical outcomes, including re-interventions and mortality, are worse than the cases with the IFU<sup><xref ref-type="bibr" rid="CR3">3</xref></sup>. Currently, medical imaging examinations, such as computed tomography (CT), are crucial and the only procedures to evaluate the shape and size of an AAA before EVAR. However, the inter-observer reproducibility of aortic shape measurement is poor at 87%, exceeding the clinical tolerance of&#x02009;&#x000b1;&#x02009;5&#x000a0;mm for aortic diameter measurement<sup><xref ref-type="bibr" rid="CR4">4</xref></sup>. This limitation negatively impacts post-procedural clinical outcomes, making automated segmentation and measurement valuable for planning EVAR and improving its overall success.</p><p id="Par3">Recently, deep learning (DL) models, particularly the U-Net architecture, have made major advancements in medical image segmentation. They have achieved significant improvements in accuracy and robustness when segmenting anatomical structures and diseases<sup><xref ref-type="bibr" rid="CR5">5</xref></sup>. These DL advancements have enabled more accurate segmentation of various medical conditions and internal structures of the human body<sup><xref ref-type="bibr" rid="CR6">6</xref>&#x02013;<xref ref-type="bibr" rid="CR9">9</xref></sup>. Generally, DL requires many medical images with labeling; however, labeling large amounts of data or complex anatomies is difficult. To address this problem, various studies have introduced active learning (AL) frameworks, which reduce the need for manual annotation. AL divides a dataset into smaller subsets and initiates training. It involves an iterative &#x0201c;human in the loop&#x0201d; process in which a model infers on unlabeled data, and human experts modify the annotations. This continues until the model achieves satisfactory performance or annotation resources<sup><xref ref-type="bibr" rid="CR10">10</xref>,<xref ref-type="bibr" rid="CR11">11</xref></sup>.</p><p id="Par4">Efficient measurement based on segmentation offers benefits, such as reduced lead times, accurate landmark measurement, and robustness. Automated 3D measurement can be performed by algorithmizing the measurement process based on python-script using tools that are basically embbeded in computer-aided design (CAD) software such as Solidwork, 3-matic, and Rahno<sup><xref ref-type="bibr" rid="CR12">12</xref>&#x02013;<xref ref-type="bibr" rid="CR14">14</xref></sup>. The conventional repetitive manual segmentation and measurement are tedious, labor intensive, and time consuming. Furthermore, manual tasks are associated with variations in inter- and intra-human variabilities. Caradu et al.<sup><xref ref-type="bibr" rid="CR15">15</xref></sup> used an automatic segmentation software to robustly segment the lumen and thrombus in AAA. The segmentation was subsequently manually corrected by senior and junior surgeons. Wyss et al.<sup><xref ref-type="bibr" rid="CR2">2</xref></sup> generated a central luminal line after the segmentation of CT images and measured specific landmarks on a cross-sectional plane in 2D images for predicting complications. However, they focused on automated segmentation-based detection with two classes and measurement with a 2D image-based cross-sectional plane, potentially causing discrepancies with the actual 3D anatomy.</p><p id="Par5">Unlike previous studies, we developed a semantic segmentation algorithm with AL for AAA using abdominal CT and automated measurement based on the 3D model obtained by the developed semantic segmentation using CAD.</p><p id="Par6">In contrast to prior investigations, our study presents a novel approach by devising a semantic segmentation algorithm with AL for AAA utilizing abdominal CT scans. Moreover, we introduce an automated measurement framework leveraging the 3D model generated through the semantic segmentation process using scripts-based application programming interface (APIs) of CAD. This holistic methodology not only enhances the accuracy of AAA segmentation but also streamlines the measurement process, signifying a substantial advancement in medical imaging analysis for AAA diagnosis and treatment planning.</p></sec><sec id="Sec2"><title>Methods</title><sec id="Sec3"><title>Dataset acquisition</title><p id="Par7">The retrospective study carried out in accordance with the principles of Declaration of Helsinki and current scientific guidelines. The institutional review board for human investigations at Asan Medical Center approved this study with a waiver of informed consent from patients because of the use of retrospective clinical and imaging data. The data were de-identified, in accordance with the Health Insurance Portability and Accountability Act privacy rule. All methods were performed in accordance with the relevant guidelines and regulations. The dataset could be available on request from the corresponding authors with allowance of our IRB.</p><p id="Par8">Three hundred subjects diagnosed with AAA were enrolled in the Asan Medical Center (AMC) between March 2007 and December 2016. All participants underwent pre-operative CT angiography scanning with a slice thickness of 2.5&#x02013;5.0&#x000a0;mm, field of view (FOV) of 512&#x02009;&#x000d7;&#x02009;512&#x02009;&#x000d7;&#x02009;z-axis, and pixel size of 0.5781&#x02013;0.9258&#x000a0;mm (Table <xref rid="Tab1" ref-type="table">1</xref>). From the 300 participants, for 96, automated measurement using CT angiography scans was performed with a tube voltage of 120 kVp, pixel size of 0.5781&#x02013;0.8164&#x000a0;mm, and slice thickness of 2.5&#x02013;5.0&#x000a0;mm (Table <xref rid="Tab1" ref-type="table">1</xref>).<table-wrap id="Tab1"><label>Table 1</label><caption><p>Details of CT scans of enrolled patients.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Dataset</th><th align="left">Semantic segmentation</th><th align="left">Automated measurement</th></tr></thead><tbody><tr><td align="left">Subject (N)</td><td align="left">300</td><td align="left">94</td></tr><tr><td align="left">Field of view (mm)</td><td align="left">512&#x02009;&#x000d7;&#x02009;512</td><td align="left">512&#x02009;&#x000d7;&#x02009;512</td></tr><tr><td align="left">Tube voltage (kV)</td><td align="left">80&#x02013;130</td><td align="left">100&#x02013;130</td></tr><tr><td align="left">Pixel size (mm)</td><td align="left">0.5781&#x02013;0.9258&#x02009;&#x000d7;&#x02009;0.5781&#x02013;0.9258</td><td align="left">0.5781&#x02013;0.8164&#x02009;&#x000d7;&#x02009;0.5781&#x02013;0.8164</td></tr><tr><td align="left">Slice thickness (mm)</td><td align="left">2.5&#x02013;5.0</td><td align="left">2.5&#x02013;5.0</td></tr></tbody></table></table-wrap></p></sec><sec id="Sec4"><title>Procedure</title><p id="Par9">The retrospective study was divided into two parts: (1) semantic segmentation and (2) automated measurement (Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>). Initially, abdominal CT images were manually segmented into the aorta, thrombus, calcification, and vessels. These sub-datasets were preprocessed and augmented before training various models: UNEt TRansformers (UNETR)<sup><xref ref-type="bibr" rid="CR16">16</xref></sup>, shifted-windows UNEt TRansformers (SwinUNETR)<sup><xref ref-type="bibr" rid="CR17">17</xref></sup>, and no-new-U-Net (nnU-Net), including 2D U-Net, 3D U-Net, 2D&#x02013;3D U-Net ensemble, and cascade 3D U-Net<sup><xref ref-type="bibr" rid="CR18">18</xref></sup>. The preprocessing, augmentation, and training processes were repeated on both the original and additional datasets until no new datasets were available for AL with five stages. In the stages, a new dataset was predicted from the previous model and then corrected by human experts in a &#x0201c;human in the loop&#x0201d; process. Three-dimensional models were generated by semantic segmentation, and automated measurement with CAD was conducted with clinically defined landmarks. Finally, performances of the various networks and stages were evaluated and compared to those of the manual measurements by medical doctors.<fig id="Fig1"><label>Figure 1</label><caption><p>Overall process of semantic segmentation with active learning (AL) and measurement using computer-aided design (CAD). In automated segmentation, after pre-processing and augmentation of CT images, training is conducted using several networks. The best-performing network is selected, and AL is performed up to Stage 5. In automated measurement, based on the data obtained from automated segmentation, the 3D model is generated. Automated measurements are then carried out using a script-based application programming interface (API), followed by evaluation against conventional CT image-based measurements. *nnU-Net is consisted of 2D, 3D U-Net, 2D-3D U-Net ensemble, and cascaded 3D U-Net. (<italic>UNETR</italic> UNEt Transformers, <italic>SwinUNETR</italic> shifted-windows UNEt TRansformers).</p></caption><graphic xlink:href="41598_2024_59735_Fig1_HTML" id="MO1"/></fig></p></sec><sec id="Sec5"><title>Dataset distribution and AL</title><p id="Par10">All participants were divided into five stages of AL, and the data distribution for the training, validation, and testing is depicted in Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref>. In the first stage, the ground truths were manually delineated for 48 training, 6 validation, and 6 test samples from the CT angiography scans of 60 participants, covering four classes. The next stage involved manually correcting the predicted segmentation obtained using a convolutional neural network, resulting in AL-corrected segmentation for 60 new data. A total of 120 subjects were trained (60 from the previous stage and 60 new subjects). Stages 3 and 4 followed a similar process to stage 2, and in the final stage, all 300 subjects were used: 240 as training, 30 as validation, and 30 as test samples. The best-performing network in the final stage was selected, and the best network trained in each stage was used to infer on the 30 test sets. The results were evaluated by manual segmentation (Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref>).<fig id="Fig2"><label>Figure 2</label><caption><p>Training and test strategy with active learning (AL). *All stages except Stage 1 contain the same dataset from the training and validation of the previous stage.</p></caption><graphic xlink:href="41598_2024_59735_Fig2_HTML" id="MO2"/></fig></p></sec><sec id="Sec6"><title>Initial manual and AL-corrected segmentation</title><p id="Par11">In the initial segmentation, the aorta was segmented by &#x0201c;Thresholding&#x0201d; positioning the FOV and &#x0201c;Region growing&#x0201d; with a seed point and &#x0201c;Close&#x0201d; under &#x0201c;Morphology Operations&#x0201d; for filling the outside of the Hounsfield units (HUs) of the aorta. The thrombus with a small amount of an injected contrast medium, small vessels, and calcification were segmented using the &#x0201c;Edit Mask&#x0201d; function to add or eliminate areas. The AL-corrected segmentation used the same techniques as the initial segmentation to correct the predicted binary masks. These masks were then superimposed onto the CT images and manually adjusted to produce the ground-truth segmentation. We used Mimics software (Materialise., Leuven, Belgium) for segmentation of all dataset.</p></sec><sec id="Sec7"><title>Preprocessing</title><p id="Par12">The data preparation involved preprocessing steps, including foreground elimination to remove irrelevant regions such as the background. Z-score normalization was performed to adjust the contrast in the CT images, and the intensity values were clipped between 0 and 1<sup><xref ref-type="bibr" rid="CR19">19</xref></sup>. The spatial properties of the images were standardized by resizing the FOV to 512&#x02009;&#x000d7;&#x02009;512&#x02009;&#x000d7;&#x02009;z-axis mm and applying a spacing of 1.0&#x02009;&#x000d7;&#x02009;1.0&#x02009;&#x000d7;&#x02009;3.0&#x000a0;mm.</p></sec><sec id="Sec8"><title>Networks and experimental settings</title><p id="Par13">nnU-Net is a framework for medical image segmentation. It utilizes a self-adapting approach to optimize hyperparameters, including preprocessing, loss optimization during training, and post-processing operations. Nested cross-validation loops are used to enhance the performance of specific segmentation tasks on different data subsets<sup><xref ref-type="bibr" rid="CR18">18</xref></sup>. UNETR and SwinUNETR are medical image segmentation architectures provided by the Medical Open Network for Artificial Intelligence (MONAI) that use transformers<sup><xref ref-type="bibr" rid="CR16">16</xref>,<xref ref-type="bibr" rid="CR17">17</xref></sup>. UNETR combines U-Net with transformers and employs self-attention. SwinUNETR is an upgraded version optimized for medical image segmentation<sup><xref ref-type="bibr" rid="CR17">17</xref></sup>. It has a hybrid design with a Swin transformer encoder for high-level features and a U-Net decoder for segmentation maps. The Swin transformer breaks down an input image into smaller patches and applies self-attention layers to capture features at various scales. The hyper-parameter of nnU-Net is applied with self-configuration. In UNETR and SwinUNETR, for augmentation, various transformations were performed: foreground extraction, random rotation (90&#x000b0;), random flips (x-, y-, and z-axes), and random intensity shifts with 3D patch (96&#x02009;&#x000d7;&#x02009;96&#x02009;&#x000d7;&#x02009;96). The training used the Adam optimizer (learning rate&#x02009;=&#x02009;0.0001, weight decay&#x02009;=&#x02009;0.00001) with the dice cross entropy loss and a batch size of 4. The training epochs for nnU-Net were set to 10,000, while UNETR and SwinUNETR were set to 100,000 steps for training. The training was performed on an NVIDIA TITAN RTX GPU with 24,220 MiB, using MONAI 0.1.0 and PyTorch 1.12.1.</p></sec><sec id="Sec9"><title>Landmarks of AAA</title><p id="Par14">To avoid complications such as endoleaks and re-intervention after EVAR and determine suitable commercially available endografts based on the anatomical size of a patient, a 3D model was generated by automatic segmentation. It was defined by seven landmarks: (1) aortic neck diameter, which is the diameter of the midpoint between the lower part of the renal artery and the starting point of the aneurysm, (2) aortic aneurysm diameter, which is the maximum diameter of the aneurysm area, (3) right iliac artery diameter, which is the maximum diameter of the right iliac artery, (4) left iliac artery diameter, which is the maximum diameter of the left iliac artery, (5) aortic neck length, which is the distance between the lower part of the renal artery and the starting point of the aneurysm, (6) common iliac artery tortuosity, which is the ratio of the centerline and the straight line between the start and end of the common iliac artery, and (7) aortic neck angulation, which is the angle between the aortic neck and aortic aneurysm (Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3</xref>).<fig id="Fig3"><label>Figure 3</label><caption><p>The measurement landmarks for determining the stent graft according to the patient&#x02019;s anatomy. It was defined by seven landmarks: aortic neck diameter, which is the diameter of the midpoint between the lower part of the renal artery and the starting point of the aneurysm, aortic aneurysm diameter, which is the maximum diameter of the aneurysm area, right and left iliac artery diameter, which is the maximum diameter of the iliac artery, aortic neck length, which is the distance between the lower part of the renal artery and the starting point of the aneurysm, common iliac artery tortuosity, which is the ratio of the centerline and the straight line between the start and end of the common iliac artery, and aortic neck angulation, which is the angle between the aortic neck and aortic aneurysm.</p></caption><graphic xlink:href="41598_2024_59735_Fig3_HTML" id="MO3"/></fig></p></sec><sec id="Sec10"><title>Conventional and automated measurement</title><p id="Par15">The conventional measurement was performed with tools of a picture archiving and communication system (PACS) based on 2D images. Seven measurements were performed using the PACS. On the selected axial slice, the measurement included the antero-posterior, transverse, and maximum diameters in any direction. On the selected sagittal slice, the measurement included the antero-posterior diameter and the diameter perpendicular to the long axis of the aneurysm. On the coronal slice images, the measurement included the transverse diameter and the diameter perpendicular to the long axis of the aneurysm<sup><xref ref-type="bibr" rid="CR4">4</xref></sup>. Two algorithms were developed for the automated measurement of the landmarks in AAA, requiring inputs such as a 3D aortic model with a thrombus, a centerline dividing the aortic neck and the aneurysm, right and left iliac arteries, and start planes of the right and left iliac arteries. The necessity of each input is as follows: (1) 3D aortic model serves as the base for all measurements, enabling the generation of centerlines and measurement. (2) Centerlines are utilized in conjunction with the 3D aortic model to measure diameter, length, tortuosity, and angulation. (3) The planes of the right and left iliac arteries are employed to separate the iliac artery model from the aortic 3D model, and the separated 3D models and their centerlines are used to determine maximum diameter.</p><p id="Par16">Algorithm 1 involved generating points along the centerline and the directions between each point and its consecutive one. This process created planes, which were used to intersect the 3D model of the AAA. The longest contour among the generated contours was selected for the measurement. The aortic neck diameter was measured with the midpoint and its consecutive one between the starting and ending points of the aortic neck centerline and lines based on these two generated points. Subsequently, the mid-plane was generated using the midpoint and the direction of the line and then intersected with the AAA 3D model. The maximum diameter of the mid-contour was measured. The aortic aneurysm diameter was obtained by feeding the AAA 3D model and the centerline of the aortic aneurysm into Algorithm 1. Determining the diameters of both iliac arteries necessitates obtaining the starting plane of the centerline for each iliac artery. This involves cutting the AAA 3D model based on the plane corresponding to the right or left side and measuring using Algorithm 1. The aortic neck length was derived by selecting the aortic neck centerline. The tortuosity of the common iliac artery was determined based on the ratio of the straight length and its centerline of the AAA 3D model. The straight length was that between the starting and ending points of the centerline for each iliac artery. Finally, obtaining the aortic neck angulation involved calculating the angle formed by two lines that were derived from the starting and ending points of the centerlines of the aortic neck and the aortic aneurysm (Algorithm 2). The inputs for the automated measurement were as follows: (1) an aortic model with a thrombus, (2) the aortic neck centerline, (3) the aortic aneurysm centerline, (4) the centerline of the right and left iliac arteries, and (5) the planes of the right and left iliac arteries. Supplementary Video <xref rid="MOESM2" ref-type="media">1</xref> was shown in the automated measurement process for 7 landmarks.<fig position="anchor" id="Figa"><label>Algorithm 1</label><caption><p>Pseudocode for determining maximum contour.</p></caption><graphic position="anchor" xlink:href="41598_2024_59735_Figa_HTML" id="MO4"/></fig><fig position="anchor" id="Figb"><label>Algorithm 2</label><caption><p>Pseudocode for automatic measurement of seven landmarks.</p></caption><graphic position="anchor" xlink:href="41598_2024_59735_Figb_HTML" id="MO5"/></fig></p></sec><sec id="Sec11"><title>Evaluation</title><p id="Par18">To assess the accuracy of the predicted labels against the ground truths in the semantic segmentation, the dice similarity coefficient (DSC) and 95% Hausdorff distance (HD95) were utilized<sup><xref ref-type="bibr" rid="CR20">20</xref></sup>. The DSC is a metric that ranges from 0 to 1, with 0 and 1 indicating no and perfect overlaps between the volumes, respectively. HD95 is similar to the maximum Hausdorff distance and it removes outliers by considering the 95th percentile of the distances. This prevents extreme values from significantly influencing the metric. The saturation concerning dataset sizes was evaluated across various classes, revealing discrepancies in predictive performance. While classes such as aorta demonstrated effective prediction with relatively small dataset sizes, others like thrombus, calcification, and vessels struggled despite larger dataset sizes. By comparing results at each stage, optimal dataset sizes were determined for individual classes, facilitating their respective optimization. In addition, manual and AL-corrected segmentation times by one observer were evaluated on the same ten patients randomly selected from the test set. All statistics were evaluated using paired t-tests. The measurements were evaluated by the Bland&#x02013;Altman analysis between our method and the manual ground truths obtained by medical doctors with limit of agreement, which used to remove outliers lying outside the 95% range.</p><p id="Par19">In addition, for the centerline, which was divided manually for the automated measurement, human variation was confirmed using three observers and by correlation analysis.</p></sec></sec><sec id="Sec12"><title>Results</title><sec id="Sec13"><title>DSC and HD95 in semantic segmentation with various networks</title><p id="Par20">Figure&#x000a0;<xref rid="Fig4" ref-type="fig">4</xref> was shown in the boxplot for the DSC and HD95 in stage 5, comparing the different networks. In stage 5, the 2D&#x02013;3D U-Net ensemble yielded the most accurate outcomes for the aorta and calcification, scoring 0.928&#x02009;&#x000b1;&#x02009;0.026 and 0.702&#x02009;&#x000b1;&#x02009;0.226, respectively. Cascade 3D U-Net achieved the highest accuracy for the thrombus, scoring 0.782&#x02009;&#x000b1;&#x02009;0.170. For vessels, the 3D U-Net was the best with a score of 0.481&#x02009;&#x000b1;&#x02009;0.155. The 3D U-Net showed the highest performance with an average DSC of 0.722&#x02009;&#x000b1;&#x02009;0.227 and exhibited statistically significant differences with all networks (p&#x02009;&#x0003c;&#x02009;0.01) except for the 2D&#x02013;3D U-Net ensemble (p&#x02009;=&#x02009;0.153) and cascade 3D U-Net (p&#x02009;=&#x02009;0.102). In terms of HD95, for the aorta, the 2D&#x02013;3D U-Net ensemble achieved the best value of 2.31&#x02009;&#x000b1;&#x02009;1.42 mm, whereas for calcification, the highest accuracy was achieved with in 3D U-Net with a score of 12.39&#x02009;&#x000b1;&#x02009;16.62 mm. UNETR was the best-performance for the thrombus and vessels with HD95 of 7.46&#x02009;&#x000b1;&#x02009;6.12 and 11.61&#x02009;&#x000b1;&#x02009;12.58 mm in stage 5, respectively. SwinUNETR yielded the best outcome in terms of the average HD95, with a value of 10.23&#x02009;&#x000b1;&#x02009;6.19&#x000a0;mm. Statistical analysis indicated a significant difference between SwinUNETR and all networks (p&#x02009;&#x0003c;&#x02009;0.01) except for UNETR (p&#x02009;=&#x02009;0.540) and 3D U-Net (p&#x02009;=&#x02009;0.118) (Fig.&#x000a0;<xref rid="Fig4" ref-type="fig">4</xref>). The DSC and HD95 values in each stage of 3D U-Net are shown in Fig.&#x000a0;<xref rid="Fig5" ref-type="fig">5</xref>. Comprehensive results for all networks, including their stages and classes, are summarized in Supplementary Table <xref rid="MOESM1" ref-type="media">1</xref> and Fig.&#x000a0;<xref rid="Fig4" ref-type="fig">4</xref>.<fig id="Fig4"><label>Figure 4</label><caption><p>DSC and HD95 of each class in stage 5 obtained using various networks. DSC and HD95 of (<bold>A,E</bold>) aorta, (<bold>B,F</bold>) thrombus, (<bold>C,G</bold>) calcification, and (<bold>D,H</bold>) vessels. Paired t-tests between stage 5 and other stages; *p&#x02009;&#x0003c;&#x02009;0.05, **p&#x02009;&#x0003c;&#x02009;0.005, ***p&#x02009;&#x0003c;&#x02009;0.0005; <italic>DSC</italic> dice similarity coefficient, <italic>HD95</italic> 95% Hausdorff distance, <italic>SwinUNETR</italic> shifted-windows UNEt transformers.</p></caption><graphic xlink:href="41598_2024_59735_Fig4_HTML" id="MO6"/></fig><fig id="Fig5"><label>Figure 5</label><caption><p>DSC of 3D U-Net and HD95 of SwinUNETR showed the best performance for each class from stage 1 to stage 5. DSC of 3D U-Net and HD95 of SwinUNETR for (<bold>A,E</bold>) aorta, (<bold>B,F</bold>) thrombus, (<bold>C,G</bold>) calcification, and (<bold>D,H</bold>) vessels. Paired t-tests between stage 5 and other stages; *p&#x02009;&#x0003c;&#x02009;0.05, **p&#x02009;&#x0003c;&#x02009;0.005, ***p&#x02009;&#x0003c;&#x02009;0.0005, <italic>DSC</italic> dice similarity coefficient, <italic>HD95</italic> 95% Hausdorff distance, <italic>SwinUNETR</italic> shifted-windows UNEt transformers.</p></caption><graphic xlink:href="41598_2024_59735_Fig5_HTML" id="MO7"/></fig></p></sec><sec id="Sec14"><title>Saturation evaluation of 3D U-Net</title><p id="Par21">In each stage of 3D U-Net, the aorta consistently showed a small DSC variation, ranging from 0.909 to 0.926, suggesting performance saturation with only 60 cases in stage 1. By contrast, the thrombus and vessels displayed larger DSC variations, ranging from 0.733 to 0.779 and 0.310 to 0.481, respectively, across the different stages. The performance continued to improve even in stage 5 with a larger dataset of 300 cases. For calcification, DSC values ranging from 0.602 to 0.703 were obtained, showing improvement until stage 4 with 240 cases (Fig.&#x000a0;<xref rid="Fig5" ref-type="fig">5</xref>, Supplementary Table <xref rid="MOESM1" ref-type="media">1</xref>).</p></sec><sec id="Sec15"><title>Segmentation times of manual and AL-corrected segmentation using 3D U-Net</title><p id="Par22">The spent time difference between the manual and AL-corrected segmentation using the best model (3D U-Net) reduced as 9.51&#x02009;&#x000b1;&#x02009;1.02, 2.09&#x02009;&#x000b1;&#x02009;1.06, 1.07&#x02009;&#x000b1;&#x02009;1.10, and 1.07&#x02009;&#x000b1;&#x02009;0.97 min for the aorta, thrombus, calcification, and vessels, respectively. These times were statistically significant difference (p&#x02009;&#x0003c;&#x02009;0.001) (Fig.&#x000a0;<xref rid="Fig6" ref-type="fig">6</xref>, Supplementary Table <xref rid="MOESM1" ref-type="media">2</xref>).<fig id="Fig6"><label>Figure 6</label><caption><p>Manual and AL-corrected (using 3D U-Net) segmentation times (*p&#x02009;&#x0003c;&#x02009;0.001).</p></caption><graphic xlink:href="41598_2024_59735_Fig6_HTML" id="MO8"/></fig></p></sec><sec id="Sec16"><title>Comparison of conventional and automated measurements</title><p id="Par23">Table <xref rid="Tab2" ref-type="table">2</xref> lists the arithmetic means and standard deviations of the differences between the conventional and automated measurements of the aortic neck diameter, aortic aneurysm, right and left iliac artery diameters, aortic neck length, and tortuosity of both iliac arteries (curve length, line length, and ratio). For the angulation between the centerline of the aortic neck and the aneurysm, errors were observed for three patients both in manual and automated measurements, particularly when the angle was not 60&#x000b0;.<table-wrap id="Tab2"><label>Table 2</label><caption><p>Manual and automated measurements of aortic neck diameter, aortic aneurysm diameter, right and left iliac artery diameters, aortic neck length, and tortuosity.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Landmark</th><th align="left">Conventional measurement</th><th align="left">Automated measurement</th><th align="left">Difference (LoA)</th></tr></thead><tbody><tr><td align="left">Aortic neck diameter (mm)</td><td align="left">23.41&#x02009;&#x000b1;&#x02009;3.25</td><td align="left">24.60&#x02009;&#x000b1;&#x02009;4.21</td><td align="left">&#x02009;&#x02212;&#x02009;1.19&#x02009;&#x000b1;&#x02009;3.92 (&#x02212;&#x02009;8.88 to 6.50)</td></tr><tr><td align="left">Aortic aneurysm diameter (mm)</td><td align="left">55.99&#x02009;&#x000b1;&#x02009;10.02</td><td align="left">55.76&#x02009;&#x000b1;&#x02009;10.10</td><td align="left">0.23&#x02009;&#x000b1;&#x02009;4.00 (&#x02212;&#x02009;7.61 to 8.06)</td></tr><tr><td align="left">Right iliac artery diameter (mm)</td><td align="left">18.76&#x02009;&#x000b1;&#x02009;6.10</td><td align="left">20.90&#x02009;&#x000b1;&#x02009;5.57</td><td align="left">&#x02009;&#x02212;&#x02009;2.14&#x02009;&#x000b1;&#x02009;4.37 (&#x02212;&#x02009;10.70 to 6.43)</td></tr><tr><td align="left">Left iliac artery diameter (mm)</td><td align="left">16.96&#x02009;&#x000b1;&#x02009;5.61</td><td align="left">19.76&#x02009;&#x000b1;&#x02009;5.34</td><td align="left">&#x02009;&#x02212;&#x02009;2.80&#x02009;&#x000b1;&#x02009;4.09 (&#x02212;&#x02009;10.82 to 5.22)</td></tr><tr><td align="left">Aortic neck length (mm)</td><td align="left">33.06&#x02009;&#x000b1;&#x02009;13.50</td><td align="left">35.71&#x02009;&#x000b1;&#x02009;15.98</td><td align="left">&#x02009;&#x02212;&#x02009;2.65&#x02009;&#x000b1;&#x02009;11.88 (&#x02212;&#x02009;25.94 to 20.64)</td></tr><tr><td align="left" colspan="4">Tortuosity of right iliac artery</td></tr><tr><td align="left">Curve length (mm)</td><td align="left">46.99&#x02009;&#x000b1;&#x02009;12.40</td><td align="left">56.93&#x02009;&#x000b1;&#x02009;17.76</td><td align="left">&#x02009;&#x02212;&#x02009;9.95&#x02009;&#x000b1;&#x02009;12.81 (&#x02212;&#x02009;35.05 to 15.16)</td></tr><tr><td align="left">Line length (mm)</td><td align="left">42.41&#x02009;&#x000b1;&#x02009;11.62</td><td align="left">49.49&#x02009;&#x000b1;&#x02009;13.98</td><td align="left">&#x02009;&#x02212;&#x02009;7.08&#x02009;&#x000b1;&#x02009;10.57 (&#x02212;&#x02009;27.79 to 13.63)</td></tr><tr><td align="left">Ratio</td><td align="left">1.13&#x02009;&#x000b1;&#x02009;0.24</td><td align="left">1.14&#x02009;&#x000b1;&#x02009;0.10</td><td align="left">&#x02009;&#x02212;&#x02009;0.01&#x02009;&#x000b1;&#x02009;0.20 (&#x02212;&#x02009;0.41 to 0.38)</td></tr><tr><td align="left" colspan="4">Tortuosity of left iliac artery</td></tr><tr><td align="left">&#x000a0;Curve length (mm)</td><td align="left">52.27&#x02009;&#x000b1;&#x02009;15.13</td><td align="left">61.95&#x02009;&#x000b1;&#x02009;17.64</td><td align="left">&#x02009;&#x02212;&#x02009;9.68&#x02009;&#x000b1;&#x02009;10.96 (&#x02212;&#x02009;31.16 to 11.81)</td></tr><tr><td align="left">&#x000a0;Line length (mm)</td><td align="left">61.01&#x02009;&#x000b1;&#x02009;17.64</td><td align="left">53.65&#x02009;&#x000b1;&#x02009;13.88</td><td align="left">8.30&#x02009;&#x000b1;&#x02009;7.78 (&#x02212;&#x02009;6.94 to 23.54)</td></tr><tr><td align="left">&#x000a0;Ratio</td><td align="left">0.86&#x02009;&#x000b1;&#x02009;0.17</td><td align="left">1.15&#x02009;&#x000b1;&#x02009;0.15</td><td align="left">&#x02009;&#x02212;&#x02009;0.29&#x02009;&#x000b1;&#x02009;0.22 (&#x02212;&#x02009;0.29 to 0.14)</td></tr><tr><td align="left" colspan="4">Angulation</td></tr><tr><td align="left">&#x000a0;&#x0003c;&#x02009;60&#x000b0; (n)</td><td align="left">86</td><td align="left">89</td><td align="left">&#x02013;</td></tr><tr><td align="left">&#x000a0;&#x0003e;&#x02009;60&#x000b0; (n)</td><td align="left">10</td><td align="left">7</td><td align="left">&#x02013;</td></tr></tbody></table><table-wrap-foot><p><italic>LoA</italic> limit of agreement, <italic>n</italic> number of patients.</p></table-wrap-foot></table-wrap></p></sec><sec id="Sec17"><title>Human variation in automated measurement</title><p id="Par24">For the automated measurement, manual input data was essential. These included a 3D aortic model with a thrombus, the centerline of the aortic model divided by the aortic neck, the aortic aneurysm, right, and left iliac arteries, and the starting planes of the right and left iliac arteries. Human variation in the automated measurement could occur owing to the manual centerline division. The correlation analysis showed a high level of agreement, with an intraclass correlation coefficient value of 1.00 for all measurements, indicating no significant human error in the automated measurement. In addition, all correlations were significant at 0.01 (Table <xref rid="Tab3" ref-type="table">3</xref>).<table-wrap id="Tab3"><label>Table 3</label><caption><p>Correlation analyses of three researchers for aortic neck diameter, aortic neck length, and right and left tortuosity including curve length, line length, and ratio.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Correlation</th><th align="left">r<sub>12</sub></th><th align="left">r<sub>13</sub></th><th align="left">R<sub>23</sub></th></tr></thead><tbody><tr><td align="left">Aortic neck diameter</td><td char="." align="char">0.999</td><td char="." align="char">1.000</td><td char="." align="char">0.998</td></tr><tr><td align="left">Aortic neck length</td><td char="." align="char">1.000</td><td char="." align="char">1.000</td><td char="." align="char">1.000</td></tr><tr><td align="left" colspan="4">Right tortuosity</td></tr><tr><td align="left">&#x000a0;Curve length</td><td char="." align="char">1.000</td><td char="." align="char">1.000</td><td char="." align="char">1.000</td></tr><tr><td align="left">&#x000a0;Line length</td><td char="." align="char">1.000</td><td char="." align="char">1.000</td><td char="." align="char">1.000</td></tr><tr><td align="left">&#x000a0;Ratio</td><td char="." align="char">0.999</td><td char="." align="char">0.999</td><td char="." align="char">1.000</td></tr><tr><td align="left" colspan="4">Left tortuosity</td></tr><tr><td align="left">&#x000a0;Curve length</td><td char="." align="char">1.000</td><td char="." align="char">1.000</td><td char="." align="char">1.000</td></tr><tr><td align="left">&#x000a0;Line length</td><td char="." align="char">1.000</td><td char="." align="char">1.000</td><td char="." align="char">1.000</td></tr><tr><td align="left">&#x000a0;Ratio</td><td char="." align="char">1.000</td><td char="." align="char">1.000</td><td char="." align="char">1.000</td></tr></tbody></table><table-wrap-foot><p><italic>r</italic><sub><italic>12</italic></sub> correlation between researchers 1 and 2, <italic>r</italic><sub><italic>13</italic></sub> correlation between researchers 1 and 3, <italic>r</italic><sub><italic>23</italic></sub> correlation between researchers 2 and 3; all correlations, p&#x02009;&#x0003c;&#x02009;0.01.</p></table-wrap-foot></table-wrap></p></sec></sec><sec id="Sec18"><title>Discussion</title><p id="Par25">We developed a semantic segmentation and measurement method for AAA using abdominal CT. The semantic segmentation involves using several DL architectures with AL. The automated measurement allows obtaining various landmarks: diameters of the aortic neck, aortic aneurysm, and both iliac arteries, aortic neck length, tortuosity of both iliac arteries, and angulation between the aortic neck and the aneurysm.</p><p id="Par26">The evaluation of the semantic segmentation with AL showed that the average DSC and HD95 of the four classes became better or remained consistent as the stages progressed. In particular, the aorta consistently demonstrated superior performance compared to other classes across all stages. Saturation of the dataset was observed at Stage 1, with a reduction in segmentation time to 9.51&#x02009;&#x000b1;&#x02009;1.02&#x000a0;min, indicating that the efficacy is closer to AI-assisted labeling techniques rather than active learning. However, for the thrombus, challenges occurred owing to the lack of contrast enhancement, making it difficult to discern surrounding structures. In addition, the vessels with thin walls posed a challenge owing to the low-resolution of the CT images. Notably, even in stage 5, on utilizing the entire dataset, a consistent performance improvement trend was observed. For calcification, which is characterized by small and randomly distributed patterns on the aortic wall, a relatively low performance was achieved. However, its relatively bright appearance led to saturation in stage 4. Despite the inferior performance compared to the aorta, they were observed that as the stages progressed, better performance was confirmed, indicating the efficacy of active learning.</p><p id="Par27">We compared the predictions of the top-performing 3D U-Net, SwinUNETR, and the ground truths. False positives were identified in the challenging areas of the thrombus, calcification, and vessels where even manual labeling could be difficult (Fig.&#x000a0;<xref rid="Fig7" ref-type="fig">7</xref>). In addition, AL-corrected segmentation significantly reduced the segmentation time by 13.74&#x02009;&#x000b1;&#x02009;2.16&#x000a0;min compared to manual segmentation.<fig id="Fig7"><label>Figure 7</label><caption><p>The challenge of labeling and false positive between ground truth and predictions. (<bold>A</bold>) Original CT images, (<bold>B</bold>) predictions of 3D U-Net, (<bold>C</bold>) predictions of SwinUNETR, and (<bold>D</bold>) ground truths. (Yellow arrow, false positive; Pink, aorta; Green, thrombus; Blue, calcification; purple; vessels).</p></caption><graphic xlink:href="41598_2024_59735_Fig7_HTML" id="MO9"/></fig></p><p id="Par28">We developed two algorithms for automated measurement. The differences between manual and automated measurement can be attributed to their methodologies. The shortcomings of the traditional image-based manual measurement include challenges of accurately detecting the maximum diameter across multiple slices and three views, inadequate reproducibility, prolonged measurement time, and subjective determination of individuals. Our automated approach provides several benefits. First, it significantly decreases repetitive and labor-intensive manual tasks. Second, it reduces time-consuming processes, leading to increased efficiency. Third, it ensures consistency of both automated workflows and researchers. Finally, it can be applied in various medical applications.</p><p id="Par29">There are several limitations in this study. First, the dataset was from only one institution; therefore, validation in multiple institutions is needed in future studies. Second, we found the best networks for each class using various networks in AL. However, to confirm the improved performance, experiments on optimized parameters such as augmentation and development of advanced networks are required. Finally, to achieve full automation, manual intervention needs to be minimized, and advanced algorithm development should be a focus of future research.</p></sec><sec id="Sec19"><title>Conclusions</title><p id="Par30">We developed a semantic segmentation method using various networks with AL and automated measurement using API of CAD. The 3D U-Net demonstrated superior performance compared to the other networks. AL identified saturation stages for each class, and its time efficiency was verified. In addition, our automated measurement approach could be highly efficient in minimizing labor-intensive, time-consuming, and repetitive manual tasks.</p></sec><sec sec-type="supplementary-material"><sec id="Sec20"><title>Supplementary Information</title><p>
<supplementary-material content-type="local-data" id="MOESM1"><media xlink:href="41598_2024_59735_MOESM1_ESM.docx"><caption><p>Supplementary Information.</p></caption></media></supplementary-material><supplementary-material content-type="local-data" id="MOESM2"><media xlink:href="41598_2024_59735_MOESM2_ESM.mp4"><caption><p>Supplementary Video 1.</p></caption></media></supplementary-material></p></sec></sec></body><back><fn-group><fn><p><bold>Publisher's note</bold></p><p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></fn><fn><p>These authors contributed equally: Jun Gyo Gwon and Namkug Kim.</p></fn></fn-group><sec><title>Supplementary Information</title><p>The online version contains supplementary material available at 10.1038/s41598-024-59735-8.</p></sec><ack><title>Acknowledgements</title><p>This study was supported by a Grant from the Korea Health Technology R&#x00026;D Project of the Korea Health Industry Development Institute (KHIDI), funded by the Ministry of Health &#x00026; Welfare, Republic of Korea (HI18C2383, HI22C0471). It was also supported by the Korean Society of Vascular Surgery Lee Yong Gak Research Fund.</p></ack><notes notes-type="author-contribution"><title>Author contributions</title><p>T.K. contributed to conceptualization, methodology, software, validation, formal analysis, data curation, writing&#x02014;original draft, writing&#x02014;review &#x00026; editing, and visualization. S.O. contributed to conceptualization, software, and data curation. J. G. G. contributed to conceptualization, investigation, resources, data curation, writing&#x02014;original draft, writing&#x02014;review &#x00026; editing, supervision, project administration, and funding acquisition. N.K. contributed to conceptualization, methodology, validation, formal analysis, resources, writing&#x02014;review &#x00026; editing, supervision, project administration, and funding acquisition.</p></notes><notes notes-type="data-availability"><title>Data availability</title><p>The dataset could be available on request from the corresponding authors with allowance of our IRB.</p></notes><notes id="FPar1" notes-type="COI-statement"><title>Competing interests</title><p id="Par31">The authors declare no competing interests.</p></notes><ref-list id="Bib1"><title>References</title><ref id="CR1"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chaikof</surname><given-names>EL</given-names></name><etal/></person-group><article-title>The Society for Vascular Surgery practice guidelines on the care of patients with an abdominal aortic aneurysm</article-title><source>J. Vasc. Surg.</source><year>2018</year><volume>67</volume><fpage>2</fpage><lpage>77</lpage><pub-id pub-id-type="doi">10.1016/j.jvs.2017.10.044</pub-id><?supplied-pmid 29268916?><pub-id pub-id-type="pmid">29268916</pub-id>
</element-citation></ref><ref id="CR2"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wyss</surname><given-names>TR</given-names></name><name><surname>Dick</surname><given-names>F</given-names></name><name><surname>Brown</surname><given-names>LC</given-names></name><name><surname>Greenhalgh</surname><given-names>RM</given-names></name></person-group><article-title>The influence of thrombus, calcification, angulation, and tortuosity of attachment sites on the time to the first graft-related complication after endovascular aneurysm repair</article-title><source>J. Vasc. Surg.</source><year>2011</year><volume>54</volume><fpage>965</fpage><lpage>971</lpage><pub-id pub-id-type="doi">10.1016/j.jvs.2011.04.007</pub-id><?supplied-pmid 21723072?><pub-id pub-id-type="pmid">21723072</pub-id>
</element-citation></ref><ref id="CR3"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hahl</surname><given-names>T</given-names></name><etal/></person-group><article-title>Long-term outcomes of endovascular aneurysm repair according to instructions for use adherence status</article-title><source>J. Vasc. Surg.</source><year>2022</year><volume>76</volume><fpage>699</fpage><lpage>706</lpage><pub-id pub-id-type="doi">10.1016/j.jvs.2022.03.010</pub-id><?supplied-pmid 35314298?><pub-id pub-id-type="pmid">35314298</pub-id>
</element-citation></ref><ref id="CR4"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mora</surname><given-names>CE</given-names></name><name><surname>Marcus</surname><given-names>CD</given-names></name><name><surname>Barbe</surname><given-names>CM</given-names></name><name><surname>Ecarnot</surname><given-names>FB</given-names></name><name><surname>Long</surname><given-names>AL</given-names></name></person-group><article-title>Maximum diameter of native abdominal aortic aneurysm measured by angio-computed tomography</article-title><source>Aorta</source><year>2015</year><volume>3</volume><fpage>47</fpage><lpage>55</lpage><pub-id pub-id-type="doi">10.12945/j.aorta.2015.14-059</pub-id><?supplied-pmid 26798757?><pub-id pub-id-type="pmid">26798757</pub-id>
</element-citation></ref><ref id="CR5"><label>5.</label><mixed-citation publication-type="other">Long, J., Shelhamer, E. &#x00026; Darrell, T. <italic>Proc. IEEE Conference on Computer Vision and Pattern Recognition</italic> 3431&#x02013;3440.</mixed-citation></ref><ref id="CR6"><label>6.</label><mixed-citation publication-type="other">&#x000c7;i&#x000e7;ek, &#x000d6;., Abdulkadir, A., Lienkamp, S. S., Brox, T. &#x00026; Ronneberger, O. <italic>Medical Image Computing and Computer-Assisted Intervention&#x02013;MICCAI 2016: 19th International Conference, Athens, Greece, October 17&#x02013;21, 2016, Proceedings, Part II 19</italic> 424&#x02013;432 (Springer).</mixed-citation></ref><ref id="CR7"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chi</surname><given-names>W</given-names></name><etal/></person-group><article-title>Deep learning-based medical image segmentation with limited labels</article-title><source>Phys. Med. Biol.</source><year>2020</year><volume>65</volume><fpage>235001</fpage><pub-id pub-id-type="doi">10.1088/1361-6560/abc363</pub-id></element-citation></ref><ref id="CR8"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>G</given-names></name><etal/></person-group><article-title>Interactive medical image segmentation using deep learning with image-specific fine tuning</article-title><source>IEEE Trans. Med. Imaging</source><year>2018</year><volume>37</volume><fpage>1562</fpage><lpage>1573</lpage><pub-id pub-id-type="doi">10.1109/TMI.2018.2791721</pub-id><?supplied-pmid 29969407?><pub-id pub-id-type="pmid">29969407</pub-id>
</element-citation></ref><ref id="CR9"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cai</surname><given-names>L</given-names></name><name><surname>Gao</surname><given-names>J</given-names></name><name><surname>Zhao</surname><given-names>D</given-names></name></person-group><article-title>A review of the application of deep learning in medical image classification and segmentation</article-title><source>Ann. Transl. Med.</source><year>2020</year><volume>8</volume><fpage>713</fpage><pub-id pub-id-type="doi">10.21037/atm.2020.02.44</pub-id><?supplied-pmid 32617333?><pub-id pub-id-type="pmid">32617333</pub-id>
</element-citation></ref><ref id="CR10"><label>10.</label><mixed-citation publication-type="other">Gorriz, M., Carlier, A., Faure, E. &#x00026; Giro-i-Nieto, X. Cost-effective active learning for melanoma segmentation. Preprint at <ext-link ext-link-type="uri" xlink:href="http://arXiv.org/1711.09168">http://arXiv.org/1711.09168</ext-link> (2017).</mixed-citation></ref><ref id="CR11"><label>11.</label><mixed-citation publication-type="other">Yang, L., Zhang, Y., Chen, J., Zhang, S. &#x00026; Chen, D. Z. <italic>Medical Image Computing and Computer Assisted Intervention&#x02014;MICCAI 2017: 20th International Conference, Quebec City, QC, Canada, September 11&#x02013;13, 2017, Proceedings, Part III 20</italic> 399&#x02013;407 (Springer).</mixed-citation></ref><ref id="CR12"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Siddesh</surname><given-names>S</given-names></name><name><surname>Suresh</surname><given-names>B</given-names></name></person-group><article-title>Automation of generating CAD models</article-title><source>J. Mech. Eng. Autom.</source><year>2015</year><volume>5</volume><fpage>55</fpage><lpage>58</lpage></element-citation></ref><ref id="CR13"><label>13.</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Moreno</surname><given-names>R</given-names></name><name><surname>Baz&#x000e1;n</surname><given-names>A</given-names></name></person-group><source>IOP Conference Series: Materials Science and Engineering</source><year>2017</year><publisher-name>IOP Publishing</publisher-name><fpage>062039</fpage></element-citation></ref><ref id="CR14"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jayesh</surname><given-names>P</given-names></name><name><surname>Khairnar</surname><given-names>H</given-names></name><name><surname>Cam</surname><given-names>MTC</given-names></name></person-group><article-title>Master model automation using NX unigraphics customization</article-title><source>Int. J. Eng. Dev. Res.</source><year>2014</year><volume>2</volume><fpage>2184</fpage><lpage>2189</lpage></element-citation></ref><ref id="CR15"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Caradu</surname><given-names>C</given-names></name><name><surname>Spampinato</surname><given-names>B</given-names></name><name><surname>Vrancianu</surname><given-names>AM</given-names></name><name><surname>B&#x000e9;rard</surname><given-names>X</given-names></name><name><surname>Ducasse</surname><given-names>E</given-names></name></person-group><article-title>Fully automatic volume segmentation of infrarenal abdominal aortic aneurysm computed tomography images with deep learning approaches versus physician controlled manual segmentation</article-title><source>J. Vasc. Surg.</source><year>2021</year><volume>74</volume><fpage>246</fpage><lpage>256</lpage><pub-id pub-id-type="doi">10.1016/j.jvs.2020.11.036</pub-id><?supplied-pmid 33309556?><pub-id pub-id-type="pmid">33309556</pub-id>
</element-citation></ref><ref id="CR16"><label>16.</label><mixed-citation publication-type="other">Hatamizadeh, A. <italic>et al. Proc. IEEE/CVF Winter Conference on Applications of Computer Vision</italic> 574&#x02013;584.</mixed-citation></ref><ref id="CR17"><label>17.</label><mixed-citation publication-type="other">Hatamizadeh, A. <italic>et al. Brainlesion: Glioma, Multiple Sclerosis, Stroke and Traumatic Brain Injuries: 7th International Workshop, BrainLes 2021, Held in Conjunction with MICCAI 2021, Virtual Event, September 27, 2021, Revised Selected Papers, Part I</italic> 272&#x02013;284 (Springer).</mixed-citation></ref><ref id="CR18"><label>18.</label><mixed-citation publication-type="other">Isensee, F. <italic>et al.</italic> nnu-net: Self-adapting framework for u-net-based medical image segmentation. Preprint at <ext-link ext-link-type="uri" xlink:href="http://arXiv.org/1809.10486">http://arXiv.org/1809.10486</ext-link> (2018).</mixed-citation></ref><ref id="CR19"><label>19.</label><mixed-citation publication-type="other">Patro, S. &#x00026; Sahu, K. K. Normalization: A preprocessing stage. Preprint at <ext-link ext-link-type="uri" xlink:href="http://arXiv.org/1503.06462">http://arXiv.org/1503.06462</ext-link> (2015).</mixed-citation></ref><ref id="CR20"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yeghiazaryan</surname><given-names>V</given-names></name><name><surname>Voiculescu</surname><given-names>I</given-names></name></person-group><article-title>Family of boundary overlap metrics for the evaluation of medical image segmentation</article-title><source>J. Med. Imaging</source><year>2018</year><volume>5</volume><fpage>015006</fpage><pub-id pub-id-type="doi">10.1117/1.JMI.5.1.015006</pub-id></element-citation></ref></ref-list></back></article>