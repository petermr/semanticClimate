<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd"> 
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><?properties open_access?><?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?><?DTDIdentifier.IdentifierType public?><?SourceDTD.DTDName A++V2.4.dtd?><?SourceDTD.Version 2.4?><?ConverterInfo.XSLTName springer2nlmx2.xsl?><?ConverterInfo.Version 1?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sci Rep</journal-id><journal-id journal-id-type="iso-abbrev">Sci Rep</journal-id><journal-title-group><journal-title>Scientific Reports</journal-title></journal-title-group><issn pub-type="epub">2045-2322</issn><publisher><publisher-name>Nature Publishing Group UK</publisher-name><publisher-loc>London</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmcid">10923834</article-id><article-id pub-id-type="publisher-id">56211</article-id><article-id pub-id-type="doi">10.1038/s41598-024-56211-1</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>MFCA-Net: a deep learning method for semantic segmentation of remote sensing images</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Li</surname><given-names>Xiujuan</given-names></name><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Li</surname><given-names>Junhuai</given-names></name><address><email>lijunhuai@xaut.edu.cn</email></address><xref ref-type="aff" rid="Aff1">1</xref></contrib><aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/038avdt50</institution-id><institution-id institution-id-type="GRID">grid.440722.7</institution-id><institution-id institution-id-type="ISNI">0000 0000 9591 9677</institution-id><institution>School of Computer Science and Engineering, </institution><institution>Xi&#x02019;an University of Technology, </institution></institution-wrap>Xi&#x02019;an, 710048 China </aff><aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/01b9y2p27</institution-id><institution-id institution-id-type="GRID">grid.464491.a</institution-id><institution-id institution-id-type="ISNI">0000 0004 1755 0877</institution-id><institution>School of Information, </institution><institution>Xi&#x02019;an University of Finance and Economics, </institution></institution-wrap>Xi&#x02019;an, 710100 China </aff></contrib-group><pub-date pub-type="epub"><day>8</day><month>3</month><year>2024</year></pub-date><pub-date pub-type="pmc-release"><day>8</day><month>3</month><year>2024</year></pub-date><pub-date pub-type="collection"><year>2024</year></pub-date><volume>14</volume><elocation-id>5745</elocation-id><history><date date-type="received"><day>13</day><month>10</month><year>2023</year></date><date date-type="accepted"><day>4</day><month>3</month><year>2024</year></date></history><permissions><copyright-statement>&#x000a9; The Author(s) 2024</copyright-statement><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p></license></permissions><abstract id="Abs1"><p id="Par1">Semantic segmentation of remote sensing images (RSI) is an important research direction in remote sensing technology. This paper proposes a multi-feature fusion and channel attention network, MFCA-Net, aiming to improve the segmentation accuracy of remote sensing images and the recognition performance of small target objects. The architecture is built on an encoding&#x02013;decoding structure. The encoding structure includes the improved MobileNet V2 (IMV2) and multi-feature dense fusion (MFDF). In IMV2, the attention mechanism is introduced twice to enhance the feature extraction capability, and the design of MFDF can obtain more dense feature sampling points and larger receptive fields. In the decoding section, three branches of shallow features of the backbone network are fused with deep features, and upsampling is performed to achieve the pixel-level classification. Comparative experimental results of the six most advanced methods effectively prove that the segmentation accuracy of the proposed network has been significantly improved. Furthermore, the recognition degree of small target objects is higher. For example, the proposed MFCA-Net achieves about 3.65&#x02013;23.55% MIoU improvement on the dataset Vaihingen.</p></abstract><kwd-group kwd-group-type="npg-subject"><title>Subject terms</title><kwd>Computer science</kwd><kwd>Information theory and computation</kwd></kwd-group><funding-group><award-group><funding-source><institution>the Scientific Research Support Program of Xi&#x02019;an University of Finance and Economics</institution></funding-source><award-id>22FCZD05</award-id><principal-award-recipient><name><surname>Li</surname><given-names>Xiujuan</given-names></name></principal-award-recipient></award-group></funding-group><funding-group><award-group><funding-source><institution>Shaanxi Water Conservancy Technology Project</institution></funding-source><award-id>2020slkj-17</award-id><principal-award-recipient><name><surname>Li</surname><given-names>Junhuai</given-names></name></principal-award-recipient></award-group></funding-group><custom-meta-group><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#x000a9; Springer Nature Limited 2024</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1"><title>Introduction</title><p id="Par2">Remote sensing technology is widely used in various fields such as urban planning<sup><xref ref-type="bibr" rid="CR1">1</xref>,<xref ref-type="bibr" rid="CR2">2</xref></sup>, land resource utilization<sup><xref ref-type="bibr" rid="CR3">3</xref>&#x02013;<xref ref-type="bibr" rid="CR5">5</xref></sup>, and precision agriculture<sup><xref ref-type="bibr" rid="CR4">4</xref>,<xref ref-type="bibr" rid="CR6">6</xref></sup>. The semantic segmentation technique is an important research direction of RSI. Various semantic segmentation methods have been developed and applied in practical applications. The threshold-based image segmentation method<sup><xref ref-type="bibr" rid="CR7">7</xref>,<xref ref-type="bibr" rid="CR8">8</xref></sup> realizes semantic segmentation by classifying the image gray histogram using different gray thresholds. The edge-based segmentation method was used by Roberts<sup><xref ref-type="bibr" rid="CR9">9</xref></sup>, Sobel<sup><xref ref-type="bibr" rid="CR10">10</xref>,<xref ref-type="bibr" rid="CR11">11</xref></sup>, Prewitt <sup><xref ref-type="bibr" rid="CR12">12</xref>,<xref ref-type="bibr" rid="CR13">13</xref></sup>, and other edge detection operators<sup><xref ref-type="bibr" rid="CR14">14</xref>,<xref ref-type="bibr" rid="CR15">15</xref></sup> in identifying and connecting the boundary pixels to form the contour of the edge. The image region segmentation method classifies the pixels and creates regions based on their similar characteristics, and methods such as region production and split merge are frequently employed<sup><xref ref-type="bibr" rid="CR16">16</xref>&#x02013;<xref ref-type="bibr" rid="CR18">18</xref></sup>. Traditional semantic segmentation approaches mentioned above need to set parameters manually, and the segmentation accuracy is low. In addition, they cannot adapt to image segmentation tasks with a large amount of semantic information.</p><p id="Par3">In recent years, deep learning has achieved profound success in remote sensing image applications<sup><xref ref-type="bibr" rid="CR19">19</xref>&#x02013;<xref ref-type="bibr" rid="CR21">21</xref></sup>, especially in semantic segmentation<sup><xref ref-type="bibr" rid="CR22">22</xref>&#x02013;<xref ref-type="bibr" rid="CR24">24</xref></sup>. Zheng et al.<sup><xref ref-type="bibr" rid="CR25">25</xref></sup> applied the U-Net<sup><xref ref-type="bibr" rid="CR26">26</xref></sup> model widely used in medical image segmentation to RSI and trained on the GF-2 RSI dataset. Xuan et al.<sup><xref ref-type="bibr" rid="CR27">27</xref></sup> suggested a multipath encoder structure for extracting the features to improve target object boundary classification accuracy in RSI. Zheng et al.<sup><xref ref-type="bibr" rid="CR28">28</xref></sup> developed a semantic segmentation model using spatial context acquisition of the Markov random field model to enhance the segmentation accuracy of different land categories. Sun et al.<sup><xref ref-type="bibr" rid="CR29">29</xref></sup> proposed an improved U-Net network that groups channels in a multitasking manner and processes heterogeneous image segmentation through information fusion. Chen et al.<sup><xref ref-type="bibr" rid="CR30">30</xref></sup> presented an improved network framework for RSI semantic segmentation based on the spatial channel fusion compression and excitation module. Fan et al.<sup><xref ref-type="bibr" rid="CR31">31</xref></sup> improved DeepLab<sup><xref ref-type="bibr" rid="CR32">32</xref></sup> for extracting cultivated land information, introducing a parameter to adjust the dilated convolution kernel and adding a more precise decoder group to the model structure. Wang et al.<sup><xref ref-type="bibr" rid="CR33">33</xref></sup> used ResNet-34<sup><xref ref-type="bibr" rid="CR34">34</xref></sup> as the backbone and built a double-branch encoder to extract lakes and water bodies from the Qinghai Tibet Plateau.</p><p id="Par4">Transformer is a deep learning model based on the self-attention mechanism. Since the transformer captures long-distance dependencies between local and global features by comparing their correlations at all spatial positions, it has more robust modeling capabilities. Therefore, more and more researchers are applying it to computer vision tasks. Zhang et al.<sup><xref ref-type="bibr" rid="CR35">35</xref></sup> propose a semantic segmentation model using a transformer as the backbone network to obtain better remote spatial dependencies. Wang et al.<sup><xref ref-type="bibr" rid="CR36">36</xref></sup> combine Swin Transformer with Densely Connected Feature Aggregation Module to propose a new semantic segmentation model for remote sensing images.</p><p id="Par5">Generative Adversarial Networks (GANs)<sup><xref ref-type="bibr" rid="CR37">37</xref></sup> belong to generative models. Luc et al.<sup><xref ref-type="bibr" rid="CR38">38</xref></sup> first introduced GANs into image semantic segmentation. Due to the high time and money costs of large-scale annotated datasets, many researchers have shifted their research direction to GAN-based semantic segmentation. Li et al.<sup><xref ref-type="bibr" rid="CR39">39</xref></sup> propose a distribution-aligned semantic segmentation network based on GAN. Ma et al.<sup><xref ref-type="bibr" rid="CR40">40</xref></sup> suggest a novel GAN network, which integrates additional discriminators to learn domain-specific features and captures cross-domain dependencies of semantic feature representations through mutually enhancing attention transformers. Algorithms based on GANs can generate samples and determine their authenticity, but their performance could be better for large-scale training.</p><p id="Par6">In summary, the feature learning ability of neural networks mentioned above has shown substantial advantages in the semantic segmentation of RSI. However, RSI is prone to the problem of unbalanced sample classification, or there may be significant differences in classification sizes. These characteristics result in insufficient network, classification errors, and missed detection of small target objects, decreasing overall segmentation accuracy. This paper presents a new deep neural network for remote sensing image segmentation in response to the above issues. The main contributions of this study can be summarized as follows:<list list-type="bullet"><list-item><p id="Par7">A new neural network, MFCA-Net, is proposed for the semantic segmentation of RSI. Moreover, the results of the proposed MFCA-Net are superior to those of other approaches under limited training sample scenarios.</p></list-item><list-item><p id="Par8">In IMV2, attention mechanisms are introduced in the shallow and deep feature maps respectively to improve the segmentation accuracy of the network.</p></list-item><list-item><p id="Par9">The MFDF module obtained a more extensive range of contextual information and denser feature sampling points, effectively solving the problems of unbalanced sample classification and low segmentation accuracy of small target objects.</p></list-item></list></p></sec><sec id="Sec2"><title>Methods</title><p id="Par10">The overall framework of MFCA-Net adopts an encoding&#x02013;decoding structure, as shown in Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>. We introduce MobileNet V2<sup><xref ref-type="bibr" rid="CR41">41</xref></sup> as the backbone and improve it. The attention mechanism is used in the shallow and deep feature layers. We add the MFDF module, which not only obtains a larger receptive field but also attempts to solve the problem of identifying small sample targets through denser sampling points. In decoding, three branches are introduced from the feature extraction module, fused, and then upsampled to achieve pixel-level classification of RSI.<fig id="Fig1"><label>Figure 1</label><caption><p>The overall architecture of the proposed MFCA-Net network (this figure was drawn by Visio 2021, which can be available at <ext-link ext-link-type="uri" xlink:href="https://www.microsoftstore.com.cn/software/office/visio-standard-2021">https://www.microsoftstore.com.cn/software/office/visio-standard-2021</ext-link>).</p></caption><graphic xlink:href="41598_2024_56211_Fig1_HTML" id="MO1"/></fig></p><sec id="Sec3"><title>Encoder</title><sec id="Sec4"><title>IMV2</title><p id="Par11">The feature extraction module uses the lightweight MobileNet V2 to ensure the learning performance and efficiency of the network. Based on depthwise and pointwise convolution, the parameter quantity of MobileNet V2 is only 1/9 to 1/8 of the standard convolution. Nevertheless, all channels in the feature map are assigned the same weight in MobileNet V2. We improve it and introduce channel attention mechanisms (CA) after the shallow feature map Bottleneck1 and deep feature map Bottleneck6, respectively. The operation process of CA includes compression, activation, and scale operations.</p><sec id="Sec5"><title>Compression operation</title><p id="Par12">Firstly, the feature map is global pooled. Then, the feature vector is compressed into a one-dimensional vector through the convolution and batch normalization (BN) layers. Each dimension of the one-dimensional vector represents the weight of each channel. The operation can be expressed as follows:<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\text{z}}={{\text{F}}}_{{\text{sq}}}\left({\text{f}}\right)=\frac{1}{{\text{H}}\times {\text{W}}}\sum_{{\text{i}}=1}^{{\text{H}}}\sum_{{\text{j}}=1}^{{\text{W}}}{\text{f}}\left({\text{i}},{\text{j}}\right),$$\end{document}</tex-math><mml:math id="M2" display="block"><mml:mrow><mml:mtext>z</mml:mtext><mml:mo>=</mml:mo><mml:msub><mml:mtext>F</mml:mtext><mml:mtext>sq</mml:mtext></mml:msub><mml:mfenced close=")" open="("><mml:mtext>f</mml:mtext></mml:mfenced><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mtext>H</mml:mtext><mml:mo>&#x000d7;</mml:mo><mml:mtext>W</mml:mtext></mml:mrow></mml:mfrac><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mtext>i</mml:mtext><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mtext>H</mml:mtext></mml:munderover><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mtext>j</mml:mtext><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mtext>W</mml:mtext></mml:munderover><mml:mtext>f</mml:mtext><mml:mfenced close=")" open="("><mml:mtext>i</mml:mtext><mml:mo>,</mml:mo><mml:mtext>j</mml:mtext></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:math><graphic xlink:href="41598_2024_56211_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq1"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{\text{F}}}_{{\text{sq}}}$$\end{document}</tex-math><mml:math id="M4"><mml:msub><mml:mtext>F</mml:mtext><mml:mtext>sq</mml:mtext></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_56211_Article_IEq1.gif"/></alternatives></inline-formula> is the compression operation function, <inline-formula id="IEq2"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\text{f}}\in {{\text{R}}}^{{\text{H}}\times {\text{W}}}$$\end{document}</tex-math><mml:math id="M6"><mml:mrow><mml:mtext>f</mml:mtext><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mrow><mml:mtext>R</mml:mtext></mml:mrow><mml:mrow><mml:mtext>H</mml:mtext><mml:mo>&#x000d7;</mml:mo><mml:mtext>W</mml:mtext></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_56211_Article_IEq2.gif"/></alternatives></inline-formula> is a set of two-dimensional feature maps; <inline-formula id="IEq3"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\text{f}}\left({\text{i}},{\text{j}}\right)$$\end{document}</tex-math><mml:math id="M8"><mml:mrow><mml:mtext>f</mml:mtext><mml:mfenced close=")" open="("><mml:mtext>i</mml:mtext><mml:mo>,</mml:mo><mml:mtext>j</mml:mtext></mml:mfenced></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_56211_Article_IEq3.gif"/></alternatives></inline-formula> is one of the elements, H and W are the height and width of the feature map, respectively; z is the output of compression operation.</p></sec><sec id="Sec6"><title>Activation operation</title><p id="Par13">The feature map vector&#x02019;s channel dimension is reduced to the original 1/r through the first full connection layer (FC1), resulting in a 1&#x02009;&#x000d7;&#x02009;1&#x02009;&#x000d7;&#x02009;C/r feature map shape, and r expresses the dimensionality reduction ratio. After that, Funnel activation (FReLU)<sup><xref ref-type="bibr" rid="CR42">42</xref></sup> performs the nonlinear processing. The activation function in the MobileNet series, whether Relu or Relu6, models the one-dimensional linear space of the pixel itself, so it is easy to lose the characteristics of the pixels around the center point and reduce the model learning ability. FReLU uses funnel conditions to obtain the maximum value between the center point and the states. The formula is as follows:<disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$FReLU={\text{max}}\left({{\text{x}}}_{{\text{c}},{\text{i}},{\text{j}}},{\text{T}}\left({{\text{x}}}_{{\text{c}},{\text{i}},{\text{j}}}\right)\right),$$\end{document}</tex-math><mml:math id="M10" display="block"><mml:mrow><mml:mi>F</mml:mi><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>L</mml:mi><mml:mi>U</mml:mi><mml:mo>=</mml:mo><mml:mtext>max</mml:mtext><mml:mfenced close=")" open="("><mml:msub><mml:mtext>x</mml:mtext><mml:mrow><mml:mtext>c</mml:mtext><mml:mo>,</mml:mo><mml:mtext>i</mml:mtext><mml:mo>,</mml:mo><mml:mtext>j</mml:mtext></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mtext>T</mml:mtext><mml:mfenced close=")" open="("><mml:msub><mml:mtext>x</mml:mtext><mml:mrow><mml:mtext>c</mml:mtext><mml:mo>,</mml:mo><mml:mtext>i</mml:mtext><mml:mo>,</mml:mo><mml:mtext>j</mml:mtext></mml:mrow></mml:msub></mml:mfenced></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:math><graphic xlink:href="41598_2024_56211_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq4"><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{\text{x}}}_{{\text{c}},{\text{i}},{\text{j}}}$$\end{document}</tex-math><mml:math id="M12"><mml:msub><mml:mtext>x</mml:mtext><mml:mrow><mml:mtext>c</mml:mtext><mml:mo>,</mml:mo><mml:mtext>i</mml:mtext><mml:mo>,</mml:mo><mml:mtext>j</mml:mtext></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_56211_Article_IEq4.gif"/></alternatives></inline-formula> is the pooling window centered with position <inline-formula id="IEq5"><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$({\text{i}},{\text{j}})$$\end{document}</tex-math><mml:math id="M14"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mtext>i</mml:mtext><mml:mo>,</mml:mo><mml:mtext>j</mml:mtext><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_56211_Article_IEq5.gif"/></alternatives></inline-formula> on channel C. <inline-formula id="IEq6"><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\text{T}}\left({{\text{x}}}_{{\text{c}},{\text{i}},{\text{j}}}\right)={{\text{x}}}_{{\text{c}},{\text{i}},{\text{j}}}^{{\text{w}}}\cdot {{\text{p}}}_{{\text{c}}}^{{\text{w}}}$$\end{document}</tex-math><mml:math id="M16"><mml:mrow><mml:mtext>T</mml:mtext><mml:mfenced close=")" open="("><mml:msub><mml:mtext>x</mml:mtext><mml:mrow><mml:mtext>c</mml:mtext><mml:mo>,</mml:mo><mml:mtext>i</mml:mtext><mml:mo>,</mml:mo><mml:mtext>j</mml:mtext></mml:mrow></mml:msub></mml:mfenced><mml:mo>=</mml:mo><mml:msubsup><mml:mtext>x</mml:mtext><mml:mrow><mml:mtext>c</mml:mtext><mml:mo>,</mml:mo><mml:mtext>i</mml:mtext><mml:mo>,</mml:mo><mml:mtext>j</mml:mtext></mml:mrow><mml:mtext>w</mml:mtext></mml:msubsup><mml:mo>&#x000b7;</mml:mo><mml:msubsup><mml:mtext>p</mml:mtext><mml:mrow><mml:mtext>c</mml:mtext></mml:mrow><mml:mtext>w</mml:mtext></mml:msubsup></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_56211_Article_IEq6.gif"/></alternatives></inline-formula>, <inline-formula id="IEq7"><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{\text{p}}}_{{\text{c}}}^{{\text{w}}}$$\end{document}</tex-math><mml:math id="M18"><mml:msubsup><mml:mtext>p</mml:mtext><mml:mrow><mml:mtext>c</mml:mtext></mml:mrow><mml:mtext>w</mml:mtext></mml:msubsup></mml:math><inline-graphic xlink:href="41598_2024_56211_Article_IEq7.gif"/></alternatives></inline-formula> is the parameters shared by this window in the same channel. Therefore, a funnel-shaped two-dimensional feature extractor can obtain more abundant image context feature information, which helps improve the segmentation accuracy. The feature map of the feature map vector is raised back to the channels&#x02019; original number through the second full connection layer (FC2). Additionally, it is transformed into a normalized weight vector, with values varying between 0 and 1, using a sigmoid function.</p></sec><sec id="Sec7"><title>Scale operation</title><p id="Par14">The normalized weight and the original input characteristic map channel are multiplied to generate the weighted distinct map. The formula is<disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\text{x}}={{\text{F}}}_{{\text{scale}}}\left({\text{f}},{\text{s}}\right)={\text{s}}\cdot {\text{f}}\left({\text{i}},{\text{j}}\right),$$\end{document}</tex-math><mml:math id="M20" display="block"><mml:mrow><mml:mtext>x</mml:mtext><mml:mo>=</mml:mo><mml:msub><mml:mtext>F</mml:mtext><mml:mtext>scale</mml:mtext></mml:msub><mml:mfenced close=")" open="("><mml:mtext>f</mml:mtext><mml:mo>,</mml:mo><mml:mtext>s</mml:mtext></mml:mfenced><mml:mo>=</mml:mo><mml:mtext>s</mml:mtext><mml:mo>&#x000b7;</mml:mo><mml:mtext>f</mml:mtext><mml:mfenced close=")" open="("><mml:mtext>i</mml:mtext><mml:mo>,</mml:mo><mml:mtext>j</mml:mtext></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:math><graphic xlink:href="41598_2024_56211_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq8"><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{\text{F}}}_{{\text{scale}}}$$\end{document}</tex-math><mml:math id="M22"><mml:msub><mml:mtext>F</mml:mtext><mml:mtext>scale</mml:mtext></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_56211_Article_IEq8.gif"/></alternatives></inline-formula> is the scale operation; <inline-formula id="IEq9"><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\text{x}}$$\end{document}</tex-math><mml:math id="M24"><mml:mtext>x</mml:mtext></mml:math><inline-graphic xlink:href="41598_2024_56211_Article_IEq9.gif"/></alternatives></inline-formula> is a value in the last output X of the attention module; <inline-formula id="IEq10"><alternatives><tex-math id="M25">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\text{X}}=\left[{{\text{x}}}_{1},{{\text{x}}}_{2},\ldots ,{{\text{x}}}_{{\text{c}}}\right]$$\end{document}</tex-math><mml:math id="M26"><mml:mrow><mml:mtext>X</mml:mtext><mml:mo>=</mml:mo><mml:mfenced close="]" open="["><mml:msub><mml:mtext>x</mml:mtext><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mtext>x</mml:mtext><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mtext>x</mml:mtext><mml:mtext>c</mml:mtext></mml:msub></mml:mfenced></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_56211_Article_IEq10.gif"/></alternatives></inline-formula>.The entire process is a parameter learnable process. The contribution weights of different channels are obtained through backpropagation training. The structure diagram of IMV2 is given in Table <xref rid="Tab1" ref-type="table">1</xref>.<table-wrap id="Tab1"><label>Table 1</label><caption><p>The structure of IMV2.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Input</th><th align="left">Operator</th><th align="left">t</th><th align="left">c</th><th align="left">n</th><th align="left">s</th></tr></thead><tbody><tr><td align="left">512<sup>2</sup>&#x02009;&#x000d7;&#x02009;3</td><td align="left">conv2d</td><td align="left">&#x02013;</td><td align="left">32</td><td align="left">1</td><td align="left">2</td></tr><tr><td align="left">256<sup>2</sup>&#x02009;&#x000d7;&#x02009;32</td><td align="left">Bottleneck1&#x02009;+&#x02009;CA</td><td align="left">1</td><td align="left">16</td><td align="left">1</td><td align="left">1</td></tr><tr><td align="left">256<sup>2</sup>&#x02009;&#x000d7;&#x02009;16</td><td align="left">Bottleneck2</td><td align="left">6</td><td align="left">24</td><td align="left">2</td><td align="left">2</td></tr><tr><td align="left">128<sup>2</sup>&#x02009;&#x000d7;&#x02009;24</td><td align="left">Bottleneck3</td><td align="left">6</td><td align="left">32</td><td align="left">3</td><td align="left">2</td></tr><tr><td align="left">64<sup>2</sup>&#x02009;&#x000d7;&#x02009;32</td><td align="left">Bottleneck4</td><td align="left">6</td><td align="left">64</td><td align="left">4</td><td align="left">2</td></tr><tr><td align="left">32<sup>2</sup>&#x02009;&#x000d7;&#x02009;64</td><td align="left">Bottleneck5</td><td align="left">6</td><td align="left">96</td><td align="left">3</td><td align="left">1</td></tr><tr><td align="left">32<sup>2</sup>&#x02009;&#x000d7;&#x02009;96</td><td align="left">Bottleneck6&#x02009;+&#x02009;CA</td><td align="left">6</td><td align="left">160</td><td align="left">3</td><td align="left">2</td></tr><tr><td align="left">16<sup>2</sup>&#x02009;&#x000d7;&#x02009;160</td><td align="left">Bottleneck7</td><td align="left">6</td><td align="left">320</td><td align="left">1</td><td align="left">1</td></tr><tr><td align="left">16<sup>2</sup>&#x02009;&#x000d7;&#x02009;320</td><td align="left">conv2d</td><td align="left">&#x02013;</td><td align="left">1280</td><td align="left">1</td><td align="left">1</td></tr><tr><td align="left">16<sup>2</sup>&#x02009;&#x000d7;&#x02009;1280</td><td align="left">avgpool</td><td align="left">&#x02013;</td><td align="left">&#x02013;</td><td align="left">1</td><td align="left">&#x02013;</td></tr><tr><td align="left">1&#x02009;&#x000d7;&#x02009;1&#x02009;&#x000d7;&#x02009;1280</td><td align="left">conv2d</td><td align="left">&#x02013;</td><td align="left">K</td><td align="left">&#x02013;</td><td align="left"/></tr></tbody></table></table-wrap></p><p id="Par15">In Table <xref rid="Tab1" ref-type="table">1</xref>, <inline-formula id="IEq11"><alternatives><tex-math id="M27">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$t$$\end{document}</tex-math><mml:math id="M28"><mml:mi>t</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_56211_Article_IEq11.gif"/></alternatives></inline-formula> is the expansion factor; <inline-formula id="IEq12"><alternatives><tex-math id="M29">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$c$$\end{document}</tex-math><mml:math id="M30"><mml:mi>c</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_56211_Article_IEq12.gif"/></alternatives></inline-formula> is the depth of the output characteristic matrix; <inline-formula id="IEq13"><alternatives><tex-math id="M31">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$n$$\end{document}</tex-math><mml:math id="M32"><mml:mi>n</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_56211_Article_IEq13.gif"/></alternatives></inline-formula> is the number of iterations of bottleneck; <inline-formula id="IEq14"><alternatives><tex-math id="M33">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\text{s}}$$\end{document}</tex-math><mml:math id="M34"><mml:mtext>s</mml:mtext></mml:math><inline-graphic xlink:href="41598_2024_56211_Article_IEq14.gif"/></alternatives></inline-formula> is the step length.</p></sec></sec><sec id="Sec8"><title>MFDF</title><p id="Par16">The atrous spatial pyramid pooling (ASPP) proposed by DeepLab V2<sup><xref ref-type="bibr" rid="CR43">43</xref></sup> contacts feature maps with different dilation rates. Although this method can get a larger receptive field, it is only effective for some large objects, and fewer sampling points can be captured for fewer categories and small target objects. The design of MFDF aims to address the above issues. The study fuses the convolution feature maps of 3, 6, 12, 18, and 24 with various dilation rates. Adaptive average pooling can integrate a broad range of spatial information and prevent overfitting, so adaptivepool2d is added to this module. These six branches are densely connected backward, and the overall MFDF structure is depicted in Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref>.<fig id="Fig2"><label>Figure 2</label><caption><p>The structure of MFDF, and c represents concatenation operation (this figure was drawn by Visio 2021, which can be available at <ext-link ext-link-type="uri" xlink:href="https://www.microsoftstore.com.cn/software/office/visio-standard-2021">https://www.microsoftstore.com.cn/software/office/visio-standard-2021</ext-link>).</p></caption><graphic xlink:href="41598_2024_56211_Fig2_HTML" id="MO2"/></fig></p><p id="Par17">Each dilation layer can be represented as follows:<disp-formula id="Equ4"><label>4</label><alternatives><tex-math id="M35">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{\text{x}}}_{{\text{l}}}={{\text{H}}}_{{\text{K}},{{\text{d}}}_{{\text{l}}}}\left(\left[{{\text{x}}}_{{\text{l}}-1},{{\text{x}}}_{{\text{l}}-2},\dots ,{{\text{x}}}_{0}\right]\right),$$\end{document}</tex-math><mml:math id="M36" display="block"><mml:mrow><mml:msub><mml:mtext>x</mml:mtext><mml:mtext>l</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mtext>H</mml:mtext><mml:mrow><mml:mtext>K</mml:mtext><mml:mo>,</mml:mo><mml:msub><mml:mtext>d</mml:mtext><mml:mtext>l</mml:mtext></mml:msub></mml:mrow></mml:msub><mml:mfenced close=")" open="("><mml:mfenced close="]" open="["><mml:msub><mml:mtext>x</mml:mtext><mml:mrow><mml:mtext>l</mml:mtext><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mtext>x</mml:mtext><mml:mrow><mml:mtext>l</mml:mtext><mml:mo>-</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x022ef;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mtext>x</mml:mtext><mml:mn>0</mml:mn></mml:msub></mml:mfenced></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:math><graphic xlink:href="41598_2024_56211_Article_Equ4.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq15"><alternatives><tex-math id="M37">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{\text{d}}}_{{\text{l}}}$$\end{document}</tex-math><mml:math id="M38"><mml:msub><mml:mtext>d</mml:mtext><mml:mtext>l</mml:mtext></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_56211_Article_IEq15.gif"/></alternatives></inline-formula> is the dilation rate of one layer; [&#x02026;] is the splicing operation of the feature layer; <inline-formula id="IEq16"><alternatives><tex-math id="M39">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\left[{{\text{x}}}_{{\text{l}}-1},{{\text{x}}}_{{\text{l}}-2},\dots ,{{\text{x}}}_{0}\right]$$\end{document}</tex-math><mml:math id="M40"><mml:mfenced close="]" open="["><mml:msub><mml:mtext>x</mml:mtext><mml:mrow><mml:mtext>l</mml:mtext><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mtext>x</mml:mtext><mml:mrow><mml:mtext>l</mml:mtext><mml:mo>-</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x022ef;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mtext>x</mml:mtext><mml:mn>0</mml:mn></mml:msub></mml:mfenced></mml:math><inline-graphic xlink:href="41598_2024_56211_Article_IEq16.gif"/></alternatives></inline-formula> is the output of all layers before splicing. For the expanded convolution layer with dilation rate d and convolution kernel size K, the receptive field size is computed as follows:<disp-formula id="Equ5"><label>5</label><alternatives><tex-math id="M41">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$R=\left(d-1\right)\times (K-1)+K$$\end{document}</tex-math><mml:math id="M42" display="block"><mml:mrow><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mfenced close=")" open="("><mml:mi>d</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mfenced><mml:mo>&#x000d7;</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>K</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:math><graphic xlink:href="41598_2024_56211_Article_Equ5.gif" position="anchor"/></alternatives></disp-formula></p><p id="Par18">Stacking the two convolution layers together can provide a large receptive field. If two convolution layers with convolution kernels K<sub>1</sub> and K<sub>2</sub> are superimposed, the new receptive field is<disp-formula id="Equ6"><label>6</label><alternatives><tex-math id="M43">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$K={K}_{1}+{K}_{2}-1.$$\end{document}</tex-math><mml:math id="M44" display="block"><mml:mrow><mml:mi>K</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>K</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>K</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo>.</mml:mo></mml:mrow></mml:math><graphic xlink:href="41598_2024_56211_Article_Equ6.gif" position="anchor"/></alternatives></disp-formula></p><p id="Par19">The above formula indicates that the receptive field of the densely connected characteristic map is 128. In contrast, the receptive field of the ASPP with the same void rate is only 51, i.e., the receptive field of MFDF is more than twice as large as that of the ASPP.</p></sec></sec><sec id="Sec9"><title>Decoder</title><p id="Par20">Relevant studies<sup><xref ref-type="bibr" rid="CR44">44</xref>,<xref ref-type="bibr" rid="CR45">45</xref></sup> have indicated that increasing the fusion of shallow feature maps containing details can improve segmentation accuracy. The present research enhances the application of low-level feature maps. After 3&#x02009;&#x000d7;&#x02009;3 and 1&#x02009;&#x000d7;&#x02009;1 convolution to adjust the channels of feature maps, bottleneck1, and bottleneck2 perform fusion operations, then achieve downsampling using convolution with stride 2. After fusing with bottleneck3, the feature maps combine with the deep feature map. Bilinear interpolation of four times is performed for upsampling to produce the segmentation image.</p></sec><sec id="Sec10"><title>Loss function</title><p id="Par21">The loss function often used in semantic segmentation is cross-entropy loss, which assigns equal weight to all categories. The present study adds weight factors to the loss function to improve the importance of a few classes in the loss function and balance the distribution of the loss function. It uses the focal loss function<sup><xref ref-type="bibr" rid="CR46">46</xref></sup>. The formula is as follows:<disp-formula id="Equ7"><label>7</label><alternatives><tex-math id="M45">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${L}_{FL}\left({p}_{t}\right)=-{\alpha }_{t}{\left(1-{p}_{t}\right)}^{\gamma }log{p}_{t}^{\prime},$$\end{document}</tex-math><mml:math id="M46" display="block"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi mathvariant="italic">FL</mml:mi></mml:mrow></mml:msub><mml:mfenced close=")" open="("><mml:msub><mml:mi>p</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mfenced><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:msub><mml:mi>&#x003b1;</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:msup><mml:mrow><mml:mfenced close=")" open="("><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mfenced></mml:mrow><mml:mi>&#x003b3;</mml:mi></mml:msup><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:msubsup><mml:mi>p</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mo>&#x02032;</mml:mo></mml:msubsup><mml:mo>,</mml:mo></mml:mrow></mml:math><graphic xlink:href="41598_2024_56211_Article_Equ7.gif" position="anchor"/></alternatives></disp-formula>where &#x003b1; is a category balance parameter used to adjust the category balance degree; &#x003b3; is the focusing parameter used to focus complex samples; Pt is the probability value of the prediction category. Experiments revealed that weight adjustment slightly improves the result.</p></sec></sec><sec id="Sec11"><title>Experiments</title><p id="Par22">We design two experiments to verify the performance of the proposed MFCA-Net: (i) an experimental investigation of the superiority of the proposed approach over six state-of-the-art methods, namely, SegNet<sup><xref ref-type="bibr" rid="CR47">47</xref></sup>, U-Net<sup><xref ref-type="bibr" rid="CR26">26</xref></sup>, PSPNet<sup><xref ref-type="bibr" rid="CR48">48</xref></sup>, DANet<sup><xref ref-type="bibr" rid="CR49">49</xref></sup>, DeepLab V3+<sup><xref ref-type="bibr" rid="CR50">50</xref></sup>, and A2-FPN<sup><xref ref-type="bibr" rid="CR51">51</xref></sup>. SegNet proposed an unpooling structure that applied the max pooling index, improving the recognition of segmentation boundaries. U-Net is an entirely symmetric semantic segmentation model. The first half of its structure is feature extraction, and the second half is upsampling. PSPNet introduces a pyramid pooling module to capture contextual information at different scales, thereby improving semantic segmentation performance. The DANet model introduces both position and channel attention, downsampling using ResNet as the backbone network, reducing it from 32 to 8 times while retaining more detailed information to improve segmentation performance. DeepLab V3+&#x02009;uses atrous spatial pyramid pooling to concatenate feature maps obtained through convolution operations with different void ratios, achieving multi-scale feature extraction. The A2-FPN model performs semantic segmentation of fine-resolution remote sensing images by adding an attention aggregation module to the feature pyramid network. (ii) An ablation experiment given promoting the widespread use of the proposed MFCA-Net.</p><p id="Par23">The present study uses pixel accuracy (PA), mean PA (MPA), mean intersection over union (MIoU), and frequency-weighted intersection over union (FWIoU) to determine segmentation accuracy. The operating system of this experiment is Windows 10, the graphics card is NVIDIA Geforce RTX3060, the Cuda version for parallel computing architecture is 11.0, and the deep learning framework is Pytorch 1.7.</p><sec id="Sec12"><title>Dataset description</title><p id="Par24">Two datasets: Vaihingen (<ext-link ext-link-type="uri" xlink:href="https://www.isprs.org/education/benchmarks/UrbanSemLab/2d-sem-label-vaihingen.aspx">https://www.isprs.org/education/benchmarks/UrbanSemLab/2d-sem-label-vaihingen.aspx</ext-link>) and Gaofen Image Dataset (GID) (<ext-link ext-link-type="uri" xlink:href="https://www.cvmart.net/dataSets/detail/765?channel_id=op10&#x00026;utm_source=cvmartmp&#x00026;utm_campaign=datasets&#x00026;utm_medium=article">https://www.cvmart.net/dataSets/detail/765?channel_id=op10&#x00026;utm_source=cvmartmp&#x00026;utm_campaign=datasets&#x00026;utm_medium=article</ext-link>) are used to assess the effect of MFCA-Net. The Vaihingen dataset is collected by airborne imaging equipment of aerial vehicles, and the image collection location is the small village of Vaihingen in Germany. The data imaging consists of three bands: near-infrared, red, and green. The average resolution is 2494&#x02009;&#x000d7;&#x02009;2064, and the dataset is trimmed to a fixed size of 512&#x02009;&#x000d7;&#x02009;512 using 75% interblock coverage. 3300 images are obtained using horizontal and vertical flip and rotation operations to enhance the image. The dataset includes six classifications: impermeable surfaces, buildings, low vegetation, trees, cars, and backgrounds. The proportions of these six categories are 27.8%, 26%, 22.9%, 21.3%, 1.2%, and 0.8%, respectively, showing the category imbalance problem.</p><p id="Par25">GID is a large-scale, high-resolution, remote-sensing, land-cover image dataset based on China's Gaofen-2 satellite data. These images were taken from over 60 cities in China, and each image is clear and of high quality, without any cloud or fog obstruction. The GID dataset has a vibrant spectrum, texture, and structure diversity, which is very close to the natural distribution characteristics of land features. GID includes 10 images with a spatial resolution of 4&#x000a0;m and an image size of 6908&#x02009;&#x000d7;&#x02009;7300 pixels. High interclass similarity and low intraclass discrimination are characteristics of GID images. Similarly, 31,500 images with the size of 512&#x02009;&#x000d7;&#x02009;512 are obtained after data enhancement methods. In light of the large dataset, the present study randomly selects 5000 images to create a small dataset. The dataset is classified into six categories: background, buildings, cultivated land, woodland, grassland, and water. The problem of sample unbalance is also apparent. The proportion of grassland is tiny, only 1.6%. Except for the background, the proportion of cultivated land is the highest, close to 30%.</p></sec></sec><sec id="Sec13"><title>Quantitative comparison and visual performance</title><sec id="Sec14"><title>Experiments on Vaihingen</title><p id="Par26">Table <xref rid="Tab2" ref-type="table">2</xref> lists the Vaihingen test set and highlights the best performance in bold. The experimental results show that the segmentation accuracy of DANet, DeepLab V3+, and A2-FPN models is similar. The A2-FPN model proposed in 2022 has higher segmentation accuracy. MFCA-Net is the highest in all other metrics except for being less than 1% lower than DeepLab V3+&#x02009;in MPA metrics. Compared to A2-FPN, MIoU and FWIoU indicators are 3.18% and 2.86% higher, respectively.<table-wrap id="Tab2"><label>Table 2</label><caption><p>Results on Vaihingen.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Method</th><th align="left">PA</th><th align="left">MPA</th><th align="left">MIoU</th><th align="left">FWIoU</th></tr></thead><tbody><tr><td align="left">SegNet<sup><xref ref-type="bibr" rid="CR47">47</xref></sup></td><td char="." align="char">78.79</td><td char="." align="char">63.91</td><td char="." align="char">53.22</td><td char="." align="char">65.03</td></tr><tr><td align="left">U-Net<sup><xref ref-type="bibr" rid="CR26">26</xref></sup></td><td char="." align="char">83.23</td><td char="." align="char">66.42</td><td char="." align="char">56.49</td><td char="." align="char">71.55</td></tr><tr><td align="left">PSPNet<sup><xref ref-type="bibr" rid="CR48">48</xref></sup></td><td char="." align="char">86.15</td><td char="." align="char">77.53</td><td char="." align="char">67.73</td><td char="." align="char">75.87</td></tr><tr><td align="left">DANet<sup><xref ref-type="bibr" rid="CR49">49</xref></sup></td><td char="." align="char">89.16</td><td char="." align="char">81.14</td><td char="." align="char">72.19</td><td char="." align="char">80.65</td></tr><tr><td align="left">DeepLab V3+<sup><xref ref-type="bibr" rid="CR50">50</xref></sup></td><td char="." align="char">89.08</td><td char="." align="char"><bold>85.36</bold></td><td char="." align="char">73.12</td><td char="." align="char">80.57</td></tr><tr><td align="left">A2-FPN<sup><xref ref-type="bibr" rid="CR51">51</xref></sup></td><td char="." align="char">89.12</td><td char="." align="char">82.61</td><td char="." align="char">73.59</td><td char="." align="char">80.72</td></tr><tr><td align="left">MFCA-Net (ours)</td><td char="." align="char"><bold>90.94</bold></td><td char="." align="char">84.77</td><td char="." align="char"><bold>76.77</bold></td><td char="." align="char"><bold>83.58</bold></td></tr></tbody></table></table-wrap></p><p id="Par27">The visual inspection is presented in Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3</xref>. We randomly select three samples and predict the pixel-wise label. Among all the methods compared, the MFCA-Net method has the greatest impact on vehicle recognition. For easily confused low vegetation and trees, the proposed MFCA-Net has a more accurate boundary delineation.<fig id="Fig3"><label>Figure 3</label><caption><p>Visualization of the results of the Vaihingen testing set: (<bold>a</bold>) image (<bold>b</bold>) ground truth, (<bold>c</bold>) SegNet<sup><xref ref-type="bibr" rid="CR47">47</xref></sup>, (<bold>d</bold>) U-Net<sup><xref ref-type="bibr" rid="CR26">26</xref></sup>, (<bold>e</bold>) PSPNet<sup><xref ref-type="bibr" rid="CR48">48</xref></sup>, (<bold>f</bold>) DANet<sup><xref ref-type="bibr" rid="CR49">49</xref></sup>, (<bold>g</bold>) DeepLab V3+<sup><xref ref-type="bibr" rid="CR50">50</xref></sup>, (<bold>h</bold>) A2-FPN<sup><xref ref-type="bibr" rid="CR51">51</xref></sup>, and (i) Our proposed approach. (This figure was drawn by Visio 2021, which can be available at <ext-link ext-link-type="uri" xlink:href="https://www.microsoftstore.com.cn/software/office/visio-standard-2021">https://www.microsoftstore.com.cn/software/office/visio-standard-2021</ext-link>, The visualization was achieved in Visdom under the PyTorch framework. Vaihingen can be available at <ext-link ext-link-type="uri" xlink:href="https://www.isprs.org/education/benchmarks/UrbanSemLab/2d-sem-label-vaihingen.aspx">https://www.isprs.org/education/benchmarks/UrbanSemLab/2d-sem-label-vaihingen.aspx</ext-link>).</p></caption><graphic xlink:href="41598_2024_56211_Fig3_HTML" id="MO3"/></fig></p></sec><sec id="Sec15"><title>Experiments on GID</title><p id="Par28">Table <xref rid="Tab3" ref-type="table">3</xref> shows the experimental results of various methods in GID. The results show that the segmentation accuracy of SegNet and U-Net is relatively low; The segmentation accuracy of PSPNet and DANet is close. DeepLab V3+&#x02009;has the highest accuracy among these six models, while A2-FPN segmentation accuracy is only higher than SegNet and U-Net. Analyzing the reasons, the variance between woodland and grassland classes is slight, and the proportion of woodland, grassland, and Buildings is also tiny, resulting in low segmentation accuracy for all three categories. After the proportion weighting calculation, the overall accuracy index was lowered. For datasets with slight inter-class variance, the segmentation accuracy of A2-FPN is low. The MFCA Net proposed in the paper outperforms the best DeepLab V3+&#x02009;in all indicators. PA, MPA, MoU, and FWIoU indicators are 2.60%, 5.19%, 4.51%, and 3.86% higher than DeepLab V3+.<table-wrap id="Tab3"><label>Table 3</label><caption><p>Results on GID.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Method</th><th align="left">PA</th><th align="left">MPA</th><th align="left">MIoU</th><th align="left">FWIoU</th></tr></thead><tbody><tr><td align="left">SegNet<sup><xref ref-type="bibr" rid="CR47">47</xref></sup></td><td char="." align="char">65.40</td><td char="." align="char">66.44</td><td char="." align="char">48.53</td><td char="." align="char">48.56</td></tr><tr><td align="left">U-Net<sup><xref ref-type="bibr" rid="CR26">26</xref></sup></td><td char="." align="char">67.11</td><td char="." align="char">62.71</td><td char="." align="char">49.25</td><td char="." align="char">50.37</td></tr><tr><td align="left">PSPNet<sup><xref ref-type="bibr" rid="CR48">48</xref></sup></td><td char="." align="char">79.86</td><td char="." align="char">72.79</td><td char="." align="char">62.13</td><td char="." align="char">66.69</td></tr><tr><td align="left">DANet<sup><xref ref-type="bibr" rid="CR49">49</xref></sup></td><td char="." align="char">79.67</td><td char="." align="char">79.54</td><td char="." align="char">64.62</td><td char="." align="char">66.48</td></tr><tr><td align="left">DeepLab V3+<sup><xref ref-type="bibr" rid="CR50">50</xref></sup></td><td char="." align="char">82.77</td><td char="." align="char">80.50</td><td char="." align="char">69.43</td><td char="." align="char">70.82</td></tr><tr><td align="left">A2-FPN<sup><xref ref-type="bibr" rid="CR51">51</xref></sup></td><td char="." align="char">63.62</td><td char="." align="char">74.47</td><td char="." align="char">53.52</td><td char="." align="char">59.36</td></tr><tr><td align="left">MFCA-Net (ours)</td><td char="." align="char">85.37</td><td char="." align="char">85.69</td><td char="." align="char">73.94</td><td char="." align="char">74.68</td></tr></tbody></table></table-wrap></p><p id="Par29">For qualitative evaluation, three samples of the GID testing set are predicted and illustrated in&#x000a0;Fig.&#x000a0;<xref rid="Fig4" ref-type="fig">4</xref>. In the dataset, the promotion of grassland is tiny; SegNet, U-Net, and the A2-FPN proposed in 2022 have poor recognition performance on grassland. A2-FPN did not perform as well as expected in identifying cultivated land and woodland. Compared with the other six models, the proposed MFCA-Net has better recognition performance for all classifications and smoother segmentation boundaries.<fig id="Fig4"><label>Figure 4</label><caption><p>Visualization of the results of the GID testing set: (<bold>a</bold>) image, (<bold>b</bold>) ground truth, (<bold>c</bold>) SegNet<sup><xref ref-type="bibr" rid="CR47">47</xref></sup>, (<bold>d</bold>) U-Net<sup><xref ref-type="bibr" rid="CR26">26</xref></sup>, (<bold>e</bold>) PSPNet<sup><xref ref-type="bibr" rid="CR48">48</xref></sup>, (<bold>f</bold>) DANet<sup><xref ref-type="bibr" rid="CR49">49</xref></sup>, (<bold>g</bold>) DeepLab V3+<sup><xref ref-type="bibr" rid="CR50">50</xref></sup>, (<bold>h</bold>) A2-FPN<sup><xref ref-type="bibr" rid="CR51">51</xref></sup>, and (<bold>i</bold>) Our proposed approach. (This figure was drawn by Visio 2021, which can be available at <ext-link ext-link-type="uri" xlink:href="https://www.microsoftstore.com.cn/software/office/visio-standard-2021">https://www.microsoftstore.com.cn/software/office/visio-standard-2021</ext-link>, The visualization was achieved in Visdom under the PyTorch framework. GID can be available at <ext-link ext-link-type="uri" xlink:href="https://www.cvmart.net/dataSets/detail/765?channel_id=op10&#x00026;utm_source=cvmartmp&#x00026;utm_campaign=datasets&#x00026;utm_medium=article">https://www.cvmart.net/dataSets/detail/765?channel_id=op10&#x00026;utm_source=cvmartmp&#x00026;utm_campaign=datasets&#x00026;utm_medium=article</ext-link>).</p></caption><graphic xlink:href="41598_2024_56211_Fig4_HTML" id="MO4"/></fig></p></sec><sec id="Sec16"><title>Ablation study</title><p id="Par30">The ablation study is implemented under the same hyperparameters and runtime environment. As presented in Table <xref rid="Tab4" ref-type="table">4</xref>, the MPA and MIoU are collected to analyze the effects. We first list the segmentation accuracy on MobileNet V2 as the baseline network. Next, we investigated how IMV2 would influence the detection performance. It was observed that the MPA index improved by 4.08% and 5.39%, respectively, on the two datasets. On the MIoU index, the segmentation accuracy has been improved by 2.23% and 1.99%, respectively. Similarly, the performance of the MFDF module was verified. The MPA index increased by 2.51% and 3.96%, respectively; the MIoU index improved by 1.58% and 1.54%, respectively.<table-wrap id="Tab4"><label>Table 4</label><caption><p>Result of the ablation study.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Moudule</th><th align="left">Vaihingen (MPA/MIoU)</th><th align="left">GID (MPA/MIoU)</th></tr></thead><tbody><tr><td align="left">Baseline</td><td align="left">78.18/72.96</td><td align="left">76.34/70.4</td></tr><tr><td align="left">IMV2</td><td align="left">82.26/75.19</td><td align="left">81.73/72.39</td></tr><tr><td align="left">IMV2&#x02009;+&#x02009;MFDF</td><td align="left">84.77/76.77</td><td align="left">85.69/73.93</td></tr></tbody></table></table-wrap></p><p id="Par31">Figure&#x000a0;<xref rid="Fig5" ref-type="fig">5</xref> shows the performance of IMV2 and MFDF by randomly selecting two images for visualization. The first two columns are input images and ground truth. The third column is the performance of the primary network, and the effect is not satisfied for the small proportion of clutter marked in red and cars marked in yellow. The fourth column shows the significant improvement after replacing MobileNet V2 with IMV2. In contrast, the last column shows the segmentation performance after continuing to add the MFDF module, which has better recognition performance for small samples and small target objects and is closer to the ground truth.<fig id="Fig5"><label>Figure 5</label><caption><p>Visualization of the effect of IMV2 and MFDF on Vaihingen: (<bold>a</bold>) image (<bold>b</bold>) ground truth, (<bold>c</bold>) baseline, (<bold>d</bold>) IMV2, and (<bold>e</bold>) IMV2&#x02009;+&#x02009;MFDF (this figure was drawn by Visio 2021, which can be available at <ext-link ext-link-type="uri" xlink:href="https://www.microsoftstore.com.cn/software/office/visio-standard-2021">https://www.microsoftstore.com.cn/software/office/visio-standard-2021</ext-link>, The visualization was achieved in Visdom under the PyTorch framework. Vaihingen can be available at <ext-link ext-link-type="uri" xlink:href="https://www.isprs.org/education/benchmarks/UrbanSemLab/2d-sem-label-vaihingen.aspx">https://www.isprs.org/education/benchmarks/UrbanSemLab/2d-sem-label-vaihingen.aspx</ext-link>).</p></caption><graphic xlink:href="41598_2024_56211_Fig5_HTML" id="MO5"/></fig></p><p id="Par32">Figure&#x000a0;<xref rid="Fig6" ref-type="fig">6</xref> shows the segmentation effect of IMV2 and MFDF on the GID dataset. The above figure did not identify the buildings marked in red based on the basic network and, after adding IMV2, ultimately identified the buildings after adding the MFDF module. It is difficult to distinguish between woodland in blue and grassland in yellow. The recognition effect is improving with the increase of IMV2 and MFDF modules.<fig id="Fig6"><label>Figure 6</label><caption><p>Visualization of the effect of IMV2 and MFDF on GID: (<bold>a</bold>) image, (<bold>b</bold>) ground truth, (<bold>c</bold>) baseline, (<bold>d</bold>) IMV2, and (<bold>e</bold>) IMV2&#x02009;+&#x02009;MFDF (this figure was drawn by Visio 2021, which can be available at <ext-link ext-link-type="uri" xlink:href="https://www.microsoftstore.com.cn/software/office/visio-standard-2021">https://www.microsoftstore.com.cn/software/office/visio-standard-2021</ext-link>, The visualization was achieved in Visdom under the PyTorch framework. GID can be available at <ext-link ext-link-type="uri" xlink:href="https://www.cvmart.net/dataSets/detail/765?channel_id=op10&#x00026;utm_source=cvmartmp&#x00026;utm_campaign=datasets&#x00026;utm_medium=article">https://www.cvmart.net/dataSets/detail/765?channel_id=op10&#x00026;utm_source=cvmartmp&#x00026;utm_campaign=datasets&#x00026;utm_medium=article</ext-link>).</p></caption><graphic xlink:href="41598_2024_56211_Fig6_HTML" id="MO6"/></fig></p></sec></sec><sec id="Sec17"><title>Conclusion</title><p id="Par33">This paper proposes a novel MFCA-Net to improve semantic segmentation performance with RSI. The analysis introduced the channel attention module into the feature extraction network's shallow and deep feature maps, respectively. Moreover, a two-dimensional activation function FReLU that can obtain context information is adopted. After deep feature extraction, the MFDF module was designed. The upsampling process fused the three branches of the shallow feature map of the backbone network. The proposed MFCA-Net achieved better performance and higher detection accuracies than the state-of-the-art methods. The advantages of the proposed MFCA-Net can be briefly summarized as follows: (1) MFCA-Net obtained advanced semantic segmentation results. The experimental results indicate that MFCA-Net outperformed six widely used semantic segmentation methods in the visual observation and quantitative evaluation criteria. (2) The proposed MFCA-Net may achieve quick and effective learning performance and be quickly promoted in practical engineering applications. The findings on the relationship between the loss value and epoch indicate the temporary learning effect of MFCA-Net. These characteristics are acceptable and even preferred in practical applications. In our future studies, we plan to collect large-area datasets with other change detection methods and apply the proposed network to test its robustness and adaptability further.</p></sec></body><back><fn-group><fn><p><bold>Publisher's note</bold></p><p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></fn></fn-group><notes notes-type="author-contribution"><title>Author contributions</title><p>Conceptualization, methodology, and writing by X.L. and J.L.; validation and experiments by X.L.; writing review and editing, X.L. and J.L. Both authors have read and agreed to the published version of the manuscript.</p></notes><notes notes-type="funding-information"><title>Funding</title><p>Shaanxi Water Conservancy Technology Project (2020slkj-17) and the Scientific Research Support Program of Xi&#x02019;an University of Finance and Economics (22FCZD05,22FCJH008).</p></notes><notes notes-type="data-availability"><title>Data availability</title><p>The datasets used or analyzed during the current study are available from the corresponding author on reasonable request.</p></notes><notes id="FPar1" notes-type="COI-statement"><title>Competing interests</title><p id="Par34">The authors declare no competing interests.</p></notes><ref-list id="Bib1"><title>References</title><ref id="CR1"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Du</surname><given-names>S</given-names></name><name><surname>Du</surname><given-names>S</given-names></name><name><surname>Liu</surname><given-names>B</given-names></name><name><surname>Zhang</surname><given-names>X</given-names></name></person-group><article-title>Mapping large-scale and fine-grained urban functional zones from VHR images using a multi-scale semantic segmentation network and object based approach</article-title><source>Remote Sens. Environ.</source><year>2021</year><volume>261</volume><fpage>112480</fpage><pub-id pub-id-type="doi">10.1016/j.rse.2021.112480</pub-id></element-citation></ref><ref id="CR2"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gao</surname><given-names>W</given-names></name><name><surname>Nan</surname><given-names>L</given-names></name><name><surname>Boom</surname><given-names>B</given-names></name><name><surname>Ledoux</surname><given-names>H</given-names></name></person-group><article-title>PSSNet: Planarity-sensible semantic segmentation of large-scale urban meshes</article-title><source>ISPRS J. Photogramm. Remote. Sens.</source><year>2023</year><volume>196</volume><fpage>32</fpage><lpage>44</lpage><pub-id pub-id-type="doi">10.1016/j.isprsjprs.2022.12.020</pub-id></element-citation></ref><ref id="CR3"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>X</given-names></name><etal/></person-group><article-title>MCANet: A joint semantic segmentation framework of optical and SAR images for land use classification</article-title><source>Int. J. Appl. Earth Obs. Geoinf.</source><year>2022</year><volume>106</volume><fpage>102638</fpage></element-citation></ref><ref id="CR4"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>H</given-names></name><etal/></person-group><article-title>Automated delineation of agricultural field boundaries from Sentinel-2 images using recurrent residual U-Net</article-title><source>Int. J. Appl. Earth Obs. Geoinf.</source><year>2021</year><volume>105</volume><fpage>102557</fpage></element-citation></ref><ref id="CR5"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wieland</surname><given-names>M</given-names></name><name><surname>Martinis</surname><given-names>S</given-names></name><name><surname>Kiefl</surname><given-names>R</given-names></name><name><surname>Gstaiger</surname><given-names>V</given-names></name></person-group><article-title>Semantic segmentation of water bodies in very high-resolution satellite and aerial images</article-title><source>Remote Sens. Environ.</source><year>2023</year><volume>287</volume><fpage>113452</fpage><pub-id pub-id-type="doi">10.1016/j.rse.2023.113452</pub-id></element-citation></ref><ref id="CR6"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xiang</surname><given-names>J</given-names></name><name><surname>Liu</surname><given-names>J</given-names></name><name><surname>Chen</surname><given-names>D</given-names></name><name><surname>Xiong</surname><given-names>Q</given-names></name><name><surname>Deng</surname><given-names>C</given-names></name></person-group><article-title>CTFuseNet: A multi-scale CNN-transformer feature fused network for crop type segmentation on UAV remote sensing imagery</article-title><source>Remote Sens.</source><year>2023</year><volume>15</volume><fpage>1151</fpage><pub-id pub-id-type="doi">10.3390/rs15041151</pub-id></element-citation></ref><ref id="CR7"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pun</surname><given-names>T</given-names></name></person-group><article-title>A new method for grey-level picture thresholding using the entropy of the histogram</article-title><source>Signal Process.</source><year>1980</year><volume>2</volume><fpage>223</fpage><lpage>237</lpage><pub-id pub-id-type="doi">10.1016/0165-1684(80)90020-1</pub-id></element-citation></ref><ref id="CR8"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yen</surname><given-names>JC</given-names></name><name><surname>Chang</surname><given-names>FJ</given-names></name><name><surname>Chang</surname><given-names>S</given-names></name></person-group><article-title>A new criterion for automatic multilevel thresholding</article-title><source>IEEE Trans. Image Process</source><year>1995</year><volume>4</volume><fpage>370</fpage><pub-id pub-id-type="doi">10.1109/83.366472</pub-id><?supplied-pmid 18289986?><pub-id pub-id-type="pmid">18289986</pub-id>
</element-citation></ref><ref id="CR9"><label>9.</label><mixed-citation publication-type="other">Rosenfeld, A. The max Roberts operator is a Hueckel-type edge detector. <italic>IEEE Trans. Pattern Anal. Mach. Intell.</italic> (1981).</mixed-citation></ref><ref id="CR10"><label>10.</label><mixed-citation publication-type="other">Lang, Y.&#x0202f;&#x00026; Zheng, D. An improved Sobel edge detection operator. In <italic>2016 6th International Conference on Mechatronics, Computer and Education Informationization (MCEI 2016)</italic> (2016).</mixed-citation></ref><ref id="CR11"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ravivarma</surname><given-names>G</given-names></name><etal/></person-group><article-title>Implementation of Sobel operator based image edge detection on FPGA</article-title><source>Mater. Today Proc.</source><year>2021</year><volume>45</volume><fpage>2401</fpage><lpage>2407</lpage><pub-id pub-id-type="doi">10.1016/j.matpr.2020.10.825</pub-id></element-citation></ref><ref id="CR12"><label>12.</label><mixed-citation publication-type="other">Yang, L., Wu, X., Zhao, D., Li, H. &#x00026; Zhai, J. An improved Prewitt algorithm for edge detection based on noised image. In <italic>2011 4th International Congress on Image and Signal Processing</italic> 1197&#x02013;1200 (IEEE, 2011) 10.1109/CISP.2011.6100495.</mixed-citation></ref><ref id="CR13"><label>13.</label><mixed-citation publication-type="other">Yadav, J. S. &#x00026; Shyamala Bharathi, P. Edge detection of images using Prewitt algorithm comparing with Sobel algorithm to improve accuracy. In <italic>2022 3rd International Conference on Intelligent Engineering and Management (ICIEM)</italic> 351&#x02013;355 (2022). 10.1109/ICIEM54221.2022.9853193.</mixed-citation></ref><ref id="CR14"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huang</surname><given-names>M</given-names></name><name><surname>Liu</surname><given-names>Y</given-names></name><name><surname>Yang</surname><given-names>Y</given-names></name></person-group><article-title>Edge detection of ore and rock on the surface of explosion pile based on improved Canny operator</article-title><source>Alex. Eng. J.</source><year>2022</year><volume>61</volume><fpage>10769</fpage><lpage>10777</lpage><pub-id pub-id-type="doi">10.1016/j.aej.2022.04.019</pub-id></element-citation></ref><ref id="CR15"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vladimir</surname><given-names>M</given-names></name><name><surname>Mile</surname><given-names>P</given-names></name><name><surname>Dragan</surname><given-names>S</given-names></name><name><surname>Branimir</surname><given-names>J</given-names></name><name><surname>Petar</surname><given-names>S</given-names></name></person-group><article-title>New approach of estimating edge detection threshold and application of adaptive detector depending on image complexity</article-title><source>Optik Zeitschrift fur Licht und Elektronenoptik J. Light-and Electronoptic</source><year>2021</year><volume>238</volume><fpage>166476</fpage></element-citation></ref><ref id="CR16"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Giacomini</surname><given-names>M</given-names></name><name><surname>Perotto</surname><given-names>S</given-names></name></person-group><article-title>Anisotropic mesh adaptation for region-based segmentation accounting for image spatial information</article-title><source>Comput. Math. Appl.</source><year>2022</year><volume>121</volume><fpage>1</fpage><lpage>17</lpage><pub-id pub-id-type="doi">10.1016/j.camwa.2022.06.025</pub-id></element-citation></ref><ref id="CR17"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Park</surname><given-names>J</given-names></name><name><surname>Cho</surname><given-names>YK</given-names></name><name><surname>Kim</surname><given-names>S</given-names></name></person-group><article-title>Deep learning-based UAV image segmentation and inpainting for generating vehicle-free orthomosaic</article-title><source>Int. J. Appl. Earth Observ. Geoinformation</source><year>2022</year><volume>115</volume><fpage>103111</fpage><pub-id pub-id-type="doi">10.1016/j.jag.2022.103111</pub-id></element-citation></ref><ref id="CR18"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Y</given-names></name><name><surname>Wu</surname><given-names>L</given-names></name><name><surname>Qi</surname><given-names>Q</given-names></name><name><surname>Wang</surname><given-names>J</given-names></name></person-group><article-title>Local scale-guided hierarchical region merging and further over- and under-segmentation processing for hybrid remote sensing image segmentation</article-title><source>IEEE Access</source><year>2022</year><volume>10</volume><fpage>81492</fpage><lpage>81505</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2022.3194047</pub-id></element-citation></ref><ref id="CR19"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Paoletti</surname><given-names>ME</given-names></name><etal/></person-group><article-title>Separable attention network in single- and mixed-precision floating point for land-cover classification of remote sensing images</article-title><source>IEEE Geosci. Remote Sens. Lett.</source><year>2022</year><pub-id pub-id-type="doi">10.1109/LGRS.2021.3108965</pub-id></element-citation></ref><ref id="CR20"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hl</surname><given-names>A</given-names></name><name><surname>Zw</surname><given-names>B</given-names></name><name><surname>Hui</surname><given-names>ZA</given-names></name></person-group><article-title>Edge protection filtering and convolutional neural network for hyperspectral remote sensing image classification</article-title><source>Infrared Phys. Technol.</source><year>2022</year><volume>122</volume><fpage>104039</fpage><pub-id pub-id-type="doi">10.1016/j.infrared.2022.104039</pub-id></element-citation></ref><ref id="CR21"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zheng</surname><given-names>H</given-names></name><etal/></person-group><article-title>HFA-Net: High frequency attention siamese network for building change detection in VHR remote sensing images</article-title><source>Pattern Recogn.</source><year>2022</year><volume>129</volume><fpage>108717</fpage><pub-id pub-id-type="doi">10.1016/j.patcog.2022.108717</pub-id></element-citation></ref><ref id="CR22"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>H</given-names></name><name><surname>Chen</surname><given-names>X</given-names></name><name><surname>Zhang</surname><given-names>T</given-names></name><name><surname>Xu</surname><given-names>Z</given-names></name><name><surname>Li</surname><given-names>J</given-names></name></person-group><article-title>CCTNet: Coupled CNN and transformer network for crop segmentation of remote sensing images</article-title><source>Remote Sens.</source><year>2022</year><volume>14</volume><fpage>1956</fpage><pub-id pub-id-type="doi">10.3390/rs14091956</pub-id></element-citation></ref><ref id="CR23"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Z</given-names></name><etal/></person-group><article-title>Semantic segmentation of high-resolution remote sensing images based on a class feature attention mechanism fused with Deeplabv3+</article-title><source>Comput. Geosci.</source><year>2022</year><volume>158</volume><fpage>104969</fpage><pub-id pub-id-type="doi">10.1016/j.cageo.2021.104969</pub-id></element-citation></ref><ref id="CR24"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhao</surname><given-names>J</given-names></name><etal/></person-group><article-title>Multi-source collaborative enhanced for remote sensing images semantic segmentation</article-title><source>Neurocomputing</source><year>2022</year><volume>493</volume><fpage>76</fpage><lpage>90</lpage><pub-id pub-id-type="doi">10.1016/j.neucom.2022.04.045</pub-id></element-citation></ref><ref id="CR25"><label>25.</label><mixed-citation publication-type="other">Zheng, X. &#x00026; Chen, T. Segmentation of high spatial resolution remote sensing image based on U-Net convolutional networks. In <italic>IGARSS 2020&#x02014;2020 IEEE International GeoSci. and Remote Sens. Symposium</italic> (2020).</mixed-citation></ref><ref id="CR26"><label>26.</label><mixed-citation publication-type="other">Ronneberger, O., Fischer, P., &#x00026; Brox, T. U-Net: Convolutional networks for biomedical image segmentation. In <italic>International Conference on Medical Image Computing and Computer-Assisted Intervention</italic> (2015).</mixed-citation></ref><ref id="CR27"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xuan</surname><given-names>Y</given-names></name><etal/></person-group><article-title>An attention-fused network for semantic segmentation of very-high-resolution remote sensing imagery</article-title><source>ISPRS J. Photogramm. Remote Sens.</source><year>2021</year><volume>177</volume><fpage>238</fpage><lpage>262</lpage><pub-id pub-id-type="doi">10.1016/j.isprsjprs.2021.05.004</pub-id></element-citation></ref><ref id="CR28"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zheng</surname><given-names>C</given-names></name><name><surname>Zhang</surname><given-names>Y</given-names></name><name><surname>Wang</surname><given-names>L</given-names></name></person-group><article-title>Multigranularity multiclass-layer Markov random field model for semantic segmentation of remote sensing images</article-title><source>IEEE Trans. Geosci. Remote Sens.</source><year>2020</year><volume>PP</volume><fpage>1</fpage><lpage>20</lpage><pub-id pub-id-type="doi">10.1109/TGRS.2020.2993861</pub-id></element-citation></ref><ref id="CR29"><label>29.</label><mixed-citation publication-type="other">Sun, S., Lei, Y., Liu, W. &#x00026; Li, R. Feature fusion through multitask CNN for large-scale remote sensing image segmentation. In <italic>2018 10th IAPR Workshop on Pattern Recognit. in Remote Sens. (PRRS)</italic> (2018).</mixed-citation></ref><ref id="CR30"><label>30.</label><mixed-citation publication-type="other">Chen, G. <italic>et al.</italic> SDFCNv2: An improved FCN framework for remote sensing images semantic segmentation. (2021).</mixed-citation></ref><ref id="CR31"><label>31.</label><mixed-citation publication-type="other">Fan, H., Wei, Q., Shu, D. Q., Li, Y. &#x00026; Yang, C. D. An improved deeplab based model for extracting cultivated land information from high definition remote sensing images. In <italic>2019 IEEE International Conference on Signal, Information and Data Process (ICSIDP)</italic> (2019).</mixed-citation></ref><ref id="CR32"><label>32.</label><mixed-citation publication-type="other">Chen, L.-C., Papandreou, G., Kokkinos, I., Murphy, K. &#x00026; Yuille, A. L. <italic>Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs</italic>. Preprint at <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1412.7062">http://arxiv.org/abs/1412.7062</ext-link> (2016).</mixed-citation></ref><ref id="CR33"><label>33.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Z</given-names></name><name><surname>Gao</surname><given-names>X</given-names></name><name><surname>Zhang</surname><given-names>Y</given-names></name></person-group><article-title>HA-Net: A lake water body extraction network based on hybrid-scale attention and transfer learning</article-title><source>Remote Sens.</source><year>2021</year><volume>13</volume><fpage>4121</fpage><pub-id pub-id-type="doi">10.3390/rs13204121</pub-id></element-citation></ref><ref id="CR34"><label>34.</label><mixed-citation publication-type="other">Wang, F. <italic>et al.</italic> Residual attention network for image classification. In <italic>2017 Proc. IEEE Conf. Comput. Vis. Pattern Recog.</italic> 6450&#x02013;6458 (2017).</mixed-citation></ref><ref id="CR35"><label>35.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>C</given-names></name><etal/></person-group><article-title>Transformer and CNN hybrid deep neural network for semantic segmentation of very-high-resolution remote sensing imagery</article-title><source>IEEE Trans. Geosci. Remote Sens.</source><year>2022</year><volume>60</volume><fpage>1</fpage><lpage>20</lpage></element-citation></ref><ref id="CR36"><label>36.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>L</given-names></name><etal/></person-group><article-title>A novel transformer based semantic segmentation scheme for fine-resolution remote sensing images</article-title><source>IEEE Geosci. Remote Sens. Lett.</source><year>2022</year><volume>19</volume><fpage>1</fpage><lpage>5</lpage></element-citation></ref><ref id="CR37"><label>37.</label><mixed-citation publication-type="other">Goodfellow, I. J. <italic>et al.</italic><italic>Generative Adversarial Networks</italic>. Preprint at 10.48550/arXiv.1406.2661 (2014).</mixed-citation></ref><ref id="CR38"><label>38.</label><mixed-citation publication-type="other">Luc, P., Couprie, C., Chintala, S. &#x00026; Verbeek, J. <italic>Semantic Segmentation using Adversarial Networks</italic>. Preprint at 10.48550/arXiv.1611.08408 (2016).</mixed-citation></ref><ref id="CR39"><label>39.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>Y</given-names></name><name><surname>Shi</surname><given-names>T</given-names></name><name><surname>Zhang</surname><given-names>Y</given-names></name><name><surname>Ma</surname><given-names>J</given-names></name></person-group><article-title>SPGAN-DA: Semantic-preserved generative adversarial network for domain adaptive remote sensing image semantic segmentation</article-title><source>IEEE Trans. Geosci. Remote Sens.</source><year>2023</year><volume>61</volume><fpage>1</fpage><lpage>17</lpage></element-citation></ref><ref id="CR40"><label>40.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ma</surname><given-names>X</given-names></name><name><surname>Zhang</surname><given-names>X</given-names></name><name><surname>Wang</surname><given-names>Z</given-names></name><name><surname>Pun</surname><given-names>M-O</given-names></name></person-group><article-title>Unsupervised domain adaptation augmented by mutually boosted attention for semantic segmentation of VHR remote sensing images</article-title><source>IEEE Trans. Geosci. Remote Sens.</source><year>2023</year><volume>61</volume><fpage>1</fpage><lpage>15</lpage></element-citation></ref><ref id="CR41"><label>41.</label><mixed-citation publication-type="other">Sandler, M., Howard, A., Zhu, M., Zhmoginov, A. &#x00026; Chen, L. C. MobileNetV2: Inverted residuals and linear bottlenecks. In <italic>2018 Proc. IEEE Conf. Comput. Vis. Pattern Recog.</italic> (2018).</mixed-citation></ref><ref id="CR42"><label>42.</label><mixed-citation publication-type="other">Ma, N., Zhang, X. &#x00026; Sun, J. <italic>Funnel Activation for Visual Recognition</italic>. Preprint at <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2007.11824">http://arxiv.org/abs/2007.11824</ext-link> (2020).</mixed-citation></ref><ref id="CR43"><label>43.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>LC</given-names></name><name><surname>Papandreou</surname><given-names>G</given-names></name><name><surname>Kokkinos</surname><given-names>I</given-names></name><name><surname>Murphy</surname><given-names>K</given-names></name><name><surname>Yuille</surname><given-names>AL</given-names></name></person-group><article-title>DeepLab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2018</year><volume>40</volume><fpage>834</fpage><lpage>848</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2017.2699184</pub-id><?supplied-pmid 28463186?><pub-id pub-id-type="pmid">28463186</pub-id>
</element-citation></ref><ref id="CR44"><label>44.</label><mixed-citation publication-type="other">Takikawa, T., Acuna, D., Jampani, V. &#x00026; Fidler, S. <italic>Gated-SCNN: Gated Shape CNNs for Semantic Segmentation</italic>. Preprint at 10.48550/arXiv.1907.05740 (2019).</mixed-citation></ref><ref id="CR45"><label>45.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Z</given-names></name><name><surname>Song</surname><given-names>R</given-names></name><name><surname>Duan</surname><given-names>P</given-names></name><name><surname>Li</surname><given-names>X</given-names></name></person-group><article-title>EFNet: Enhancement-fusion network for semantic segmentation</article-title><source>Pattern Recogn.</source><year>2021</year><volume>118</volume><fpage>108023</fpage><pub-id pub-id-type="doi">10.1016/j.patcog.2021.108023</pub-id></element-citation></ref><ref id="CR46"><label>46.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lin</surname><given-names>TY</given-names></name><name><surname>Goyal</surname><given-names>P</given-names></name><name><surname>Girshick</surname><given-names>R</given-names></name><name><surname>He</surname><given-names>K</given-names></name><name><surname>Doll&#x000e1;r</surname><given-names>P</given-names></name></person-group><article-title>Focal loss for dense object detection</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2017</year><volume>5</volume><fpage>2999</fpage><lpage>3007</lpage></element-citation></ref><ref id="CR47"><label>47.</label><mixed-citation publication-type="other">Badrinarayanan, V., Kendall, A. &#x00026; Cipolla, R. <italic>SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation</italic>. (2017).</mixed-citation></ref><ref id="CR48"><label>48.</label><mixed-citation publication-type="other">Zhao, H., Shi, J., Qi, X., Wang, X. &#x00026; Jia, J. Pyramid scene parsing network. In <italic>IEEE Computer Soc.</italic> (2016).</mixed-citation></ref><ref id="CR49"><label>49.</label><mixed-citation publication-type="other">Fu, J. <italic>et al.</italic> Dual attention network for scene segmentation. In <italic>2019 Proc. IEEE Conf. Comput. Vis. Pattern Recog.</italic> (2020).</mixed-citation></ref><ref id="CR50"><label>50.</label><mixed-citation publication-type="other">Chen, L.-C., Zhu, Y., Papandreou, G., Schroff, F. &#x00026; Adam, H. <italic>Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation</italic> (2018).</mixed-citation></ref><ref id="CR51"><label>51.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>R</given-names></name><name><surname>Wang</surname><given-names>L</given-names></name><name><surname>Zhang</surname><given-names>C</given-names></name><name><surname>Duan</surname><given-names>C</given-names></name><name><surname>Zheng</surname><given-names>S</given-names></name></person-group><article-title>A2-FPN for semantic segmentation of fine-resolution remotely sensed images</article-title><source>Int. J. Remote Sens.</source><year>2022</year><volume>43</volume><fpage>1131</fpage><lpage>1155</lpage><pub-id pub-id-type="doi">10.1080/01431161.2022.2030071</pub-id></element-citation></ref></ref-list></back></article>