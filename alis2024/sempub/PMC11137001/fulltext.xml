<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd"> 
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="data-paper" dtd-version="1.3"><?properties open_access?><?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?><?DTDIdentifier.IdentifierType public?><?SourceDTD.DTDName A++V2.4.dtd?><?SourceDTD.Version 2.4?><?ConverterInfo.XSLTName springer2nlmx2.xsl?><?ConverterInfo.Version 1?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sci Data</journal-id><journal-id journal-id-type="iso-abbrev">Sci Data</journal-id><journal-title-group><journal-title>Scientific Data</journal-title></journal-title-group><issn pub-type="epub">2052-4463</issn><publisher><publisher-name>Nature Publishing Group UK</publisher-name><publisher-loc>London</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmcid">11137001</article-id><article-id pub-id-type="publisher-id">3398</article-id><article-id pub-id-type="doi">10.1038/s41597-024-03398-7</article-id><article-categories><subj-group subj-group-type="heading"><subject>Data Descriptor</subject></subj-group></article-categories><title-group><article-title>ChineseEEG: A Chinese Linguistic Corpora EEG Dataset for Semantic Alignment and Neural Decoding</article-title></title-group><contrib-group><contrib contrib-type="author" equal-contrib="yes"><contrib-id contrib-id-type="orcid">http://orcid.org/0009-0004-3273-9704</contrib-id><name><surname>Mou</surname><given-names>Xinyu</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author" equal-contrib="yes"><name><surname>He</surname><given-names>Cuilin</given-names></name><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author" equal-contrib="yes"><name><surname>Tan</surname><given-names>Liwei</given-names></name><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author"><name><surname>Yu</surname><given-names>Junjie</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author"><name><surname>Liang</surname><given-names>Huadong</given-names></name><xref ref-type="aff" rid="Aff3">3</xref></contrib><contrib contrib-type="author"><name><surname>Zhang</surname><given-names>Jianyu</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author"><name><surname>Tian</surname><given-names>Yan</given-names></name><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author"><name><surname>Yang</surname><given-names>Yu-Fang</given-names></name><xref ref-type="aff" rid="Aff4">4</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-0065-3832</contrib-id><name><surname>Xu</surname><given-names>Ting</given-names></name><xref ref-type="aff" rid="Aff5">5</xref></contrib><contrib contrib-type="author"><name><surname>Wang</surname><given-names>Qing</given-names></name><xref ref-type="aff" rid="Aff6">6</xref></contrib><contrib contrib-type="author"><name><surname>Cao</surname><given-names>Miao</given-names></name><xref ref-type="aff" rid="Aff7">7</xref></contrib><contrib contrib-type="author"><name><surname>Chen</surname><given-names>Zijiao</given-names></name><xref ref-type="aff" rid="Aff8">8</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-7503-5131</contrib-id><name><surname>Hu</surname><given-names>Chuan-Peng</given-names></name><xref ref-type="aff" rid="Aff9">9</xref></contrib><contrib contrib-type="author"><name><surname>Wang</surname><given-names>Xindi</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Liu</surname><given-names>Quanying</given-names></name><address><email>liuqy@sustech.edu.cn</email></address><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author" corresp="yes"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-8869-6636</contrib-id><name><surname>Wu</surname><given-names>Haiyan</given-names></name><address><email>haiyanwu@um.edu.mo</email></address><xref ref-type="aff" rid="Aff2">2</xref></contrib><aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/049tv2d57</institution-id><institution-id institution-id-type="GRID">grid.263817.9</institution-id><institution-id institution-id-type="ISNI">0000 0004 1773 1790</institution-id><institution>Department of Biomedical Engineering, </institution><institution>Southern University of Science and Technology, </institution></institution-wrap>Shenzhen, China </aff><aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="GRID">grid.437123.0</institution-id><institution-id institution-id-type="ISNI">0000 0004 1794 8068</institution-id><institution>Centre for Cognitive and Brain Sciences, Department of Psychology, Faculty of Social Sciences, </institution><institution>University of Macau, </institution></institution-wrap>Taipa, Macau SAR China </aff><aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="GRID">grid.519944.4</institution-id><institution-id institution-id-type="ISNI">0000 0005 0628 5884</institution-id><institution>AI Research Institute, iFLYTEK Co., </institution><institution>LTD, </institution></institution-wrap>Hefei, China </aff><aff id="Aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/046ak2485</institution-id><institution-id institution-id-type="GRID">grid.14095.39</institution-id><institution-id institution-id-type="ISNI">0000 0000 9116 4836</institution-id><institution>Division of Experimental Psychology and Neuropsychology, Department of Education and Psychology, </institution><institution>Freie Universit&#x000e4;t Berlin, </institution></institution-wrap>Berlin, Germany </aff><aff id="Aff5"><label>5</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/01bfgxw09</institution-id><institution-id institution-id-type="GRID">grid.428122.f</institution-id><institution-id institution-id-type="ISNI">0000 0004 7592 9033</institution-id><institution>Center for the Integrative Developmental Neuroscience, </institution><institution>Child Mind Institute, </institution></institution-wrap>New York, NY USA </aff><aff id="Aff6"><label>6</label><institution-wrap><institution-id institution-id-type="GRID">grid.16821.3c</institution-id><institution-id institution-id-type="ISNI">0000 0004 0368 8293</institution-id><institution>Shanghai Mental Health Center, School of Medicine, </institution><institution>Shanghai Jiao Tong University, </institution></institution-wrap>600 S. Wanping Rd., Shanghai, 200030 China </aff><aff id="Aff7"><label>7</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/031rekg67</institution-id><institution-id institution-id-type="GRID">grid.1027.4</institution-id><institution-id institution-id-type="ISNI">0000 0004 0409 2862</institution-id><institution>Australian National Imaging Facility and Swinburne Neuroimaging Facility, </institution><institution>Swinburne University of Technology, </institution></institution-wrap>Victoria, Australia </aff><aff id="Aff8"><label>8</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/01tgyzw49</institution-id><institution-id institution-id-type="GRID">grid.4280.e</institution-id><institution-id institution-id-type="ISNI">0000 0001 2180 6431</institution-id><institution>Centre for Cognitive and Cognition, Yong Loo Lin School of Medicine, </institution><institution>National University of Singapore, </institution></institution-wrap>Kent Ridge, Singapore </aff><aff id="Aff9"><label>9</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/036trcv74</institution-id><institution-id institution-id-type="GRID">grid.260474.3</institution-id><institution-id institution-id-type="ISNI">0000 0001 0089 5711</institution-id><institution>School of Psychology, </institution><institution>Nanjing Normal University, </institution></institution-wrap>Nanjing, China </aff></contrib-group><pub-date pub-type="epub"><day>29</day><month>5</month><year>2024</year></pub-date><pub-date pub-type="pmc-release"><day>29</day><month>5</month><year>2024</year></pub-date><pub-date pub-type="collection"><year>2024</year></pub-date><volume>11</volume><elocation-id>550</elocation-id><history><date date-type="received"><day>11</day><month>2</month><year>2024</year></date><date date-type="accepted"><day>21</day><month>5</month><year>2024</year></date></history><permissions><copyright-statement>&#x000a9; The Author(s) 2024</copyright-statement><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article&#x02019;s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article&#x02019;s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p></license></permissions><abstract id="Abs1"><p id="Par1">An Electroencephalography (EEG) dataset utilizing rich text stimuli can advance the understanding of how the brain encodes semantic information and contribute to semantic decoding in brain-computer interface (BCI). Addressing the scarcity of EEG datasets featuring Chinese linguistic stimuli, we present the ChineseEEG dataset, a high-density EEG dataset complemented by simultaneous eye-tracking recordings. This dataset was compiled while 10 participants silently read approximately 13&#x02009;hours of Chinese text from two well-known novels. This dataset provides long-duration EEG recordings, along with pre-processed EEG sensor-level data and semantic embeddings of reading materials extracted by a pre-trained natural language processing (NLP) model. As a pilot EEG dataset derived from natural Chinese linguistic stimuli, ChineseEEG can significantly support research across neuroscience, NLP, and linguistics. It establishes a benchmark dataset for Chinese semantic decoding, aids in the development of BCIs, and facilitates the exploration of alignment between large language models and human cognitive processes. It can also aid research into the brain&#x02019;s mechanisms of language processing within the context of the Chinese natural language.</p></abstract><kwd-group kwd-group-type="npg-subject"><title>Subject terms</title><kwd>Language</kwd><kwd>Neural decoding</kwd></kwd-group><funding-group><award-group><funding-source><institution-wrap><institution-id institution-id-type="FundRef">https://doi.org/10.13039/501100003453</institution-id><institution>Natural Science Foundation of Guangdong Province (Guangdong Natural Science Foundation)</institution></institution-wrap></funding-source><award-id>2021A1515012509</award-id><principal-award-recipient><name><surname>Wu</surname><given-names>Haiyan</given-names></name></principal-award-recipient></award-group></funding-group><funding-group><award-group><funding-source><institution-wrap><institution-id institution-id-type="FundRef">https://doi.org/10.13039/100011045</institution-id><institution>British Sedimentological Research Group (BSRG)</institution></institution-wrap></funding-source></award-group></funding-group><funding-group><award-group><funding-source><institution>the MindD project of Tianqiao and Chrissy Chen Institute(TCCI), the Science and Technology Development Fund (FDCT) of Macau [0127/2020/A3, 0041/2022/A], Shenzhen-Hong Kong-Macao Science and Technology Innovation Project (Category C) (SGDX2020110309280100), and the SRG of University of Macau (SRG2020-00027-ICI).</institution></funding-source></award-group></funding-group><custom-meta-group><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#x000a9; Springer Nature Limited 2024</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1"><title>Background &#x00026; Summary</title><p id="Par2">The human brain&#x02019;s ability to rapidly comprehend linguistic information and generate corresponding linguistic expressions is an indicator of its complex processing capabilities<sup><xref ref-type="bibr" rid="CR1">1</xref></sup>. When exposed to linguistic stimuli, the human brain encodes the semantic information through neural activities<sup><xref ref-type="bibr" rid="CR2">2</xref></sup>. By analyzing such neural activities, we can uncover the encoding mechanisms of semantics in the brain<sup><xref ref-type="bibr" rid="CR3">3</xref></sup>. A variety of neural signals, including EEG, Functional Magnetic Resonance Imaging (fMRI), Electrocorticography (ECoG) are employed in language-related tasks, from academic research like investigating language processing mechanisms&#x000a0;in the brain to practical applications like language decoding in BCI<sup><xref ref-type="bibr" rid="CR4">4</xref>&#x02013;<xref ref-type="bibr" rid="CR9">9</xref></sup>. Recently, a lot of studies on neurolinguistics utilized both traditional&#x000a0;machine learning methods and modern deep learning methods in NLP to explore linguistic-related problems<sup><xref ref-type="bibr" rid="CR10">10</xref>&#x02013;<xref ref-type="bibr" rid="CR16">16</xref></sup>. However, these data-driven methods rely heavily on massive and comprehensive datasets<sup><xref ref-type="bibr" rid="CR17">17</xref></sup>. In the field of NLP, it is relatively easy to collect large amounts of natural language data. In contrast, acquiring a large volume of neural signals generated in response to natural language stimuli poses significant challenges. To utilize the strong ability of modern data-driven methods, it is important to scale neural datasets to commensurate the state-of-the-art NLP to encompass the wide range of language expressions encountered in daily life. Among all neuroimaging techniques, EEG holds great potential to meet this demand. EEG is non-invasive and cost-effective<sup><xref ref-type="bibr" rid="CR18">18</xref></sup>, which allows the creation of long-duration neural signal datasets enriched with semantic information. Meanwhile, EEG features high temporal resolution<sup><xref ref-type="bibr" rid="CR19">19</xref></sup>, which enables it to precisely capture the brain&#x02019;s rapid dynamic changes in the language processing process.</p><p id="Par3">Despite the abundance of EEG datasets for natural visual stimuli (e.g., THINGS-EEG)<sup><xref ref-type="bibr" rid="CR20">20</xref>&#x02013;<xref ref-type="bibr" rid="CR23">23</xref></sup>, those for natural language stimuli remain scarce. Currently, only a few language-related EEG datasets exist, such as the ZuCo dataset<sup><xref ref-type="bibr" rid="CR24">24</xref></sup>. However, the majority of these datasets are collected using stimuli from English language corpora. This leads to limited research on the neural representations of other languages like Chinese. The brain&#x02019;s processing mechanisms differ for various languages. For example, the brain exhibits specificity in response to Chinese compared to English<sup><xref ref-type="bibr" rid="CR25">25</xref></sup>. Therefore, it is important to create an EEG dataset based on other language stimuli. Chinese, being distinct from English in both structure and semantics, provides an opportunity to expand our understanding of neural responses to linguistic stimuli. An EEG dataset stimulated by Chinese corpora can facilitate the investigation of cross-linguistic commonalities and variations in language processing in the brain, bringing new perspectives to our understanding of language processing mechanisms.</p><p id="Par4">To address these gaps, we have collected an EEG dataset, named the &#x0201c;ChineseEEG&#x0201d; (Chinese Linguistic Corpora EEG Dataset)<sup><xref ref-type="bibr" rid="CR26">26</xref>,<xref ref-type="bibr" rid="CR27">27</xref></sup>. It contains high-density EEG data and simultaneous eye-tracking data recorded from 10 participants, each silently reading Chinese text for about 13&#x02009;hours. The text materials are sourced from two well-known novels, <italic>the Little Prince</italic> and <italic>Garnett Dream</italic>, both in their Chinese versions. This dataset further comprises multiple versions of pre-processed EEG sensor-level data generated under different parameter settings, offering researchers a diverse range of selections. Additionally, we provide embeddings of the Chinese text materials encoded from BERT-base-chinese model, which is a pre-trained NLP model specifically used for Chinese<sup><xref ref-type="bibr" rid="CR28">28</xref></sup>, aiding researchers in exploring the alignment between text embeddings from NLP models and brain information representations in neural signals.</p><p id="Par5">ChineseEEG<sup><xref ref-type="bibr" rid="CR26">26</xref>,<xref ref-type="bibr" rid="CR27">27</xref></sup> is a pilot EEG dataset specifically stimulated by Chinese text. It offers several advantages. Firstly, each participant was exposed to around 13&#x02009;hours of diverse Chinese linguistic stimuli, encompassing a broad spectrum of semantic information. The extensive exposure is significant for studying the long-term neural dynamics of language processing in the brain. Secondly, we employed 128 channels of high-density EEG data, which offers superior spatial resolution for precise localization of brain regions involved in language processing. Besides, with a sampling rate of 1&#x02009;kHz, it effectively captures the dynamics of neural representations during reading. Furthermore, the inclusion of the pre-processed EEG data and text embeddings is beneficial for scholars in both neuroscience and computer science domains who lack inter-disciplinary experience, enabling them to directly utilize well processed data from fields they may not familiar with.</p><p id="Par6">ChineseEEG<sup><xref ref-type="bibr" rid="CR26">26</xref>,<xref ref-type="bibr" rid="CR27">27</xref></sup> can serve as a valuable resource in neuroscience, linguistics, and other related fields. EEG data generated from Chinese language stimuli will significantly support research within the Chinese context, aiding researchers in revealing the characteristics of brain signal representations under Chinese stimuli, and promoting the development of brain-to-text translation, semantic decoding and other practical applications tailored to Chinese context. The dataset can also bring diversity to languages used in related research, encouraging the exploration of similarities and differences in language processing stimulated by different languages. It can also aid in multi-linguistic alignment in NLP by aligning multi-lingual brain signals with natural languages<sup><xref ref-type="bibr" rid="CR29">29</xref>,<xref ref-type="bibr" rid="CR30">30</xref></sup>. Given the dataset&#x02019;s inclusion of widely used text materials like <italic>the Little Prince</italic> in multilingual neuroimaging research, ChineseEEG can be combined with prior datasets to extend its potential. For example, combining ChineseEEG with neural signal datasets for auditory language comprehension tasks under similar semantic stimuli<sup><xref ref-type="bibr" rid="CR31">31</xref>,<xref ref-type="bibr" rid="CR32">32</xref></sup> can help to uncover the neural mechanisms of language understanding in multi-modal perceptions. Besides, researchers can integrate semantically rich EEG data in ChineseEEG with other neuroimaging modalities, such as fMRI and MEG, in language comprehension tasks<sup><xref ref-type="bibr" rid="CR33">33</xref>,<xref ref-type="bibr" rid="CR34">34</xref></sup> to precisely uncover brain&#x02019;s spatio-temporal dynamics and thus enhance the understanding of neural mechanisms of language processing in the brain.</p></sec><sec id="Sec2"><title>Methods</title><sec id="Sec3"><title>Participants and task overview</title><p id="Par7">We recruited 15 participants (18&#x02013;29 years old, averaged 21.9 years old, and 9 males). 3 participants participated the pre-experimental test before the official experiment to ensure the rationality of the experimental procedure and the stability of the devices. In the official experiment, 2 participants withdrew halfway due to scheduling conflicts (After communicating with the experimenter, they decided to withdraw from the experiment). In total, data from only 10 participants were used (18&#x02013;29 years old, averaged 22.7 years old, and 5 males). No participant reported neurological or psychiatric history. All participants are right-handed and have normal or corrected-to-normal vision. Each participant voluntarily enrolled in and signed the informed consent form before the experiment and got a coupon compensation of approximately 50 MOP (MOP is the official currency of the Macao Special Administrative Region of China) for each experimental run (25 runs in total). This study complied with the Declaration of Helsinki and was performed according to the ethics committee approval of the Institutional Review Board of the University of Macau (Approval No. BSERE20-APP011-ICI).</p></sec><sec id="Sec4"><title>Experimental material</title><p id="Par8">The experimental materials consist of two novels, both in the genre of children&#x02019;s literature. The first is the Chinese translation of <italic>the Little Prince</italic> (<ext-link ext-link-type="uri" xlink:href="http://www.xiaowangzi.org/index.html">http://www.xiaowangzi.org/index.html</ext-link>) and the second is <italic>Garnett Dream</italic> (<ext-link ext-link-type="uri" xlink:href="https://www.feiku6.com/read/s3-langwangmeng/18242419.html">https://www.feiku6.com/read/s3-langwangmeng/18242419.html</ext-link>), both sourced from the Internet. Using novels, especially children&#x02019;s literature provides several advantages for research, especially within a naturalistic paradigm. Firstly, given their extensive size, these novels offer vast and diverse linguistic content, encompassing the majority of frequently utilized Chinese characters and daily expressions. Besides, children&#x02019;s literature can create an engaging environment for participants, making them more focused and emotionally engaged in the experiment.</p><p id="Par9">Each novel was used as the material for a single session in the experiment. Each session was divided into several runs. For <italic>the Little Prince</italic>, the preface was used as the material for the practice reading phase. The main body of the novel was then used for seven runs in the formal reading phase. The first six runs each includes 4 chapters of the novel, while the seventh run includes the last 3 chapters. For <italic>Garnett Dream</italic>, the first 18 chapters were used for 18 runs in the formal reading stage, with each run including a complete chapter. Due to the loss of markers during the EEG collection process, run 18 of ses-GarnettDream of sub-07 is unusable. We requested this participant to re-complete the reading task using chapter 19 of <italic>Garnett Dream</italic>.</p><p id="Par10">To properly present the text on the screen during the experiment, the content of each run was segmented into a series of units, with each unit containing no more than 10 Chinese characters. These segmented contents were saved in Excel (.xlsx) format for subsequent usage. During the experiment, three adjacent units from each run&#x02019;s content will be displayed on the screen in three separate lines, with the middle line highlighted for the participant to read. The relevant code has been uploaded to the GitHub repository. See Code availability section for detailed information.</p><p id="Par11">The overview of experimental materials is shown in Table&#x000a0;<xref rid="Tab1" ref-type="table">1</xref>. In summary, a total of 115,233 characters (24,324 in <italic>the Little Prince</italic> and 90,909 in <italic>Garnett Dream</italic>), of which 2,985 characters are unique, are used as experimental stimuli in ChineseEEG dataset.<table-wrap id="Tab1"><label>Table 1</label><caption><p>An overview of the experiment.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Session</th><th>Run</th><th>Chapter</th><th>Number of Chinese characters</th><th>Duration</th></tr></thead><tbody><tr><td rowspan="8">LittlePrince</td><td/><td>Preface</td><td>210</td><td/></tr><tr><td>1</td><td>1&#x02013;4</td><td>3,805</td><td>24min34s</td></tr><tr><td>2</td><td>5&#x02013;8</td><td>3,734</td><td>24min5s</td></tr><tr><td>3</td><td>9&#x02013;12</td><td>3,218</td><td>20min50s</td></tr><tr><td>4</td><td>13&#x02013;16</td><td>4,030</td><td>25min59s</td></tr><tr><td>5</td><td>17&#x02013;20</td><td>1,713</td><td>11min11s</td></tr><tr><td>6</td><td>21&#x02013;24</td><td>3,635</td><td>23min27s</td></tr><tr><td>7</td><td>25&#x02013;27</td><td>4,189</td><td>26min54s</td></tr><tr><td rowspan="19">GarnettDream</td><td>1</td><td>1</td><td>5,267</td><td>34min17s</td></tr><tr><td>2</td><td>2</td><td>4,406</td><td>28min39s</td></tr><tr><td>3</td><td>3</td><td>5,327</td><td>34min35s</td></tr><tr><td>4</td><td>4</td><td>3,906</td><td>25min15s</td></tr><tr><td>5</td><td>5</td><td>4,989</td><td>32min14s</td></tr><tr><td>6</td><td>6</td><td>4,413</td><td>28min29s</td></tr><tr><td>7</td><td>7</td><td>3,912</td><td>25min25s</td></tr><tr><td>8</td><td>8</td><td>5,537</td><td>35min52s</td></tr><tr><td>9</td><td>9</td><td>4,171</td><td>27min2s</td></tr><tr><td>10</td><td>10</td><td>5,943</td><td>38min30s</td></tr><tr><td>11</td><td>11</td><td>4,351</td><td>28min21s</td></tr><tr><td>12</td><td>12</td><td>4,830</td><td>31min13s</td></tr><tr><td>13</td><td>13</td><td>3,799</td><td>24min31s</td></tr><tr><td>14</td><td>14</td><td>4,963</td><td>32min9s</td></tr><tr><td>15</td><td>15</td><td>4,656</td><td>29min55s</td></tr><tr><td>16</td><td>16</td><td>4,615</td><td>29min42s</td></tr><tr><td>17</td><td>17</td><td>5,273</td><td>33min57s</td></tr><tr><td>18</td><td>18</td><td>5,113</td><td>32min57s</td></tr><tr><td>19</td><td>19</td><td>5,438</td><td>35min10s</td></tr></tbody></table></table-wrap></p></sec><sec id="Sec5"><title>Experimental procedures</title><p id="Par12">Participants were instructed to sit in an adjustable chair, with their eyes positioned approximately 67&#x02009;cm away from the monitor (Dell, width: 54&#x02009;cm, height: 30.375&#x02009;cm, resolution: 1,920 &#x000d7; 1,080 pixels, vertical refresh rate: 60&#x02009;Hz), see Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1b</xref>. They were tasked with reading a novel and were required to keep their heads still and keep their gaze on the highlighted (red) Chinese characters moving across the screen, reading at a pace set by the program. Eye-tracking technique was utilized to confirm that participants followed the highlighted characters.<fig id="Fig1"><label>Fig. 1</label><caption><p>Overview of the experiment and the modalities included in the dataset. (<bold>a</bold>) Equipment utilized in the experiment, including the EGI device for collecting EEG data and the Tobii Pro&#x000a0;Glasses 3 eye-tracker for tracking eye movements. (<bold>b</bold>) The experiment setup. Participants were instructed to sit quietly approximately 67&#x02009;cm from the screen and sequentially read the highlighted text. (<bold>c</bold>) The experimental protocol. Participants&#x02019; 128-channel EEG signals and eye-tracking data were recorded while reading the highlighted text. (<bold>d</bold>) The data modalities in the dataset. The dataset comprises raw data such as the original textual stimuli, eye movement data, EEG data, and derivatives such as text embeddings from pre-trained NLP models and pre-processed EEG data.</p></caption><graphic xlink:href="41597_2024_3398_Fig1_HTML" id="d33e809"/></fig></p><p id="Par13">Each participant was required to complete a total of 1 practice reading phase and 2 formal reading sessions. <italic>the Little Prince</italic> session was divided into 7 experimental runs and <italic>Garnett Dream</italic> session was divided into 18 experimental runs. The schedule for the entire experiment is as follows: participants were required to finish all experimental runs over the span of 8 days. The total daily reading duration was set at approximately 1.5&#x02009;hours to avoid fatigue. Specifically, the reading tasks for the first day comprised the practice reading phase and runs 1&#x02013;4 of <italic>the Little Prince</italic> session. The tasks for the second day comprised runs 5&#x02013;8 of <italic>the Little Prince</italic> session. From the third to the eighth day, each day&#x02019;s reading tasks comprised 3 runs of <italic>Garnett Dream</italic> session. While participants were afforded the flexibility to adjust their schedules in the experiment, they were required to complete all reading tasks within one month.</p><p id="Par14">Each experimental run lasted approximately 30&#x02009;minutes and was divided into two phases: the eye-tracker calibration phase and the reading phase.</p><sec id="Sec6"><title>Phase 1: Eye-tracker calibration phase</title><p id="Par15">At the beginning of each run, participants were required to undergo an eye-tracker calibration process. Initially, the message &#x0201c;Hello! Please press the spacebar to start calibration&#x0201d; was displayed at the screen&#x02019;s center. Participants were instructed to keep their gaze at a fixation point, which sequentially appeared at the four corners and the center of the screen, each for 5&#x02009;seconds. If the calibration failed, participants were prompted to start another calibration. Upon successful calibration, the message &#x0201c;Calibration successful! The page will automatically redirect in 5&#x02009;seconds&#x0201d; was displayed at the center of the screen.</p><p id="Par16">During the reading process, the accuracy of eye-tracking data can be influenced by several factors, including drift errors resulting from involuntary eye movements, as well as head movements and equipment positioning discrepancies. By performing calibrations at the beginning of each experimental run, these potential errors can be effectively mitigated, thereby ensuring the precision of the eye-tracking data.</p></sec><sec id="Sec7"><title>Phase 2: Reading phase</title><p id="Par17">After the calibration phase, participants were automatically directed to the reading phase. During the reading process, the screen initially displayed the serial number of the current chapter. Subsequently, the text appeared with three lines per page, ensuring each line contained no more than ten Chinese characters (excluding punctuation). On each page, the middle line was highlighted as the focal point, while the upper and lower lines were displayed with reduced intensity as the background. Each character in the middle line was sequentially highlighted with red color for 0.35&#x02009;s, and participants were required to read the novel content following the highlighted cues. To facilitate a smooth reading experience, the text was designed to scroll automatically on the screen. Once participants finished reading the highlighted middle line, the text would scroll, moving the third line up to become the new middle line on the subsequent page.</p><p id="Par18">The reading speed, which is slower than the typical speeds reported in previous studies<sup><xref ref-type="bibr" rid="CR35">35</xref></sup>, was deliberately chosen. This speed was selected based on feedback from the pre-experimental test to maintain participants&#x02019; attention and minimize fatigue throughout the relatively long experimental run. The reading speed was fixed to enable character-level alignment between EEG segments and text. Additionally, fixed speed can also minimize the impact of external interference in the experiment and eliminate the impact of different reading speeds of different participants on subsequent analyses.</p><p id="Par19">To ensure the accuracy of both EEG and eye-tracking data, participants were instructed to consistently focus on the highlighted text, while avoiding significant body movements to maintain a stable reading position. This protocol was strictly enforced to reduce any potential drifts and artifacts in the recordings.</p><p id="Par20">After each run, participants were given sufficient time to rest. They were instructed to start the subsequent run only when they explicitly reported being ready to proceed. Adequate rest time can mitigate fatigue and enable participants to sustain their attention throughout the experiment, thus ensuring the quality of both EEG and eye-tracking data. The experimenter also evaluated each participant&#x02019;s performance and fatigue level through oral inquiries after each experimental run to ensure they could fully maintain their attention in subsequent runs. During each rest period, the experimenter replenished the saline solution on the electrodes of the EEG cap, which helped to maintain a low impedance, ensuring the collection of high-quality EEG data. Additionally, the experimenter checked the power status of the eye-tracker and replaced the batteries as necessary to ensure its continuous operation.</p><p id="Par21">It should be noted that during the initial participation in the experiment, participants were required to complete a practice reading phase. The preface chapter of <italic>the Little Prince</italic> was selected as the reading material for this phase. All settings remained the same as those of the formal reading stage, to familiarize participants with the eye-tracker calibration process and the reading task.</p><p id="Par22">The presentation of stimuli was managed using PsychoPy v2023.2.3<sup><xref ref-type="bibr" rid="CR36">36</xref></sup>, with the EGI PyNetstation v1.0.1 module facilitating the connection between PsychoPy and EGI Netstation. We also utilized g3pylib package to control our eye-tracker to follow the eye movement trajectories of the participants.</p></sec></sec><sec id="Sec8"><title>Data collection and analysis</title><p id="Par23">This section shows the details of the data collection, pre-processing, and data analysis procedure. The modalities included in our dataset<sup><xref ref-type="bibr" rid="CR26">26</xref>,<xref ref-type="bibr" rid="CR27">27</xref></sup> are shown in Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1d</xref>, including raw data and derivatives. Raw data contains the raw EEG data, eye-tracking data, raw text materials, and derivatives contain pre-processed EEG data and text embeddings generated by a pre-trained NLP model BERT-base-chinese.</p><sec id="Sec9"><title>EEG data collection</title><p id="Par24">EEG data was acquired using an EGI 128-channel cap based on the GSN-HydroCel-128 montage with the Geodesic Sensor Net system (see Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1a</xref>). The egi-pynetstation v1.0.1 package was used to control the EGI system. Before recording, the experimenter used a soft ruler to locate the position of the Cz electrode (i.e., the center of the brain) for each participant, ensuring the alignment of the electrodes in each experimental run. During recording, the sampling rate was 1&#x02009;kHz. The impedance of each electrode was kept below 50 k&#x003a9; during the experiment. Setups and recording parameters are similar to our previous EEG dataset<sup><xref ref-type="bibr" rid="CR37">37</xref></sup>. To precisely co-register EEG segments with individual characters during the experiment, we marked the EEG data with triggers (Table&#x000a0;<xref rid="Tab2" ref-type="table">2</xref>). The raw EEG data was exported to metafile format (.mff) files on the macOS system.<table-wrap id="Tab2"><label>Table 2</label><caption><p>EEG triggers.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Trigger</th><th>Description</th></tr></thead><tbody><tr><td>EYES</td><td>Start of eye-tracker recording</td></tr><tr><td>EYEE</td><td>End of eye-tracker recording</td></tr><tr><td>CALS</td><td>Start of the calibration stage before reading</td></tr><tr><td>CALE</td><td>End of the calibration stage</td></tr><tr><td>BEGN</td><td>Start of EEG data collection by the EGI device</td></tr><tr><td>STOP</td><td>Stop collecting EEG data</td></tr><tr><td>CHxx</td><td>Start of each chapter, where xx is the chapter number (e.g., the first chapter is CH01)</td></tr><tr><td>ROWS</td><td>Start of a new line of text</td></tr><tr><td>ROWE</td><td>End of a line</td></tr><tr><td>PRES</td><td>Start of the preface reading phase</td></tr><tr><td>PREE</td><td>End of the preface reading phase</td></tr></tbody></table></table-wrap></p></sec><sec id="Sec10"><title>Eye-tracking data collection</title><p id="Par25">Eye-tracking data was acquired using Tobii Pro Glasses 3. The device features 16 illuminators and 4 eye cameras integrated into scratch-resistant lenses, along with a wide-angle scene camera, allowing for a comprehensive capture of participant behavior and environmental context (see Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1a</xref>). Due to the extensive duration of our experiments, the requirement for a lightweight eye-tracker was prioritized. The Tobii Pro Glass 3 fulfilled this criterion. Tobii Pro Glass 3 has a maximum sampling rate of 100&#x02009;Hz. Given the relatively slow reading speed in our experiment, a sampling rate of 100&#x02009;Hz is adequate for capturing the eye movement trajectories of the participants and assessing whether they were fixating on highlighted text at specific moments. More information about Tobii Pro Glass 3 can be found on the official website (<ext-link ext-link-type="uri" xlink:href="https://www.tobii.com/products/eye-trackers/wearables/tobii-pro-glasses-3">https://www.tobii.com/products/eye-trackers/wearables/tobii-pro-glasses-3</ext-link>). We utilized the package g3pylib to control the glasses. The raw data was exported to .rar files.</p></sec><sec id="Sec11"><title>EEG data pre-processing</title><p id="Par26">To retain maximum amount of valid information in the data, we performed minimal pre-processing on the data, allowing researchers to further process the data according to their specific research needs. The pre-processing pipeline is shown in Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref>. These pre-processing steps include data segmentation, downsampling, powerline filtering, band-pass filtering, bad channel interpolation, independent component analysis (ICA), and re-referencing. The MNE v1.6.0<sup><xref ref-type="bibr" rid="CR38">38</xref></sup> package was utilized to implement all pre-processing steps.<fig id="Fig2"><label>Fig. 2</label><caption><p>EEG pre-processing pipeline. (<bold>a</bold>) Data segmentation: Data is segmented based on markers, retaining only the data from the formal reading phase. (<bold>b</bold>) Band-pass filtering: Two versions of filtered data are provided, with band-pass ranges of 0.5&#x02013;30&#x02009;Hz and 0.5&#x02013;80&#x02009;Hz respectively. (<bold>c</bold>) Bad channel interpolation: Our bad channel detection includes automatic detection implemented with the pyprep package and manual checking. For interpolation, the spherical spline interpolation implemented in MNE is utilized. (<bold>d</bold>) ICA denoising: In this part, the automatic labeling method in mne-iclabel package is utilized followed by a manual checking to remove noisy independent components such as eye movements and heartbeats. (<bold>e</bold>) Dataset organization: Our dataset is organized in the BIDS<sup><xref ref-type="bibr" rid="CR41">41</xref>,<xref ref-type="bibr" rid="CR42">42</xref></sup> format. The detailed file structure is shown in Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3</xref>.</p></caption><graphic xlink:href="41597_2024_3398_Fig2_HTML" id="d33e1013"/></fig></p><p id="Par27">During the data segmentation phase, we only retained data from the formal reading phase of the experiment. Based on the event markers during the data collection phase, we segmented the data, removing sections irrelevant to the formal experiment such as calibration and preface reading. To minimize the impact of subsequent filtering steps on the beginning and end of the signal, an additional 10&#x02009;seconds of data was retained before the start of the formal reading phase. Subsequently, the signal was downsampled to 256&#x02009;Hz. This specific sampling rate ensures effective capture of information related to language comprehension while reducing the burden of subsequent data processing and storage. Additionally, it aligns with the principle of minimal pre-processing, leaving necessary room for researchers to conduct personalized pre-processing based on their needs.</p><p id="Par28">Following downsampling, a 50&#x02009;Hz notch filter was applied to remove the powerline noise from the signal. Next, we performed band-pass overlap-add FIR filter on the signal to eliminate the low-frequency direct current components and high-frequency noise. Here, two versions of filtered data were offered. The first one has a filter band of 0.5&#x02013;80&#x02009;Hz and the second one has a filter band of 0.5&#x02013;30&#x02009;Hz. Researchers can choose the appropriate version based on their specific needs. After filtering, we performed an interpolation of bad channels. The bad channels were selected automatically using a Python-implemented EEG pre-processing package pyprep v0.4.3<sup><xref ref-type="bibr" rid="CR39">39</xref></sup>. After automatic detection, we manually checked to avoid mislabeling or errors before interpolation. The spherical spline interpolation in the MNE package was utilized in this process.</p><p id="Par29">Independent Component Analysis (ICA) was then applied to the data, utilizing the infomax algorithm available in the MNE package. The number of independent components was set to 20, ensuring that they contain the majority of information while not being so numerous to increase the burden of manual processing. Additionally, we set the random seed of the ICA algorithm to 97 to ensure the reproducibility of the ICA results. An automatic method was used to inspect and label components. It was implemented using mne-iclabel v0.5.1<sup><xref ref-type="bibr" rid="CR40">40</xref></sup>, which is a Python-implemented package for automatic independent component labeling. By manually inspecting the independent components after automatic labeling, we excluded obvious noise components such as Electrooculography (EOG) and Electrocardiogram (ECG). Finally, the data was re-referenced using the average method.</p><p id="Par30">The process of manually identifying bad channels and excluding independent components during the ICA step can be conducted through annotations in a Graphical User Interface (GUI), making the annotation process quicker and more user-friendly.</p></sec></sec></sec><sec id="Sec12"><title>Data Records</title><p id="Par31">The full dataset is publicly accessible via the ChineseNeuro Symphony community (CHNNeuro) in the Science Data Bank (ScienceDB) platform (10.57760/sciencedb.CHNNeuro.00007)<sup><xref ref-type="bibr" rid="CR26">26</xref></sup> or via the Openneuro platform (10.18112/openneuro.ds004952.v1.2.0)<sup><xref ref-type="bibr" rid="CR27">27</xref></sup>. Public data is distributed under the the Creative Commons Attribution 4.0 International Public License (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/publicdomain/zero/1.0">https://creativecommons.org/publicdomain/zero/1.0</ext-link>).</p><sec id="Sec13"><title>Data organization</title><p id="Par32">The dataset<sup><xref ref-type="bibr" rid="CR26">26</xref>,<xref ref-type="bibr" rid="CR27">27</xref></sup> is organized following the EEG-BIDS<sup><xref ref-type="bibr" rid="CR41">41</xref>,<xref ref-type="bibr" rid="CR42">42</xref></sup> specification, which is an extension to the brain imaging data structure for EEG. The overview directory tree of our dataset is shown in Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3</xref>. The dataset contains some regular BIDS files, 10 participants&#x02019; data folders, and a <italic>derivatives</italic> folder. The stand-alone files offer an overview of the dataset: i) <italic>dataset_description.json</italic> is a JSON file depicting the information of the dataset, such as the name, dataset type and authors; ii) <italic>participants.tsv</italic> contains participants&#x02019; information, such as age, sex, and handedness; iii) <italic>participants.json</italic> describes the column attributes in <italic>participants.tsv</italic>; iv) <italic>README.md</italic> contains a detailed introduction of the dataset.<fig id="Fig3"><label>Fig. 3</label><caption><p>File structure of the dataset. (<bold>a</bold>) Eye-tracking data: Each experimental run is associated with a&#x000a0;.rar file that contains eye-tracking data. (<bold>b</bold>) Electrode information files: These include detailed information of electrodes such as the location, type, and sampling rate, as well as information on any channels marked as bad during pre-processing. (<bold>c</bold>) EEG data and event-related files: Including EEG data in BrainVision format and event files that record marker information. (<bold>d</bold>) ICA-related files: Containing independent components in numpy format, records of removed components during pre-processing, and topographic maps of the components. (<bold>e</bold>) Text materials: Containing original and segmented text. (<bold>f</bold>) Text embedding files: Each file corresponds to an experimental run and is stored in .npy format. (<bold>g</bold>) Raw EEG data.</p></caption><graphic xlink:href="41597_2024_3398_Fig3_HTML" id="d33e1123"/></fig></p><p id="Par33">Each participant&#x02019;s folder contains two folders named <italic>ses-LittlePrince</italic> and <italic>ses-GarnettDream</italic>, which store the data of this participant reading two novels, respectively. Each of the two folders contains a folder <italic>eeg</italic> and one file <italic>sub-xx_scans.tsv</italic>. The tsv file contains information about the scanning time of each file. The <italic>eeg</italic> folder contains the source raw EEG data of several runs, channels, and marker events files. Each run includes an <italic>eeg.json</italic> file, which encompasses detailed information for that run, such as the sampling rate and the number of channels. Events are stored in <italic>events.tsv</italic> with onset and event ID. The EEG data is converted from raw metafile format (<italic>.mff</italic> file) to BrainVision format (<italic>.vhdr</italic>,<italic>.vmrk</italic> and<italic>.eeg</italic> files) since EEG-BIDS<sup><xref ref-type="bibr" rid="CR41">41</xref>,<xref ref-type="bibr" rid="CR42">42</xref></sup> is not officially compatible with <italic>.mff</italic> format. All data is formatted to EEG-BIDS<sup><xref ref-type="bibr" rid="CR41">41</xref>,<xref ref-type="bibr" rid="CR42">42</xref></sup> using the mne-bids v0.14<sup><xref ref-type="bibr" rid="CR42">42</xref>,<xref ref-type="bibr" rid="CR43">43</xref></sup> package in Python.</p><p id="Par34">The <italic>derivatives</italic> folder contains six folders: <italic>eyetracking_data</italic>, <italic>filtered_0.5_80</italic>, <italic>filtered_0.5_30</italic>, <italic>preproc</italic>, <italic>novels</italic>, and <italic>text_embeddings</italic>. The <italic>eyetracking_data</italic> folder contains all the eye-tracking data. Each eye-tracking data is formatted in a <italic>.rar</italic> file with eye moving trajectories and other parameters like sampling rate saved in different files. The <italic>filtered_0.5_80</italic> folder and <italic>filtered_0.5_30</italic> folder contain data that has been processed up to the pre-processing step of 0.5&#x02013;80&#x02009;Hz and 0.5&#x02013;30&#x02009;Hz band-pass filtering respectively. This data is suitable for researchers who have specific requirements and want to perform customized processing on subsequent pre-processing steps like ICA and re-referencing. The <italic>preproc</italic> folder contains minimally pre-processed EEG data that is processed using the whole pre-processing pipeline. It includes four additional types of files compared to the participants&#x02019; raw data folders in the root directory: i) <italic>bad_channels.json</italic> contains bad channels marked during bad channel rejection phase. ii) <italic>ica_components.npy</italic> stores the values of all independent components in the ICA phase. iii) <italic>ica_components.json</italic> includes the independent components excluded in ICA (the ICA random seed is fixed, allowing for reproducible results). iv) <italic>ica_components_topography.png</italic> is a picture of the topographic maps of all independent components, where the excluded components are labeled in grey. The <italic>novels</italic> folder contains the original and segmented text stimuli materials. The original novels are saved in .txt format and the segmented novels corresponding to each experimental run are saved in Excel (.xlsx) files. The <italic>text_embeddings</italic> folder contains embeddings of the two novels. The embeddings corresponding to each experimental run are stored in NumPy (.npy) files.</p></sec></sec><sec id="Sec14"><title>Technical Validation</title><sec id="Sec15"><title>Classic sensor-level EEG analysis</title><p id="Par35">The EEG data in the dataset<sup><xref ref-type="bibr" rid="CR26">26</xref>,<xref ref-type="bibr" rid="CR27">27</xref></sup> can be used to do classic time-frequency analysis. In this section, pre-processed EEG data was used to extract neural oscillations in different frequency bands. Specifically, we targeted the segment corresponding to the sentence &#x0201c;Draw me a sheep&#x0201d; in <italic>the Little Prince</italic> from the 0.5&#x02013;80&#x02009;Hz filtered pre-processed data of sub-07. The analysis was exclusively focused on the C3 electrode to investigate the neural activities at the scalp location overlying the temporal lobe, which is a language processing related area.</p><p id="Par36">To dissect the frequency components inherent in the C3 electrode&#x02019;s signal, we applied the Fast Fourier Transform (FFT) algorithm to the data. This mathematical technique transforms the time-domain signal into the frequency domain, revealing the spectrum of frequencies present in the neural recordings. We defined frequency bands of interest&#x02014;Theta (4&#x02013;8&#x02009;Hz), Alpha (8&#x02013;12&#x02009;Hz), Beta (12&#x02013;30&#x02009;Hz), and Gamma (30&#x02013;100&#x02009;Hz)&#x02014;to categorize the neural oscillations according to their respective frequency ranges.</p><p id="Par37">For each frequency band, we separated the components from the FFT results and conducted an inverse FFT to retrieve the time-domain signal representing the band&#x02019;s oscillatory activity. This step allows for the quantitative analysis of the amplitude of oscillations within each frequency band, offering insights into the neurophysiological activity in these specific ranges. The results of different frequency bands are shown in Fig.&#x000a0;<xref rid="Fig4" ref-type="fig">4</xref>.<fig id="Fig4"><label>Fig. 4</label><caption><p>EEG time course and the neural oscillations under different frequency bands (i.e., Theta, Alpha, Beta, and Gamma) corresponding to the Chinese sentence meaning &#x0201c;Draw me a sheep&#x0201d;. The pre-processed EEG data using 0.5&#x02013;80&#x02009;Hz band-pass filter from ses-LittlePrince of sub-07 was used in the analysis. We illustrated the EEG signals from electrode C3, which locates at a language processing related area overlying the temporal lobe.</p></caption><graphic xlink:href="41597_2024_3398_Fig4_HTML" id="d33e1273"/></fig></p></sec><sec id="Sec16"><title>EEG source reconstruction</title><p id="Par38">Apart from the sensor level analysis, the EEG data allows for conducting source localization. Here, three segments of the data were utilized as an example to perform the source-level analysis using the MNE package. The fsaverage MRI template (<ext-link ext-link-type="uri" xlink:href="https://surfer.nmr.mgh.harvard.edu/fswiki/FsAverage">https://surfer.nmr.mgh.harvard.edu/fswiki/FsAverage</ext-link>)<sup><xref ref-type="bibr" rid="CR44">44</xref></sup> in MNE package was utilized to complete the surface reconstruction process. A 3-layer Boundary Element Method (BEM) model with 15360 triangles and conductivities of 0.3&#x02009;S/m, 0.006&#x02009;S/m, and 0.3&#x02009;S/m for the brain, skull, and scalp compartments respectively was created. Source spaces consisted of 10242 sources per hemisphere. Three segments of the pre-processed EEG data with a band-pass frequency band of 0.5&#x02013;80&#x02009;Hz corresponding to one line displayed in the experiment were used to calculate the inverse solution. Inverse solutions were calculated using dynamic Statistical Parametric Maps (dSPM). The method was selected because it is widely used by researchers and is representative of currently used methods<sup><xref ref-type="bibr" rid="CR45">45</xref></sup>. We offer the code of source reconstruction in our GitHub repository. See Code availability section for detailed information.</p><p id="Par39">The visualization of the source activities is shown in Fig.&#x000a0;<xref rid="Fig5" ref-type="fig">5b</xref>. Results for the left and right hemispheres are presented separately. The moments of peak activation in the left and right brain regions are chosen for visualization. The source localization results for the first segment reveal a dispersed activation area, encompassing the anterior temporal lobe and temporo-parietal region, which are associated with language comprehension and primary processing<sup><xref ref-type="bibr" rid="CR46">46</xref></sup>. The results of the second segment exhibit more focused activation, particularly near the left middle temporal gyrus, an area (encompassing Wernicke&#x02019;s area) intimately related to language comprehension<sup><xref ref-type="bibr" rid="CR47">47</xref></sup>. The activation areas for the third segment are localized in the left temporal and frontal lobes, potentially representing high-level stages of language processing, including sentence construction, semantic processing, and language expression<sup><xref ref-type="bibr" rid="CR48">48</xref></sup>. Fig.&#x000a0;<xref rid="Fig5" ref-type="fig">5c</xref> presents plots of source activities over time, derived from 12 sources in the corresponding region with strongest activities. The first two curves in each plot correspond to sources in the left and right hemispheres that reach maximum peak values.<fig id="Fig5"><label>Fig. 5</label><caption><p>EEG source localization analysis. (<bold>a</bold>) EEG sensor-level data: Three segments of pre-processed EEG data using 0.5&#x02013;80&#x02009;Hz band-pass filter were selected for analysis, accompanied by the corresponding text segments shown above the EEG segments. (<bold>b</bold>) Visualization of brain activation after source analysis: The dSPM method was utilized to solve the inverse problem. Results for the left and right hemispheres are presented separately. The moments of peak activation in the left and right brain regions are chosen for visualization. (<bold>c</bold>) Plots of source activity over time: Each plot contains the activities of 12 sources in the region with the strongest activity.</p></caption><graphic xlink:href="41597_2024_3398_Fig5_HTML" id="d33e1327"/></fig></p></sec><sec id="Sec17"><title>Text embeddings with pre-trained language model</title><p id="Par40">To assist researchers in efficiently exploring the alignment between EEG and text representations, as well as in text decoding based on EEG, this study provides embeddings of two novels calculated using a pre-trained language model, accompanied by the code to compute these embeddings. This work employed Google&#x02019;s pre-trained language model BERT-base-Chinese<sup><xref ref-type="bibr" rid="CR28">28</xref></sup>. This model, pre-trained on Chinese corpora, effectively encodes Chinese semantic features. Given that Chinese characters are the smallest unit of composition in Chinese writing and cannot be further decomposed<sup><xref ref-type="bibr" rid="CR49">49</xref></sup>, bert-base-chinese adopts a character-based tokenization approach, treating each Chinese character as a token for embedding. During the experimental procedure, each displayed line of text contains <italic>n</italic> Chinese characters. The BERT-base-Chinese model processes these n Chinese characters, yielding an embedding of size <italic>(n</italic>, 768<italic>)</italic>, where <italic>n</italic> represents the number of Chinese characters, and <italic>768</italic> the dimensionality of the embedding. To ensure displayed lines of varying length to have embeddings of the same shape, the first dimension of the embeddings is averaged to standardize the embedding size to <italic>(1, 768)</italic> for each instance. This processing procedure was implemented using the Hugging Face Transformers v4.36.2<sup><xref ref-type="bibr" rid="CR50">50</xref></sup> package.</p></sec><sec id="Sec18"><title>Temporal alignment between EEG, text sequences, and eye-tracking data</title><p id="Par41">This section provides a comprehensive explanation on how to align the EEG data with its corresponding text content and eye-tracking data in the temporal domain.</p><p id="Par42">To facilitate semantic decoding, it is necessary to align specific text with its corresponding EEG segment in the temporal domain. During the marking process when collecting the data, the start and end of each line of the stimuli were annotated, thereby enabling the alignment of each text line with a corresponding segment of EEG data. Given the consistent highlighting duration for each character, the EEG segment can be equally divided to match the corresponding character. In the GitHub repository, we offer the script to align the EEG segments to their corresponding text and text embeddings.</p><p id="Par43">The recorded eye-tracking data can be aligned with EEG data to verify whether participants were focusing on the text as expected. The eye-tracking data captures both the scene viewed by the participants at each moment and the coordinates of the gaze points. During each experimental run, the marker &#x0201c;EYES&#x0201d; and &#x0201c;EYEE&#x0201d; were inserted into the EEG recordings when the eye-tracker was activated and deactivated. These markers enable precise alignment between the eye-tracking data and the EEG recordings. Once the alignment is complete, markers in the EEG recordings enable the extraction of specific eye-tracking data segments corresponding to particular EEG segments. These eye-tracking data segments can be used to check whether the eye fixation locations align with the anticipated positions on the screen, thus reflecting the quality of the EEG data.</p></sec></sec><sec id="Sec19"><title>Usage Notes</title><p id="Par44">The code for the experiment and data analysis has been uploaded to GitHub to facilitate sharing and utilization, which is accessible at <ext-link ext-link-type="uri" xlink:href="https://github.com/ncclabsustech/Chinese_reading_task_eeg_processing">https://github.com/ncclabsustech/Chinese_reading_task_eeg_processing</ext-link>.</p><p id="Par45">The code repository contains four main modules, each including scripts desired to reproduce the experiment and data analysis procedures. The script <italic>cut_chinese_novel.py</italic> in the <italic>novel_segmentation_and_text_embeddings</italic> folder contains the code to prepare the stimulation materials from source materials. The script <italic>play_novel.py</italic> in the <italic>experiment</italic> module contains code for the experiment, including text stimuli presentation and control of the EGI device and Tobii Pro&#x000a0;Glasses 3 eye-tracker. The script <italic>preprocessing.py</italic> in <italic>data_preprocessing_and_alignment</italic> module contains the main part of the code to apply pre-processing on EEG data. The script <italic>align_eeg_with_sentence.py</italic> in the same module contains code to align the EEG segments with corresponding text contents and text embeddings. The <italic>docker</italic> module contains the Docker image required for deploying and running the code, as well as tutorials on how to use Docker for environment deployment.</p><p id="Par46">The code for EEG data pre-processing is highly configurable, permitting flexible adjustments of various pre-processing parameters, such as data segmentation range, downsampling rate, filtering range, and choice of ICA algorithm, thereby ensuring convenience and efficiency. Researchers can modify and optimize this code according to their specific requirements.</p><p id="Par47">Before using our ChineseEEG dataset<sup><xref ref-type="bibr" rid="CR26">26</xref>,<xref ref-type="bibr" rid="CR27">27</xref></sup>, we encourage all users to check the <italic>README.md</italic> and the updated information in the GitHub repository.</p></sec><sec sec-type="supplementary-material"><sec id="Sec20"><title>Supplementary information</title><p>
<supplementary-material content-type="local-data" id="MOESM1"><media xlink:href="41597_2024_3398_MOESM1_ESM.docx"><caption><p>Human data submission checklist</p></caption></media></supplementary-material>
</p></sec></sec></body><back><fn-group><fn><p><bold>Publisher&#x02019;s note</bold> Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></fn><fn><p>These authors contributed equally: Xinyu Mou, Cuilin He, Liwei Tan.</p></fn></fn-group><sec><title>Supplementary information</title><p>The online version contains supplementary material available at 10.1038/s41597-024-03398-7.</p></sec><ack><title>Acknowledgements</title><p>This work was mainly supported by the MindD project of Tianqiao and Chrissy Chen Institute(TCCI), the Science and Technology Development Fund (FDCT) of Macau [0127/2020/A3, 0041/2022/A], the Natural Science Foundation of Guangdong Province(2021A1515012509), Shenzhen-Hong Kong-Macao Science and Technology Innovation Project (Category C) (SGDX2020110309280100), and the SRG of University of Macau (SRG2020-00027-ICI). We also thank all research assistants who provided general support in participant recruiting and data collection.</p></ack><notes notes-type="author-contribution"><title>Author contributions</title><p>H.Wu, Q.Liu and X.Wang designed the study, H.Wu, Q.Liu and X.Wang, X.Mou, C.He, and L.Tan designed the experiments, [movie, Chinese text&#x02026;], X.Mou, C.He and L.Tan, H.Liang and J.Zhang conducted the experiments, X.Mou, C.He, L.Tan, H.Liang, J.Zhang and J.Yu analyzed the results. X.Mou, C.He and L.Tan wrote the first draft. All authors checked the code, wrote the manuscript, reviewed the manuscript, and approved the final manuscript.</p></notes><notes notes-type="data-availability"><title>Code availability</title><p>The code for all modules is openly available on GitHub (<ext-link ext-link-type="uri" xlink:href="https://github.com/ncclabsustech/Chinese_reading_task_eeg_processing">https://github.com/ncclabsustech/Chinese_reading_task_eeg_processing</ext-link>). All scripts were developed in Python 3.10<sup><xref ref-type="bibr" rid="CR51">51</xref></sup>. Package openpyxl v3.1.2 was utilized to export segmented text in Excel (.xlsx) files, and egi-pynetstation v1.0.1, g3pylib v0.1.1, psychopy v2023.2.3<sup><xref ref-type="bibr" rid="CR36">36</xref></sup> were used to implement the scripts for EGI device control, Tobii eye-tracker control, stimuli presentation respectively. In the data pre-processing scripts, MNE v1.6.0<sup><xref ref-type="bibr" rid="CR38">38</xref></sup>, pybv v0.7.5<sup><xref ref-type="bibr" rid="CR52">52</xref></sup>, pyprep v0.4.3<sup><xref ref-type="bibr" rid="CR39">39</xref></sup>, mne-iclabel v0.5.1<sup><xref ref-type="bibr" rid="CR40">40</xref></sup> were used to implement the pre-processing pipeline, while mne-bids v0.14<sup><xref ref-type="bibr" rid="CR42">42</xref>,<xref ref-type="bibr" rid="CR43">43</xref></sup> was used to organize the data into BIDS<sup><xref ref-type="bibr" rid="CR41">41</xref>,<xref ref-type="bibr" rid="CR42">42</xref></sup> format. The text embeddings were calculated using Hugging Face transformers v4.36.2<sup><xref ref-type="bibr" rid="CR50">50</xref></sup>. For more details about code usage, please refer to the GitHub repository.</p></notes><notes id="FPar1" notes-type="COI-statement"><title>Competing interests</title><p id="Par48">The authors declare no competing interests.</p></notes><ref-list id="Bib1"><title>References</title><ref id="CR1"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friederici</surname><given-names>AD</given-names></name></person-group><article-title>The brain basis of language processing: From structure to function</article-title><source>Physiological Reviews</source><year>2011</year><volume>91</volume><fpage>1357</fpage><lpage>1392</lpage><pub-id pub-id-type="doi">10.1152/physrev.00006.2011</pub-id><?supplied-pmid 22013214?><pub-id pub-id-type="pmid">22013214</pub-id>
</element-citation></ref><ref id="CR2"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frisby</surname><given-names>SL</given-names></name><name><surname>Halai</surname><given-names>AD</given-names></name><name><surname>Cox</surname><given-names>CR</given-names></name><name><surname>Lambon Ralph</surname><given-names>MA</given-names></name><name><surname>Rogers</surname><given-names>TT</given-names></name></person-group><article-title>Decoding semantic representations in mind and brain</article-title><source>Trends in Cognitive Sciences</source><year>2023</year><volume>27</volume><fpage>258</fpage><lpage>281</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2022.12.006</pub-id><?supplied-pmid 36631371?><pub-id pub-id-type="pmid">36631371</pub-id>
</element-citation></ref><ref id="CR3"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pereira</surname><given-names>F</given-names></name><etal/></person-group><article-title>Toward a universal decoder of linguistic meaning from brain activation</article-title><source>Nature Communications</source><year>2018</year><volume>9</volume><fpage>963</fpage><pub-id pub-id-type="doi">10.1038/s41467-018-03068-4</pub-id><?supplied-pmid 29511192?><pub-id pub-id-type="pmid">29511192</pub-id>
</element-citation></ref><ref id="CR4"><label>4.</label><mixed-citation publication-type="other">Wang, Y., Ji, Q., Zhou, C. &#x00026; Wang, Y. Brain mechanisms linking language processing and open motor skill training. <italic>Frontiers in Human Neuroscience</italic><bold>16</bold> (2022).</mixed-citation></ref><ref id="CR5"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fuseda</surname><given-names>K</given-names></name><etal/></person-group><article-title>Impact of depressed state on attention and language processing during news broadcasts: EEG analysis and machine learning approach</article-title><source>Scientific Report</source><year>2022</year><volume>12</volume><fpage>20492</fpage><pub-id pub-id-type="doi">10.1038/s41598-022-24319-x</pub-id></element-citation></ref><ref id="CR6"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McClelland</surname><given-names>JL</given-names></name><name><surname>Hill</surname><given-names>F</given-names></name><name><surname>Rudolph</surname><given-names>M</given-names></name><name><surname>Baldridge</surname><given-names>J</given-names></name><name><surname>Sch&#x000fc;tze</surname><given-names>H</given-names></name></person-group><article-title>Placing language in an integrated understanding system: Next steps toward human-level performance in neural language models</article-title><source>Proceedings of the National Academy of Sciences</source><year>2020</year><volume>117</volume><fpage>25966</fpage><lpage>25974</lpage><pub-id pub-id-type="doi">10.1073/pnas.1910416117</pub-id></element-citation></ref><ref id="CR7"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tang</surname><given-names>J</given-names></name><etal/></person-group><article-title>Semantic reconstruction of continuous language from non-invasive brain recordings</article-title><source>Nature Neuroscience</source><year>2023</year><volume>26</volume><fpage>858</fpage><lpage>866</lpage><pub-id pub-id-type="doi">10.1038/s41593-023-01304-9</pub-id><?supplied-pmid 37127759?><pub-id pub-id-type="pmid">37127759</pub-id>
</element-citation></ref><ref id="CR8"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sun</surname><given-names>P</given-names></name><name><surname>Anumanchipalli</surname><given-names>GK</given-names></name><name><surname>Chang</surname><given-names>EF</given-names></name></person-group><article-title>Brain2char: a deep architecture for decoding text from brain recordings</article-title><source>Journal of neural engineering</source><year>2020</year><volume>17</volume><fpage>066015</fpage><pub-id pub-id-type="doi">10.1088/1741-2552/abc742</pub-id></element-citation></ref><ref id="CR9"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>D&#x000e9;fossez</surname><given-names>A</given-names></name><name><surname>Caucheteux</surname><given-names>C</given-names></name><name><surname>Rapin</surname><given-names>J</given-names></name><name><surname>Kabeli</surname><given-names>O</given-names></name><name><surname>King</surname><given-names>J-R</given-names></name></person-group><article-title>Decoding speech perception from non-invasive brain recordings</article-title><source>Nature Machine Intelligence</source><year>2023</year><volume>5</volume><fpage>1097</fpage><lpage>1107</lpage><pub-id pub-id-type="doi">10.1038/s42256-023-00714-5</pub-id></element-citation></ref><ref id="CR10"><label>10.</label><mixed-citation publication-type="other">Jeong, J., Cho, J., Lee, B. &#x00026; Lee, S. Real-time deep neurolinguistic learning enhances noninvasive neural language decoding for brain&#x02013;machine interaction. <italic>IEEE. Trans. Cybern</italic>. (2022).</mixed-citation></ref><ref id="CR11"><label>11.</label><mixed-citation publication-type="other">Artemova, E., Bakarov, A., Artemov, A., Burnaev, E. &#x00026; Sharaev, M. Data-driven models and computational tools for neurolinguistics: a language technology perspective. Preprint at <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2003.10540">https://arxiv.org/abs/2003.10540</ext-link> (2020).</mixed-citation></ref><ref id="CR12"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zock</surname><given-names>M</given-names></name></person-group><article-title>AI at the crossroads of NLP and neurosciences</article-title><source>Journal of Cognitive Science</source><year>2020</year><volume>21</volume><fpage>1</fpage><lpage>14</lpage></element-citation></ref><ref id="CR13"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Herff</surname><given-names>C</given-names></name><etal/></person-group><article-title>Brain-to-text: Decoding spoken phrases from phone representations in the brain</article-title><source>Frontiers in Neuroscience</source><year>2015</year><volume>9</volume><fpage>217</fpage><pub-id pub-id-type="doi">10.3389/fnins.2015.00217</pub-id><?supplied-pmid 26124702?><pub-id pub-id-type="pmid">26124702</pub-id>
</element-citation></ref><ref id="CR14"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Anumanchipalli</surname><given-names>GK</given-names></name><name><surname>Chartier</surname><given-names>J</given-names></name><name><surname>Chang</surname><given-names>EF</given-names></name></person-group><article-title>Speech synthesis from neural decoding of spoken sentences</article-title><source>Nature</source><year>2019</year><volume>568</volume><fpage>493</fpage><lpage>498</lpage><pub-id pub-id-type="doi">10.1038/s41586-019-1119-1</pub-id><?supplied-pmid 31019317?><pub-id pub-id-type="pmid">31019317</pub-id>
</element-citation></ref><ref id="CR15"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Makin</surname><given-names>JG</given-names></name><name><surname>Moses</surname><given-names>DA</given-names></name><name><surname>Chang</surname><given-names>EF</given-names></name></person-group><article-title>Machine translation of cortical activity to text with an encoder&#x02013;decoder framework</article-title><source>Nature Neuroscience</source><year>2020</year><volume>23</volume><fpage>575</fpage><lpage>582</lpage><pub-id pub-id-type="doi">10.1038/s41593-020-0608-8</pub-id><?supplied-pmid 32231340?><pub-id pub-id-type="pmid">32231340</pub-id>
</element-citation></ref><ref id="CR16"><label>16.</label><mixed-citation publication-type="other">Sun, J., Wang, S., Zhang, J. &#x00026; Zong, C. Towards sentence-level brain decoding with distributed representations. In <italic>AAAI. Conf. Artif. Intell</italic>., 7047&#x02013;7054 (2019).</mixed-citation></ref><ref id="CR17"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sarker</surname><given-names>I</given-names></name></person-group><article-title>Deep learning: A comprehensive overview on techniques, taxonomy, applications and research directions</article-title><source>SN comput. sci.</source><year>2021</year><volume>2</volume><fpage>420</fpage><pub-id pub-id-type="doi">10.1007/s42979-021-00815-1</pub-id><?supplied-pmid 34426802?><pub-id pub-id-type="pmid">34426802</pub-id>
</element-citation></ref><ref id="CR18"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Teplan</surname><given-names>M</given-names></name><etal/></person-group><article-title>Fundamentals of EEG measurement</article-title><source>Meas. Sci. Rev.</source><year>2002</year><volume>2</volume><fpage>1</fpage><lpage>11</lpage></element-citation></ref><ref id="CR19"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dietrich</surname><given-names>A</given-names></name><name><surname>Kanso</surname><given-names>R</given-names></name></person-group><article-title>A review of EEG, ERP, and neuroimaging studies of creativity and insight</article-title><source>Psychol. Bull.</source><year>2010</year><volume>136</volume><fpage>822</fpage><pub-id pub-id-type="doi">10.1037/a0019749</pub-id><?supplied-pmid 20804237?><pub-id pub-id-type="pmid">20804237</pub-id>
</element-citation></ref><ref id="CR20"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grootswagers</surname><given-names>T</given-names></name><etal/></person-group><article-title>Human EEG recordings for 1,854 concepts presented in rapid serial visual presentation streams</article-title><source>Sci. Data.</source><year>2022</year><volume>9</volume><fpage>3</fpage><pub-id pub-id-type="doi">10.1038/s41597-021-01102-7</pub-id><?supplied-pmid 35013331?><pub-id pub-id-type="pmid">35013331</pub-id>
</element-citation></ref><ref id="CR21"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gifford</surname><given-names>AT</given-names></name><name><surname>Dwivedi</surname><given-names>K</given-names></name><name><surname>Roig</surname><given-names>G</given-names></name><name><surname>Cichy</surname><given-names>RM</given-names></name></person-group><article-title>A large and rich EEG dataset for modeling human visual object recognition</article-title><source>NeuroImage</source><year>2022</year><volume>264</volume><fpage>119754</fpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2022.119754</pub-id><?supplied-pmid 36400378?><pub-id pub-id-type="pmid">36400378</pub-id>
</element-citation></ref><ref id="CR22"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Telesford</surname><given-names>Q</given-names></name><etal/></person-group><article-title>An open-access dataset of naturalistic viewing using simultaneous EEG-fMRI</article-title><source>Sci. Data.</source><year>2023</year><volume>10</volume><fpage>554</fpage><pub-id pub-id-type="doi">10.1038/s41597-023-02458-8</pub-id><?supplied-pmid 37612297?><pub-id pub-id-type="pmid">37612297</pub-id>
</element-citation></ref><ref id="CR23"><label>23.</label><mixed-citation publication-type="other">Liu, J., Feng, K., Song, L. &#x00026; Zeng, X. A visual EEG paradigm and dataset for recognizing the size transformation of images. In <italic>2021 International Conference on Networking Systems of AI (INSAI)</italic>, 171&#x02013;175 (IEEE, 2021).</mixed-citation></ref><ref id="CR24"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hollenstein</surname><given-names>N</given-names></name><etal/></person-group><article-title>ZuCo, a simultaneous EEG and eye-tracking resource for natural sentence reading</article-title><source>Sci. Data.</source><year>2018</year><volume>5</volume><fpage>1</fpage><lpage>13</lpage><pub-id pub-id-type="doi">10.1038/sdata.2018.291</pub-id><pub-id pub-id-type="pmid">30482902</pub-id>
</element-citation></ref><ref id="CR25"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Siok</surname><given-names>WT</given-names></name><name><surname>Perfetti</surname><given-names>CA</given-names></name><name><surname>Jin</surname><given-names>Z</given-names></name><name><surname>Tan</surname><given-names>LH</given-names></name></person-group><article-title>Biological abnormality of impaired reading is constrained by culture</article-title><source>Nature</source><year>2004</year><volume>431</volume><fpage>71</fpage><lpage>76</lpage><pub-id pub-id-type="doi">10.1038/nature02865</pub-id><?supplied-pmid 15343334?><pub-id pub-id-type="pmid">15343334</pub-id>
</element-citation></ref><ref id="CR26"><label>26.</label><element-citation publication-type="data"><name><surname>Mou</surname><given-names>X</given-names></name><etal/><year>2024</year><data-title>ChineseEEG: A Chinese linguistic corpora EEG dataset for semantic alignment and neural decoding</data-title><source>Scidb</source><pub-id pub-id-type="doi">10.57760/sciencedb.CHNNeuro.00007</pub-id></element-citation></ref><ref id="CR27"><label>27.</label><element-citation publication-type="data"><name><surname>Mou</surname><given-names>X</given-names></name><etal/><year>2024</year><data-title>ChineseEEG: A Chinese linguistic corpora EEG dataset for semantic alignment and neural decoding</data-title><source>Openneuro</source><pub-id pub-id-type="doi">10.18112/openneuro.ds004952.v1.2.0</pub-id></element-citation></ref><ref id="CR28"><label>28.</label><mixed-citation publication-type="other">Devlin, J., Chang, M.-W., Lee, K. &#x00026; Toutanova, K. BERT: Pre-training of deep bidirectional transformers for language understanding. In <italic>North American Chapter of the Association for Computational Linguistics</italic> (2019).</mixed-citation></ref><ref id="CR29"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xu</surname><given-names>M</given-names></name><name><surname>Li</surname><given-names>D</given-names></name><name><surname>Li</surname><given-names>P</given-names></name></person-group><article-title>Brain decoding in multiple languages: Can cross-language brain decoding work?</article-title><source>Brain and Language</source><year>2021</year><volume>215</volume><fpage>104922</fpage><pub-id pub-id-type="doi">10.1016/j.bandl.2021.104922</pub-id><?supplied-pmid 33556764?><pub-id pub-id-type="pmid">33556764</pub-id>
</element-citation></ref><ref id="CR30"><label>30.</label><mixed-citation publication-type="other">Hollenstein, N. <italic>et al</italic>. Decoding EEG brain activity for multi-modal natural language processing. <italic>Frontiers in Human Neuroscience</italic><bold>15</bold> (2021).</mixed-citation></ref><ref id="CR31"><label>31.</label><mixed-citation publication-type="other">Stehwien, S., Henke, L., Hale, J., Brennan, J. &#x00026; Meyer, L. The Little Prince in 26 languages: Towards a multilingual neuro-cognitive corpus. In Chersoni, E., Devereux, B. &#x00026; Huang, C.-R. (eds.) <italic>Proceedings of the Second Workshop on Linguistic and Neurocognitive Resources</italic>, 43&#x02013;49 (European Language Resources Association, 2020).</mixed-citation></ref><ref id="CR32"><label>32.</label><mixed-citation publication-type="other">Bhattasali, S., Brennan, J., Luh, W.-M., Franzluebbers, B. &#x00026; Hale, J. The Alice datasets: fMRI &#x00026; EEG observations of natural language comprehension. In <italic>Proceedings of the Twelfth Language Resources and Evaluation Conference</italic>, 120&#x02013;125 (European Language Resources Association, 2020).</mixed-citation></ref><ref id="CR33"><label>33.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>S</given-names></name><etal/></person-group><article-title>A synchronized multimodal neuroimaging dataset for studying brain language processing</article-title><source>Sci. Data.</source><year>2022</year><volume>9</volume><fpage>590</fpage><pub-id pub-id-type="doi">10.1038/s41597-022-01708-5</pub-id><?supplied-pmid 36180444?><pub-id pub-id-type="pmid">36180444</pub-id>
</element-citation></ref><ref id="CR34"><label>34.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>J</given-names></name><etal/></person-group><article-title>Le Petit Prince multilingual naturalistic fMRI corpus</article-title><source>Sci. Data.</source><year>2022</year><volume>9</volume><fpage>530</fpage><pub-id pub-id-type="doi">10.1038/s41597-022-01625-7</pub-id><?supplied-pmid 36038567?><pub-id pub-id-type="pmid">36038567</pub-id>
</element-citation></ref><ref id="CR35"><label>35.</label><mixed-citation publication-type="other">Wang, J., Chen, H.-C., Radach, R. &#x00026; Inhoff, A. <italic>Reading Chinese script: A cognitive analysis</italic> (Psychology Press, 1999).</mixed-citation></ref><ref id="CR36"><label>36.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peirce</surname><given-names>J</given-names></name><etal/></person-group><article-title>Psychopy2: Experiments in behavior made easy</article-title><source>Behavior Research Methods</source><year>2019</year><volume>51</volume><fpage>195</fpage><lpage>203</lpage><pub-id pub-id-type="doi">10.3758/s13428-018-01193-y</pub-id><?supplied-pmid 30734206?><pub-id pub-id-type="pmid">30734206</pub-id>
</element-citation></ref><ref id="CR37"><label>37.</label><mixed-citation publication-type="other">Chen, K. <italic>et al</italic>. A resource for assessing dynamic binary choices in the adult brain using EEG and mouse-tracking. <italic>Sci. Data</italic>. <bold>9</bold> (2022).</mixed-citation></ref><ref id="CR38"><label>38.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gramfort</surname><given-names>A</given-names></name><etal/></person-group><article-title>MNE software for processing MEG and EEG data</article-title><source>NeuroImage</source><year>2014</year><volume>86</volume><fpage>446</fpage><lpage>460</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.10.027</pub-id><?supplied-pmid 24161808?><pub-id pub-id-type="pmid">24161808</pub-id>
</element-citation></ref><ref id="CR39"><label>39.</label><mixed-citation publication-type="other">Bigdely-Shamlo, N., Mullen, T., Kothe, C., Su, K.-M. &#x00026; Robbins, K. A. The PREP pipeline: standardized preprocessing for large-scale EEG analysis. <italic>Front. Neuroinform</italic>. <bold>9</bold> (2015).</mixed-citation></ref><ref id="CR40"><label>40.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>A</given-names></name><name><surname>Feitelberg</surname><given-names>J</given-names></name><name><surname>Saini</surname><given-names>AP</given-names></name><name><surname>H&#x000f6;chenberger</surname><given-names>R</given-names></name><name><surname>Scheltienne</surname><given-names>M</given-names></name></person-group><article-title>MNE-ICALabel: Automatically annotating ICA components with ICLabel in Python</article-title><source>Journal of Open Source Software</source><year>2022</year><volume>7</volume><fpage>4484</fpage><pub-id pub-id-type="doi">10.21105/joss.04484</pub-id></element-citation></ref><ref id="CR41"><label>41.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gorgolewski</surname><given-names>K</given-names></name><etal/></person-group><article-title>The brain imaging data structure, a format for organizing and describing outputs of neuroimaging experiments</article-title><source>Scientific Data</source><year>2016</year><volume>3</volume><fpage>160044</fpage><pub-id pub-id-type="doi">10.1038/sdata.2016.44</pub-id><?supplied-pmid 27326542?><pub-id pub-id-type="pmid">27326542</pub-id>
</element-citation></ref><ref id="CR42"><label>42.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pernet</surname><given-names>CR</given-names></name><etal/></person-group><article-title>EEG-BIDS, an extension to the brain imaging data structure for electroencephalography</article-title><source>Sci. Data.</source><year>2019</year><volume>6</volume><fpage>1</fpage><lpage>5</lpage><pub-id pub-id-type="doi">10.1038/s41597-019-0104-8</pub-id><pub-id pub-id-type="pmid">30647409</pub-id>
</element-citation></ref><ref id="CR43"><label>43.</label><mixed-citation publication-type="other">Appelhoff, S. <italic>et al</italic>. MNE-BIDS: Organizing electrophysiological data into the BIDS format and facilitating their analysis. <italic>Journal of Open Source Software</italic><bold>4</bold> (2019).</mixed-citation></ref><ref id="CR44"><label>44.</label><mixed-citation publication-type="other">Fischl, B. R., Sereno, M. I., Tootell, R. B. H. &#x00026; Dale, A. M. High&#x02014;resolution intersubject averaging and a coordinate system for the cortical surface. <italic>Human Brain Mapping</italic><bold>8</bold> (1999).</mixed-citation></ref><ref id="CR45"><label>45.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grech</surname><given-names>R</given-names></name><etal/></person-group><article-title>Review on solving the inverse problem in EEG source analysis</article-title><source>Journal of NeuroEngineering and Rehabilitation</source><year>2008</year><volume>5</volume><fpage>1</fpage><lpage>33</lpage><pub-id pub-id-type="doi">10.1186/1743-0003-5-25</pub-id><pub-id pub-id-type="pmid">18171465</pub-id>
</element-citation></ref><ref id="CR46"><label>46.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bi</surname><given-names>Y</given-names></name><etal/></person-group><article-title>The role of the left anterior temporal lobe in language processing revisited: Evidence from an individual with ATL resection</article-title><source>Cortex.</source><year>2011</year><volume>47</volume><fpage>575</fpage><lpage>587</lpage><pub-id pub-id-type="doi">10.1016/j.cortex.2009.12.002</pub-id><?supplied-pmid 20074721?><pub-id pub-id-type="pmid">20074721</pub-id>
</element-citation></ref><ref id="CR47"><label>47.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pobric</surname><given-names>G</given-names></name><name><surname>Ralph</surname><given-names>MAL</given-names></name><name><surname>Jefferies</surname><given-names>E</given-names></name></person-group><article-title>The role of the anterior temporal lobes in the comprehension of concrete and abstract words: rTMS evidence</article-title><source>Cortex.</source><year>2009</year><volume>45</volume><fpage>1104</fpage><lpage>1110</lpage><pub-id pub-id-type="doi">10.1016/j.cortex.2009.02.006</pub-id><?supplied-pmid 19303592?><pub-id pub-id-type="pmid">19303592</pub-id>
</element-citation></ref><ref id="CR48"><label>48.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fedorenko</surname><given-names>E</given-names></name><name><surname>Behr</surname><given-names>MK</given-names></name><name><surname>Kanwisher</surname><given-names>NG</given-names></name></person-group><article-title>Functional specificity for high-level linguistic processing in the human brain</article-title><source>Proceedings of the National Academy of Sciences</source><year>2011</year><volume>108</volume><fpage>16428</fpage><lpage>16433</lpage><pub-id pub-id-type="doi">10.1073/pnas.1112937108</pub-id></element-citation></ref><ref id="CR49"><label>49.</label><mixed-citation publication-type="other">Po-Ching, Y. &#x00026; Rimmington, D. <italic>Chinese: A comprehensive grammar</italic> (Routledge, 2015).</mixed-citation></ref><ref id="CR50"><label>50.</label><mixed-citation publication-type="other">Wolf, T. <italic>et al</italic>. Transformers: State-of-the-art natural language processing. In <italic>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</italic>, 38&#x02013;45 (Online, 2020).</mixed-citation></ref><ref id="CR51"><label>51.</label><mixed-citation publication-type="other">Van Rossum, G. &#x00026; Drake, F. L. <italic>Python 3 Reference Manual</italic> (CreateSpace, Scotts Valley, CA, 2009).</mixed-citation></ref><ref id="CR52"><label>52.</label><mixed-citation publication-type="other">Appelhoff, S. <italic>et al</italic>. pybv &#x02013; A lightweight I/O utility for the BrainVision data format.</mixed-citation></ref></ref-list></back></article>