<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd"> 
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN?><?DTDIdentifier.IdentifierType public?><?SourceDTD.DTDName JATS-journalpublishing1.dtd?><?SourceDTD.Version 1.2?><?ConverterInfo.XSLTName jats2jats3.xsl?><?ConverterInfo.Version 1?><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Natl Sci Rev</journal-id><journal-id journal-id-type="iso-abbrev">Natl Sci Rev</journal-id><journal-id journal-id-type="publisher-id">nsr</journal-id><journal-title-group><journal-title>National Science Review</journal-title></journal-title-group><issn pub-type="ppub">2095-5138</issn><issn pub-type="epub">2053-714X</issn><publisher><publisher-name>Oxford University Press</publisher-name></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmcid">10862086</article-id><article-id pub-id-type="doi">10.1093/nsr/nwad317</article-id><article-id pub-id-type="publisher-id">nwad317</article-id><article-categories><subj-group subj-group-type="heading"><subject>Brief Communication</subject><subj-group subj-group-type="category-toc-heading"><subject>Information Science</subject></subj-group></subj-group><subj-group subj-group-type="category-taxonomy-collection"><subject>Nsr/3</subject></subj-group><subj-group subj-group-type="category-taxonomy-collection"><subject>AcademicSubjects/MED00010</subject><subject>AcademicSubjects/SCI00010</subject></subj-group></article-categories><title-group><article-title>Emergence of machine language: towards symbolic intelligence with neural networks</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-6360-1431</contrib-id><name><surname>Wang</surname><given-names>Yuqi</given-names></name><aff>
<institution>State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences</institution>, <country country="CN">China</country></aff></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Zhang</surname><given-names>Xu-Yao</given-names></name><!--xyz@nlpr.ia.ac.cn--><aff>
<institution>State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences</institution>, <country country="CN">China</country></aff><xref rid="cor2" ref-type="corresp"/></contrib><contrib contrib-type="author"><name><surname>Liu</surname><given-names>Cheng-Lin</given-names></name><aff>
<institution>State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences</institution>, <country country="CN">China</country></aff></contrib><contrib contrib-type="author"><name><surname>Tan</surname><given-names>Tieniu</given-names></name><aff>
<institution>State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences</institution>, <country country="CN">China</country></aff></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Zhang</surname><given-names>Zhaoxiang</given-names></name><!--zhaoxiang.zhang@ia.ac.cn--><aff>
<institution>State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences</institution>, <country country="CN">China</country></aff><xref rid="cor1" ref-type="corresp"/></contrib></contrib-group><author-notes><corresp id="cor1">
<bold>Corresponding authors.</bold> E-mails: <email>zhaoxiang.zhang@ia.ac.cn</email></corresp><corresp id="cor2">
<bold>Corresponding authors.</bold> E-mails: <email>xyz@nlpr.ia.ac.cn</email></corresp></author-notes><pub-date pub-type="collection"><month>4</month><year>2024</year></pub-date><pub-date pub-type="epub" iso-8601-date="2024-01-02"><day>02</day><month>1</month><year>2024</year></pub-date><pub-date pub-type="pmc-release"><day>02</day><month>1</month><year>2024</year></pub-date><volume>11</volume><issue>4</issue><elocation-id>nwad317</elocation-id><history><date date-type="received"><day>25</day><month>7</month><year>2023</year></date><date date-type="rev-recd"><day>02</day><month>11</month><year>2023</year></date><date date-type="corrected-typeset"><day>10</day><month>1</month><year>2024</year></date></history><permissions><copyright-statement>&#x000a9; The Author(s) 2024. Published by Oxford University Press on behalf of China Science Publishing &#x00026; Media Ltd.</copyright-statement><copyright-year>2024</copyright-year><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p></license></permissions><self-uri xlink:href="nwad317.pdf"/><abstract abstract-type="teaser"><p>Inspired by human language, machine language is a novel discrete representation learned from visual data only through playing the speak, guess, and draw game.</p></abstract><funding-group><award-group award-type="grant"><funding-source>
<institution-wrap><institution>National Key Research and Development Program of China</institution><institution-id institution-id-type="DOI">10.13039/501100012166</institution-id></institution-wrap>
</funding-source><award-id>2022ZD0160102</award-id></award-group><award-group award-type="grant"><funding-source>
<institution-wrap><institution>National Natural Science Foundation of China</institution><institution-id institution-id-type="DOI">10.13039/501100001809</institution-id></institution-wrap>
</funding-source><award-id>61836014</award-id><award-id>U21B2042</award-id><award-id>62072457</award-id><award-id>62006231</award-id></award-group></funding-group><counts><page-count count="3"/></counts></article-meta></front><body><p>Representation learning is a core issue in artificial intelligence (AI). Currently, there exists a disparity in the choice of representation between humans and machines. Humans rely on discrete language for communication and learning, whereas machines utilize continuous features for computation and representation. Discrete symbols are low-dimensional, decoupled and offer robust reasoning abilities, while continuous features are high-dimensional, coupled and possess remarkable abstracting capabilities. In recent years, deep learning&#x000a0;[<xref rid="bib1" ref-type="bibr">1</xref>] has developed the idea of continuous representation to the extreme, using billions of parameters to achieve high accuracies. Although this is reasonable from a statistical perspective, it has other major problems, such as a lack of interpretability, poor generalization and being easily attacked. Both paradigms have strengths and weaknesses, and a better choice is to seek reconciliation.</p><p>Inspired by the strengths of human language, we propose a novel approach that combines deep neural networks with symbolic intelligence to create a new form of representation called &#x02018;machine language&#x02019;. We aim to create a language specifically tailored for machines, combining deep neural networks with symbolic reasoning. Through this fusion, we aim to create a representation that inherits the reasoning abilities of discrete symbols and the abstracting capabilities of continuous features, thereby leveraging the advantages of both paradigms.</p><p>Human language is a highly complex and dynamic system that evolves alongside social and cultural changes. Our focus is primarily on the emergence of machine language, specifically from a semantic perspective. Drawing inspiration from the characteristics of human language, we propose three essential properties that machine language should possess. Similar to the early languages found in tribes, which may have been simple in form and grammar, these basic properties are fundamental for the development of a language.</p><list list-type="order"><list-item><p>
<italic toggle="yes">Spontaneous.</italic> The process of language emergence should be spontaneous, resembling the natural evolution of language in early human communities. It should not depend on prior knowledge of human language or require additional data annotations. The development of machine language should be unsupervised or self-supervised&#x000a0;[<xref rid="bib2" ref-type="bibr">2</xref>], occurring through interactions with others and the environment.</p></list-item><list-item><p>
<italic toggle="yes">Flexible.</italic> The form of machine language should exhibit flexibility, characterized by variable-length discrete symbol sequences. This variability is essential because different individuals may describe the same objects or concepts using varying lengths of language, ranging from concise to elaborate expressions.</p></list-item><list-item><p>
<italic toggle="yes">Semantic.</italic> A language, in the context of machine language, should possess semantics that can be conveyed through the permutation and combination of basic symbols. It should enable machines to communicate and comprehend information, allowing them to perform specific tasks such as describing objects or providing instructions.</p></list-item></list><p>To achieve the aforementioned objectives, a basic idea is to leverage the cooperation among multiple agents to facilitate the automatic learning of a language. This process entails multiple agents working together to solve various tasks within complex environments&#x000a0;[<xref rid="bib3" ref-type="bibr">3&#x02013;6</xref>]. We begin by simulating this process in the simplest scenario of a two-agent game, aiming to generate a language through their interactions. As depicted in Fig.&#x000a0;<xref rid="fig1" ref-type="fig">1</xref>, two agents, speaker A and listener B, engage in a collaborative game. The process of language emergence can be divided into three stages, represented by the three scenes illustrated as follows.</p><list list-type="order"><list-item><p>
<italic toggle="yes">Perception.</italic> Agent A perceives a target image, which in this case is a bird sitting on a tree.</p></list-item><list-item><p>
<italic toggle="yes">Communication.</italic> Agent A and agent B engage in a communication exchange, utilizing a sequence of symbols to convey information. Agent A, having observed the bird on the tree, attempts to describe what he saw using language. The symbols used in the communication are the targets that both agents aim to learn and understand.</p></list-item><list-item><p>
<italic toggle="yes">Cooperation.</italic> Agent A and agent B collaborate to solve tasks based on their communication. In this scenario, agent B needs to interpret agent&#x000a0;A&#x02019;s description and identify the bird sitting on the tree based on the communicated symbols. The successful cooperation between the two agents allows them to complete tasks effectively. Specifically, agent B&#x02019;s tasks involve understanding the language used in communication and making accurate guesses about what agent A is describing. Additionally, agent B needs to visually represent his understanding by drawing the original image.</p></list-item></list><fig position="float" id="fig1"><label>Figure 1.</label><caption><p>(a) The emergence of language is facilitated through the speak, guess and draw game, depicted from left to right. Given a random image, agent A attempts to describe it using the novel machine language. Agent B, the listener, must comprehend the language and accurately guess what A is describing. Simultaneously, B draws the image based on their understanding. (b) The network structure of the speaker and listener is based on an encoder&#x02013;decoder architecture. The speaker first perceives an image and generates a sequence of symbols representing the machine language. Subsequently, the listener receives the machine language as input, and outputs a query to make a guess for the correct target within a batch. Additionally, the listener draws the image according to the comprehension of the machine language. (c) To evaluate the effectiveness of our approach, we conduct experiments on five datasets. The left plot illustrates the training accuracy across learning epochs, demonstrating a clear improvement in guessing accuracy with the aid of machine language. The right plot compares the test accuracy compared to random guessing.</p></caption><graphic xlink:href="nwad317fig1" position="float"/></fig><p>Rewards are provided for successful performance in the game, while punishments are given for poor performance. This game is referred to as the speak, guess and draw (SGD) game in this paper. The game can be characterized by a tuple <italic toggle="yes">G</italic> = &#x02329;<italic toggle="yes">D, V, R, A<sub>s</sub></italic>, <italic toggle="yes">A<sub>l</sub></italic>, <italic toggle="yes">M</italic>&#x0232a;,&#x02009;<italic toggle="yes">B</italic> = &#x02329;<italic toggle="yes">T</italic>, &#x02026;&#x0232a;. Here, <italic toggle="yes">D</italic> represents the set of all images, <italic toggle="yes">V</italic> denotes the vocabulary restricting the symbols that the agents can use (e.g.&#x000a0;26 characters in English) and <italic toggle="yes">R</italic> defines the range of sequence lengths (e.g.&#x000a0;8&#x02013;16 symbols). For each turn, a random length <italic toggle="yes">r</italic> &#x02208; <italic toggle="yes">R</italic> is assigned. Agent A, denoted as <italic toggle="yes">A<sub>s</sub></italic>, serves as the speaker who observes the target image and generates a variable-length sequence <italic toggle="yes">M</italic> = (<italic toggle="yes">m</italic><sub>1</sub>, <italic toggle="yes">m</italic><sub>2</sub>, &#x02026;, <italic toggle="yes">m<sub>r</sub></italic>), representing the machine language. Each <italic toggle="yes">m<sub>i</sub></italic> represents a discrete symbol. Agent B, referred to as <italic toggle="yes">A<sub>l</sub></italic>, acts as the listener who receives the machine language <italic toggle="yes">M</italic> and decodes the information to solve two tasks: guessing the target image among distractors and drawing the target based on the provided information.</p><p>We employ a neural network model to simulate the SGD game process, depicted in Fig.&#x000a0;<xref rid="fig1" ref-type="fig">1</xref>. The model follows an encoder&#x02013;decoder structure, with the speaker acting as the encoder and the listener as the decoder. For a given random image, the speaker first processes it using a convolutional neural network (CNN) to extract the embedded feature. This feature is then fed into a recurrent neural network (RNN) with long short-term memory units to generate a variable-length sequence, representing the machine language to be learned. The listener receives this sequence and processes it using another RNN to produce a feature vector, denoted as query <italic toggle="yes">q</italic>. The game involves three tasks based on this query. The first is <italic toggle="yes">guessing</italic>, where the listener has not seen the target image before and relies solely on the language input from the speaker to make a guess. A batch of images <italic toggle="yes">B</italic> is randomly selected, including distractor images and the target image seen by the speaker. The second task is <italic toggle="yes">drawing</italic>, where the listener, based on their understanding of the language, is asked to draw an image reflecting their interpretation. The drawn image is then compared with the original image, and the reconstruction error defines the loss <italic toggle="yes">L</italic><sub>draw</sub>. This generative task enhances the semantic understanding of the language. To promote language flexibility, a <italic toggle="yes">regularization</italic> task is introduced. The speaker can describe an image multiple times with different sequence lengths, resulting in different queries <italic toggle="yes">q</italic> from the listener. The consistency of these queries is measured using a regularization loss <italic toggle="yes">L</italic><sub>regularization</sub>. By incorporating these tasks, the model simulates the collaborative process of speaking, guessing and drawing.</p><p>Besides showing the emergence of machine language, we also verified its functionality by comparing discrete language with the continuous feature from three aspects of interpretability, generalization and robustness on diverse datasets and tasks. Discrete language offers inherent interpretability, allowing for the manipulation and modification of semantic meanings. Our experiments demonstrated that discrete language can be purposefully altered to convey different semantic interpretations. Robustness is a crucial characteristic in practical applications, and our experiment evaluated the robustness of discrete language and continuous features. We compared their classification accuracy under different conditions, including the presence of noise and adversarial samples&#x000a0;[<xref rid="bib7" ref-type="bibr">7</xref>]. The results revealed that continuous features suffered a significant decrease in performance when subjected to perturbations; discrete language remained more robust and stable. This can be attributed to the abstract nature of language, which focuses on conveying higher-level semantic information rather than relying on specific visual details. From the generalization perspective, continuous features have shown strong performance in independent and identical distribution settings. However, we argue that the compositionality of language enables better generalization, particularly in out-of-distribution scenarios. While there may be little difference in accuracy between discrete language and continuous features for known categories, language-based representations excel when dealing with new and unknown categories.</p><p>In conclusion, the study of machine language represents an exciting and valuable direction in AI research. As AI progresses towards cognitive intelligence, there is a growing interest in integrating symbolic intelligence with neural networks, as advocated by Yoshua Bengio <italic toggle="yes">et&#x000a0;al.</italic> in their Turing lecture&#x000a0;[<xref rid="bib8" ref-type="bibr">8</xref>]. Recent advancements, exemplified by models like CLIP&#x000a0;[<xref rid="bib9" ref-type="bibr">9</xref>], DALLE-2&#x000a0;[<xref rid="bib10" ref-type="bibr">10</xref>] and GPT-4, have demonstrated AI&#x02019;s potential to learn from cross-modal information, moving beyond traditional methods that rely on human language for learning visual concepts. Our work takes a different perspective, exploring whether machines can develop their own language, known as machine language, through cooperative visual tasks, without relying on human language. By harnessing the potential of visual big data in shaping machine language, we aim to reconcile symbolic intelligence with neural networks, paving the way for more advanced AI systems. This research direction aligns with the transition towards cognitive intelligence and offers a fresh perspective on language learning in AI, going beyond human language-driven approaches. We firmly believe that emphasizing visual information in language emergence can lead to significant progress in AI capability.</p><sec sec-type="supplementary-material"><title>Supplementary Material</title><supplementary-material id="sup1" position="float" content-type="local-data"><label>nwad317_Supplemental_File</label><media xlink:href="nwad317_supplemental_file.pdf"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material></sec></body><back><sec id="sec1"><title>FUNDING</title><p>This work is supported in part by the 2035 Innovation Program of CAS, the National Key R&#x00026;D Program of China (2022ZD0160102), and the National Natural Science Foundation of China (61836014, U21B2042, 62072457 and 62006231).</p></sec><sec id="sec2"><p>
<bold>
<italic toggle="yes">Conflict of interest statement</italic>.</bold> None declared.</p></sec><ref-list id="ref1"><title>REFERENCES</title><ref id="bib1"><label>1.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Yan</surname> &#x000a0;<given-names>LC</given-names></string-name>, <string-name><surname>Yoshua</surname> &#x000a0;<given-names>B</given-names></string-name>, <string-name><surname>Geoffrey</surname> &#x000a0;<given-names>H</given-names></string-name></person-group> . <source>Nature</source> &#x000a0;<year>2015</year>; <volume>521</volume>: <fpage>436</fpage>&#x02013;<lpage>44</lpage>.<pub-id pub-id-type="doi">10.1038/nature14539</pub-id><pub-id pub-id-type="pmid">26017442</pub-id>
</mixed-citation></ref><ref id="bib2"><label>2.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Liu</surname> &#x000a0;<given-names>X</given-names></string-name>, <string-name><surname>Zhang</surname> &#x000a0;<given-names>F</given-names></string-name>, <string-name><surname>Hou</surname> &#x000a0;<given-names>Z</given-names></string-name> &#x000a0;<etal>et al.</etal></person-group> &#x000a0;<source>IEEE Trans Knowl Data Eng</source> &#x000a0;<year>2021</year>; <volume>35</volume>: <fpage>857</fpage>&#x02013;<lpage>76</lpage>.<pub-id pub-id-type="doi">10.1109/TKDE.2021.3090866</pub-id></mixed-citation></ref><ref id="bib3"><label>3.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Lazaridou</surname> &#x000a0;<given-names>A</given-names></string-name>, <string-name><surname>Peysakhovich</surname> &#x000a0;<given-names>A</given-names></string-name>, <string-name><surname>Baroni</surname> &#x000a0;<given-names>M</given-names></string-name></person-group>. <article-title>Multi-Agent Cooperation and the Emergence of (Natural) Language</article-title>. <comment><italic toggle="yes">ICLR 2017: 5th International Conference on Learning Representations, Toulon, France</italic>, 24&#x02013;26 April</comment>, <year>2017</year>.</mixed-citation></ref><ref id="bib4"><label>4.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Havrylov</surname> &#x000a0;<given-names>S</given-names></string-name>, <string-name><surname>Titov</surname> &#x000a0;<given-names>I</given-names></string-name></person-group>. <article-title>Emergence of language with multi-agent games: learning to communicate with sequences of symbols</article-title>. In: <source>Proceedings of the 31st International Conference on Neural Information Processing Systems</source>. <publisher-loc>Red Hook, NY</publisher-loc>: <publisher-name>Curran Associates</publisher-name>, <year>2017</year>, <fpage>2146</fpage>&#x02013;<lpage>56</lpage>.</mixed-citation></ref><ref id="bib5"><label>5.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Lee</surname> &#x000a0;<given-names>J</given-names></string-name>, <string-name><surname>Cho</surname> &#x000a0;<given-names>K</given-names></string-name>, <string-name><surname>Weston</surname> &#x000a0;<given-names>J</given-names></string-name> &#x000a0;<etal>et al.</etal></person-group> &#x000a0;<article-title>Emergent translation in multi-agent communication</article-title>. <source>ICLR 2018: 6th International Conference on Learning Representations, Vancouver, Canada</source>, <year>30 April&#x02013;3 May, 2018</year>.</mixed-citation></ref><ref id="bib6"><label>6.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Lazaridou</surname> &#x000a0;<given-names>A</given-names></string-name>, <string-name><surname>Baroni</surname> &#x000a0;<given-names>M</given-names></string-name></person-group>. <comment>arXiv: 2006.02419</comment>.</mixed-citation></ref><ref id="bib7"><label>7.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Szegedy</surname> &#x000a0;<given-names>C</given-names></string-name>, <string-name><surname>Zaremba</surname> &#x000a0;<given-names>W</given-names></string-name>, <string-name><surname>Sutskever</surname> &#x000a0;<given-names>I</given-names></string-name> &#x000a0;<etal>et al.</etal></person-group> &#x000a0;<article-title>Intriguing properties of neural networks</article-title>. <source>ICLR 2014: International Conference on Learning Representations, Banff, Canada</source>, <comment>14&#x02013;16 April, 2014</comment>.</mixed-citation></ref><ref id="bib8"><label>8.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Bengio</surname> &#x000a0;<given-names>Y</given-names></string-name>, <string-name><surname>Lecun</surname> &#x000a0;<given-names>Y</given-names></string-name>, <string-name><surname>Hinton</surname> &#x000a0;<given-names>G</given-names></string-name></person-group>. <source>Commun ACM</source> &#x000a0;<year> 2021</year>; <volume>64</volume>: <fpage>58</fpage>&#x02013;<lpage>65</lpage>.<pub-id pub-id-type="doi">10.1145/3448250</pub-id></mixed-citation></ref><ref id="bib9"><label>9.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Radford</surname> &#x000a0;<given-names>A</given-names></string-name>, <string-name><surname>Kim</surname> &#x000a0;<given-names>JW</given-names></string-name>, <string-name><surname>Hallacy</surname> &#x000a0;<given-names>C</given-names></string-name> &#x000a0;<etal>et al.</etal></person-group> &#x000a0;<article-title>Learning transferable visual models from natural language supervision</article-title>. In: <person-group person-group-type="editor"><string-name><surname>Meila</surname> &#x000a0;<given-names>M</given-names></string-name>, <string-name><surname>Zhang</surname> &#x000a0;<given-names>T</given-names></string-name></person-group> (eds) <source>Proceedings of the 38th International Conference on Machine Learning</source>. <publisher-name>PMLR</publisher-name>, <year>2021</year>, <fpage>8748</fpage>&#x02013;<lpage>63</lpage>.</mixed-citation></ref><ref id="bib10"><label>10.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Ramesh</surname> &#x000a0;<given-names>A</given-names></string-name>, <string-name><surname>Dhariwal</surname> &#x000a0;<given-names>P</given-names></string-name>, <string-name><surname>Nichol</surname> &#x000a0;<given-names>A</given-names></string-name> &#x000a0;<etal>et al.</etal></person-group> &#x000a0;<comment>arXiv: 2204.06125.</comment></mixed-citation></ref></ref-list></back></article>