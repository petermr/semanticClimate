<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd"> 
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><?properties open_access?><?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?><?DTDIdentifier.IdentifierType public?><?SourceDTD.DTDName A++V2.4.dtd?><?SourceDTD.Version 2.4?><?ConverterInfo.XSLTName springer2nlmx2.xsl?><?ConverterInfo.Version 1?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sci Rep</journal-id><journal-id journal-id-type="iso-abbrev">Sci Rep</journal-id><journal-title-group><journal-title>Scientific Reports</journal-title></journal-title-group><issn pub-type="epub">2045-2322</issn><publisher><publisher-name>Nature Publishing Group UK</publisher-name><publisher-loc>London</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmcid">10984971</article-id><article-id pub-id-type="pmid">38561396</article-id>
<article-id pub-id-type="publisher-id">58190</article-id><article-id pub-id-type="doi">10.1038/s41598-024-58190-9</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Spatial-temporal graph neural ODE networks for skeleton-based action recognition</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Pan</surname><given-names>Longji</given-names></name><xref ref-type="aff" rid="Aff1"/></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Lu</surname><given-names>Jianguang</given-names></name><address><email>jglu@gzu.edu.cn</email></address><xref ref-type="aff" rid="Aff1"/></contrib><contrib contrib-type="author"><name><surname>Tang</surname><given-names>Xianghong</given-names></name><xref ref-type="aff" rid="Aff1"/></contrib><aff id="Aff1"><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/02wmsc916</institution-id><institution-id institution-id-type="GRID">grid.443382.a</institution-id><institution-id institution-id-type="ISNI">0000 0004 1804 268X</institution-id><institution>Guizhou University, State Key Laboratory of Public Big Data, </institution></institution-wrap>Guiyang, 550025 China </aff></contrib-group><pub-date pub-type="epub"><day>1</day><month>4</month><year>2024</year></pub-date><pub-date pub-type="pmc-release"><day>1</day><month>4</month><year>2024</year></pub-date><pub-date pub-type="collection"><year>2024</year></pub-date><volume>14</volume><elocation-id>7629</elocation-id><history><date date-type="received"><day>12</day><month>1</month><year>2024</year></date><date date-type="accepted"><day>26</day><month>3</month><year>2024</year></date></history><permissions><copyright-statement>&#x000a9; The Author(s) 2024</copyright-statement><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article&#x02019;s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article&#x02019;s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p></license></permissions><abstract id="Abs1"><p id="Par1">In the field of skeleton-based action recognition, accurately recognizing human actions is crucial for applications such as virtual reality and motion analysis. However, this task faces challenges such intraindividual action differences and long-term temporal dependencies. To address these challenges, we propose an innovative model called spatial-temporal graph neural ordinary differential equations (STG-NODE). First, in the data preprocessing stage, the dynamic time warping (DTW) algorithm is used to normalize and calculate 3D skeleton data to facilitate the derivation of customized adjacency matrices for improving the influence of intraindividual action differences. Secondly, a custom ordinary differential equation (ODE) integrator is applied based on the initial conditions of the temporal features, producing a solution function that simulates the dynamic evolution trend of the events of interest. Finally, the outstanding ODE solver is used to numerically solve the time features based on the solution function to increase the influence of long-term dependencies on the recognition accuracy of the model and provide it with a more powerful temporal modeling ability. Through extensive experiments conducted on the NTU RGB+D 60 and Kinetics Skeleton 400 benchmark datasets, we demonstrate the superior performance of STG-NODE in the action recognition domain. The success of the STG-NODE model also provides new ideas and methods for the future development of the action recognition field.</p></abstract><kwd-group kwd-group-type="npg-subject"><title>Subject terms</title><kwd>Computational science</kwd><kwd>Computer science</kwd><kwd>Information technology</kwd><kwd>Software</kwd></kwd-group><funding-group><award-group><funding-source><institution>The Science and Technology Foundation of Guizhou Province</institution></funding-source><award-id>QKHJC-ZK(2021)YB015</award-id><principal-award-recipient><name><surname>Lu</surname><given-names>Jianguang</given-names></name></principal-award-recipient></award-group></funding-group><funding-group><award-group><funding-source><institution>Guizhou Provincial Key Technology R&#x00026;D Program</institution></funding-source><award-id>QKHZC(2022)YB074</award-id><principal-award-recipient><name><surname>Tang</surname><given-names>Xianghong</given-names></name></principal-award-recipient></award-group></funding-group><custom-meta-group><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#x000a9; Springer Nature Limited 2024</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1"><title>Introduction</title><p id="Par2">Rapid advancements within the field of computer vision have had profound and far-reaching impacts across various domains<sup><xref ref-type="bibr" rid="CR1">1</xref>&#x02013;<xref ref-type="bibr" rid="CR3">3</xref></sup>. Within this realm, action recognition stands as a pivotal branch, that is dedicated to the comprehension and analysis of human actions in images and videos. However, to further improve the robustness and practicality of recognition, the field of skeleton-based action recognition is emerging. Within this subfield, traditional methods rely on red-green-blue (RGB) data<sup><xref ref-type="bibr" rid="CR4">4</xref>,<xref ref-type="bibr" rid="CR5">5</xref></sup>; in contrast, skeletal data encompass time series that encapsulate the 2D or 3D positional coordinates of multiple human joints. These data can be directly captured by sensor devices or extracted from images using pose estimation techniques. Compared with conventional RGB video recognition approaches, action recognition based on skeleton data demonstrates reduced sensitivity to disruptive factors such as changes in lighting, environmental backgrounds, and occlusions that occur during the recognition process. This resilience to environmental variations enhances the robustness and practicality of action recognition systems, making them more reliable across a range of real-world scenarios. Notably, skeleton-based action recognition technology offers potent solutions in various applications, encompassing video surveillance, human-computer interaction, and video comprehension, among others<sup><xref ref-type="bibr" rid="CR6">6</xref>,<xref ref-type="bibr" rid="CR7">7</xref></sup>. It is an efficient, noninvasive<sup><xref ref-type="bibr" rid="CR8">8</xref></sup> and robust recognition method and is indispensable in the field of computer vision.<fig id="Fig1"><label>Figure 1</label><caption><p>The main idea of this work. Most traditional methods address static images based on frame intervals and suffer from reduced accuracy because they ignore the diversity of different individual motions. In contrast, STG-NODE produces ODE-based dynamic graphs and properly aligns the key points of actions.</p></caption><graphic xlink:href="41598_2024_58190_Fig1_HTML" id="MO1"/></fig></p><p id="Par3">The development of skeleton-based action recognition methods has gone through three different stages.</p><p id="Par4">1) Traditional feature engineering stage: In this early stage, researchers mainly relied on hand-designed feature extraction methods for processing skeleton data and performing action recognition. These methods often require expertise and experience to select and design features. The focus of this research concerns how to extract meaningful information from skeleton data. With the emergence of the machine learning era, researchers have begun to manually design skeleton data and shape them into pseudoimages<sup><xref ref-type="bibr" rid="CR9">9</xref>&#x02013;<xref ref-type="bibr" rid="CR11">11</xref></sup> or coordinate vector sequences<sup><xref ref-type="bibr" rid="CR12">12</xref>,<xref ref-type="bibr" rid="CR13">13</xref></sup>. The methods developed in this stage cannot fully express the complex information contained in skeleton data, and they have difficulty coping with the diversity and complexity of different actions. Therefore, the attained recognition accuracy is greatly limited, making it difficult for skeleton-based action recognition algorithms to be promoted or employed in wider application fields.</p><p id="Par5">2) RNN and CNN stage: With the rise of deep learning technology, skeleton-based action recognition has undergone a revolutionary change. Deep learning models such as recurrent neural networks (RNNs) and convolutional neural networks (CNNs) are beginning to be employed for processing skeleton data. The focus of an RNN is to model the time series information contained in skeleton data to capture the time series characteristics of actions<sup><xref ref-type="bibr" rid="CR14">14</xref>&#x02013;<xref ref-type="bibr" rid="CR16">16</xref></sup>. RNN models, such as long short-term memory (LSTM), can handle long-term dependencies, automatically extract key features, and map them to action categories. This capability enables them to achieve action recognition<sup><xref ref-type="bibr" rid="CR17">17</xref>&#x02013;<xref ref-type="bibr" rid="CR19">19</xref></sup>. In contrast, the main goal of CNNs is to extract local features from skeleton data through convolution operations to identify key skeleton patterns. A CNN exhibits spatial invariance and can ignore the specific positions of joints. It gradually extracts hierarchical features through multilayer convolution and is finally combined with a classifier to achieve action recognition<sup><xref ref-type="bibr" rid="CR20">20</xref>&#x02013;<xref ref-type="bibr" rid="CR22">22</xref></sup>. Nonetheless, the effectiveness of the abovementioned RNN and CNN methods in terms of recognizing actions from skeletal data is still limited. This limitation stems from the inability of these methods to openly represent the spatial relationships between joints, preventing neural networks from directly and proficiently capturing the collaborative spatial interactions between joints.</p><p id="Par6">3) GCN stage: In recent years, the concept of graph convolutional networks (GCNs), as an extension of the convolution paradigm from images to graphs, has notably penetrated various fields<sup><xref ref-type="bibr" rid="CR23">23</xref>&#x02013;<xref ref-type="bibr" rid="CR26">26</xref></sup>. The intrinsic graph structure of a non-Euclidean space is seamlessly coordinated with the intrinsic configuration of the human skeleton and is essentially a graph-like structure. Joints act as vertices and are connected to each other through edges that reflect the connections between bones in the human body. This complex architecture enables the description of dependencies in interconnected joints. The pioneering work<sup><xref ref-type="bibr" rid="CR27">27</xref></sup> concerning spatial-temporal graph convolutional networks (ST-GCNs), which encapsulate human skeleton data within graph frameworks, is particularly important. In this approach, a GCN is used for skeleton-based action recognition. This impetus has pushed GCN-based methods to the forefront of recognition tasks, cleverly capturing the subtleties of space and time by constructing spatiotemporal graphs. It is worth noting that this type of method demonstrates not only significant robustness but also commendable computational efficiency<sup><xref ref-type="bibr" rid="CR14">14</xref>,<xref ref-type="bibr" rid="CR28">28</xref></sup>.</p><p id="Par7">Specifically, the AS-GCN model proposed by Li et al.<sup><xref ref-type="bibr" rid="CR29">29</xref></sup> successfully captures richer dependencies and more detailed patterns in actions. The model proposed by Peng et al.<sup><xref ref-type="bibr" rid="CR30">30</xref></sup> uses a neural architecture search (NAS) to explore the spatiotemporal correlations between nodes and employs multiple dynamic graph modules to construct a search space. However, most GCN variants, including the abovementioned models, ignore the issue of &#x0201c;intraindividual action differences&#x0201d;. For example, the same action performed by the same person at different times or locations will produce different action attributes and differences, and these attributes and differences can seriously affect the recognition accuracy of the utilized model. The AGC-LSTM model proposed by Si et al.<sup><xref ref-type="bibr" rid="CR31">31</xref></sup> successfully captures discriminative spatial configuration features and temporal dynamics and successfully explores the co-occurrence relationship between the spatial and temporal domains. Subsequently, Shi et al.<sup><xref ref-type="bibr" rid="CR32">32</xref></sup> proposed the MS-AAGCN model that uses a data-driven approach to increase its flexibility and generalization capabilities; the authors confirmed that the adaptive learning graph topology is more suitable for action recognition tasks than human-based graphs. The above approaches are all valid spatiotemporal network models, but they mainly consider short-range connections. However, the MST-GCN model proposed by Chen et al.<sup><xref ref-type="bibr" rid="CR33">33</xref></sup> proved that long-range dependencies are also important for action recognition. Compared with traditional deep neural networks, an ordinary GCN or a spatiotemporal GCN model significantly improves the resulting recognition accuracy. However, according to our analysis, the current mainstream models face at least the following two challenges. 1) The intraindividual differences among actions are neglected. The current GCN-based models often ignore the impact of intraindividual action differences on the accuracy achieved in skeleton-based action recognition tasks. 2) The susceptibility of the existing graph neural networks (GNNs) to oversmoothing reflects an inherent limitation of these networks. As the network layers deepen, all node representations tend to converge to a uniform value, which greatly affects the ability of the employed model to capture long-term dependencies, especially long-term temporal dependencies.</p><p id="Par8">To this end, the spatial-temporal graph neural ODE network (STG-NODE) proposed herein integrates well-designed components to overcome these challenges. As shown in Figure&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>, compared with the traditional methods, STG-NODE has excellent advantages in terms of accurately identifying key actions. First, its discretization layer with residual connections, inspired by<sup><xref ref-type="bibr" rid="CR34">34</xref></sup>, can be viewed as a discretization of ODEs. Subsequently, a continuous graph neural network (CGNN)<sup><xref ref-type="bibr" rid="CR35">35</xref></sup> is derived to alleviate the oversmoothing problem. Taking advantage of this strategy, an ODE-temporal convolutional network (TCN) module is developed to enhance the temporal modeling ability of the model so that it can simulate long-term temporal dependencies. It is worth mentioning that the dynamics introduced by the ODE-TCN module improve the interpretability of the model in this task domain. Second, STG-NODE designs a semantic-based adjacency matrix for skeleton-based action recognition. This innovation is based on an elaborate data preprocessing pipeline, which includes skeleton alignment, semantic feature extraction, category labeling, and dynamic time warping (DTW)-based similarity computation steps, resulting in a semantic adjacency matrix. This unique approach significantly improves the flexibility of the STG-NODE model by effectively mitigating the impact of intraindividual action differences on the accuracy of the skeleton-based action recognition process, ultimately improving its performance in terms of recognizing complex human motions from skeletal data. To verify the superiority of our proposed model (i.e., STG-NODE), we conduct extensive experiments on two large datasets: the NTU RGB+D 60 dataset<sup><xref ref-type="bibr" rid="CR13">13</xref></sup> and the Kinetics Skeleton 400 dataset<sup><xref ref-type="bibr" rid="CR36">36</xref></sup>. Our model achieves superior performance to that of the competing methods on both datasets.</p><p id="Par9">The main contributions of our work are as follows.<list list-type="bullet"><list-item><p id="Par10">The main contribution of STG-NODE is the introduction of tensor-based ordinary differential equations for conducting skeleton-based action recognition. Specifically, this study designs an ODE-TCN module to enhance the temporal modeling capabilities of the model. This enhancement enables the model to effectively model long-term temporal dependencies, thereby improving its suitability for tasks involving complex temporal patterns. Notably, the dynamics introduced by the ODE-TCN module enhance the interpretability of the resulting model in the context of skeleton-based action recognition.</p></list-item><list-item><p id="Par11">A semantic adjacency matrix customized for skeleton-based action recognition is proposed. This innovation is based on a customized data preprocessing pipeline and a similarity calculation with DTW, ultimately creating a semantic-based semantic adjacency matrix. This unique approach enhances the flexibility of the model and, ultimately, its performance with respect to recognizing complex human motions from skeletal data.</p></list-item><list-item><p id="Par12">To validate the effectiveness of our proposed STG-NODE model, we conduct extensive experiments on two large datasets: NTU RGB+D 60 and Kinetics Skeleton 400. Our model consistently achieves superior performance on both datasets, demonstrating its ability to address the challenges encountered in skeleton-based action recognition tasks.</p></list-item></list>The remainder of this paper is organized as follows. Section &#x0201c;<xref rid="Sec2" ref-type="sec">The proposed approach</xref>&#x0201d; first briefly describes the motivation of this paper, followed by detailed descriptions of the key components in the proposed STG-NODE model. Section &#x0201c;<xref rid="Sec9" ref-type="sec">Experiments</xref>&#x0201d; verifies the effectiveness of our method through comparative and ablation experiments and analyses. Finally, we conclude the paper in Section &#x0201c;<xref rid="Sec14" ref-type="sec">Conclusion</xref>&#x0201d;.</p></sec><sec id="Sec2"><title>The proposed approach</title><sec id="Sec3"><title>Motivation</title><p id="Par13">Repeatedly performing the same action introduces diversity in skeletal data due to variations in physiological characteristics and action execution conditions. This phenomenon, which is known as intraindividual action differences, affects recognition accuracy. Furthermore, the importance of long-term temporal dependencies is evident in action recognition tasks since actions are coherent and sequential in nature. Neglecting temporal dependencies may lead to information losses and limited recognition accuracy.</p><p id="Par14">These challenges drive our innovative approach. First, the motivation behind the introduction of tensor-based ODEs stems from the need to capture and model the long-term temporal dependencies inherent in skeleton data. Theoretically, the dynamics resulting from ODEs fit the inherent coherence properties of skeleton-based action recognition tasks. To this end, the ODE-TCN module is introduced to enhance the temporal modeling capabilities of our model and simultaneously inject dynamics to better simulate the temporal attributes of real actions, allowing the model to more accurately capture the dynamic changes exhibited by action sequences. Second, the motivation behind introducing semantic-based category similarity matrices is to address the intraindividual action differences encountered in skeleton-based action recognition scenarios. Theoretically, we plan to obtain semantic action features through specific operations and then calculate the target matrix. To this end, by carefully designing a data preprocessing strategy, including alignment, semantic feature extraction, category labeling, and DTW-based similarity calculations, this study creates a novel semantic adjacency matrix. This matrix is expected to enhance the adaptability of the developed model by effectively mitigating the effect of intraindividual action differences, thereby improving the accuracy of skeleton-based action recognition. These efforts ultimately lead to the development of the STG-NODE model.<fig id="Fig2"><label>Figure 2</label><caption><p>(<bold>a</bold>) Basic network framework. The POOL-FC layer and the final class score component form the output module. Asp represents the spatial adjacency matrix; Ase represents the semantic adjacency matrix. (<bold>b</bold>) The details of Ase. fin denotes the original data; fout1 and fout2 are the category similarity matrices.</p></caption><graphic xlink:href="41598_2024_58190_Fig2_HTML" id="MO2"/></fig></p></sec><sec id="Sec4"><title>Model framework</title><p id="Par15">Figure&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref>(a) shows the basic framework of our proposed STG-NODE model. It mainly consists of three parts: an ordinary differential equation-temporal convolutional network (ODE-TCN) module, a graph convolutional network-temporal convolutional network (GCN-TCN) module and an output module. The ODE-TCN module is composed of an integrator, a solver and a temporal convolutional network connected in series. The integrator is implemented with an integral function to generate a solution function, which is obtained by integrating the input data in the temporal dimension. The solver is implemented with an ordinary differential equation solver based on the numerical solution with respect to the time characteristics of the solution function so that the model can effectively model long-term time dependencies. The GCN-TCN module is composed of a graph convolutional network and a temporal convolutional network in series; this module empowers the model by ensuring that it comprehensively considers and leverages the joint relationships and spatial structures contained within skeleton data. This leads to an improved understanding and analysis of the spatial features and relationships in human body movements, ultimately resulting in enhanced model performance. Functionality of the Output Module: This module meticulously consolidates and summarizes the features acquired from skeleton data. This process enables the amalgamation of skeleton features into higher-level representations, effectively capturing the abstract characteristics of various actions. Subsequently, these refined features are mapped to various potential action categories, yielding probability distributions for each category. Furthermore, both the ODE-TCN and GCN-TCN modules conduct feature extraction in parallel across different layers. Following feature fusion, these modules seamlessly feed the features into the subsequent parallel structure. Ultimately, the amalgamated features are fed into the output module, enabling the model to perform action classification and accurately determine the action category to which the input data belong. To better present the details of the ODE-TCN module, we show the Ase submodule separately in Figure&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref>(b).</p></sec><sec id="Sec5"><title>Adjacency matrix construction</title><p id="Par16">In our model, we use two types of adjacency matrices. Drawing inspiration from the ST-GCN<sup><xref ref-type="bibr" rid="CR27">27</xref></sup>, the spatial adjacency matrix is formulated as follows:<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} A_{ij}^{sp}= \left\{ \begin{matrix} A_{ij}\cdot D_{ij},\ if\ A_{ij}^{hop}\ne inf\\ 0,\ otherwise \end{matrix}\right. \end{aligned}$$\end{document}</tex-math><mml:math id="M2" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msubsup><mml:mi>A</mml:mi><mml:mrow><mml:mi mathvariant="italic">ij</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">sp</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mfenced open="{"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi mathvariant="italic">ij</mml:mi></mml:mrow></mml:msub><mml:mo>&#x000b7;</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi mathvariant="italic">ij</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mspace width="4pt"/><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mspace width="4pt"/><mml:msubsup><mml:mi>A</mml:mi><mml:mrow><mml:mi mathvariant="italic">ij</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">hop</mml:mi></mml:mrow></mml:msubsup><mml:mo>&#x02260;</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mrow/><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mspace width="4pt"/><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>w</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_58190_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula>A is an adjacency matrix, D is a diagonal matrix whose diagonal elements are node degrees, <inline-formula id="IEq1"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$A^{hop}$$\end{document}</tex-math><mml:math id="M4"><mml:msup><mml:mi>A</mml:mi><mml:mrow><mml:mi mathvariant="italic">hop</mml:mi></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="41598_2024_58190_Article_IEq1.gif"/></alternatives></inline-formula> is the shortest number of hops between nodes, and inf means that the corresponding nodes are not reachable.</p><p id="Par17">In addition, it is crucial to consider the impact of intraindividual action differences on the accuracy of skeleton-based action recognition tasks. For example, the same person performing the same action at different times or locations produces different action properties and differences that cannot be revealed in a spatial graph. To capture the above semantic variability, we use the FastDTW<sup><xref ref-type="bibr" rid="CR37">37</xref></sup> algorithm to calculate the joint nodes of a human body action based on the node characteristics (all node coordinates within a period of time), thereby constructing a semantic adjacency matrix. This semantic adjacency matrix quantifies the degrees and strengths of the correlations between joint points in human movements through an algorithm, thereby extracting semantic information. By providing semantic information concerning action execution, the semantic adjacency matrix enables the model to better understand the intrinsic correlations and meanings of actions. As shown in Figure&#x000a0;<xref rid="Fig3" ref-type="fig">3</xref>, with the FastDTW algorithm, point a of a series M is correlated with point z of another series N but not with point y of series N. Specifically, given two time series <inline-formula id="IEq2"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$M = (m_{1},\ m_{2},\cdots ,\ m_{k})$$\end{document}</tex-math><mml:math id="M6"><mml:mrow><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mspace width="4pt"/><mml:msub><mml:mi>m</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x022ef;</mml:mo><mml:mo>,</mml:mo><mml:mspace width="4pt"/><mml:msub><mml:mi>m</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_58190_Article_IEq2.gif"/></alternatives></inline-formula> and <inline-formula id="IEq3"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$N = (n_{1}, n_{2},\cdots , n_{l})$$\end{document}</tex-math><mml:math id="M8"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x022ef;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_58190_Article_IEq3.gif"/></alternatives></inline-formula>, FastDTW is a dynamic programming algorithm that is defined as follows:<disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} \small D(i,\ j)=dist(m_{i},\ n_{j})+min(D(i-1,\ j),\ D(i,\ j-1),\ \varepsilon *D(i-1,\ j-1)) \end{aligned}$$\end{document}</tex-math><mml:math id="M10" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mstyle mathsize="0.6em"><mml:mi mathvariant="normal">D</mml:mi></mml:mstyle><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mspace width="4pt"/><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mspace width="4pt"/><mml:msub><mml:mi>n</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mspace width="4pt"/><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mspace width="4pt"/><mml:mi>D</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mspace width="4pt"/><mml:mi>j</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mspace width="4pt"/><mml:mi>&#x003b5;</mml:mi><mml:mrow/><mml:mo>&#x02217;</mml:mo><mml:mi>D</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mspace width="4pt"/><mml:mi>j</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_58190_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula><fig id="Fig3"><label>Figure 3</label><caption><p>An example of the difference between the Euclidean distance and the FastDTW distance.</p></caption><graphic xlink:href="41598_2024_58190_Fig3_HTML" id="MO3"/></fig></p><p id="Par18">Where <inline-formula id="IEq4"><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$D(i,\ j)$$\end{document}</tex-math><mml:math id="M12"><mml:mrow><mml:mi>D</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mspace width="4pt"/><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_58190_Article_IEq4.gif"/></alternatives></inline-formula> denotes the shortest distance between subseries <inline-formula id="IEq5"><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$M=(m_{1},\ m_{2},\cdots ,\ m_{i})$$\end{document}</tex-math><mml:math id="M14"><mml:mrow><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mspace width="4pt"/><mml:msub><mml:mi>m</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x022ef;</mml:mo><mml:mo>,</mml:mo><mml:mspace width="4pt"/><mml:msub><mml:mi>m</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_58190_Article_IEq5.gif"/></alternatives></inline-formula> and <inline-formula id="IEq6"><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$N=(n_{1},\ n_{2},\cdots ,\ n_{j})$$\end{document}</tex-math><mml:math id="M16"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mspace width="4pt"/><mml:msub><mml:mi>n</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x022ef;</mml:mo><mml:mo>,</mml:mo><mml:mspace width="4pt"/><mml:msub><mml:mi>n</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_58190_Article_IEq6.gif"/></alternatives></inline-formula>, with <inline-formula id="IEq7"><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$dist(m_{i},\ n_{j})$$\end{document}</tex-math><mml:math id="M18"><mml:mrow><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mspace width="4pt"/><mml:msub><mml:mi>n</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_58190_Article_IEq7.gif"/></alternatives></inline-formula> representing a distance metric such as the absolute distance measure. Consequently, <inline-formula id="IEq8"><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$FastDTW(M,\ N)=D(k,\ l)$$\end{document}</tex-math><mml:math id="M20"><mml:mrow><mml:mi>F</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>D</mml:mi><mml:mi>T</mml:mi><mml:mi>W</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>M</mml:mi><mml:mo>,</mml:mo><mml:mspace width="4pt"/><mml:mi>N</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>D</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mspace width="4pt"/><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_58190_Article_IEq8.gif"/></alternatives></inline-formula> signifies the ultimate distance between <italic>M</italic> and <italic>N</italic>, providing a more accurate assessment of the similarity between two time series than that provided by the Euclidean distance. The value of the multiplier <inline-formula id="IEq9"><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\epsilon$$\end{document}</tex-math><mml:math id="M22"><mml:mi>&#x003f5;</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_58190_Article_IEq9.gif"/></alternatives></inline-formula> ranges from 0 <inline-formula id="IEq10"><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$&#x0003c; \epsilon \leqslant$$\end{document}</tex-math><mml:math id="M24"><mml:mrow><mml:mo>&#x0003c;</mml:mo><mml:mi>&#x003f5;</mml:mi><mml:mo>&#x02a7d;</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_58190_Article_IEq10.gif"/></alternatives></inline-formula> 1. By adjusting the value of <inline-formula id="IEq11"><alternatives><tex-math id="M25">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\epsilon$$\end{document}</tex-math><mml:math id="M26"><mml:mi>&#x003f5;</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_58190_Article_IEq11.gif"/></alternatives></inline-formula>, the approximation degree of the FastDTW algorithm can be controlled, which reduces its computational complexity to a certain extent and improves the computational speed of the model.</p><p id="Par19">Accordingly, we define the semantic adjacency matrix through the FastDTW distance as follows:<disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="M27">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} A_{ij}^{se}=\left\{ \begin{matrix} 1,&#x00026;{}FastDTW(X^{i},\ X^{j})&#x0003c;\varepsilon \\ 0,&#x00026;{}otherwise \end{matrix}\right. \end{aligned}$$\end{document}</tex-math><mml:math id="M28" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msubsup><mml:mi>A</mml:mi><mml:mrow><mml:mi mathvariant="italic">ij</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">se</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mfenced open="{"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mrow/><mml:mi>F</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>D</mml:mi><mml:mi>T</mml:mi><mml:mi>W</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>X</mml:mi><mml:mi>i</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:mspace width="4pt"/><mml:msup><mml:mi>X</mml:mi><mml:mi>j</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x0003c;</mml:mo><mml:mi>&#x003b5;</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mrow/><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mrow/><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>w</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_58190_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq12"><alternatives><tex-math id="M29">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$X^{i}$$\end{document}</tex-math><mml:math id="M30"><mml:msup><mml:mi>X</mml:mi><mml:mi>i</mml:mi></mml:msup></mml:math><inline-graphic xlink:href="41598_2024_58190_Article_IEq12.gif"/></alternatives></inline-formula> represents the time series of node <italic>i</italic>, and <inline-formula id="IEq13"><alternatives><tex-math id="M31">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\epsilon$$\end{document}</tex-math><mml:math id="M32"><mml:mi>&#x003f5;</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_58190_Article_IEq13.gif"/></alternatives></inline-formula> controls the sparsity level of the adjacency matrix.</p></sec><sec id="Sec6"><title>Customized ODE integrator and solver</title><p id="Par20">GCNs update node embeddings by aggregating features derived from both the nodes themselves and their neighbors using a graph convolution operation. The conventional form of this convolution operation can be expressed as follows:<disp-formula id="Equ4"><label>4</label><alternatives><tex-math id="M33">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} f_{out}=GCN(f_{in})=\alpha (f_{in}{\hat{A}}W) \end{aligned}$$\end{document}</tex-math><mml:math id="M34" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi mathvariant="italic">out</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>G</mml:mi><mml:mi>C</mml:mi><mml:mi>N</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi mathvariant="italic">in</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>&#x003b1;</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi mathvariant="italic">in</mml:mi></mml:mrow></mml:msub><mml:mover accent="true"><mml:mi>A</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>W</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_58190_Article_Equ4.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq14"><alternatives><tex-math id="M35">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$f_{in}\in {\mathbb {R}}^{N\times C}$$\end{document}</tex-math><mml:math id="M36"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi mathvariant="italic">in</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_58190_Article_IEq14.gif"/></alternatives></inline-formula> denotes the input of the previous graph convolution layer, <inline-formula id="IEq15"><alternatives><tex-math id="M37">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\hat{A}}\in {\mathbb {R}}^{N\times N}$$\end{document}</tex-math><mml:math id="M38"><mml:mrow><mml:mover accent="true"><mml:mi>A</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_58190_Article_IEq15.gif"/></alternatives></inline-formula> is the normalized adjacency matrix, and <inline-formula id="IEq16"><alternatives><tex-math id="M39">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$W\in {\mathbb {R}}^{C\times {C}'}$$\end{document}</tex-math><mml:math id="M40"><mml:mrow><mml:mi>W</mml:mi><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:msup><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mo>&#x02032;</mml:mo></mml:msup></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_58190_Article_IEq16.gif"/></alternatives></inline-formula> is a learnable parameter matrix that models the interactions among different features. However, such GCNs have been shown to suffer from the issue of oversmoothing as the networks become deeper<sup><xref ref-type="bibr" rid="CR38">38</xref>,<xref ref-type="bibr" rid="CR39">39</xref></sup>, which significantly restricts their ability to model long-range dependencies. In response to this limitation, we introduce our novel STG-NODE block.</p><p id="Par21">To allow interactions between the adjacency matrices and modules, we are inspired by the success of the CGNN<sup><xref ref-type="bibr" rid="CR35">35</xref></sup> and consider a more powerful discrete dynamic function:<disp-formula id="Equ5"><label>5</label><alternatives><tex-math id="M41">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} f_{out}=f_{in}\times _{1} {\hat{A}}\times _{2} Z\times _{3} W+h_{0} \end{aligned}$$\end{document}</tex-math><mml:math id="M42" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi mathvariant="italic">out</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi mathvariant="italic">in</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mo>&#x000d7;</mml:mo><mml:mn>1</mml:mn></mml:msub><mml:mover accent="true"><mml:mi>A</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:msub><mml:mo>&#x000d7;</mml:mo><mml:mn>2</mml:mn></mml:msub><mml:mi>Z</mml:mi><mml:msub><mml:mo>&#x000d7;</mml:mo><mml:mn>3</mml:mn></mml:msub><mml:mi>W</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_58190_Article_Equ5.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq17"><alternatives><tex-math id="M43">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$f_{in}\in {\mathbb {R}}^{N\times T\times F}$$\end{document}</tex-math><mml:math id="M44"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi mathvariant="italic">in</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>T</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>F</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_58190_Article_IEq17.gif"/></alternatives></inline-formula> is a space-time tensor that represents the hidden embedding of the examined node in the previous layer, <inline-formula id="IEq18"><alternatives><tex-math id="M45">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\times _{i}$$\end{document}</tex-math><mml:math id="M46"><mml:msub><mml:mo>&#x000d7;</mml:mo><mml:mi>i</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_58190_Article_IEq18.gif"/></alternatives></inline-formula> denotes the tensor matrix multiplication operation executed on mode <italic>i</italic>, <inline-formula id="IEq19"><alternatives><tex-math id="M47">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\hat{A}}$$\end{document}</tex-math><mml:math id="M48"><mml:mover accent="true"><mml:mi>A</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="41598_2024_58190_Article_IEq19.gif"/></alternatives></inline-formula> is the regularized adjacency matrix, <italic>Z</italic> is the temporal transformation matrix, <italic>W</italic> is the feature transformation matrix, and <inline-formula id="IEq20"><alternatives><tex-math id="M49">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$h_{0}$$\end{document}</tex-math><mml:math id="M50"><mml:msub><mml:mi>h</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_58190_Article_IEq20.gif"/></alternatives></inline-formula> denotes the initial input of the GCN, which can be acquired through another neural network. Motivated by the CGNN, a restart distribution <inline-formula id="IEq21"><alternatives><tex-math id="M51">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$H_{0}$$\end{document}</tex-math><mml:math id="M52"><mml:msub><mml:mi>H</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_58190_Article_IEq21.gif"/></alternatives></inline-formula> is used to alleviate the oversmoothing problem.</p><p id="Par22">Although the residual structure shown in Eq. 5 is powerful, training it can be challenging due to the large number of parameters involved. Therefore, our goal is to extend the discrete formulation to a continuous expression in the skeleton-based action recognition domain. To effectively convert the residual structure into an ODE structure, we follow the successful practices of previous researchers, such as the series of practices adopted in<sup><xref ref-type="bibr" rid="CR40">40</xref></sup>.</p><p id="Par23">Specifically, the continuous expression of Eq. 5 is shown as follows:<disp-formula id="Equ6"><label>6</label><alternatives><tex-math id="M53">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} \frac{df(t)}{dt}=f(t)\times _{1} ({\hat{A}}-I)+f(t)\times _{2} (Z-I)+f(t)\times _{3} (W-I)+H_{0} \end{aligned}$$\end{document}</tex-math><mml:math id="M54" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="italic">dt</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:msub><mml:mo>&#x000d7;</mml:mo><mml:mn>1</mml:mn></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi>A</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>-</mml:mo><mml:mi>I</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:msub><mml:mo>&#x000d7;</mml:mo><mml:mn>2</mml:mn></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>Z</mml:mi><mml:mo>-</mml:mo><mml:mi>I</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:msub><mml:mo>&#x000d7;</mml:mo><mml:mn>3</mml:mn></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>W</mml:mi><mml:mo>-</mml:mo><mml:mi>I</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_58190_Article_Equ6.gif" position="anchor"/></alternatives></disp-formula>Finally, we draw inspiration from neural ODEs<sup><xref ref-type="bibr" rid="CR34">34</xref></sup> and introduce our STG-NODE framework. The continuous form of the hidden representation is as follows:<disp-formula id="Equ7"><label>7</label><alternatives><tex-math id="M55">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} f(t)=ODESolver(\frac{df(t)}{dt},h_{0},t) \end{aligned}$$\end{document}</tex-math><mml:math id="M56" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>O</mml:mi><mml:mi>D</mml:mi><mml:mi>E</mml:mi><mml:mi>S</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="italic">dt</mml:mi></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_58190_Article_Equ7.gif" position="anchor"/></alternatives></disp-formula>Runge-Kutta solvers are generally more stable than Euler solvers, which is critical for accurately capturing the subtle changes and characteristics of action sequences. In addition, a Runge-Kutta solver has higher accuracy when processing action sequences with nonlinear characteristics and rapid changes and can more accurately capture details and important features from actions. Based on these considerations, we choose the Runge-Kutta method as the ODE solver in our model.</p></sec><sec id="Sec7"><title>STG-NODE module and performance analysis</title><p id="Par24">The preceding sections have provided a detailed exposition of the key components contained within STG-NODE. This section operates from a macro perspective, delineating the holistic STG-NODE module. As illustrated in Figure 1, the model adopts a serial-parallel structure comprising an ODE-TCN block and a GCN-TCN block. This inventive architecture not only facilitates the seamless amalgamation of spatiotemporal information but also harnesses the inherent strengths of ODEs, thereby enhancing the precision achieved in skeleton-based action recognition tasks.</p><p id="Par25">On the one hand, STG-NODE presents a plethora of advantages over traditional GCNs and TCNs, significantly bolstering the foundational aspects of skeletal action recognition models. ODEs fundamentally capture dynamic behaviors by modeling state evolution trends over consecutive time intervals. This property impeccably aligns with the nuanced nature of human motion, enabling the model to discern the subtle temporal intricacies contained within skeletal data. Furthermore, the ODE-based framework exhibits superior generalization capabilities to those of the traditional methods and adeptly handles irregularly sampled or missing data points within the input skeleton sequence. Its rich feature reservoir and adept module combinations substantially augment the ability of the model to unravel intricate the correlations inherent in skeletal actions. However, compared to models that lack DTW integration (e.g.,<sup><xref ref-type="bibr" rid="CR27">27</xref>,<xref ref-type="bibr" rid="CR41">41</xref></sup>), the ODE-TCN module utilizes FastDTW to compute the semantic adjacency matrix. This matrix enables the ODE-TCN module to focus more on nodes with greater relevance when propagating information in the temporal dimension during the training process, thereby capturing key frames in actions with varying lengths. This allows the model to better adapt to situations where the execution speed is faster or slower than the action features are learned. Ultimately, this mechanism allows STG-NODE to mitigate the impact of intraindividual action differences.</p><p id="Par26">In summary, the STG-NODE model has the advantages of ODEs and DTW enhancement. In theory, this comprehensive advantage makes the STG-NODE model significantly better than the traditional models. This enhancement is reflected in its ability to effectively capture the spatiotemporal complexity of skeletal actions, resulting in significant performance advantages in action recognition tasks.</p></sec><sec id="Sec8"><title>Training loss expression</title><p id="Par27">The cross-entropy loss is chosen as the loss function, as it is well suited for addressing multiclassification tasks and exhibits strong sensitivity to variations in predicted probability distributions. This property encourages the model to prioritize the correct category. The formulation of the cross-entropy loss is outlined below:<disp-formula id="Equ8"><label>8</label><alternatives><tex-math id="M57">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} L(y,t)=-\sum _{c=1}^{C}t_{c}log(y_{c}) \end{aligned}$$\end{document}</tex-math><mml:math id="M58" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>L</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>C</mml:mi></mml:munderover><mml:msub><mml:mi>t</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_58190_Article_Equ8.gif" position="anchor"/></alternatives></disp-formula>where <italic>C</italic> is the number of categories, <inline-formula id="IEq22"><alternatives><tex-math id="M59">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$t_{c}$$\end{document}</tex-math><mml:math id="M60"><mml:msub><mml:mi>t</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_58190_Article_IEq22.gif"/></alternatives></inline-formula> is the value of the <italic>c</italic>-th category in the real labels, <inline-formula id="IEq23"><alternatives><tex-math id="M61">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$y_{c}$$\end{document}</tex-math><mml:math id="M62"><mml:msub><mml:mi>y</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_58190_Article_IEq23.gif"/></alternatives></inline-formula> is the predicted probability output by the model for the <italic>c</italic>-th category, <italic>y</italic> is the output of the model, indicating the probability predicted by the model for each category, <italic>t</italic> is the real label, only one element is 1, and the others are 0.</p><p id="Par28">The goal of the loss function is to guide the optimization process of the model parameters by minimizing the difference between the predicted probability of each category and the one-hot encoding of the actual corresponding label, thereby enabling the model to more accurately predict action categories.</p></sec></sec><sec id="Sec9"><title>Experiments</title><p id="Par29">In this section, an extensive performance evaluation of the devised STG-NODE model is implemented across two expansive datasets: NTU RGB+D 60<sup><xref ref-type="bibr" rid="CR13">13</xref></sup> and Kinetics Skeleton 400<sup><xref ref-type="bibr" rid="CR36">36</xref></sup>. Given the relatively modest size of the NTU RGB+D 60 dataset, a meticulous ablation study is conducted to ascertain the efficacy of the enhancements incorporated into the model. Subsequently, a comprehensive comparative analysis is performed, benchmarking our STG-NODE model against other approaches. This multifaceted evaluation, spanning two datasets (two completely different benchmarks plus two different indicator scales), serves to corroborate both the broad applicability and the precision of our proposed framework in terms of achieving definitive recognition outcomes.</p><sec id="Sec10"><title>Datasets</title><p id="Par30">The <bold>NTU RGB+D 60</bold> dataset, which contains an extensive collection of 56,000 action clips classified into 60 different action categories, is of crucial importance in the field of 3D human action recognition. This comprehensive range covers a variety of action types, from the nuances of everyday behaviors to health-related routines and complex two-person interactions. These captured segments were recorded from the perspective of three synchronized camera angles within the controlled confines of a laboratory environment. These visual narratives reveal the complex spatial coordinates of joints (X, Y, Z) in 3D with the help of the discriminative capabilities provided by the Kinect depth sensor.</p><p id="Par31">The evaluation paradigm for this dataset is carefully constructed around two strong paradigms: cross-subject (X-Sub) and cross-view (X-View) protocols. Under the X-Sub benchmark, the partitioning strategy is based on individual IDs and ultimately allocates 40,320 samples for fine-grained training and an additional 16,560 samples for rigorous testing. At the same time, the X-View framework utilizes perspectives derived from different camera angles to form similar partitioning patterns. In this configuration, a selective subset of 18,960 samples from camera 1 is reserved for exhaustive testing purposes, while a large repository of 37,920 samples acquired from cameras 2 and 3 strongly supports the comprehensive training scheme.</p><p id="Par32">The <bold>Kinetics Skeleton 400</bold> dataset is a comprehensive compilation of approximately 300,000 video clips that were carefully curated from various sources on YouTube. This massive dataset contains 400 different human action categories, covering a wide range of scenarios from everyday activities to dynamic motion scenarios and complex interactions with complex actions. Notably, each video clip in the Kinetics Skeleton 400 dataset maintains a consistent temporal structure with an average duration of approximately 10 seconds. The clips were captured at a standard frame rate of 30 frames/second, yielding a total of 300 frames.</p><p id="Par33">Each frame in these clips is meticulously analyzed, during which up to two joints with the highest average confidence levels are selected. This rigorous process culminates in the precise definition of 18 joints for each bone structure, each of which characterized by its 2D coordinates and corresponding confidence. The resulting skeletal representation provides the basis for a comprehensive action analysis.</p><p id="Par34">Furthermore, in the context of the Kinetics Skeleton 400 experiment, we evaluate the achieved recognition performance according to the top-1 and top-5 classification accuracy metrics, which evaluate the determinism and robustness of the tested model. The estimated 2D joint positions generated by the OpenPose pose estimation framework<sup><xref ref-type="bibr" rid="CR42">42</xref></sup> provided by the ST-GCN are used as inputs. This choice ensures that the experiments are performed on a consistent and reputable basis, allowing for a robust and accurate analysis of dataset-rich human behavioral dynamics.</p></sec><sec id="Sec11"><title>Experimental settings</title><p id="Par35">The experiments are conducted on a Linux server equipped with an Intel(R) Xeon(R) Silver 4316 CPU running at 2.3 GHz and four NVIDIA TESLA A100 GPUs. Adhering to the foundational framework of the ST-GCN, STG-NODE follows a similar structural setup. The architecture encompasses 10 STG-NODE blocks within the STG-NODE model, with each block consisting of one ODE-TCN and one GCN-TCN. For the semantic adjacency matrix, precise calibration is achieved by setting the thresholds <inline-formula id="IEq24"><alternatives><tex-math id="M63">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\sigma$$\end{document}</tex-math><mml:math id="M64"><mml:mi>&#x003c3;</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_58190_Article_IEq24.gif"/></alternatives></inline-formula> and <inline-formula id="IEq25"><alternatives><tex-math id="M65">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\epsilon$$\end{document}</tex-math><mml:math id="M66"><mml:mi>&#x003f5;</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_58190_Article_IEq25.gif"/></alternatives></inline-formula> to 0.1 and 0.6, respectively.</p><p id="Par36">Stochastic gradient descent (SGD) with Nesterov momentum (0.9) is applied as the optimization strategy with a learning rate of 0.15, and it is accompanied by a decay ratio of 0.0001. The cross-entropy loss function is chosen for gradient backpropagation. This process is executed using a batch size of 64, spanning a training period that extends to 80 iterations. Notably, the experimental setup applies several supplementary preprocessing strategies to each dataset. Initially, the coordinate information of the samples is extracted through the utilization of a dedicated skeleton sequence visualization tool, as provided in<sup><xref ref-type="bibr" rid="CR27">27</xref></sup>. Subsequently, the extracted data are carefully processed using the DTW algorithm, and finally, a customized semantic adjacency matrix is derived. This strategic preprocessing procedure augments the discernment capabilities of the model and contributes to the overall efficacy of the experimental analysis.</p></sec><sec id="Sec12"><title>Comparison</title><p id="Par37">To evaluate the performance of STG-NODE, we compare our proposed STG-NODE model with other skeleton-based action recognition methods on the Kinetics Skeleton 400 dataset and the NTU RGB+D 60 dataset. The comparison results are shown in Table&#x000a0;<xref rid="Tab1" ref-type="table">1</xref>, which shows that our model has strong performance advantages on both datasets.<table-wrap id="Tab1"><label>Table 1</label><caption><p>Accuracy comparisons between our proposed STG-NODE model and other methods on NTU RGB+D 60 and Kinetics Skeleton 400.&#x000a0;Significant values are in bold.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Method</th><th align="left">X-Sub</th><th align="left">X-View</th><th align="left">Kinetics Top1</th><th align="left">Kinetics Top5</th><th align="left">Years</th></tr></thead><tbody><tr><td align="left">Deep LSTM<sup><xref ref-type="bibr" rid="CR13">13</xref></sup></td><td align="left">60.7</td><td align="left">67.3</td><td align="left">16.4</td><td align="left">35.3</td><td align="left">2016</td></tr><tr><td align="left">TCN<sup><xref ref-type="bibr" rid="CR11">11</xref></sup></td><td align="left">74.3</td><td align="left">83.1</td><td align="left">20.3</td><td align="left">40.0</td><td align="left">2017</td></tr><tr><td align="left">ST-GCN<sup><xref ref-type="bibr" rid="CR27">27</xref></sup></td><td align="left">81.5</td><td align="left">88.3</td><td align="left">30.7</td><td align="left">52.8</td><td align="left">2018</td></tr><tr><td align="left">DS-LSTN<sup><xref ref-type="bibr" rid="CR43">43</xref></sup></td><td align="left">75.5</td><td align="left">84.2</td><td align="left">-</td><td align="left">-</td><td align="left">2020</td></tr><tr><td align="left">STD+RGB-DI<sup><xref ref-type="bibr" rid="CR44">44</xref></sup></td><td align="left">79.4</td><td align="left">84.1</td><td align="left">-</td><td align="left">-</td><td align="left">2020</td></tr><tr><td align="left">GFNet<sup><xref ref-type="bibr" rid="CR45">45</xref></sup></td><td align="left">82.0</td><td align="left">89.9</td><td align="left">-</td><td align="left">-</td><td align="left">2020</td></tr><tr><td align="left">STA<sup><xref ref-type="bibr" rid="CR46">46</xref></sup></td><td align="left">72.4</td><td align="left">79.7</td><td align="left">-</td><td align="left">-</td><td align="left">2021</td></tr><tr><td align="left">CNN+LSTM<sup><xref ref-type="bibr" rid="CR47">47</xref></sup></td><td align="left">81.9</td><td align="left">88.7</td><td align="left">-</td><td align="left">-</td><td align="left">2021</td></tr><tr><td align="left">PoT2I<sup><xref ref-type="bibr" rid="CR48">48</xref></sup></td><td align="left">83.9</td><td align="left">90.3</td><td align="left">-</td><td align="left">-</td><td align="left">2021</td></tr><tr><td align="left">C-CNN+HTLN<sup><xref ref-type="bibr" rid="CR49">49</xref></sup></td><td align="left">83.5</td><td align="left">86.8</td><td align="left">-</td><td align="left">-</td><td align="left">2022</td></tr><tr><td align="left">Custom ST-GCN<sup><xref ref-type="bibr" rid="CR41">41</xref></sup></td><td align="left">82.7</td><td align="left">90.2</td><td align="left">32.3</td><td align="left">54.5</td><td align="left">2023</td></tr><tr><td align="left">STG-NODE (ours)</td><td align="left"><bold>84</bold>.<bold>0</bold></td><td align="left"><bold>91</bold>.<bold>1</bold></td><td align="left"><bold>32</bold>.<bold>6</bold></td><td align="left"><bold>55</bold>.<bold>0</bold></td><td align="left">2023</td></tr></tbody></table></table-wrap></p><p id="Par38">Specifically, our model produces remarkable recognition results on the Kinetics Skeleton 400 and NTU RGB+D 60 datasets, achieving the best performance. However, when delving into the nuances of the Kinetics dataset, the recognition accuracy improvement achieved over the method of<sup><xref ref-type="bibr" rid="CR41">41</xref></sup> is not sufficiently large. A plausible explanation for this phenomenon can be attributed to the composition of the Kinetics Skeleton 400 test set. This dataset contains individuals whose behavioral styles or unique characteristics are not fully represented within the scope of the training set. The emergence of this new individual variability poses urgent challenges and may require the inclusion of additional data to enhance the generalizability of the model. Thus, it becomes evident that our model yearns for a more substantial sample size to adeptly assimilate and adapt to the diverse intricacies of different individuals, thereby elevating its performance on the Kinetics dataset.</p><p id="Par39">Furthermore, notably, it performs well on the NTU RGB+D dataset, surpassing the benchmarks set by models such as that of<sup><xref ref-type="bibr" rid="CR41">41</xref></sup>. This serves as a compelling testament to the exceptional prowess of our model with respect to addressing action recognition tasks from varying viewpoints. The undeniable significance of this capacity becomes even more pronounced in real-world applications.</p><p id="Par40">To more clearly highlight the advantages of STG-NODE over the existing models, we select some action categories with obvious coherence differences for comparison. Generally, people have a better memory and understanding of common action sequences and therefore perform more naturally and coherently when performing such actions. We call these action categories &#x0201c;strong-coherence action categories&#x0201d;, as shown in the red box in Figure&#x000a0;<xref rid="Fig4" ref-type="fig">4</xref>, including drinking water (label: 1), sitting down (label: 8), standing (label: 9), etc. Conversely, some actions may feel confusing or unnatural, with less coherence. We call these action categories &#x0201c;weakly coherent action categories&#x0201d;, as shown in the green box in Figure&#x000a0;<xref rid="Fig4" ref-type="fig">4</xref>, including typing on a keyboard (label: 30), experiencing back pain (label: 46) and vomiting (label: 48), etc. In Figure&#x000a0;<xref rid="Fig4" ref-type="fig">4</xref>, we clearly observe that in the weakly coherent action category, STG-NODE has a smaller performance improvement over the ST-GCN (dark areas remain in the green box). However, in the strongly coherent action category, STG-NODE yields significantly improved performance over that of the ST-GCN model (the dark areas in the red box have basically disappeared).<fig id="Fig4"><label>Figure 4</label><caption><p>Confusion matrices produced for the NTU RGB+D 60 dataset. (<bold>a</bold>) shows the experimental results of STG-NODE; (<bold>b</bold>) presents the experimental result of the ST-GCN.</p></caption><graphic xlink:href="41598_2024_58190_Fig4_HTML" id="MO4"/></fig></p><p id="Par41">The two datasets used in the experiments exhibit distinctly disparate properties. While the Kinetic dataset employs a 2D skeleton detected by a deep neural network as the input, the NTU RGB+D 60 dataset employs data from a Kinect depth sensor. Further differentiating the two datasets, NTU RGB+D 60 employs a stationary camera, whereas the Kinetics dataset often captures videos using handheld devices, thus introducing significant camera motion. The noteworthy efficacy exhibited by the proposed STG-NODE model across both datasets underscores the prowess of our spatiotemporal dependency modeling approach. This accomplishment can be attributed to two key factors.<list list-type="bullet"><list-item><p id="Par42">Leveraging the tensor-based ODE framework significantly augments the temporal modeling ability of the model. Simultaneously, the dynamics introduced by the ODE can be construed as the evolutionary journey of an action sequence, thereby providing insights into the rationale used by the model for action recognition.</p></list-item><list-item><p id="Par43">Employing a strategic approach, the DTW algorithm serves as a conduit that introduces the semantic adjacency matrix. This augmentation bolsters the semantic acumen of the model and adeptly mitigates the influence of individual intraindividual action discrepancies on the precision achieved in skeleton-based action recognition tasks.</p></list-item></list></p></sec><sec id="Sec13"><title>Ablation study</title><p id="Par44">Ablation experiments concerning action recognition are performed on the NTU RGB+D 60 dataset and the Kinetics Skeleton 400 dataset to examine the effectiveness of the proposed components in the above STG-NODE model. Then, different learning rates are set for a verification implemented on the X-Sub benchmark and X-View benchmark to achieve the best recognition accuracy.</p><p id="Par45"><bold>Evaluation of the effectiveness of each STG-NODE module:</bold> To determine the necessity and effectiveness of the individual modules in the STG-NODE model, a system analysis is performed by iteratively omitting certain modules from the original architecture and subsequently comparing the performances of the ablated versions. Two different variants of the STG-NODE model are designed for this purpose.</p><p id="Par46">1. STG-Semantic: In this model, a semantic adjacency matrix is constructed based on the semantic similarity of the target skeleton. However, the ODE solver is replaced by a regular GCN to verify the effectiveness of the ODE structure in terms of capturing long-range dependencies.</p><p id="Par47">2. STG-ODE*: This model contains ODE modules but does not involve the creation of specialized adjacency matrices. This omission is intended to identify the necessity of introducing a semantic adjacency matrix.<table-wrap id="Tab2"><label>Table 2</label><caption><p>Comparisons between the accuracy (%) of our model and that of each variant on the NTU RGB+D 60 dataset and the Kinetics Skeleton 400 dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Method</th><th align="left">X-Sub</th><th align="left">X-view</th><th align="left">Kinetics Top1</th><th align="left">Kinetics Top5</th></tr></thead><tbody><tr><td align="left">STG-semantic</td><td align="left">80.8</td><td align="left">89.0</td><td align="left">30.8</td><td align="left">53.1</td></tr><tr><td align="left">STG-ODE*</td><td align="left">81.9</td><td align="left">90.1</td><td align="left">31.4</td><td align="left">53.7</td></tr><tr><td align="left">STG-NODE</td><td align="left">84.0</td><td align="left">91.1</td><td align="left">32.6</td><td align="left">55.0</td></tr></tbody></table></table-wrap></p><p id="Par48">The results are shown in Table&#x000a0;<xref rid="Tab2" ref-type="table">2</xref>, which shows that the accuracy and efficiency of a model adding any module exceed those of the baseline model. It is worth noting that the best performance is achieved when all modules are integrated. This synergy leads to significant improvements in the accuracies achieved on the X-View and X-Sub benchmarks of the NTU RGB+D 60 dataset (2.1% and 3.2%, respectively), while improving the top-1 and top-5 accuracies attained on the Kinetics Skeleton 400 dataset by up to 1.8% and 1.9%, respectively.<fig id="Fig5"><label>Figure 5</label><caption><p>Visualization of kicking actions. (<bold>a</bold>) is the first action of person No. 1, (<bold>b</bold>) is the second action of person No. 1, (<bold>c</bold>) is the first action of person No. 2, and (<bold>d</bold>) is the second action of person No. 2. The two actions occur at different angles, heights and distances.</p></caption><graphic xlink:href="41598_2024_58190_Fig5_HTML" id="MO5"/></fig></p><p id="Par49">These results underscore the significant performance improvement provided by our STG-NODE architecture due to its innovative temporal modeling approach and specialized semantic adjacency matrix. In addition, to more clearly demonstrate the effect of integrating the semantic adjacency matrix into the ODE-TCN module, we draw action visualization diagrams of different individuals performing the same action (such as kicking something). As shown in Figure&#x000a0;<xref rid="Fig5" ref-type="fig">5</xref>, the yellow highlighted part represents the edge composed of joint points with higher correlations when executing the action. When the model encounters different individuals performing the same action at different speeds, it focuses on these parts to help mitigate the impact of intraindividual action differences on the accuracy of skeleton-based action recognition.<fig id="Fig6"><label>Figure 6</label><caption><p>Recognition accuracy fluctuations observed in two accuracy evaluations conducted on the NTU RGB+D dataset with different learning rates.</p></caption><graphic xlink:href="41598_2024_58190_Fig6_HTML" id="MO6"/></fig></p><p id="Par50"><bold>Selection of the Optimal Learning Rate for STG-NODE:</bold> We conduct a comprehensive evaluation of the accuracy achieved by STG-NODE across 7 distinct learning rates. Figure&#x000a0;<xref rid="Fig6" ref-type="fig">6</xref> (b) compares the top-5 accuracy results obtained on the two benchmarks, and Figure&#x000a0;<xref rid="Fig6" ref-type="fig">6</xref> (a) compares the top-1 accuracy results obtained on the two benchmarks. Notably, as depicted in the figure, the experimental accuracy peaks when the learning rate is set to 0.15.</p></sec></sec><sec id="Sec14"><title>Conclusion</title><p id="Par51">Many efforts have been made to address the complex challenge of action recognition. However, little attention has been given to solving the difficult problem of extracting long-range dependencies without succumbing to the oversmoothing problem that is inherent in GCN-related architectures. This paper presents a groundbreaking ODE-based spatiotemporal forecasting model called STG-NODE. To the best of our knowledge, this is the first attempt to link continuous differential equations to node representations for developing skeleton-based action recognition networks, and STG-NODE provides the ability to shape deeper architectures and exploit a wider range of dependencies than can other methods. Furthermore, the incorporation of a customized semantic adjacency matrix greatly improves the efficiency of the model. The performance achieved by STG-NODE in four challenging tests (two benchmarks plus two metrics) is better than that of many existing methods. In future research, we will delve into the extraction of complex local features from skeletons and consider further exploiting a graph structure to capture the relationships between different parts of an input sequence, such as the interaction dependencies between different body parts in human activity recognition tasks.</p></sec></body><back><fn-group><fn><p><bold>Publisher's note</bold></p><p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></fn></fn-group><ack><title>Acknowledgements</title><p>This work is supported by The Science and Technology Foundation of Guizhou Province (QKHJC-ZK(2021)YB015) and Guizhou Provincial Key Technology R &#x00026;D Program (QKHZC(2022)YB074).</p></ack><notes notes-type="author-contribution"><title>Author contributions</title><p>J.L. and X.T. conceived the experiment(s), L.P. conducted the experiment(s), L.P. analysed the results. All authors reviewed the manuscript.</p></notes><notes notes-type="data-availability"><title>Data availability</title><p>All data included in this study are available upon request by contact with the corresponding author.</p></notes><notes id="FPar1" notes-type="COI-statement"><title>Competing interests</title><p id="Par52">The authors declare no competing interests.</p></notes><ref-list id="Bib1"><title>References</title><ref id="CR1"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xu</surname><given-names>C</given-names></name><etal/></person-group><article-title>Uncertainty-aware multiview deep learning for internet of things applications</article-title><source>IEEE Trans. Industr. Inf.</source><year>2022</year><volume>19</volume><fpage>1456</fpage><lpage>1466</lpage><pub-id pub-id-type="doi">10.1109/TII.2022.3206343</pub-id></element-citation></ref><ref id="CR2"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhao</surname><given-names>W</given-names></name><etal/></person-group><article-title>Telecomnet: Tag-based weakly-supervised modally cooperative hashing network for image retrieval</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2021</year><volume>44</volume><fpage>7940</fpage><lpage>7954</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2021.3114089</pub-id></element-citation></ref><ref id="CR3"><label>3.</label><mixed-citation publication-type="other">Xu, C. <italic>et&#x000a0;al.</italic> Reliable conflictive multi-view learning. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2402.16897">arXiv:2402.16897</ext-link> (2024).</mixed-citation></ref><ref id="CR4"><label>4.</label><mixed-citation publication-type="other">Carreira, J. &#x00026; Zisserman, A. Quo vadis, action recognition? a new model and the kinetics dataset. In <italic>proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</italic>, 6299&#x02013;6308 (2017).</mixed-citation></ref><ref id="CR5"><label>5.</label><mixed-citation publication-type="other">Duan, H., Zhao, Y., Xiong, Y., Liu, W. &#x00026; Lin, D. Omni-sourced webly-supervised learning for video recognition. In <italic>European Conference on Computer Vision</italic>, 670&#x02013;688 (Springer, 2020).</mixed-citation></ref><ref id="CR6"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aggarwal</surname><given-names>JK</given-names></name><name><surname>Ryoo</surname><given-names>MS</given-names></name></person-group><article-title>Human activity analysis: A review</article-title><source>Acm Comput. Surv. (Csur)</source><year>2011</year><volume>43</volume><fpage>1</fpage><lpage>43</lpage><pub-id pub-id-type="doi">10.1145/1922649.1922653</pub-id></element-citation></ref><ref id="CR7"><label>7.</label><mixed-citation publication-type="other">Hu, W., Tan, T., Wang, L. &#x00026; Maybank, S. A survey on visual surveillance of object motion and behaviors. <italic>IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)</italic><bold>34</bold>, 334&#x02013;352 (2004).</mixed-citation></ref><ref id="CR8"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Su</surname><given-names>Y</given-names></name><name><surname>Zhu</surname><given-names>H</given-names></name><name><surname>Tan</surname><given-names>Y</given-names></name><name><surname>An</surname><given-names>S</given-names></name><name><surname>Xing</surname><given-names>M</given-names></name></person-group><article-title>Prime: privacy-preserving video anomaly detection via motion exemplar guidance</article-title><source>Knowl.-Based Syst.</source><year>2023</year><volume>278</volume><fpage>110872</fpage><pub-id pub-id-type="doi">10.1016/j.knosys.2023.110872</pub-id></element-citation></ref><ref id="CR9"><label>9.</label><mixed-citation publication-type="other">Li, B. <italic>et&#x000a0;al.</italic> Skeleton based action recognition using translation-scale invariant image mapping and multi-scale deep cnn. In <italic>2017 IEEE International Conference on Multimedia &#x00026; Expo Workshops (ICMEW)</italic>, 601&#x02013;604 (IEEE, 2017).</mixed-citation></ref><ref id="CR10"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>M</given-names></name><name><surname>Liu</surname><given-names>H</given-names></name><name><surname>Chen</surname><given-names>C</given-names></name></person-group><article-title>Enhanced skeleton visualization for view invariant human action recognition</article-title><source>Pattern Recogn.</source><year>2017</year><volume>68</volume><fpage>346</fpage><lpage>362</lpage><pub-id pub-id-type="doi">10.1016/j.patcog.2017.02.030</pub-id></element-citation></ref><ref id="CR11"><label>11.</label><mixed-citation publication-type="other">Soo&#x000a0;Kim, T. &#x00026; Reiter, A. Interpretable 3d human action analysis with temporal convolutional networks. In <italic>Proceedings of the IEEE conference on computer vision and pattern recognition workshops</italic>, 20&#x02013;28 (2017).</mixed-citation></ref><ref id="CR12"><label>12.</label><mixed-citation publication-type="other">Liu, J., Shahroudy, A., Xu, D. &#x00026; Wang, G. Spatio-temporal lstm with trust gates for 3d human action recognition. In <italic>Computer Vision&#x02013;ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part III 14</italic>, 816&#x02013;833 (Springer, 2016).</mixed-citation></ref><ref id="CR13"><label>13.</label><mixed-citation publication-type="other">Shahroudy, A., Liu, J., Ng, T.-T. &#x00026; Wang, G. Ntu rgb+ d: A large scale dataset for 3d human activity analysis. In <italic>Proceedings of the IEEE conference on computer vision and pattern recognition</italic>, 1010&#x02013;1019 (2016).</mixed-citation></ref><ref id="CR14"><label>14.</label><mixed-citation publication-type="other">Du, Y., Wang, W. &#x00026; Wang, L. Hierarchical recurrent neural network for skeleton based action recognition. In <italic>Proceedings of the IEEE conference on computer vision and pattern recognition</italic>, 1110&#x02013;1118 (2015).</mixed-citation></ref><ref id="CR15"><label>15.</label><mixed-citation publication-type="other">Li, S., Li, W., Cook, C., Zhu, C. &#x00026; Gao, Y. Independently recurrent neural network (indrnn): Building a longer and deeper rnn. In <italic>Proceedings of the IEEE conference on computer vision and pattern recognition</italic>, 5457&#x02013;5466 (2018).</mixed-citation></ref><ref id="CR16"><label>16.</label><mixed-citation publication-type="other">Si, C., Jing, Y., Wang, W., Wang, L. &#x00026; Tan, T. Skeleton-based action recognition with spatial reasoning and temporal stack learning. In <italic>Proceedings of the European conference on computer vision (ECCV)</italic>, 103&#x02013;118 (2018).</mixed-citation></ref><ref id="CR17"><label>17.</label><mixed-citation publication-type="other">Song, S., Lan, C., Xing, J., Zeng, W. &#x00026; Liu, J. An end-to-end spatio-temporal attention model for human action recognition from skeleton data. In <italic>Proceedings of the AAAI conference on artificial intelligence</italic>, vol.&#x000a0;31 (2017).</mixed-citation></ref><ref id="CR18"><label>18.</label><mixed-citation publication-type="other">Zhang, P. <italic>et&#x000a0;al.</italic> View adaptive recurrent neural networks for high performance human action recognition from skeleton data. In <italic>Proceedings of the IEEE international conference on computer vision</italic>, 2117&#x02013;2126 (2017).</mixed-citation></ref><ref id="CR19"><label>19.</label><mixed-citation publication-type="other">Zhu, W. <italic>et&#x000a0;al.</italic> Co-occurrence feature learning for skeleton based action recognition using regularized deep lstm networks. In <italic>Proceedings of the AAAI conference on artificial intelligence</italic>, vol.&#x000a0;30 (2016).</mixed-citation></ref><ref id="CR20"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>C</given-names></name><name><surname>Hou</surname><given-names>Y</given-names></name><name><surname>Wang</surname><given-names>P</given-names></name><name><surname>Li</surname><given-names>W</given-names></name></person-group><article-title>Multiview-based 3-d action recognition using deep networks</article-title><source>IEEE Trans. Hum.-Mach. Syst.</source><year>2018</year><volume>49</volume><fpage>95</fpage><lpage>104</lpage><pub-id pub-id-type="doi">10.1109/THMS.2018.2883001</pub-id></element-citation></ref><ref id="CR21"><label>21.</label><mixed-citation publication-type="other">Ke, Q., Bennamoun, M., An, S., Sohel, F. &#x00026; Boussaid, F. A new representation of skeleton sequences for 3d action recognition. In <italic>Proceedings of the IEEE conference on computer vision and pattern recognition</italic>, 3288&#x02013;3297 (2017).</mixed-citation></ref><ref id="CR22"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cao</surname><given-names>C</given-names></name><etal/></person-group><article-title>Skeleton-based action recognition with gated convolutional neural networks</article-title><source>IEEE Trans. Circuits Syst. Video Technol.</source><year>2018</year><volume>29</volume><fpage>3247</fpage><lpage>3257</lpage><pub-id pub-id-type="doi">10.1109/TCSVT.2018.2879913</pub-id></element-citation></ref><ref id="CR23"><label>23.</label><mixed-citation publication-type="other">Atwood, J. &#x00026; Towsley, D. Diffusion-convolutional neural networks. <italic>Advances in neural information processing systems</italic><bold>29</bold> (2016).</mixed-citation></ref><ref id="CR24"><label>24.</label><mixed-citation publication-type="other">Duvenaud, D.&#x000a0;K. <italic>et&#x000a0;al.</italic> Convolutional networks on graphs for learning molecular fingerprints. <italic>Advances in neural information processing systems</italic><bold>28</bold> (2015).</mixed-citation></ref><ref id="CR25"><label>25.</label><mixed-citation publication-type="other">Hamilton, W., Ying, Z. &#x00026; Leskovec, J. Inductive representation learning on large graphs. <italic>Adv. Neural Inf. Proc. syst.</italic><bold>30</bold> (2017).</mixed-citation></ref><ref id="CR26"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Su</surname><given-names>Y</given-names></name><name><surname>Xing</surname><given-names>M</given-names></name><name><surname>An</surname><given-names>S</given-names></name><name><surname>Peng</surname><given-names>W</given-names></name><name><surname>Feng</surname><given-names>Z</given-names></name></person-group><article-title>Vdarn: video disentangling attentive relation network for few-shot and zero-shot action recognition</article-title><source>Ad Hoc Netw.</source><year>2021</year><volume>113</volume><fpage>102380</fpage><pub-id pub-id-type="doi">10.1016/j.adhoc.2020.102380</pub-id></element-citation></ref><ref id="CR27"><label>27.</label><mixed-citation publication-type="other">Yan, S., Xiong, Y. &#x00026; Lin, D. Spatial temporal graph convolutional networks for skeleton-based action recognition. In <italic>Proceedings of the AAAI conference on artificial intelligence</italic>, vol.&#x000a0;32 (2018).</mixed-citation></ref><ref id="CR28"><label>28.</label><mixed-citation publication-type="other">Vemulapalli, R., Arrate, F. &#x00026; Chellappa, R. Human action recognition by representing 3d skeletons as points in a lie group. In <italic>Proceedings of the IEEE conference on computer vision and pattern recognition</italic>, 588&#x02013;595 (2014).</mixed-citation></ref><ref id="CR29"><label>29.</label><mixed-citation publication-type="other">Li, M. <italic>et&#x000a0;al.</italic> Actional-structural graph convolutional networks for skeleton-based action recognition. In <italic>Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</italic>, 3595&#x02013;3603 (2019).</mixed-citation></ref><ref id="CR30"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peng</surname><given-names>W</given-names></name><name><surname>Hong</surname><given-names>X</given-names></name><name><surname>Chen</surname><given-names>H</given-names></name><name><surname>Zhao</surname><given-names>G</given-names></name></person-group><article-title>Learning graph convolutional network for skeleton-based human action recognition by neural searching</article-title><source>In Proc. AAAI Conf. Artif. Intell.</source><year>2020</year><volume>34</volume><fpage>2669</fpage><lpage>2676</lpage></element-citation></ref><ref id="CR31"><label>31.</label><mixed-citation publication-type="other">Si, C., Chen, W., Wang, W., Wang, L. &#x00026; Tan, T. An attention enhanced graph convolutional lstm network for skeleton-based action recognition. In <italic>Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</italic>, 1227&#x02013;1236 (2019).</mixed-citation></ref><ref id="CR32"><label>32.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shi</surname><given-names>L</given-names></name><name><surname>Zhang</surname><given-names>Y</given-names></name><name><surname>Cheng</surname><given-names>J</given-names></name><name><surname>Lu</surname><given-names>H</given-names></name></person-group><article-title>Skeleton-based action recognition with multi-stream adaptive graph convolutional networks</article-title><source>IEEE Trans. Image Process.</source><year>2020</year><volume>29</volume><fpage>9532</fpage><lpage>9545</lpage><pub-id pub-id-type="doi">10.1109/TIP.2020.3028207</pub-id></element-citation></ref><ref id="CR33"><label>33.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>Z</given-names></name><name><surname>Li</surname><given-names>S</given-names></name><name><surname>Yang</surname><given-names>B</given-names></name><name><surname>Li</surname><given-names>Q</given-names></name><name><surname>Liu</surname><given-names>H</given-names></name></person-group><article-title>Multi-scale spatial temporal graph convolutional network for skeleton-based action recognition</article-title><source>In Proc. AAAI Conf. Artif. Intell.</source><year>2021</year><volume>35</volume><fpage>1113</fpage><lpage>1122</lpage></element-citation></ref><ref id="CR34"><label>34.</label><mixed-citation publication-type="other">Chen, R.&#x000a0;T., Rubanova, Y., Bettencourt, J. &#x00026; Duvenaud, D.&#x000a0;K. Neural ordinary differential equations. <italic>Adv. Neural Inf. Process. Syst.</italic><bold>31</bold> (2018).</mixed-citation></ref><ref id="CR35"><label>35.</label><mixed-citation publication-type="other">Xhonneux, L.-P., Qu, M. &#x00026; Tang, J. Continuous graph neural networks. In <italic>International Conference on Machine Learning</italic>, 10432&#x02013;10441 (PMLR, 2020).</mixed-citation></ref><ref id="CR36"><label>36.</label><mixed-citation publication-type="other">Kay, W. <italic>et&#x000a0;al.</italic> The kinetics human action video dataset. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1705.06950">arXiv:1705.06950</ext-link> (2017).</mixed-citation></ref><ref id="CR37"><label>37.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Salvador</surname><given-names>S</given-names></name><name><surname>Chan</surname><given-names>P</given-names></name></person-group><article-title>Toward accurate dynamic time warping in linear time and space</article-title><source>Intell. Data Anal.</source><year>2007</year><volume>11</volume><fpage>561</fpage><lpage>580</lpage><pub-id pub-id-type="doi">10.3233/IDA-2007-11508</pub-id></element-citation></ref><ref id="CR38"><label>38.</label><mixed-citation publication-type="other">Li, Q., Han, Z. &#x00026; Wu, X.-M. Deeper insights into graph convolutional networks for semi-supervised learning. In <italic>Proceedings of the AAAI conference on artificial intelligence</italic>, vol.&#x000a0;32 (2018).</mixed-citation></ref><ref id="CR39"><label>39.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhou</surname><given-names>J</given-names></name><etal/></person-group><article-title>Graph neural networks: A review of methods and applications</article-title><source>AI open</source><year>2020</year><volume>1</volume><fpage>57</fpage><lpage>81</lpage><pub-id pub-id-type="doi">10.1016/j.aiopen.2021.01.001</pub-id></element-citation></ref><ref id="CR40"><label>40.</label><mixed-citation publication-type="other">Fang, Z., Long, Q., Song, G. &#x00026; Xie, K. Spatial-temporal graph ode networks for traffic flow forecasting. In <italic>Proceedings of the 27th ACM SIGKDD conference on knowledge discovery &#x00026; data mining</italic>, 364&#x02013;373 (2021).</mixed-citation></ref><ref id="CR41"><label>41.</label><mixed-citation publication-type="other">Lovanshi, M. &#x00026; Tiwari, V. Human skeleton pose and spatio-temporal feature-based activity recognition using st-gcn. <italic>Multimedia Tools Appl.</italic> 1&#x02013;26 (2023).</mixed-citation></ref><ref id="CR42"><label>42.</label><mixed-citation publication-type="other">Cao, Z., Simon, T., Wei, S.-E. &#x00026; Sheikh, Y. Realtime multi-person 2d pose estimation using part affinity fields. In <italic>Proceedings of the IEEE conference on computer vision and pattern recognition</italic>, 7291&#x02013;7299 (2017).</mixed-citation></ref><ref id="CR43"><label>43.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jiang</surname><given-names>X</given-names></name><name><surname>Xu</surname><given-names>K</given-names></name><name><surname>Sun</surname><given-names>T</given-names></name></person-group><article-title>Action recognition scheme based on skeleton representation with ds-lstm network</article-title><source>IEEE Trans. Circuits Syst. Video Technol.</source><year>2019</year><volume>30</volume><fpage>2129</fpage><lpage>2140</lpage><pub-id pub-id-type="doi">10.1109/TCSVT.2019.2914137</pub-id></element-citation></ref><ref id="CR44"><label>44.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dhiman</surname><given-names>C</given-names></name><name><surname>Vishwakarma</surname><given-names>DK</given-names></name></person-group><article-title>View-invariant deep architecture for human action recognition using two-stream motion and shape temporal dynamics</article-title><source>IEEE Trans. Image Process.</source><year>2020</year><volume>29</volume><fpage>3835</fpage><lpage>3844</lpage><pub-id pub-id-type="doi">10.1109/TIP.2020.2965299</pub-id></element-citation></ref><ref id="CR45"><label>45.</label><mixed-citation publication-type="other">Liu, H., Zhang, L., Guan, L. &#x00026; Liu, M. Gfnet: A lightweight group frame network for efficient human action recognition. In <italic>ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</italic>, 2583&#x02013;2587 (IEEE, 2020).</mixed-citation></ref><ref id="CR46"><label>46.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ding</surname><given-names>C</given-names></name><name><surname>Liu</surname><given-names>K</given-names></name><name><surname>Cheng</surname><given-names>F</given-names></name><name><surname>Belyaev</surname><given-names>E</given-names></name></person-group><article-title>Spatio-temporal attention on manifold space for 3d human action recognition</article-title><source>Appl. Intell.</source><year>2021</year><volume>51</volume><fpage>560</fpage><lpage>570</lpage><pub-id pub-id-type="doi">10.1007/s10489-020-01803-3</pub-id></element-citation></ref><ref id="CR47"><label>47.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shao</surname><given-names>Z</given-names></name><name><surname>Li</surname><given-names>Y</given-names></name><name><surname>Zhang</surname><given-names>H</given-names></name></person-group><article-title>Learning representations from skeletal self-similarities for cross-view action recognition</article-title><source>IEEE Trans. Circuits Syst. Video Technol.</source><year>2020</year><volume>31</volume><fpage>160</fpage><lpage>174</lpage><pub-id pub-id-type="doi">10.1109/TCSVT.2020.2965574</pub-id></element-citation></ref><ref id="CR48"><label>48.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huynh-The</surname><given-names>T</given-names></name><name><surname>Hua</surname><given-names>C-H</given-names></name><name><surname>Ngo</surname><given-names>T-T</given-names></name><name><surname>Kim</surname><given-names>D-S</given-names></name></person-group><article-title>Image representation of pose-transition feature for 3d skeleton-based action recognition</article-title><source>Inf. Sci.</source><year>2020</year><volume>513</volume><fpage>112</fpage><lpage>126</lpage><pub-id pub-id-type="doi">10.1016/j.ins.2019.10.047</pub-id></element-citation></ref><ref id="CR49"><label>49.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vishwakarma</surname><given-names>DK</given-names></name><name><surname>Jain</surname><given-names>K</given-names></name></person-group><article-title>Three-dimensional human activity recognition by forming a movement polygon using posture skeletal data from depth sensor</article-title><source>ETRI J.</source><year>2022</year><volume>44</volume><fpage>286</fpage><lpage>299</lpage><pub-id pub-id-type="doi">10.4218/etrij.2020-0101</pub-id></element-citation></ref></ref-list></back></article>