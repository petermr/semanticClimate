<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd"> 
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN?><?DTDIdentifier.IdentifierType public?><?SourceDTD.DTDName JATS-journalpublishing1.dtd?><?SourceDTD.Version 39.96?><?ConverterInfo.XSLTName jats2jats3.xsl?><?ConverterInfo.Version 1?><?subarticle pone.0298650.r001?><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">PLoS One</journal-id><journal-id journal-id-type="iso-abbrev">PLoS One</journal-id><journal-id journal-id-type="publisher-id">plos</journal-id><journal-title-group><journal-title>PLOS ONE</journal-title></journal-title-group><issn pub-type="epub">1932-6203</issn><publisher><publisher-name>Public Library of Science</publisher-name><publisher-loc>San Francisco, CA USA</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmcid">10965085</article-id><article-id pub-id-type="publisher-id">PONE-D-23-19326</article-id><article-id pub-id-type="doi">10.1371/journal.pone.0298650</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social Sciences</subject><subj-group><subject>Linguistics</subject><subj-group><subject>Semantics</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social Sciences</subject><subj-group><subject>Linguistics</subject><subj-group><subject>Semantics</subject><subj-group><subject>Lexical Semantics</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and Analysis Methods</subject><subj-group><subject>Mathematical and Statistical Techniques</subject><subj-group><subject>Mathematical Models</subject><subj-group><subject>Random Walk</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social Sciences</subject><subj-group><subject>Linguistics</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social Sciences</subject><subj-group><subject>Linguistics</subject><subj-group><subject>Languages</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and Information Sciences</subject><subj-group><subject>Information Technology</subject><subj-group><subject>Natural Language Processing</subject><subj-group><subject>Word Embedding</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social Sciences</subject><subj-group><subject>Linguistics</subject><subj-group><subject>Evolutionary Linguistics</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Engineering and Technology</subject><subj-group><subject>Measurement</subject><subj-group><subject>Distance Measurement</subject></subj-group></subj-group></subj-group></article-categories><title-group><article-title>Anomalous diffusion analysis of semantic evolution in major Indo-European languages</article-title><alt-title alt-title-type="running-head">Anomalous diffusion analysis of semantic evolution in major Indo-European languages</alt-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-1085-1939</contrib-id><name><surname>Asztalos</surname><given-names>Bogd&#x000e1;n</given-names></name><role content-type="http://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role content-type="http://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role content-type="http://credit.niso.org/contributor-roles/investigation/">Investigation</role><role content-type="http://credit.niso.org/contributor-roles/methodology/">Methodology</role><role content-type="http://credit.niso.org/contributor-roles/software/">Software</role><role content-type="http://credit.niso.org/contributor-roles/visualization/">Visualization</role><role content-type="http://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#x02013; original draft</role><role content-type="http://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><xref rid="aff001" ref-type="aff">
<sup>1</sup>
</xref></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Palla</surname><given-names>Gergely</given-names></name><role content-type="http://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role content-type="http://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role content-type="http://credit.niso.org/contributor-roles/investigation/">Investigation</role><role content-type="http://credit.niso.org/contributor-roles/methodology/">Methodology</role><role content-type="http://credit.niso.org/contributor-roles/supervision/">Supervision</role><role content-type="http://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#x02013; original draft</role><role content-type="http://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><xref rid="aff001" ref-type="aff">
<sup>1</sup>
</xref><xref rid="aff002" ref-type="aff">
<sup>2</sup>
</xref><xref rid="cor001" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-5722-1598</contrib-id><name><surname>Cz&#x000e9;gel</surname><given-names>D&#x000e1;niel</given-names></name><role content-type="http://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role content-type="http://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role content-type="http://credit.niso.org/contributor-roles/investigation/">Investigation</role><role content-type="http://credit.niso.org/contributor-roles/methodology/">Methodology</role><role content-type="http://credit.niso.org/contributor-roles/supervision/">Supervision</role><role content-type="http://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#x02013; original draft</role><role content-type="http://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><xref rid="aff003" ref-type="aff">
<sup>3</sup>
</xref><xref rid="aff004" ref-type="aff">
<sup>4</sup>
</xref></contrib></contrib-group><aff id="aff001">
<label>1</label>
<addr-line>Deptartment of Biological Physics, E&#x000f6;tv&#x000f6;s University, Budapest, Hungary</addr-line>
</aff><aff id="aff002">
<label>2</label>
<addr-line>Health Services Management Training Centre, Semmelweis University, Budapest, Hungary</addr-line>
</aff><aff id="aff003">
<label>3</label>
<addr-line>Institute of Evolution, Centre for Ecological Research, Budapest, Hungary</addr-line>
</aff><aff id="aff004">
<label>4</label>
<addr-line>Parmenides Center for the Conceptual Foundations of Science, P&#x000f6;cking, Germany</addr-line>
</aff><contrib-group><contrib contrib-type="editor"><name><surname>Ribeiro</surname><given-names>Haroldo V.</given-names></name><role>Editor</role><xref rid="edit1" ref-type="aff"/></contrib></contrib-group><aff id="edit1">
<addr-line>Universidade Estadual de Maringa, BRAZIL</addr-line>
</aff><author-notes><fn fn-type="COI-statement" id="coi001"><p><bold>Competing Interests: </bold>The authors have declared that no competing interests exist.</p></fn><corresp id="cor001">* E-mail: <email>palla.gergely@emk.semmelweis.hu</email></corresp></author-notes><pub-date pub-type="collection"><year>2024</year></pub-date><pub-date pub-type="epub"><day>26</day><month>3</month><year>2024</year></pub-date><volume>19</volume><issue>3</issue><elocation-id>e0298650</elocation-id><history><date date-type="received"><day>21</day><month>6</month><year>2023</year></date><date date-type="accepted"><day>30</day><month>1</month><year>2024</year></date></history><permissions><copyright-statement>&#x000a9; 2024 Asztalos et al</copyright-statement><copyright-year>2024</copyright-year><copyright-holder>Asztalos et al</copyright-holder><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="pone.0298650.pdf"/><abstract><p>How do words change their meaning? Although semantic evolution is driven by a variety of distinct factors, including linguistic, societal, and technological ones, we find that there is one law that holds universally across five major Indo-European languages: that semantic evolution is subdiffusive. Using an automated pipeline of diachronic distributional semantic embedding that controls for underlying symmetries, we show that words follow stochastic trajectories in meaning space with an anomalous diffusion exponent <italic toggle="yes">&#x003b1;</italic> = 0.45 &#x000b1; 0.05 across languages, in contrast with diffusing particles that follow <italic toggle="yes">&#x003b1;</italic> = 1. Randomization methods indicate that preserving temporal correlations in semantic change <italic toggle="yes">directions</italic> is necessary to recover strongly subdiffusive behavior; however, correlations in change <italic toggle="yes">sizes</italic> play an important role too. We furthermore show that strong subdiffusion is a robust phenomenon under a wide variety of choices in data analysis and interpretation, such as the choice of fitting an ensemble average of displacements or averaging best-fit exponents of individual word trajectories.</p></abstract><funding-group><award-group id="award001"><funding-source>
<institution-wrap><institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100010661</institution-id><institution>Horizon 2020 Framework Programme</institution></institution-wrap>
</funding-source><award-id>101021607</award-id></award-group><award-group id="award002"><funding-source>
<institution-wrap><institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100003827</institution-id><institution>Nemzeti Kutat&#x000e1;si &#x000e9;s Technol&#x000f3;giai Hivatal</institution></institution-wrap>
</funding-source><award-id>K128780</award-id><principal-award-recipient>
<name><surname>Palla</surname><given-names>Gergely</given-names></name>
</principal-award-recipient></award-group><award-group id="award003"><funding-source>
<institution-wrap><institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100020626</institution-id><institution>Mesters&#x000e9;ges Intelligencia Nemzeti Laborat&#x000f3;rium</institution></institution-wrap>
</funding-source><award-id>RRF-2.3.1-21-2022-00004</award-id><principal-award-recipient>
<name><surname>Palla</surname><given-names>Gergely</given-names></name>
</principal-award-recipient></award-group><funding-statement>This project has received funding from the European Union&#x02019;s Horizon 2020 research and innovation programme under grant agreement no. 101021607 and was partially supported by the National Research, Development and Innovation Office under grant no. K128780 and by the the European Union project RRF-2.3.1-21-2022-00004 within the framework of the Artificial Intelligence National Laboratory. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement></funding-group><counts><fig-count count="3"/><table-count count="1"/><page-count count="18"/></counts><custom-meta-group><custom-meta id="data-availability"><meta-name>Data Availability</meta-name><meta-value>Data is downloaded from Google Books Ngrams Viewer Database, version 2, available at <ext-link xlink:href="https://storage.googleapis.com/books/ngrams/books/datasetsv2.html" ext-link-type="uri">https://storage.googleapis.com/books/ngrams/books/datasetsv2.html</ext-link>. Our Python code is available at <ext-link xlink:href="https://github.com/abogdan271/histwords" ext-link-type="uri">https://github.com/abogdan271/histwords</ext-link>.</meta-value></custom-meta></custom-meta-group></article-meta><notes><title>Data Availability</title><p>Data is downloaded from Google Books Ngrams Viewer Database, version 2, available at <ext-link xlink:href="https://storage.googleapis.com/books/ngrams/books/datasetsv2.html" ext-link-type="uri">https://storage.googleapis.com/books/ngrams/books/datasetsv2.html</ext-link>. Our Python code is available at <ext-link xlink:href="https://github.com/abogdan271/histwords" ext-link-type="uri">https://github.com/abogdan271/histwords</ext-link>.</p></notes></front><body><sec sec-type="intro" id="sec001"><title>Introduction</title><p>Cumulative cultural evolution at enormous scale and speed makes us a strikingly different species than the rest of the living world [<xref rid="pone.0298650.ref001" ref-type="bibr">1</xref>]. The ability of accumulating techniques and solutions one bit at a time, aggregating them across time and space gave us unprecedented power, that we use to shape the world. But how does this process of human cultural evolution unfold? Are there universal patterns that hold across cultures and eras? One human activity that allows us to zoom into the cumulative patterns of human thought is language production. We all comprehend and produce, all the time, keeping the cogwheels of language evolution moving. How do these cogwheels move?</p><p>The study of human language evolution necessitates an interdisciplinary approach and has significant theoretical and practical ramifications in a variety of fields from the core linguistic subfield of etymology to the more computational areas, like natural language processing [<xref rid="pone.0298650.ref002" ref-type="bibr">2</xref>&#x02013;<xref rid="pone.0298650.ref004" ref-type="bibr">4</xref>]. Mathematical models, simulations, and controlled experiments suggest that several factors play a role in semantic change, including the efficiency of communication as well as cognitive pressures on language acquisition [<xref rid="pone.0298650.ref005" ref-type="bibr">5</xref>, <xref rid="pone.0298650.ref006" ref-type="bibr">6</xref>]. In the case of empirical quantitative investigations, recent efforts focused on uncovering universal (i.e. language, time, genre, etc. independent) dynamical rules that govern <italic toggle="yes">frequency changes</italic> of (variants of) words or ordered combinations of them, called n-grams [<xref rid="pone.0298650.ref007" ref-type="bibr">7</xref>]. This effort provided us a handful of remarkable discoveries, including peaks or valleys of individual terms mirroring specific societal-political processes (e.g., censorship, propaganda, ideology shifts, cultural-technological drift, natural or social catastrophes, etc.), aggregate behavior of groups of terms (e.g., decay rate of fame of a cohort of people across historical time) [<xref rid="pone.0298650.ref008" ref-type="bibr">8</xref>], competition dynamics among linguistic variants (e.g., [<xref rid="pone.0298650.ref009" ref-type="bibr">9</xref>]) or synonyms, and even a marked difference between the temporal correlational patterns of word frequencies referring to natural or social processes [<xref rid="pone.0298650.ref010" ref-type="bibr">10</xref>].</p><p>N-gram frequencies also serve as the basis of the automated estimation of semantic similarities between words. Building on the distributional hypothesis, paraphrased as &#x0201c;a word is characterized by the company it keeps&#x0201d; [<xref rid="pone.0298650.ref011" ref-type="bibr">11</xref>]: similarity in meaning can be approximated by comparing neighborhoods of words over large corpora [<xref rid="pone.0298650.ref012" ref-type="bibr">12</xref>]. Many flavors of the distributional hypothesis have been formalized with classical methods [<xref rid="pone.0298650.ref013" ref-type="bibr">13</xref>, <xref rid="pone.0298650.ref014" ref-type="bibr">14</xref>], yet the advent of large-scale estimation of semantic similarity came with the &#x0201c;machine learning revolution&#x0201d; in the 2010s [<xref rid="pone.0298650.ref015" ref-type="bibr">15</xref>]. From these techniques, a successful and computationally efficient variant is the Word2vec embedding algorithm [<xref rid="pone.0298650.ref016" ref-type="bibr">16</xref>&#x02013;<xref rid="pone.0298650.ref018" ref-type="bibr">18</xref>]. Instead of calculating the vector representation of words explicitly, Word2vec solves a prediction problem by estimating semantic similarity based on sampling co-occurrences of words. This implements dimension reduction over the set of pairwise semantic similarities to embed words in a relatively low dimensional space (e.g., tens of thousands of words in a few hundred dimensional Euclidean space) [<xref rid="pone.0298650.ref019" ref-type="bibr">19</xref>]. With a corpus of time-labeled co-occurrences in hand, one can, in principle, track changes in these semantic similarities in an automated way by comparing embeddings corresponding to different times.</p><p>Indeed, a first endeavor utilizing this approach has identified two novel statistical patterns governing the <italic toggle="yes">semantic change</italic> of words: the law of conformity, stating that words with lower frequency change their meaning faster, and the law of innovation, finding that more polysemous words also tend to exhibit higher rate of semantic change [<xref rid="pone.0298650.ref020" ref-type="bibr">20</xref>]. Although the authors applied multiple word embedding variants, the validity of these results mirroring social-technological-linguistic effects as opposed to being mathematical artifacts is still debated [<xref rid="pone.0298650.ref021" ref-type="bibr">21</xref>]. It is because although state-of-the-art word embedding methods match reported semantic similarities across experiments, languages, and training corpora, they produce systematic biases over non-semantic features. One particular bias is due to the power-law distribution of word occurrences, known as Zipf&#x02019;s law [<xref rid="pone.0298650.ref022" ref-type="bibr">22</xref>, <xref rid="pone.0298650.ref023" ref-type="bibr">23</xref>], resulting in embeddings where low-frequency words tend to appear close to the center of the embedding whereas high-frequency words are pushed to the periphery. Such implicit biases point to the necessity of careful comparison of obtained results with those based on various randomized replicas of the dataset, systematically removing statistical dependencies until the phenomena at hand is no longer apparent.</p><p>Furthermore, with <italic toggle="yes">diachronic</italic> embeddings, three additional issues arise. First, embedding dimensions are arbitrary. There is no guarantee that dimensions match across subsequent timesteps. Second, Word2vec is a sampling-based method, and therefore, it is non-deterministic. Third, there are underlying symmetries along which embeddings are degenerate: namely, a set of transformations that change embeddings (and the embedding of context words) yet leave co-occurrences invariant. In order to tackle all four aforementioned obstacles, in this paper we develop a dynamical alignment method that is i) symmetry-agnostic, ii) averages over many runs to yield a robust estimate of embedding positions and their variances, iii) based on these principles, finds the best alignment over all timesteps, and iv) compares obtained results with those of systematically randomized versions of input data, removing various statistical dependencies in a step-by-step manner. Current diachronic embedding methods mostly focus on point iii) [<xref rid="pone.0298650.ref024" ref-type="bibr">24</xref>&#x02013;<xref rid="pone.0298650.ref026" ref-type="bibr">26</xref>]; here we suggest that all points above are needed for a robust estimation of semantic trajectories.</p><p>Starting from word trajectories, we construct the ensemble of all trajectories and ask whether such an ensemble of trajectories obeys any robust statistical regularities. We focus on measuring the systematic deviation of such trajectories from those of standard diffusing particles (i.e., a random walk), quantified by the <italic toggle="yes">anomalous diffusion exponent</italic>
<italic toggle="yes">&#x003b1;</italic>, defined as [<xref rid="pone.0298650.ref027" ref-type="bibr">27</xref>&#x02013;<xref rid="pone.0298650.ref030" ref-type="bibr">30</xref>]
<disp-formula id="pone.0298650.e001"><alternatives><graphic xlink:href="pone.0298650.e001.jpg" id="pone.0298650.e001g" position="anchor"/><mml:math id="M1" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msup><mml:mrow><mml:mo>|</mml:mo><mml:mo>&#x00394;</mml:mo><mml:mi>x</mml:mi><mml:mo>|</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>&#x0223c;</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mi>&#x003b1;</mml:mi></mml:msup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives>
<label>(1)</label></disp-formula>
where &#x00394;<italic toggle="yes">x</italic> = <italic toggle="yes">x</italic>(<italic toggle="yes">t</italic>) &#x02212; <italic toggle="yes">x</italic>(0) is the displacement of the position vector <italic toggle="yes">x</italic> of a word in the <italic toggle="yes">D</italic>-dimensional embedding space at time <italic toggle="yes">t</italic> compared to its starting position at time <italic toggle="yes">t</italic> = 0.</p><p>Standard diffusion, corresponding to random walk-like trajectories, is characterized by an anomalous diffusion exponent <italic toggle="yes">&#x003b1;</italic> = 1. There are, however, a variety of more general microscopic rules that generate trajectories that still belong to the <italic toggle="yes">&#x003b1;</italic> = 1 class, including i) trajectories with exponentially decaying memory, ii) the collective motion of weakly interacting particles, iii) varying step sizes or waiting times between steps unless those are not power-law distributed, iv) random walk in not very statistically heterogeneous media. Consequently, the diffusive (<italic toggle="yes">&#x003b1;</italic> = 1) class is very general, and potential deviations from it indicate the violation of i), ii), iii), or iv) above, or other characteristics that are not discussed here. In other words, an observed anomalous diffusion exponent <italic toggle="yes">&#x003b1;</italic> &#x02260; 1 implies (a combination of) particular underlying microscopic rules that generate the dynamics, notably power-law decaying memory (violation of i) above, modeled by e.g., fractional Brownian motion), the collective motion of strongly interacting particles, such as in &#x0201c;jamming&#x0201d; (violation of ii)), power-law distributed step sizes or waiting times (violation of iii)), diffusion in disordered, such as self-similar, media (violation of iv)), or changing dynamical rules, such as aging [<xref rid="pone.0298650.ref027" ref-type="bibr">27</xref>&#x02013;<xref rid="pone.0298650.ref030" ref-type="bibr">30</xref>]. This makes the anomalous diffusion exponent <italic toggle="yes">&#x003b1;</italic> a simple yet powerful indicator of the possible underlying microscopic statistical rules that govern the unfolding of the process, in this case, the meaning change of words. Anomalous diffusion is further subdivided into superdiffusion and subdiffusion, with <italic toggle="yes">&#x003b1;</italic> &#x0003e; 1 and <italic toggle="yes">&#x003b1;</italic> &#x0003c; 1, respectively, with different possible underlying generative mechanisms.</p><p>Deviation from diffusing behavior is not an unknown phenomenon: as an example, subdiffusive behavior has been observed in various within-cell processes, such as in the stochastic trajectory of messenger RNA inside living E. coli cells, <italic toggle="yes">&#x003b1;</italic> &#x02248; 0.7, [<xref rid="pone.0298650.ref031" ref-type="bibr">31</xref>], channel proteins in the membranes of living cells, <italic toggle="yes">&#x003b1;</italic> &#x02248; 0.9, [<xref rid="pone.0298650.ref032" ref-type="bibr">32</xref>], and telomeres within eucaryotic cell nuclei, <italic toggle="yes">&#x003b1;</italic> &#x02248; 0.3, [<xref rid="pone.0298650.ref033" ref-type="bibr">33</xref>], indicating the statistical structure of the ambient space or the type of active motion by the macromolecules themselves. <xref rid="pone.0298650.g001" ref-type="fig">Fig 1d</xref> illustrates subdiffusive trajectories, corresponding to various <italic toggle="yes">&#x003b1;</italic> &#x0003c; 1 exponents, generated by fractional Brownian motion (fBm) [<xref rid="pone.0298650.ref034" ref-type="bibr">34</xref>]. Note that we chose fBm for visualization purposes only; fBm generates trajectories with long-range temporal correlations, corresponding to the violation of point i) above.</p><fig position="float" id="pone.0298650.g001"><object-id pub-id-type="doi">10.1371/journal.pone.0298650.g001</object-id><label>Fig 1</label><caption><title>Illustration of word embedding, semantic change, and subdiffusive trajectories.</title><p>(a) 2D projection of a high-dimensional word embedding, illustrating semantic similarity. 300D embedding of lemmas in the Google ngram English fiction database are generated by Word2vec Skip-gram model and are nonlinearly projected to 2D using t-sne [<xref rid="pone.0298650.ref035" ref-type="bibr">35</xref>]. (b) Alignment of embeddings. First, multiple embeddings of the same year are aligned and averaged to reduce embedding noise, then, these averaged embeddings are aligned across time to achieve maximally smoothened word trajectories. (c) The resulting diachronic embedding makes it possible to visualize semantic change. Three selected words (gay, cancel, outlet) with most change within a decade are shown with respect to the time averaged position of other semantically related words. (d) Illustration of subdiffusive trajectories generated by fractional Brownian motion. Inset: mean squared distance scales as (&#x00394;<italic toggle="yes">x</italic>)<sup>2</sup> &#x0223c; <italic toggle="yes">t</italic><sup><italic toggle="yes">&#x003b1;</italic></sup> with anomalous diffusion exponent <italic toggle="yes">&#x003b1;</italic>.</p></caption><graphic xlink:href="pone.0298650.g001" position="float"/></fig><p>Actual semantic trajectories might be governed by a mixture of underlying dynamical rules, as discussed above. Measuring the anomalous diffusion exponent <italic toggle="yes">&#x003b1;</italic> classifies the semantic change in the context of stochastic processes, and helps to explore the possible stochastic dynamical rules that drive linguistic change.</p></sec><sec sec-type="materials|methods" id="sec002"><title>Methods</title><sec id="sec003"><title>Corpus</title><p>To construct semantic trajectories of words, we use the downloadable version of the Google Books Ngram Viewer database version 2, downloaded from <ext-link xlink:href="https://storage.googleapis.com/books/ngrams/books/datasetsv2.html" ext-link-type="uri">https://storage.googleapis.com/books/ngrams/books/datasetsv2.html</ext-link>, as time-labelled corpora available in multiple languages [<xref rid="pone.0298650.ref008" ref-type="bibr">8</xref>, <xref rid="pone.0298650.ref036" ref-type="bibr">36</xref>]. In this work, we included five languages from the Indo-European family with the most available data, English, French, German, Spanish, and Italian. Notable limitations of the Google Ngram corpus include the lack of meta-data, and correspondingly, the equal importance assigned to a large variety of sources, including those by non-native speakers. As a special case, the English dataset suffers from significant overrepresentation of academic literature [<xref rid="pone.0298650.ref037" ref-type="bibr">37</xref>, <xref rid="pone.0298650.ref038" ref-type="bibr">38</xref>]; in order to correct for this bias, we use the <italic toggle="yes">English fiction</italic> corpus as a representation of English [<xref rid="pone.0298650.ref037" ref-type="bibr">37</xref>].</p></sec><sec id="sec004"><title>Language processing</title><p>We used the snowball stemmer of the natural language toolkit (nltk) module of Python [<xref rid="pone.0298650.ref039" ref-type="bibr">39</xref>] to lemmatize words, and then we filtered out stopwords [<xref rid="pone.0298650.ref040" ref-type="bibr">40</xref>], before collecting all co-occurrences within a window size 2, as suggested by Levy et al. [<xref rid="pone.0298650.ref015" ref-type="bibr">15</xref>] and Hamilton et al. [<xref rid="pone.0298650.ref020" ref-type="bibr">20</xref>], illustrated by <xref rid="pone.0298650.g002" ref-type="fig">Fig 2a&#x02013;2c</xref>.</p><fig position="float" id="pone.0298650.g002"><object-id pub-id-type="doi">10.1371/journal.pone.0298650.g002</object-id><label>Fig 2</label><caption><title>Key steps of the pipeline that generates diachronic embeddings from google ngram data.</title><p>(a) Lemmatization of words. (b) Filtering stopwords. (c) Counting co-occurrences with context window of length 2. (d) Smoothening co-occurrences by applying a sliding time window to co-occurrence counts. (e) Vocabulary, i.e., the final set of lemmas we include in our analysis, is defined by the lemmas that occur at least 100 times at all time windows. (f) Training dataset was created by subsampling all co-occurrences in order to have a constant number of co-occurrences <italic toggle="yes">N</italic><sub><italic toggle="yes">c</italic></sub> = 10<sup>7</sup> at each time window. (g) Word embedding by Word2vec with Skip-gram. (h) Alignment of different embeddings (both within and across years). (i) Randomization of the temporal order of step <italic toggle="yes">sizes</italic> of each word trajectory separately. (j) Randomization of the temporal order of step <italic toggle="yes">directions</italic> of each word trajectory separately.</p></caption><graphic xlink:href="pone.0298650.g002" position="float"/></fig></sec><sec id="sec005"><title>Temporal grouping</title><p>Since temporal annotation of ngram occurrences follow a yearly resolution, we first collected co-occurrences by year, and then we applied a 10-year sliding window to increase vocabulary size and to smoothen data. Although setting the sliding window size to 10 years is an arbitrary choice, we show in <xref rid="pone.0298650.s001" ref-type="supplementary-material">S1 File</xref> (namely in S5 Fig in <xref rid="pone.0298650.s001" ref-type="supplementary-material">S1 File</xref>) that as window size increases gradually from 1 year to 10 years, the exponent <italic toggle="yes">&#x003b1;</italic> approximately converges to its value at 10 years. We then filtered out words that do not appear at least 100 times in each time window.</p><p>While Google&#x02019;s raw database initially provides ngram data from almost all years in the past few centuries, the number of occurrences decreases significantly as we look further back in time. This is a crucial factor in our methodology, since a certain amount of co-occurrence statistics is required for the embedding model to find a proper representation for the word meanings. Since the minimum number of words known by a native speaker is roughly 10 000 [<xref rid="pone.0298650.ref041" ref-type="bibr">41</xref>], we prescribed that we have a vocabulary with at least that size in each time step by only keeping data from years after 1950. If we went back further in time, we would have had a much smaller set of words to work with. This process is shown in <xref rid="pone.0298650.g002" ref-type="fig">Fig 2d and 2e</xref>.</p></sec><sec id="sec006"><title>Semantic embedding</title><p>For constructing word embeddings, we use Word2vec&#x02019;s Skip-gram model with negative sampling (SGNS), which is one of the most widely used word embedding algorithms due to its computational efficiency and ability to capture semantic relationships in a simple mathematical form [<xref rid="pone.0298650.ref016" ref-type="bibr">16</xref>, <xref rid="pone.0298650.ref042" ref-type="bibr">42</xref>]. It uses a two-layer neural network to represent each word <italic toggle="yes">i</italic> with two <italic toggle="yes">D</italic>-dimensional vectors called &#x02018;word vector&#x02019; (<italic toggle="yes">v</italic><sub><italic toggle="yes">i</italic></sub>) and &#x02018;context vector&#x02019; (<italic toggle="yes">w</italic><sub><italic toggle="yes">i</italic></sub>) such that a global cost function <italic toggle="yes">C</italic>, depending on all word vectors and all context vectors is (approximately) minimized. The objective of the Skip-gram model is to optimize the estimated empirical log-probabilities
<disp-formula id="pone.0298650.e002"><alternatives><graphic xlink:href="pone.0298650.e002.jpg" id="pone.0298650.e002g" position="anchor"/><mml:math id="M2" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mtext>log</mml:mtext><mml:mspace width="2pt"/><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mspace width="0.166667em"/><mml:mi>j</mml:mi><mml:mspace width="0.166667em"/><mml:mo>|</mml:mo><mml:mspace width="0.166667em"/><mml:mi>i</mml:mi><mml:mspace width="0.166667em"/><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="3.33333pt"/><mml:mo>&#x0221d;</mml:mo><mml:mspace width="3.33333pt"/><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x000b7;</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives>
<label>(2)</label></disp-formula>
of word <italic toggle="yes">j</italic> occurring in the context of word <italic toggle="yes">i</italic>, using the cost function <italic toggle="yes">C</italic> defined as
<disp-formula id="pone.0298650.e003"><alternatives><graphic xlink:href="pone.0298650.e003.jpg" id="pone.0298650.e003g" position="anchor"/><mml:math id="M3" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>L</mml:mi></mml:mfrac><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>L</mml:mi></mml:munderover><mml:munder><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi mathvariant="script">C</mml:mi><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:munder><mml:mspace width="2pt"/><mml:mtext>log</mml:mtext><mml:mspace width="2pt"/><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mspace width="0.166667em"/><mml:mi>j</mml:mi><mml:mspace width="0.166667em"/><mml:mo>|</mml:mo><mml:mspace width="0.166667em"/><mml:mi>i</mml:mi><mml:mspace width="0.166667em"/><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives>
<label>(3)</label></disp-formula>
where <italic toggle="yes">L</italic> is the length of the text and <inline-formula id="pone.0298650.e004"><alternatives><graphic xlink:href="pone.0298650.e004.jpg" id="pone.0298650.e004g" position="anchor"/><mml:math id="M4" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">C</mml:mi><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> is the linguistic context of word <italic toggle="yes">i</italic>. Following the recommendations of [<xref rid="pone.0298650.ref015" ref-type="bibr">15</xref>, <xref rid="pone.0298650.ref020" ref-type="bibr">20</xref>], we set the dimension <italic toggle="yes">D</italic> of the embedding space to <italic toggle="yes">D</italic> = 300 and the context size to 2. <xref rid="pone.0298650.g001" ref-type="fig">Fig 1a</xref> visualizes a single embedding projected to 2 dimensions by t-sne [<xref rid="pone.0298650.ref035" ref-type="bibr">35</xref>], a non-linear dimension reduction method.</p></sec><sec id="sec007"><title>Measuring semantic distance</title><p>In this paper, we self-consistently define semantic distance |<italic toggle="yes">x</italic> &#x02212; <italic toggle="yes">y</italic>| between vectors <italic toggle="yes">x</italic> and <italic toggle="yes">y</italic> in the D-dimensional embedding space as Euclidean distance, <inline-formula id="pone.0298650.e005"><alternatives><graphic xlink:href="pone.0298650.e005.jpg" id="pone.0298650.e005g" position="anchor"/><mml:math id="M5" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mo>|</mml:mo><mml:mi>x</mml:mi><mml:mo>-</mml:mo><mml:mi>y</mml:mi><mml:mo>|</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>D</mml:mi></mml:msubsup><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>. This is a departure from the most commonly used semantic similarity measure, cosine similarity |<italic toggle="yes">x</italic> &#x02212; <italic toggle="yes">y</italic>|<sub>cos</sub>, which is favored because it accounts for the rescaling symmetry, <italic toggle="yes">x</italic> &#x02192; &#x003bb;<italic toggle="yes">x</italic>, by setting |&#x003bb;<sub>1</sub><italic toggle="yes">x</italic> &#x02212; &#x003bb;<sub>2</sub><italic toggle="yes">y</italic>|<sub>cos</sub> = |<italic toggle="yes">x</italic> &#x02212; <italic toggle="yes">y</italic>|<sub>cos</sub> [<xref rid="pone.0298650.ref013" ref-type="bibr">13</xref>, <xref rid="pone.0298650.ref042" ref-type="bibr">42</xref>]. We argue, however, that finding the most general class of underlying symmetries and measuring distance separately is a more principled procedure, especially when semantic distances have numerical and not just ordinal meaning. In particular, we choose Euclidean distance for the following reasons.</p><list list-type="order"><list-item><p>In the Skip-gram model, the difference of two word vectors, <italic toggle="yes">v</italic><sub><italic toggle="yes">i</italic></sub> &#x02212; <italic toggle="yes">v</italic><sub><italic toggle="yes">j</italic></sub>, has semantic meaning. By interpreting the space of context words as a dual space (see <xref rid="pone.0298650.s001" ref-type="supplementary-material">S1 File</xref> for details), the meaning of <italic toggle="yes">v</italic><sub><italic toggle="yes">i</italic></sub> &#x02212; <italic toggle="yes">v</italic><sub><italic toggle="yes">j</italic></sub> can be understood by projecting it to various context words, akin to the definition of linear functionals in dual space,
<disp-formula id="pone.0298650.e006"><alternatives><graphic xlink:href="pone.0298650.e006.jpg" id="pone.0298650.e006g" position="anchor"/><mml:math id="M6" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x000b7;</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x000b7;</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>&#x000b7;</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:mo>&#x0221d;</mml:mo><mml:mtext>log</mml:mtext><mml:mspace width="2pt"/><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mspace width="0.166667em"/><mml:mi>k</mml:mi><mml:mspace width="0.166667em"/><mml:mo>|</mml:mo><mml:mspace width="0.166667em"/><mml:mi>i</mml:mi><mml:mspace width="0.166667em"/><mml:mo>)</mml:mo><mml:mo>-</mml:mo><mml:mtext>log</mml:mtext><mml:mspace width="2pt"/><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mspace width="0.166667em"/><mml:mi>k</mml:mi><mml:mspace width="0.166667em"/><mml:mo>|</mml:mo><mml:mspace width="0.166667em"/><mml:mi>j</mml:mi><mml:mspace width="0.166667em"/><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:mo>&#x0221d;</mml:mo><mml:mtext>log</mml:mtext><mml:mspace width="2pt"/><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mspace width="0.166667em"/><mml:mi>k</mml:mi><mml:mspace width="0.166667em"/><mml:mo>|</mml:mo><mml:mspace width="0.166667em"/><mml:mi>i</mml:mi><mml:mspace width="0.166667em"/><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mspace width="0.166667em"/><mml:mi>k</mml:mi><mml:mspace width="0.166667em"/><mml:mo>|</mml:mo><mml:mspace width="0.166667em"/><mml:mi>j</mml:mi><mml:mspace width="0.166667em"/><mml:mo>)</mml:mo></mml:mrow></mml:mfrac><mml:mo>)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives>
<label>(4)</label></disp-formula>
Since this result is valid for any context word <italic toggle="yes">k</italic>, <italic toggle="yes">v</italic><sub><italic toggle="yes">i</italic></sub> &#x02212; <italic toggle="yes">v</italic><sub><italic toggle="yes">j</italic></sub> measures the relative log-probability of word <italic toggle="yes">i</italic> versus word <italic toggle="yes">j</italic> occurring in the context of word <italic toggle="yes">k</italic>, for all contexts <italic toggle="yes">k</italic>. This is in line with the starting hypothesis of distributional semantics, which states that two words have a similar meaning if their co-occurrence statistics are similar.</p></list-item><list-item><p>Euclidean distance is a <italic toggle="yes">metric</italic>, obeying triangle inequality, commutativity, and other axioms, whereas other measures, such as cosine similarity are not. This is important when semantic trajectories are interpreted as trajectories in metric spaces, for example, by measuring their squared displacement over time, |&#x00394;<italic toggle="yes">x</italic>|<sup>2</sup>(<italic toggle="yes">t</italic>) = |<italic toggle="yes">x</italic>(<italic toggle="yes">t</italic>) &#x02212; <italic toggle="yes">x</italic>(<italic toggle="yes">t</italic> = 0)|<sup>2</sup>.</p></list-item><list-item><p>As discussed above, embedding dimensions are arbitrary. They need to be aligned for meaningful comparison across multiple embeddings (regardless of whether it coming from the same co-occurrence data or a different one). Here we follow the most commonly used method, called the Procrustes algorithm [<xref rid="pone.0298650.ref043" ref-type="bibr">43</xref>], that finds an orthogonal transformation that minimizes the total Euclidean distance between corresponding points in the two embeddings. Measuring semantic distance as Euclidean distance is consistent with this alignment method.</p></list-item><list-item><p>As a consequence of keeping the size of the word cloud constant, defined by <xref rid="pone.0298650.e029" ref-type="disp-formula">Eq (11)</xref>, two embeddings belong to the same equivalence class if they can be orthogonally transformed to each other, discussed above and shown below in Methods. Orthogonal transformations are defined by keeping Euclidean distance, also consistent with our definition.</p></list-item></list></sec><sec id="sec008"><title>Diachronic embedding</title><p>We first subsample the co-occurrences corresponding to each time window to eliminate systematic sample size bias (the amount of data included in the Google Ngram database steadily increases with time in all five languages) as shown by <xref rid="pone.0298650.g002" ref-type="fig">Fig 2f</xref>. We set this sample size to <italic toggle="yes">N</italic><sub><italic toggle="yes">c</italic></sub> = 10<sup>7</sup> co-occurrences, set by the first (few) time windows with the least amount of data. We observe that <italic toggle="yes">N</italic><sub><italic toggle="yes">c</italic></sub> = 10<sup>7</sup> sets a good tradeoff between sample size (and thus trajectory noise), vocabulary size, and trajectory length for this database. As shown in <xref rid="pone.0298650.s001" ref-type="supplementary-material">S1 File</xref> (namely in S1 Fig in <xref rid="pone.0298650.s001" ref-type="supplementary-material">S1 File</xref>), using larger samples will not give us embeddings with more information. This is because increasing the number of input co-occurrences only makes the word cloud larger without altering its structure.</p><p>After generating the <italic toggle="yes">D</italic> = 300 dimensional embeddings for each time window <italic toggle="yes">M</italic> = 80 times by Word2vec with Skip-gram, we first align all <italic toggle="yes">M</italic> embeddings within a time window. Then, word positions across embeddings are averaged to obtain a more robust estimate of semantic positions for each word at a given time window. These average embeddings, one for each time window, are then aligned across time, as shown by <xref rid="pone.0298650.g002" ref-type="fig">Fig 2g and 2h</xref>.</p><p>The alignment of two embeddings consists of two steps. First, one has to define the class of transformations that keep semantic relationships invariant, and second, the transformation within this class that minimizes the difference between the two embeddings has to be selected. For the first step, we find (see below for a proof) that the requirement of keeping the total size of the word cloud constant, as defined by <xref rid="pone.0298650.e029" ref-type="disp-formula">Eq (11)</xref>, restricts all linear transformations to those that are orthogonal. This is also in line with both measuring semantic distance as Euclidean distance, as discussed above, and also with the most commonly used alignment method called the orthogonal Procrustes method, which finds that the best transformation <italic toggle="yes">R</italic> that accounts for this rotational (orthogonal) freedom is given by
<disp-formula id="pone.0298650.e007"><alternatives><graphic xlink:href="pone.0298650.e007.jpg" id="pone.0298650.e007g" position="anchor"/><mml:math id="M7" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mtext>arg</mml:mtext><mml:mspace width="2pt"/><mml:munder><mml:mtext>min</mml:mtext><mml:mrow><mml:mi>Q</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi mathvariant="normal">O</mml:mi><mml:mo>(</mml:mo><mml:mi>D</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:munder><mml:mspace width="2pt"/><mml:mrow><mml:mo>|</mml:mo><mml:mo>|</mml:mo></mml:mrow><mml:msup><mml:mi>W</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup><mml:mi>Q</mml:mi><mml:mo>-</mml:mo><mml:mi>W</mml:mi><mml:msub><mml:mrow><mml:mo>|</mml:mo><mml:mo>|</mml:mo></mml:mrow><mml:mi>F</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives>
<label>(5)</label></disp-formula>
where the rows of the <italic toggle="yes">N</italic> &#x000d7; <italic toggle="yes">D</italic> matrix <italic toggle="yes">W</italic>&#x02032; and <italic toggle="yes">W</italic> contain the word vectors of the aligned and reference embedding, respectively, and ||&#x022c5;||<sub><italic toggle="yes">F</italic></sub> is the Frobenius matrix norm. We use the analytical solution [<xref rid="pone.0298650.ref043" ref-type="bibr">43</xref>] for <italic toggle="yes">R</italic> to find optimal alignments both within a time window and among time windows.</p></sec><sec id="sec009"><title>Constant word cloud size defines an orthogonal embedding symmetry</title><p>Word cloud size, defined by <xref rid="pone.0298650.e029" ref-type="disp-formula">Eq (11)</xref>, is written as
<disp-formula id="pone.0298650.e008"><alternatives><graphic xlink:href="pone.0298650.e008.jpg" id="pone.0298650.e008g" position="anchor"/><mml:math id="M8" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mtext>Tr</mml:mtext><mml:mspace width="2pt"/><mml:mi>V</mml:mi><mml:mo>(</mml:mo><mml:mi>W</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>D</mml:mi></mml:munderover><mml:munder><mml:mtext>Var</mml:mtext><mml:mi>i</mml:mi></mml:munder><mml:mo>(</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives>
<label>(6)</label></disp-formula>
where <italic toggle="yes">V</italic>(<italic toggle="yes">W</italic>) is the empirical covariance matrix of word positions, extracted from <italic toggle="yes">W</italic>, the <italic toggle="yes">N</italic> &#x000d7; <italic toggle="yes">D</italic> matrix with its rows corresponding to the <italic toggle="yes">D</italic>-dimensional word vectors of all <italic toggle="yes">N</italic> words, and Var<sub><italic toggle="yes">i</italic></sub> (<italic toggle="yes">W</italic><sub><italic toggle="yes">ij</italic></sub>) is the variance of the word positions along dimension <italic toggle="yes">j</italic>. Using the definition of variance,
<disp-formula id="pone.0298650.e010"><alternatives><graphic xlink:href="pone.0298650.e010.jpg" id="pone.0298650.e010g" position="anchor"/><mml:math id="M10" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mtext>Tr</mml:mtext><mml:mspace width="2pt"/><mml:mi>V</mml:mi><mml:mo>(</mml:mo><mml:mi>W</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>D</mml:mi></mml:munderover><mml:munder><mml:mtext>Var</mml:mtext><mml:mi>i</mml:mi></mml:munder><mml:mo>(</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>D</mml:mi></mml:munderover><mml:mo>[</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:mo>(</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mo>)</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msup><mml:mi>N</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mfrac><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>D</mml:mi></mml:munderover><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mo>[</mml:mo><mml:mi>N</mml:mi><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>]</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives>
<label>(7)</label></disp-formula>
The size of the transformed word cloud <italic toggle="yes">W</italic>&#x02032;, defined as <italic toggle="yes">W</italic>&#x02032; = <italic toggle="yes">RW</italic> is
<disp-formula id="pone.0298650.e011"><alternatives><graphic xlink:href="pone.0298650.e011.jpg" id="pone.0298650.e011g" position="anchor"/><mml:math id="M11" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mtext>Tr</mml:mtext><mml:mspace width="2pt"/><mml:mi>V</mml:mi><mml:mo>(</mml:mo><mml:msup><mml:mi>W</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msup><mml:mi>N</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mfrac><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>D</mml:mi></mml:munderover><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mo>[</mml:mo><mml:mi>N</mml:mi><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>D</mml:mi></mml:munderover><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>D</mml:mi></mml:munderover><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>D</mml:mi></mml:munderover><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>D</mml:mi></mml:munderover><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msup><mml:mi>N</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mfrac><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>D</mml:mi></mml:munderover><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>[</mml:mo><mml:mi>N</mml:mi><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>]</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives>
<label>(8)</label></disp-formula>
The difference between Tr <italic toggle="yes">V</italic>(<italic toggle="yes">W</italic>) and Tr <italic toggle="yes">V</italic>(<italic toggle="yes">W</italic>&#x02032;) is
<disp-formula id="pone.0298650.e012"><alternatives><graphic xlink:href="pone.0298650.e012.jpg" id="pone.0298650.e012g" position="anchor"/><mml:math id="M12" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mtext>Tr</mml:mtext><mml:mspace width="2pt"/><mml:mi>V</mml:mi><mml:mo>(</mml:mo><mml:mi>W</mml:mi><mml:mo>)</mml:mo><mml:mo>-</mml:mo><mml:mtext>Tr</mml:mtext><mml:mspace width="2pt"/><mml:mi>V</mml:mi><mml:mo>(</mml:mo><mml:msup><mml:mi>W</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msup><mml:mi>N</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mfrac><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>D</mml:mi></mml:munderover><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mo>(</mml:mo><mml:msub><mml:mi>&#x003b4;</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>D</mml:mi></mml:munderover><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>[</mml:mo><mml:mi>N</mml:mi><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>]</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives>
<label>(9)</label></disp-formula>
The transformation <italic toggle="yes">R</italic> satisfies the requirement of constant cloud size Tr <italic toggle="yes">V</italic>(<italic toggle="yes">W</italic>) &#x02212; Tr <italic toggle="yes">V</italic>(<italic toggle="yes">W</italic>&#x02032;) = 0 only if
<disp-formula id="pone.0298650.e013"><alternatives><graphic xlink:href="pone.0298650.e013.jpg" id="pone.0298650.e013g" position="anchor"/><mml:math id="M13" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>D</mml:mi></mml:munderover><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>&#x003b4;</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives>
<label>(10)</label></disp-formula>
also written in matrix form as <italic toggle="yes">RR</italic><sup><italic toggle="yes">T</italic></sup> = <bold>1</bold>, i.e., <italic toggle="yes">R</italic> needs to be orthogonal.</p></sec><sec id="sec010"><title>Mean versus ensemble average anomalous diffusion exponent</title><p>As explained in the Introduction, distributional semantics does not directly define the meanings of words; it treats them as statistical properties based on their contexts. Studying word meanings in this framework only makes sense in relation to other words, which are also defined relationally. This means that one cannot extract all the semantic information of an individual word from its embedded trajectory alone, but other word trajectories also have to be considered, and their collective behaviour should be studied. When dealing with measurable quantities, such as anomalous diffusion exponents, this requires analyzing their distribution and averages.</p><p>Averages of anomalous diffusion exponents corresponding to single-word trajectories are calculated in two different ways. The <italic toggle="yes">mean anomalous diffusion exponent</italic>
<inline-formula id="pone.0298650.e014"><alternatives><graphic xlink:href="pone.0298650.e014.jpg" id="pone.0298650.e014g" position="anchor"/><mml:math id="M14" display="inline" overflow="scroll"><mml:mover accent="true"><mml:mi>&#x003b1;</mml:mi><mml:mo>&#x000af;</mml:mo></mml:mover></mml:math></alternatives></inline-formula> is computed by first fitting an anomalous diffusion exponent <italic toggle="yes">&#x003b1;</italic><sub><italic toggle="yes">i</italic></sub> to the trajectory of each word <italic toggle="yes">i</italic>, and then averaging the fitted exponents, <inline-formula id="pone.0298650.e015"><alternatives><graphic xlink:href="pone.0298650.e015.jpg" id="pone.0298650.e015g" position="anchor"/><mml:math id="M15" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>&#x003b1;</mml:mi><mml:mo>&#x000af;</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup><mml:msub><mml:mi>&#x003b1;</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>/</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>, where <italic toggle="yes">N</italic> is the total number of words. To compute the <italic toggle="yes">ensemble average anomalous diffusion exponent</italic>, we first average |&#x00394;<italic toggle="yes">x</italic><sub><italic toggle="yes">i</italic></sub>(<italic toggle="yes">t</italic>)|<sup>2</sup>, the squared displacement of word <italic toggle="yes">i</italic> over time to obtain the mean squared displacement over time, <inline-formula id="pone.0298650.e016"><alternatives><graphic xlink:href="pone.0298650.e016.jpg" id="pone.0298650.e016g" position="anchor"/><mml:math id="M16" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mo>|</mml:mo><mml:mo>&#x00394;</mml:mo><mml:mi>x</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup><mml:msup><mml:mrow><mml:mo>|</mml:mo><mml:mo>&#x00394;</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>/</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>, and then we fit an anomalous diffusion exponent to |&#x00394;<italic toggle="yes">x</italic>(<italic toggle="yes">t</italic>)|<sup>2</sup>, called ensemble average anomalous diffusion exponent &#x02329;<italic toggle="yes">&#x003b1;</italic>&#x0232a;.</p></sec><sec id="sec011"><title>Trajectory randomization methods</title><p>
<xref rid="pone.0298650.g002" ref-type="fig">Fig 2i and 2j</xref> illustrate the two types of trajectory randomization methods we use in this paper to generate the results shown by <xref rid="pone.0298650.g003" ref-type="fig">Fig 3c and 3d</xref> and <xref rid="pone.0298650.t001" ref-type="table">Table 1</xref>: randomization of step sizes and randomization of step directions. Within each type, &#x0201c;randomization&#x0201d; refers to drawing step sizes and directions from various distributions constructed from the original trajectories, described as follows.</p><list list-type="simple"><list-item><p><italic toggle="yes">Random sizes</italic>: step sizes were sampled from a normal distribution with mean and standard deviation corresponding to that of the step sizes of the original trajectory; <italic toggle="yes">sizes from distribution</italic>: step sizes were sampled from the set of all step sizes corresponding to all words; <italic toggle="yes">shuffled sizes</italic>: step sizes were sampled from the set of step sizes corresponding to the same word; in other words, the temporal order of step sizes of a trajectory has been shuffled; <italic toggle="yes">original sizes</italic>: step size for every word at every time step was set to its original value.</p></list-item><list-item><p><italic toggle="yes">Random directions</italic>: directions were sampled uniformly over a D-dimensional sphere; <italic toggle="yes">shuffled directions</italic>: the temporal order of step directions of a trajectory has been shuffled; <italic toggle="yes">original directions</italic>: the direction for every word at every time step was set to its original value.</p></list-item></list><fig position="float" id="pone.0298650.g003"><object-id pub-id-type="doi">10.1371/journal.pone.0298650.g003</object-id><label>Fig 3</label><caption><title>Anomalous diffusion analysis of semantic trajectories.</title><p>(a) Squared displacement (&#x00394;<italic toggle="yes">x</italic>)<sup>2</sup> of selected English words over time, along with average squared displacement over all words, shown in blue, which can be approximated by &#x02329;(&#x00394;<italic toggle="yes">x</italic>)<sup>2</sup>&#x0232a; &#x0223c; <italic toggle="yes">t</italic><sup>&#x02329;<italic toggle="yes">&#x003b1;</italic>&#x0232a;</sup> with ensemble-average anomalous diffusion exponent &#x02329;<italic toggle="yes">&#x003b1;</italic>&#x0232a;&#x02248;0.44. (b) Average squared displacement of words over time in five languages, English, French, German, Italian, and Spanish. This is compared to an ensemble of random walkers under the same constraint given by a constant total word cloud size, defined by <xref rid="pone.0298650.e029" ref-type="disp-formula">Eq (11)</xref>. (c) Average squared displacement &#x02329;(&#x00394;<italic toggle="yes">x</italic>)<sup>2</sup>&#x0232a; of English words under various combinations of trajectory randomization methods. Columns and rows differ in the way step <italic toggle="yes">sizes</italic> and <italic toggle="yes">directions</italic> are randomized, respectively, see details in text. Keeping the original sequence of step directions is necessary to recover strongly subdiffusive behavior (bottom row), but not sufficient: the more information regarding step sizes is kept, the more its best-fit diffusion exponent approaches its original value of &#x02329;<italic toggle="yes">&#x003b1;</italic>&#x0232a; &#x02248; 0.44. (d) Distribution of diffusion exponents <italic toggle="yes">&#x003b1;</italic> fitted to the trajectory of each word separately. Their average <inline-formula id="pone.0298650.e017"><alternatives><graphic xlink:href="pone.0298650.e017" id="pone.0298650.e017g" position="anchor"/><mml:math id="M17" display="inline" overflow="scroll"><mml:mover accent="true"><mml:mi>&#x003b1;</mml:mi><mml:mo>&#x000af;</mml:mo></mml:mover></mml:math></alternatives></inline-formula> is close to the ensemble averages &#x02329;<italic toggle="yes">&#x003b1;</italic>&#x0232a; shown in c).</p></caption><graphic xlink:href="pone.0298650.g003" position="float"/></fig><table-wrap position="float" id="pone.0298650.t001"><object-id pub-id-type="doi">10.1371/journal.pone.0298650.t001</object-id><label>Table 1</label><caption><title>Ensemble-average anomalous diffusion exponent &#x02329;<italic toggle="yes">&#x003b1;</italic>&#x0232a; and the average <inline-formula id="pone.0298650.e018"><alternatives><graphic xlink:href="pone.0298650.e018" id="pone.0298650.e018g" position="anchor"/><mml:math id="M18" display="inline" overflow="scroll"><mml:mover accent="true"><mml:mi>&#x003b1;</mml:mi><mml:mo>&#x000af;</mml:mo></mml:mover></mml:math></alternatives></inline-formula> of the anomalous diffusion exponents of all words, under various combinations of trajectory randomization methods, for all five languages.</title><p>Grey cells show the exponents of the original, non-randomized trajectories. Note that the exponents for English are extracted from <xref rid="pone.0298650.g003" ref-type="fig">Fig 3c and 3d</xref>; for the other four languages, the analogous Figures are included in S3 and S4 Figs in <xref rid="pone.0298650.s001" ref-type="supplementary-material">S1 File</xref>.</p></caption><alternatives><graphic xlink:href="pone.0298650.t001" id="pone.0298650.t001g" position="float"/><table frame="box" rules="all" border="0"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><tbody><tr><td align="center" rowspan="2" colspan="1">
<bold>ENGLISH</bold>
</td><td align="center" colspan="2" rowspan="1">random directions</td><td align="center" colspan="2" rowspan="1">original directions</td></tr><tr><td align="center" rowspan="1" colspan="1">&#x02329;<italic toggle="yes">&#x003b1;</italic>&#x0232a;</td><td align="center" rowspan="1" colspan="1">
<inline-formula id="pone.0298650.e019">
<alternatives><graphic xlink:href="pone.0298650.e019" id="pone.0298650.e019g" position="anchor"/><mml:math id="M19" display="inline" overflow="scroll"><mml:mover accent="true"><mml:mi>&#x003b1;</mml:mi><mml:mo>&#x000af;</mml:mo></mml:mover></mml:math></alternatives>
</inline-formula>
</td><td align="center" rowspan="1" colspan="1">&#x02329;<italic toggle="yes">&#x003b1;</italic>&#x0232a;</td><td align="center" rowspan="1" colspan="1">
<inline-formula id="pone.0298650.e020">
<alternatives><graphic xlink:href="pone.0298650.e020" id="pone.0298650.e020g" position="anchor"/><mml:math id="M20" display="inline" overflow="scroll"><mml:mover accent="true"><mml:mi>&#x003b1;</mml:mi><mml:mo>&#x000af;</mml:mo></mml:mover></mml:math></alternatives>
</inline-formula>
</td></tr><tr><td align="center" rowspan="1" colspan="1">random sizes</td><td align="center" rowspan="1" colspan="1">1.00</td><td align="center" rowspan="1" colspan="1">1.02</td><td align="center" rowspan="1" colspan="1">0.87</td><td align="center" rowspan="1" colspan="1">0.90</td></tr><tr><td align="center" rowspan="1" colspan="1">sampled sizes</td><td align="center" rowspan="1" colspan="1">1.00</td><td align="center" rowspan="1" colspan="1">1.04</td><td align="center" rowspan="1" colspan="1">0.87</td><td align="center" rowspan="1" colspan="1">0.94</td></tr><tr><td align="center" rowspan="1" colspan="1">shuffled sizes</td><td align="center" rowspan="1" colspan="1">1.00</td><td align="center" rowspan="1" colspan="1">1.00</td><td align="center" rowspan="1" colspan="1">0.59</td><td align="center" rowspan="1" colspan="1">0.68</td></tr><tr><td align="center" rowspan="1" colspan="1">original sizes</td><td align="center" rowspan="1" colspan="1">0.95</td><td align="center" rowspan="1" colspan="1">0.91</td><td align="center" style="background-color:#D7D7D7" rowspan="1" colspan="1">
<bold>0.44</bold>
</td><td align="center" style="background-color:#D7D7D7" rowspan="1" colspan="1">
<bold>0.48</bold>
</td></tr><tr><td align="center" rowspan="2" colspan="1">
<bold>FRENCH</bold>
</td><td align="center" colspan="2" rowspan="1">random directions</td><td align="center" colspan="2" rowspan="1">original directions</td></tr><tr><td align="center" rowspan="1" colspan="1">&#x02329;<italic toggle="yes">&#x003b1;</italic>&#x0232a;</td><td align="center" rowspan="1" colspan="1">
<inline-formula id="pone.0298650.e021">
<alternatives><graphic xlink:href="pone.0298650.e021" id="pone.0298650.e021g" position="anchor"/><mml:math id="M21" display="inline" overflow="scroll"><mml:mover accent="true"><mml:mi>&#x003b1;</mml:mi><mml:mo>&#x000af;</mml:mo></mml:mover></mml:math></alternatives>
</inline-formula>
</td><td align="center" rowspan="1" colspan="1">&#x02329;<italic toggle="yes">&#x003b1;</italic>&#x0232a;</td><td align="center" rowspan="1" colspan="1">
<inline-formula id="pone.0298650.e022">
<alternatives><graphic xlink:href="pone.0298650.e022" id="pone.0298650.e022g" position="anchor"/><mml:math id="M22" display="inline" overflow="scroll"><mml:mover accent="true"><mml:mi>&#x003b1;</mml:mi><mml:mo>&#x000af;</mml:mo></mml:mover></mml:math></alternatives>
</inline-formula>
</td></tr><tr><td align="center" rowspan="1" colspan="1">random sizes</td><td align="center" rowspan="1" colspan="1">1.00</td><td align="center" rowspan="1" colspan="1">1.03</td><td align="center" rowspan="1" colspan="1">0.90</td><td align="center" rowspan="1" colspan="1">0.93</td></tr><tr><td align="center" rowspan="1" colspan="1">sampled sizes</td><td align="center" rowspan="1" colspan="1">1.00</td><td align="center" rowspan="1" colspan="1">1.05</td><td align="center" rowspan="1" colspan="1">0.90</td><td align="center" rowspan="1" colspan="1">0.98</td></tr><tr><td align="center" rowspan="1" colspan="1">shuffled sizes</td><td align="center" rowspan="1" colspan="1">1.00</td><td align="center" rowspan="1" colspan="1">1.00</td><td align="center" rowspan="1" colspan="1">0.57</td><td align="center" rowspan="1" colspan="1">0.68</td></tr><tr><td align="center" rowspan="1" colspan="1">original sizes</td><td align="center" rowspan="1" colspan="1">0.98</td><td align="center" rowspan="1" colspan="1">1.02</td><td align="center" style="background-color:#D7D7D7" rowspan="1" colspan="1">
<bold>0.49</bold>
</td><td align="center" style="background-color:#D7D7D7" rowspan="1" colspan="1">
<bold>0.64</bold>
</td></tr><tr><td align="center" rowspan="2" colspan="1">
<bold>GERMAN</bold>
</td><td align="center" colspan="2" rowspan="1">random directions</td><td align="center" colspan="2" rowspan="1">original directions</td></tr><tr><td align="center" rowspan="1" colspan="1">&#x02329;<italic toggle="yes">&#x003b1;</italic>&#x0232a;</td><td align="center" rowspan="1" colspan="1">
<inline-formula id="pone.0298650.e023">
<alternatives><graphic xlink:href="pone.0298650.e023" id="pone.0298650.e023g" position="anchor"/><mml:math id="M23" display="inline" overflow="scroll"><mml:mover accent="true"><mml:mi>&#x003b1;</mml:mi><mml:mo>&#x000af;</mml:mo></mml:mover></mml:math></alternatives>
</inline-formula>
</td><td align="center" rowspan="1" colspan="1">&#x02329;<italic toggle="yes">&#x003b1;</italic>&#x0232a;</td><td align="center" rowspan="1" colspan="1">
<inline-formula id="pone.0298650.e024">
<alternatives><graphic xlink:href="pone.0298650.e024" id="pone.0298650.e024g" position="anchor"/><mml:math id="M24" display="inline" overflow="scroll"><mml:mover accent="true"><mml:mi>&#x003b1;</mml:mi><mml:mo>&#x000af;</mml:mo></mml:mover></mml:math></alternatives>
</inline-formula>
</td></tr><tr><td align="center" rowspan="1" colspan="1">random sizes</td><td align="center" rowspan="1" colspan="1">1.00</td><td align="center" rowspan="1" colspan="1">1.02</td><td align="center" rowspan="1" colspan="1">0.81</td><td align="center" rowspan="1" colspan="1">0.84</td></tr><tr><td align="center" rowspan="1" colspan="1">sampled sizes</td><td align="center" rowspan="1" colspan="1">1.00</td><td align="center" rowspan="1" colspan="1">1.03</td><td align="center" rowspan="1" colspan="1">0.81</td><td align="center" rowspan="1" colspan="1">0.87</td></tr><tr><td align="center" rowspan="1" colspan="1">shuffled sizes</td><td align="center" rowspan="1" colspan="1">1.00</td><td align="center" rowspan="1" colspan="1">1.00</td><td align="center" rowspan="1" colspan="1">0.51</td><td align="center" rowspan="1" colspan="1">0.57</td></tr><tr><td align="center" rowspan="1" colspan="1">original sizes</td><td align="center" rowspan="1" colspan="1">0.97</td><td align="center" rowspan="1" colspan="1">0.98</td><td align="center" style="background-color:#D7D7D7" rowspan="1" colspan="1">
<bold>0.41</bold>
</td><td align="center" style="background-color:#D7D7D7" rowspan="1" colspan="1">
<bold>0.48</bold>
</td></tr><tr><td align="center" rowspan="2" colspan="1">
<bold>ITALIAN</bold>
</td><td align="center" colspan="2" rowspan="1">random directions</td><td align="center" colspan="2" rowspan="1">original directions</td></tr><tr><td align="center" rowspan="1" colspan="1">&#x02329;<italic toggle="yes">&#x003b1;</italic>&#x0232a;</td><td align="center" rowspan="1" colspan="1">
<inline-formula id="pone.0298650.e025">
<alternatives><graphic xlink:href="pone.0298650.e025" id="pone.0298650.e025g" position="anchor"/><mml:math id="M25" display="inline" overflow="scroll"><mml:mover accent="true"><mml:mi>&#x003b1;</mml:mi><mml:mo>&#x000af;</mml:mo></mml:mover></mml:math></alternatives>
</inline-formula>
</td><td align="center" rowspan="1" colspan="1">&#x02329;<italic toggle="yes">&#x003b1;</italic>&#x0232a;</td><td align="center" rowspan="1" colspan="1">
<inline-formula id="pone.0298650.e026">
<alternatives><graphic xlink:href="pone.0298650.e026" id="pone.0298650.e026g" position="anchor"/><mml:math id="M26" display="inline" overflow="scroll"><mml:mover accent="true"><mml:mi>&#x003b1;</mml:mi><mml:mo>&#x000af;</mml:mo></mml:mover></mml:math></alternatives>
</inline-formula>
</td></tr><tr><td align="center" rowspan="1" colspan="1">random sizes</td><td align="center" rowspan="1" colspan="1">1.00</td><td align="center" rowspan="1" colspan="1">1.02</td><td align="center" rowspan="1" colspan="1">0.77</td><td align="center" rowspan="1" colspan="1">0.79</td></tr><tr><td align="center" rowspan="1" colspan="1">sampled sizes</td><td align="center" rowspan="1" colspan="1">1.00</td><td align="center" rowspan="1" colspan="1">1.02</td><td align="center" rowspan="1" colspan="1">0.78</td><td align="center" rowspan="1" colspan="1">0.81</td></tr><tr><td align="center" rowspan="1" colspan="1">shuffled sizes</td><td align="center" rowspan="1" colspan="1">1.00</td><td align="center" rowspan="1" colspan="1">1.00</td><td align="center" rowspan="1" colspan="1">0.51</td><td align="center" rowspan="1" colspan="1">0.55</td></tr><tr><td align="center" rowspan="1" colspan="1">original sizes</td><td align="center" rowspan="1" colspan="1">0.94</td><td align="center" rowspan="1" colspan="1">0.96</td><td align="center" style="background-color:#D7D7D7" rowspan="1" colspan="1">
<bold>0.40</bold>
</td><td align="center" style="background-color:#D7D7D7" rowspan="1" colspan="1">
<bold>0.44</bold>
</td></tr><tr><td align="center" rowspan="2" colspan="1">
<bold>SPANISH</bold>
</td><td align="center" colspan="2" rowspan="1">random directions</td><td align="center" colspan="2" rowspan="1">original directions</td></tr><tr><td align="center" rowspan="1" colspan="1">&#x02329;<italic toggle="yes">&#x003b1;</italic>&#x0232a;</td><td align="center" rowspan="1" colspan="1">
<inline-formula id="pone.0298650.e027">
<alternatives><graphic xlink:href="pone.0298650.e027" id="pone.0298650.e027g" position="anchor"/><mml:math id="M27" display="inline" overflow="scroll"><mml:mover accent="true"><mml:mi>&#x003b1;</mml:mi><mml:mo>&#x000af;</mml:mo></mml:mover></mml:math></alternatives>
</inline-formula>
</td><td align="center" rowspan="1" colspan="1">&#x02329;<italic toggle="yes">&#x003b1;</italic>&#x0232a;</td><td align="center" rowspan="1" colspan="1">
<inline-formula id="pone.0298650.e028">
<alternatives><graphic xlink:href="pone.0298650.e028" id="pone.0298650.e028g" position="anchor"/><mml:math id="M28" display="inline" overflow="scroll"><mml:mover accent="true"><mml:mi>&#x003b1;</mml:mi><mml:mo>&#x000af;</mml:mo></mml:mover></mml:math></alternatives>
</inline-formula>
</td></tr><tr><td align="center" rowspan="1" colspan="1">random sizes</td><td align="center" rowspan="1" colspan="1">1.00</td><td align="center" rowspan="1" colspan="1">1.03</td><td align="center" rowspan="1" colspan="1">0.93</td><td align="center" rowspan="1" colspan="1">0.97</td></tr><tr><td align="center" rowspan="1" colspan="1">sampled sizes</td><td align="center" rowspan="1" colspan="1">1.00</td><td align="center" rowspan="1" colspan="1">1.06</td><td align="center" rowspan="1" colspan="1">0.93</td><td align="center" rowspan="1" colspan="1">1.02</td></tr><tr><td align="center" rowspan="1" colspan="1">shuffled sizes</td><td align="center" rowspan="1" colspan="1">1.00</td><td align="center" rowspan="1" colspan="1">1.01</td><td align="center" rowspan="1" colspan="1">0.63</td><td align="center" rowspan="1" colspan="1">0.78</td></tr><tr><td align="center" rowspan="1" colspan="1">original sizes</td><td align="center" rowspan="1" colspan="1">0.93</td><td align="center" rowspan="1" colspan="1">0.94</td><td align="center" style="background-color:#D7D7D7" rowspan="1" colspan="1">
<bold>0.49</bold>
</td><td align="center" style="background-color:#D7D7D7" rowspan="1" colspan="1">
<bold>0.66</bold>
</td></tr></tbody></table></alternatives></table-wrap></sec></sec><sec sec-type="results" id="sec012"><title>Results</title><sec id="sec013"><title>Maximally representation-agnostic temporal embedding</title><p>While constructing semantic trajectories of words from time-labelled co-occurrence data, one needs to account for two sources of arbitrariness in the process: stochasticity and symmetry. The first, stochasticity, comes from the nature of modern data-efficient semantic embedding methods, such as Word2vec. Since these approaches use a neural network to find the best possible embedding, sampling techniques, initial weights, and the choice of numerical optimization method (such as stochastic gradient descent) lead to non-deterministic embeddings. Second, the cost <italic toggle="yes">C</italic> itself exhibits multiple global minima associated with the same input data. In particular, the form of the estimated log-probabilities defined in (<xref rid="pone.0298650.e002" ref-type="disp-formula">2</xref>) implies that any transformation that leaves all dot products <italic toggle="yes">v</italic><sub><italic toggle="yes">i</italic></sub> &#x022c5; <italic toggle="yes">w</italic><sub><italic toggle="yes">j</italic></sub> invariant results in the exact same cost <italic toggle="yes">C</italic>. The simplest example of such transformation is a linear rescaling of word vectors <italic toggle="yes">v</italic> &#x021a6; &#x003bb;<italic toggle="yes">v</italic> and inverse rescaling of context word vectors <italic toggle="yes">w</italic> &#x021a6; &#x003bb;<sup>&#x02212;1</sup><italic toggle="yes">w</italic>, but in principle, any invertible linear transformation of word vectors <italic toggle="yes">v</italic> &#x021a6; <italic toggle="yes">Rv</italic>, together with (the transpose of) its inverse applied to context vectors <italic toggle="yes">w</italic> &#x021a6; (<italic toggle="yes">R</italic><sup>&#x02212;1</sup>)<sup><italic toggle="yes">T</italic></sup><italic toggle="yes">w</italic> leaves the dot product invariant (see <xref rid="sec002" ref-type="sec">Methods</xref> for details).</p><p>However, an additional constraint comes from focusing solely on words (and not contexts) when constructing temporal trajectories: the constraint that ensures that the ambient embedding space does not shrink or expand over time, formalized as
<disp-formula id="pone.0298650.e029"><alternatives><graphic xlink:href="pone.0298650.e029.jpg" id="pone.0298650.e029g" position="anchor"/><mml:math id="M29" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mtext>Tr</mml:mtext><mml:mspace width="2pt"/><mml:mi>V</mml:mi><mml:mo>=</mml:mo><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>D</mml:mi></mml:munderover><mml:msubsup><mml:mi>&#x003c3;</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:mtext>const</mml:mtext><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives>
<label>(11)</label></disp-formula>
where <italic toggle="yes">V</italic> is the <italic toggle="yes">D</italic> &#x000d7; <italic toggle="yes">D</italic> empirical covariance matrix of word positions (with <italic toggle="yes">D</italic> = 300 being the embedding dimension) and <italic toggle="yes">&#x003c3;</italic><sub><italic toggle="yes">i</italic></sub> is the standard deviation of word positions along principal dimension <italic toggle="yes">i</italic>. As shown in Methods, this reduces the possible transformations <italic toggle="yes">R</italic> to the orthogonal ones, obeying <italic toggle="yes">R</italic><sup>&#x02212;1</sup> = <italic toggle="yes">R</italic><sup><italic toggle="yes">T</italic></sup>. Consequently, a single embedding is identified with an equivalence class containing {<italic toggle="yes">Rv</italic><sub><italic toggle="yes">i</italic></sub>, <italic toggle="yes">Rw</italic><sub><italic toggle="yes">j</italic></sub>} with any orthogonal <italic toggle="yes">R</italic>.</p><p>We use this orthogonal freedom to define maximally representation-agnostic trajectories in two steps as shown in <xref rid="pone.0298650.g001" ref-type="fig">Fig 1b</xref>. First, we minimize the effect of stochasticity, described below, in any single time. Without stochasticity, any embedding starting from the same co-occurrence data would belong to the same equivalence class, and therefore it would be possible to find a transformation <italic toggle="yes">R</italic><sub><italic toggle="yes">j</italic></sub> for each embedding <italic toggle="yes">j</italic> such that they all numerically coincide. With stochasticity, however, different embedding realizations, starting from the same data, land in (slightly) different equivalence classes, and as a consequence, perfect alignment among them is not possible. The best one can do is to find a transformation for each embedding such that an overall distance measure between all embeddings is minimized (see <xref rid="sec002" ref-type="sec">Methods</xref> for details). This allows us to &#x0201c;average out&#x0201d;, i.e., minimize the effect of stochasticity at any single timestep by taking the average of all aligned embeddings. Second, we align averaged embeddings corresponding to <italic toggle="yes">different</italic> times in the same way: we find an orthogonal transformation that minimizes the distance between the embedding at subsequent times to construct word trajectories. Such word trajectories are thus maximally smoothened. Although this raises the question of whether maximal smoothening washes away real phenomena, we will see that this smoothening procedure applied to random walk does not change measured observables such as the anomalous diffusion exponent <italic toggle="yes">&#x003b1;</italic> of the process. <xref rid="pone.0298650.g001" ref-type="fig">Fig 1c</xref> shows the measured semantic change of three selected words within a decade, projected to 2 dimensions, visualized over a static background.</p></sec><sec id="sec014"><title>Semantic subdiffusion across languages</title><p>As detailed in Methods, to measure the actual value of the anomalous diffusion exponent <italic toggle="yes">&#x003b1;</italic>, we need to take averages over the whole vocabulary. We proceed from single trajectories to an ensemble of trajectories in two alternative ways: (i) we first average |&#x00394;<italic toggle="yes">x</italic>|<sup>2</sup>(<italic toggle="yes">t</italic>) over individual trajectories to obtain &#x02329;|&#x00394;<italic toggle="yes">x</italic>|<sup>2</sup>(<italic toggle="yes">t</italic>)&#x0232a; and then we fit the <italic toggle="yes">ensemble-average anomalous diffusion exponent</italic>, &#x02329;<italic toggle="yes">&#x003b1;</italic>&#x0232a;, based on &#x02329;|&#x00394;<italic toggle="yes">x</italic>|<sup>2</sup>(<italic toggle="yes">t</italic>)&#x0232a; &#x0223c; <italic toggle="yes">t</italic><sup>&#x02329;<italic toggle="yes">&#x003b1;</italic>&#x0232a;</sup>; (ii) alternatively, we first fit the anomalous diffusion exponent <italic toggle="yes">&#x003b1;</italic> to single trajectories and then we average over words to obtain the <italic toggle="yes">mean anomalous diffusion exponent</italic>
<inline-formula id="pone.0298650.e030"><alternatives><graphic xlink:href="pone.0298650.e030.jpg" id="pone.0298650.e030g" position="anchor"/><mml:math id="M30" display="inline" overflow="scroll"><mml:mover accent="true"><mml:mi>&#x003b1;</mml:mi><mml:mo>&#x000af;</mml:mo></mml:mover></mml:math></alternatives></inline-formula>. Although individual trajectories deviate considerably from the simple scaling behavior given by |&#x00394;<italic toggle="yes">x</italic>|<sup>2</sup> &#x0223c; <italic toggle="yes">t</italic><sup><italic toggle="yes">&#x003b1;</italic></sup>, somewhat surprisingly, their ensemble average, &#x02329;|&#x00394;<italic toggle="yes">x</italic>|<sup>2</sup>(<italic toggle="yes">t</italic>)&#x0232a;, follows the scaling given by <italic toggle="yes">t</italic><sup>&#x02329;<italic toggle="yes">&#x003b1;</italic>&#x0232a;</sup> with high accuracy. This is illustrated in <xref rid="pone.0298650.g003" ref-type="fig">Fig 3a</xref>, showing the squared displacement |&#x00394;<italic toggle="yes">x</italic>|<sup>2</sup>(<italic toggle="yes">t</italic>) of several individual English words as well as the ensemble average &#x02329;|&#x00394;<italic toggle="yes">x</italic>|<sup>2</sup>(<italic toggle="yes">t</italic>)&#x0232a; of all English words.</p><p>The grey cells of <xref rid="pone.0298650.t001" ref-type="table">Table 1</xref> list obtained exponents &#x02329;<italic toggle="yes">&#x003b1;</italic>&#x0232a; and <inline-formula id="pone.0298650.e031"><alternatives><graphic xlink:href="pone.0298650.e031.jpg" id="pone.0298650.e031g" position="anchor"/><mml:math id="M31" display="inline" overflow="scroll"><mml:mover accent="true"><mml:mi>&#x003b1;</mml:mi><mml:mo>&#x000af;</mml:mo></mml:mover></mml:math></alternatives></inline-formula> for five languages, English, French, German, Italian, and Spanish. In all languages, both the ensemble-average anomalous diffusion exponent &#x02329;<italic toggle="yes">&#x003b1;</italic>&#x0232a; and the mean anomalous diffusion exponent <inline-formula id="pone.0298650.e032"><alternatives><graphic xlink:href="pone.0298650.e032.jpg" id="pone.0298650.e032g" position="anchor"/><mml:math id="M32" display="inline" overflow="scroll"><mml:mover accent="true"><mml:mi>&#x003b1;</mml:mi><mml:mo>&#x000af;</mml:mo></mml:mover></mml:math></alternatives></inline-formula> are significantly lower than <italic toggle="yes">&#x003b1;</italic> = 1, i.e., semantic trajectories show subdiffusive behaviour on this time scale. In particular, we measured an ensemble-average anomalous diffusion exponent 0.4 &#x0003c; &#x02329;<italic toggle="yes">&#x003b1;</italic>&#x0232a; &#x0003c; 0.5 for all five languages. This is in strong contrast with a random walk generated using the same parameters (see <xref rid="sec002" ref-type="sec">Methods</xref> for details), corresponding to a fitted <italic toggle="yes">&#x003b1;</italic> &#x02248; 1, as shown by <xref rid="pone.0298650.g003" ref-type="fig">Fig 3b</xref>.</p></sec><sec id="sec015"><title>Comparison with randomized trajectories</title><p>Given the robust observation that the ensemble-average semantic behaviour of words follows subdiffusion with &#x02329;<italic toggle="yes">&#x003b1;</italic>&#x0232a; &#x02248; 0.4 &#x02212; 0.5 across languages, one might ask the following two questions. (i) Is this an artifact of the diachronic alignment procedure? (ii) If not, what is behind the observed subdiffusion? In other words, what (combination of) microscopic models of stochastic collective dynamics might explain this macroscopic result? In the following, we focus on these two questions. We apply a series of randomization methods to the original trajectories, gradually removing temporal correlations in step sizes and step directions of individual trajectories to see which of these, if any, play a role behind subdiffusion. As shown in <xref rid="pone.0298650.g003" ref-type="fig">Fig 3c</xref> and <xref rid="pone.0298650.t001" ref-type="table">Table 1</xref>, step sizes of a trajectory are randomized three different ways, which we call <italic toggle="yes">random sizes</italic>, <italic toggle="yes">sizes from distribution</italic>, and <italic toggle="yes">shuffled sizes</italic>; step directions are randomized two ways, <italic toggle="yes">random directions</italic>, <italic toggle="yes">shuffled directions</italic> (see <xref rid="sec002" ref-type="sec">Methods</xref>), which, together with the original trajectories, gives three times four combinations.</p><p>
<xref rid="pone.0298650.g003" ref-type="fig">Fig 3c and 3d</xref> show the average squared displacement &#x02329;(&#x00394;<italic toggle="yes">x</italic>)<sup>2</sup>&#x0232a; of all English words, and the distribution of anomalous diffusion exponents <italic toggle="yes">&#x003b1;</italic> fitted to individual word trajectories separately, under all twelve combinations of trajectory randomization methods (including the original trajectories). The same plots for French, German, Italian, and Spanish are shown in S3 and S4 Figs in <xref rid="pone.0298650.s001" ref-type="supplementary-material">S1 File</xref>; fitted anomalous diffusion exponents &#x02329;<italic toggle="yes">&#x003b1;</italic>&#x0232a; and <inline-formula id="pone.0298650.e009"><alternatives><graphic xlink:href="pone.0298650.e009.jpg" id="pone.0298650.e009g" position="anchor"/><mml:math id="M9" display="inline" overflow="scroll"><mml:mover accent="true"><mml:mi>&#x003b1;</mml:mi><mml:mo>&#x000af;</mml:mo></mml:mover></mml:math></alternatives></inline-formula> are listed for all five languages in <xref rid="pone.0298650.t001" ref-type="table">Table 1</xref>. The top left panel (random step sizes, random step directions) corresponds to a random walk; the bottom right panel (original step sizes, original step directions) corresponds to the original, non-randomized semantic trajectories.</p><p>Comparing the results of original trajectories with the randomized trajectories (<xref rid="pone.0298650.g003" ref-type="fig">Fig 3c</xref>) conveys three important messages: (i) temporal alignment, applied to an ensemble of uncorrelated trajectories (top left panel), results &#x02329;<italic toggle="yes">&#x003b1;</italic>&#x0232a; = 1 (see top left panel), equivalent to that of uncorrelated trajectories <italic toggle="yes">without</italic> alignment. (ii) Both correlations in step sizes and step directions are important factors behind subdiffusion. Neither of them alone can produce trajectories with an <italic toggle="yes">&#x003b1;</italic> lower than 0.8, yet the two combined give <italic toggle="yes">&#x003b1;</italic> &#x02248; 0.5. (iii) If step sizes do not vary significantly, the shuffling of the directions can alter the <italic toggle="yes">total</italic> displacement along a trajectory only by a small amount, thus, the last data points in the middle row (shuffled directions) must be very close to the last data points of the corresponding panels of the last row (original directions). On the other hand, shuffled directions do remove temporal correlations in step directions, making trajectories follow approximately diffusion-like behavior until the constraint on total displacement does not affect them. This can be clearly seen in the middle row of <xref rid="pone.0298650.g003" ref-type="fig">Fig 3c</xref>; in <xref rid="pone.0298650.g003" ref-type="fig">Fig 3d</xref>, we decided not to fit exponents to individual word trajectories in the middle row (shuffled directions) to avoid systematic bias in the exponents depending on the fitting range.</p><p>We further investigate a related effect, the effect of keeping the total size of the word <italic toggle="yes">cloud</italic> constant, as formalized by <xref rid="pone.0298650.e029" ref-type="disp-formula">Eq (11)</xref>. Ensembles of randomized trajectories in <xref rid="pone.0298650.g003" ref-type="fig">Fig 3c and 3d</xref> do not obey this constraint; we, therefore, generated a random walk, corresponding to random step directions and random step sizes, with the additional constraint on keeping the total cloud size constant. This simulated trajectory is illustrated in <xref rid="pone.0298650.g003" ref-type="fig">Fig 3b</xref> (&#x0201c;random walk&#x0201d;). While the total displacement is limited by the size of the word cloud, this only appears to affect the random walk trajectory when the displacement reaches the radius of the cloud (and then converges to approximately to the square root of two times the radius, corresponding to two orthogonal vectors with length equals to the radius). This, along with the middle row of <xref rid="pone.0298650.g003" ref-type="fig">Fig 3c</xref>, suggests that global constraints on total displacement do not cause the observed subdiffusive behavior.</p><p>Finally, we compare the original semantic trajectories to another randomized model, where time labels of the original data are randomly reshuffled (see details in Section S6 in <xref rid="pone.0298650.s001" ref-type="supplementary-material">S1 File</xref>). As shown by S5 Fig in <xref rid="pone.0298650.s001" ref-type="supplementary-material">S1 File</xref>, such randomized &#x0201c;trajectories&#x0201d; display a non-zero anomalous diffusion exponent, <italic toggle="yes">&#x003b1;</italic> &#x02248; 0.2, which can be attributed to the <italic toggle="yes">alignment</italic> of embeddings corresponding to subsequent times. This is an unavoidable effect whenever embedding dimensions are arbitrary and thus alignment is necessary; this baseline non-zero anomalous diffusion exponent is present even when time labels of diffusive trajectories (<italic toggle="yes">&#x003b1;</italic> = 1) are reshuffled before the same pipeline, including alignment, is applied (S8 Fig in <xref rid="pone.0298650.s001" ref-type="supplementary-material">S1 File</xref>). Trajectories built from reshuffled time labels, however, display significantly larger fluctuations (S6 Fig in <xref rid="pone.0298650.s001" ref-type="supplementary-material">S1 File</xref>), and correspondingly, the goodness of fit of the anomalous diffusion exponent is significantly lower (S7 Fig in <xref rid="pone.0298650.s001" ref-type="supplementary-material">S1 File</xref>).</p></sec></sec><sec sec-type="conclusions" id="sec016"><title>Discussion</title><p>Cumulative culture is arguably one of the most distinct characteristic features of human behavior. Understanding &#x0201c;laws&#x0201d; that govern cultural evolution is a crucial step towards understanding Homo Sapiens itself. Here we study cultural evolution through the evolution of <italic toggle="yes">meaning</italic> of words, as formalized by the distributional hypothesis: &#x0201c;a word is characterized by the company it keeps&#x0201d; [<xref rid="pone.0298650.ref044" ref-type="bibr">44</xref>]. Cultural evolution is notoriously difficult to study without making a large number of subjective assumptions and interpretations. One of the main contributions of this work is that it tries to make underlying assumptions (that are in the form of mathematical formalizations and their interpretation) as explicit as possible. We build on the work by Hamilton et al. [<xref rid="pone.0298650.ref020" ref-type="bibr">20</xref>] to find statistical regularities of <italic toggle="yes">global</italic> meaning change, i.e., with respect to a global semantic embedding, as opposed to a local semantic neighborhood, indicative of a stronger role of linguistic drift than cultural shift. We note that a classification of <italic toggle="yes">local</italic> stochastic trajectories (with respect to the semantic neighborhood) would provide a basis for a robust comparison of the above, a possible subject of future work.</p><p>Our data processing and analysis pipeline consists of three phases. First, semantic relations between all words are extracted through a state-of-the-art implementation of the distributional hypothesis: Word2vec, trained with the so-called Skip-gram with a negative sampling method. This algorithm provides a high-dimensional embedding of all words such that pairwise distances reflect semantic similarity. Apart from being fast and data-efficient, it is also well-anchored in human language representation through psycholinguistic studies. Second, to extract evolutionary trajectories, embeddings at different times need to be weaved together. This is a highly non-trivial process since the mapping between embedding and co-occurrence statistics is degenerate: many embeddings are consistent with the same co-occurrence data. When constructing trajectories, we need to break this symmetry: one specific embedding from each equivalence class needs to be chosen. We jointly construct equivalence classes and choose one specific representative of each class, both informed by the dynamics. In particular, we choose trajectories to be maximally smoothened. As we show, this maximal smoothening, applied to an ensemble of diffusing particles, does not alter the anomalous diffusion exponent <italic toggle="yes">&#x003b1;</italic> of the process, suggesting that it would not alter <italic toggle="yes">&#x003b1;</italic> significantly when actual semantic trajectories are considered, either. The third phase of the process is the comparison of the ensemble of semantic trajectories of words with various randomized counterpart ensembles. The randomization method we consider dissects various temporal correlations in semantic trajectories by randomizing step directions and step sizes separately, while still applying the same method for temporal alignment (i.e. trajectory smoothening).</p><p>With all this, we seek to answer the following questions: (i) is there any robust statistical regularity regarding the ensemble of actual semantic trajectories of words? (ii) If yes, what might be the reason? What microscopic dynamical rules can explain the observed macroscopic (ensemble-level) statistical regularities? This work provides an answer to the first question: semantic trajectories show different behaviour on the examined time scale from ordinary random walk (diffusion); semantic trajectories seem to be subdiffusive. In particular, on actual semantic trajectories extracted from data with our processing methodology we measure an anomalous diffusion exponent <italic toggle="yes">&#x003b1;</italic> &#x02248; 0.4 to 0.5, in strong contrast with a random walk belonging to the <italic toggle="yes">&#x003b1;</italic> = 1 class. We note that the measured numerical value depends on the parameters of the process, especially the size of the applied sliding window. Since word embedding is a non-linear conversion of linguistic statistics to vector representation, it is not surprising that parameters introduced before that step have non-trivial effect on the result. The investigation of this dependence would require a deep analysis of the Word2vec algorithm, which goes beyond the scope of this paper, however we show in <xref rid="pone.0298650.s001" ref-type="supplementary-material">S1 File</xref> that this subdiffusive behavior appears with other parameter choices, and the approximate value of 0.4&#x02013;0.5 is the upper limit where the exponent converges to. Hence the result of our analysis are in accordance with the classification of semantic drift as subdiffusion. This deviation from normal diffusion suggests that at least one of the criteria for diffusion mentioned in the Introduction must not hold. In other words, semantic change cannot be entirely random; correlations must exist. Considering how human memory and collective knowledge influence language use, this might seem obvious. Still, the subdiffusive behavior offers a fresh perspective on the subject.</p><p>We point out here that short-range temporal correlations in trajectories, not extremely inhomogeneous step sizes, or weak correlations between trajectories do not cause the resulting trajectories to deviate from <italic toggle="yes">&#x003b1;</italic> = 1. Instead, subdiffusion can be explained by qualitatively different microscopic dynamical rules. These include (i) long temporal correlations in trajectories, (ii) extremely inhomogeneous step size distributions, (iii) stochastic dynamics of &#x0201c;jamming&#x0201d; (overly densely packed) particles (here, words), (iv) changing average step sizes over time (corresponding to a changing diffusion coefficient), (v) diffusion in disordered media, and many others [<xref rid="pone.0298650.ref027" ref-type="bibr">27</xref>&#x02013;<xref rid="pone.0298650.ref030" ref-type="bibr">30</xref>]. If one would like to describe the microscopic dynamics of the cognitive effects that can influence semantic change, the above observable statistical properties or some kind of combination of them should be obtained on an ensemble level. Although investigating possible combinations of these microscopic dynamics as explanations of semantic subdiffusion is a subject of future work, we can at least exclude some of them based on our results. Step size distribution (&#x0201c;Brownian vs Levy flight&#x0201d;) and even correlations in step sizes do not seem to contribute to subdiffusion at all. Correlations in step directions explain some but far from all: ensembles of trajectories where step sizes are randomized but step directions are kept still exhibit <italic toggle="yes">&#x003b1;</italic> &#x02248; 0.8&#x02013;0.9 in contrast with actual trajectories that follow <italic toggle="yes">&#x003b1;</italic> &#x02248; 0.4&#x02013;0.5 (that include correlations both among step sizes and directions, and possibly cross-correlations between these categories too).</p><p>We highlight the following limitations of this analysis. First, semantic embedding methods, such as Word2Vec, compress pairwise statistical relationships between words by embedding them in a space with arbitrary dimensions. Consequently, embeddings corresponding to different times, such as subsequent years, have to be aligned to each other. This introduces an unavoidable bias in the properties of semantic trajectories; comparison with randomized models are necessary to distinguish artifacts of the pipeline from properties of the data. Second, semantic embedding and temporal alignment generates semantic trajectories of words jointly; the position of a word is determined by the position of all other words by minimizing a global cost function, suggesting that trajectories of individual words might be less informative than summary statistics of an ensemble of trajectories. Third, available data is highly unevenly distributed across time, necessiating a trade-off between the length of the trajectories and the embedding noise due to sparse data.</p><p>As in the biological examples where similar subdiffusive behavior arises, semantic change also raises questions both at a mechanistic, proximal level (what microscopic dynamical rules underlie subdiffusion?) and at an evolutionary, distal level (is subdiffusion adaptive or is it a consequence of physical-informational constraints? If it is adaptive, what is it <italic toggle="yes">for</italic> and what selection pressures led to its emergence?). In these cases, however, it is certainly known that the movement is governed by microscopic natural (physical or biological) laws which cause emergent macroscopic statistical effects. Even though such microscopic laws of language evolution are more context-dependent and noisy, and thus simple microscopic models provide a less powerful explanation to social and cognitive notions like semantic change or cultural memory, we indicate in this paper that their macroscopic (ensemble-level) behavior might converge to statistical classes, akin to universality classes that describe effective separation of scales in physics and computation.</p></sec><sec id="sec017" sec-type="supplementary-material"><title>Supporting information</title><supplementary-material id="pone.0298650.s001" position="float" content-type="local-data"><label>S1 File</label><caption><title>Supporting information.</title><p>We present and explain in this file all the auxiliary results and arguments that are related to this paper.</p><p>(PDF)</p></caption><media xlink:href="pone.0298650.s001.pdf"/></supplementary-material></sec></body><back><ack><p>The authors thank Douglas Moore, Yanbo Zhang, Jake Hanson, Andr&#x000e1;s Sz&#x000e1;nt&#x000f3;, J&#x000f3;zsef Venczeli, and P&#x000e9;ter Pollner for useful discussions at various stages of the project.</p></ack><ref-list><title>References</title><ref id="pone.0298650.ref001"><label>1</label><mixed-citation publication-type="book">
<name><surname>Mesoudi</surname><given-names>Alex</given-names></name>. <part-title>Cultural evolution</part-title>. In <source><italic toggle="yes">Cultural Evolution</italic></source>. <publisher-name>University of Chicago Press</publisher-name>, <year>2011</year>.</mixed-citation></ref><ref id="pone.0298650.ref002"><label>2</label><mixed-citation publication-type="journal">
<name><surname>Hauser</surname><given-names>Marc D</given-names></name>, <name><surname>Chomsky</surname><given-names>Noam</given-names></name>, and <name><surname>Fitch</surname><given-names>W Tecumseh</given-names></name>. <article-title>The faculty of language: what is it, who has it, and how did it evolve?</article-title>
<source><italic toggle="yes">science</italic></source>, <volume>298</volume>(<issue>5598</issue>):<fpage>1569</fpage>&#x02013;<lpage>1579</lpage>, <year>2002</year>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1126/science.298.5598.1569</pub-id>
<?supplied-pmid 12446899?><pub-id pub-id-type="pmid">12446899</pub-id>
</mixed-citation></ref><ref id="pone.0298650.ref003"><label>3</label><mixed-citation publication-type="book">
<name><surname>Malkiel</surname><given-names>Yakov</given-names></name>. <source><italic toggle="yes">Etymology</italic></source>. <publisher-name>Cambridge University Press</publisher-name>, <year>1993</year>.</mixed-citation></ref><ref id="pone.0298650.ref004"><label>4</label><mixed-citation publication-type="journal">
<name><surname>Nowak</surname><given-names>Martin A</given-names></name>, <name><surname>Komarova</surname><given-names>Natalia L</given-names></name>, and <name><surname>Niyogi</surname><given-names>Partha</given-names></name>. <article-title>Computational and evolutionary aspects of language</article-title>. <source><italic toggle="yes">Nature</italic></source>, <volume>417</volume>(<issue>6889</issue>):<fpage>611</fpage>&#x02013;<lpage>617</lpage>, <year>2002</year>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1038/nature00771</pub-id>
<?supplied-pmid 12050656?><pub-id pub-id-type="pmid">12050656</pub-id>
</mixed-citation></ref><ref id="pone.0298650.ref005"><label>5</label><mixed-citation publication-type="journal">
<name><surname>Kirby</surname><given-names>Simon</given-names></name>, <name><surname>Tamariz</surname><given-names>Monica</given-names></name>, <name><surname>Cornish</surname><given-names>Hannah</given-names></name>, and <name><surname>Smith</surname><given-names>Kenny</given-names></name>. <article-title>Compression and communication in the cultural evolution of linguistic structure</article-title>. <source><italic toggle="yes">Cognition</italic></source>, <volume>141</volume>:<fpage>87</fpage>&#x02013;<lpage>102</lpage>, <year>2015</year>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.cognition.2015.03.016</pub-id>
<?supplied-pmid 25966840?><pub-id pub-id-type="pmid">25966840</pub-id>
</mixed-citation></ref><ref id="pone.0298650.ref006"><label>6</label><mixed-citation publication-type="journal">
<name><surname>Landsbergen</surname><given-names>Frank</given-names></name>, <name><surname>Lachlan</surname><given-names>Robert</given-names></name>, <name><surname>Cate</surname><given-names>ten Carel</given-names></name>, and <name><surname>Verhagen</surname><given-names>Arie</given-names></name>. <article-title>A cultural evolutionary model of patterns in semantic change</article-title>. <source><italic toggle="yes">Linguistics</italic></source>, <volume>48</volume>(<issue>2</issue>):<fpage>363</fpage>&#x02013;<lpage>390</lpage>, <year>2010</year>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1515/ling.2010.012</pub-id></mixed-citation></ref><ref id="pone.0298650.ref007"><label>7</label><mixed-citation publication-type="journal">
<name><surname>Petersen</surname><given-names>Alexander M</given-names></name>, <name><surname>Tenenbaum</surname><given-names>Joel</given-names></name>, <name><surname>Havlin</surname><given-names>Shlomo</given-names></name>, and <name><surname>Stanley</surname><given-names>H Eugene</given-names></name>. <article-title>Statistical laws governing fluctuations in word use from word birth to word death</article-title>. <source><italic toggle="yes">Scientific reports</italic></source>, <volume>2</volume>(<issue>1</issue>):<fpage>1</fpage>&#x02013;<lpage>9</lpage>, <year>2012</year>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1038/srep00313</pub-id>
<?supplied-pmid 22423321?><pub-id pub-id-type="pmid">22423321</pub-id>
</mixed-citation></ref><ref id="pone.0298650.ref008"><label>8</label><mixed-citation publication-type="journal">
<name><surname>Michel</surname><given-names>Jean-Baptiste</given-names></name>, <name><surname>Shen</surname><given-names>Yuan Kui</given-names></name>, <name><surname>Aiden</surname><given-names>Aviva Presser</given-names></name>, <name><surname>Veres</surname><given-names>Adrian</given-names></name>, <name><surname>Gray</surname><given-names>Matthew K</given-names></name>, <name><surname>Pickett</surname><given-names>Joseph P</given-names></name>, <etal>et al</etal>. <article-title>Quantitative analysis of culture using millions of digitized books</article-title>. <source><italic toggle="yes">science</italic></source>, <volume>331</volume>(<issue>6014</issue>):<fpage>176</fpage>&#x02013;<lpage>182</lpage>, <year>2011</year>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1126/science.1199644</pub-id>
<?supplied-pmid 21163965?><pub-id pub-id-type="pmid">21163965</pub-id>
</mixed-citation></ref><ref id="pone.0298650.ref009"><label>9</label><mixed-citation publication-type="journal">
<name><surname>Lieberman</surname><given-names>Erez</given-names></name>, <name><surname>Michel</surname><given-names>Jean-Baptiste</given-names></name>, <name><surname>Jackson</surname><given-names>Joe</given-names></name>, <name><surname>Tang</surname><given-names>Tina</given-names></name>, and <name><surname>Nowak</surname><given-names>Martin A</given-names></name>. <article-title>Quantifying the evolutionary dynamics of language</article-title>. <source><italic toggle="yes">Nature</italic></source>, <volume>449</volume>(<issue>7163</issue>):<fpage>713</fpage>&#x02013;<lpage>716</lpage>, <year>2007</year>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1038/nature06137</pub-id>
<?supplied-pmid 17928859?><pub-id pub-id-type="pmid">17928859</pub-id>
</mixed-citation></ref><ref id="pone.0298650.ref010"><label>10</label><mixed-citation publication-type="journal">
<name><surname>Gao</surname><given-names>Jianbo</given-names></name>, <name><surname>Hu</surname><given-names>Jing</given-names></name>, <name><surname>Mao</surname><given-names>Xiang</given-names></name>, and <name><surname>Perc</surname><given-names>Matja&#x0017e;</given-names></name>. <article-title>Culturomics meets random fractal theory: insights into long-range correlations of social and natural phenomena over the past two centuries</article-title>. <source><italic toggle="yes">Journal of The Royal Society Interface</italic></source>, <volume>9</volume>(<issue>73</issue>):<fpage>1956</fpage>&#x02013;<lpage>1964</lpage>, <year>2012</year>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1098/rsif.2011.0846</pub-id>
<?supplied-pmid 22337632?><pub-id pub-id-type="pmid">22337632</pub-id>
</mixed-citation></ref><ref id="pone.0298650.ref011"><label>11</label><mixed-citation publication-type="journal">
<name><surname>Harris</surname><given-names>Zellig S</given-names></name>. <article-title>Distributional structure</article-title>. <source><italic toggle="yes">Word</italic></source>, <volume>10</volume>(<issue>2-3</issue>):<fpage>146</fpage>&#x02013;<lpage>162</lpage>, <year>1954</year>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1080/00437956.1954.11659520</pub-id></mixed-citation></ref><ref id="pone.0298650.ref012"><label>12</label><mixed-citation publication-type="journal">
<name><surname>Lenci</surname><given-names>Alessandro</given-names></name>. <article-title>Distributional models of word meaning</article-title>. <source><italic toggle="yes">Annual review of Linguistics</italic></source>, <volume>4</volume>:<fpage>151</fpage>&#x02013;<lpage>171</lpage>, <year>2018</year>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1146/annurev-linguistics-030514-125254</pub-id></mixed-citation></ref><ref id="pone.0298650.ref013"><label>13</label><mixed-citation publication-type="journal">
<name><surname>Landauer</surname><given-names>Thomas K</given-names></name>, <name><surname>Foltz</surname><given-names>Peter W</given-names></name>, and <name><surname>Laham</surname><given-names>Darrell</given-names></name>. <article-title>An introduction to latent semantic analysis</article-title>. <source><italic toggle="yes">Discourse processes</italic></source>, <volume>25</volume>(<issue>2-3</issue>):<fpage>259</fpage>&#x02013;<lpage>284</lpage>, <year>1998</year>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1080/01638539809545028</pub-id></mixed-citation></ref><ref id="pone.0298650.ref014"><label>14</label><mixed-citation publication-type="journal">
<name><surname>Papadimitriou</surname><given-names>Christos H</given-names></name>, <name><surname>Raghavan</surname><given-names>Prabhakar</given-names></name>, <name><surname>Tamaki</surname><given-names>Hisao</given-names></name>, and <name><surname>Vempala</surname><given-names>Santosh</given-names></name>. <article-title>Latent semantic indexing: A probabilistic analysis</article-title>. <source><italic toggle="yes">Journal of Computer and System Sciences</italic></source>, <volume>61</volume>(<issue>2</issue>):<fpage>217</fpage>&#x02013;<lpage>235</lpage>, <year>2000</year>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1006/jcss.2000.1711</pub-id></mixed-citation></ref><ref id="pone.0298650.ref015"><label>15</label><mixed-citation publication-type="journal">
<name><surname>Levy</surname><given-names>Omer</given-names></name>, <name><surname>Goldberg</surname><given-names>Yoav</given-names></name>, and <name><surname>Dagan</surname><given-names>Ido</given-names></name>. <article-title>Improving distributional similarity with lessons learned from word embeddings</article-title>. <source><italic toggle="yes">Transactions of the Association for Computational Linguistics</italic></source>, <volume>3</volume>:<fpage>211</fpage>&#x02013;<lpage>225</lpage>, <year>2015</year>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1162/tacl_a_00134</pub-id></mixed-citation></ref><ref id="pone.0298650.ref016"><label>16</label><mixed-citation publication-type="other">Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. Distributed representations of words and phrases and their compositionality. <italic toggle="yes">arXiv preprint arXiv:1310.4546</italic>, 2013.</mixed-citation></ref><ref id="pone.0298650.ref017"><label>17</label><mixed-citation publication-type="other">Yoav Goldberg and Omer Levy. word2vec explained: deriving mikolov et al.&#x02019;s negative-sampling word-embedding method. <italic toggle="yes">arXiv preprint arXiv:1402.3722</italic>, 2014.</mixed-citation></ref><ref id="pone.0298650.ref018"><label>18</label><mixed-citation publication-type="other">Akira Matsui and Emilio Ferrara. Word embedding for social sciences: An interdisciplinary survey. <italic toggle="yes">arXiv preprint arXiv:2207.03086</italic>, 2022.</mixed-citation></ref><ref id="pone.0298650.ref019"><label>19</label><mixed-citation publication-type="journal">
<name><surname>Levy</surname><given-names>Omer</given-names></name> and <name><surname>Goldberg</surname><given-names>Yoav</given-names></name>. <article-title>Neural word embedding as implicit matrix factorization</article-title>. <source><italic toggle="yes">Advances in neural information processing systems</italic></source>, <volume>27</volume>:<fpage>2177</fpage>&#x02013;<lpage>2185</lpage>, <year>2014</year>.</mixed-citation></ref><ref id="pone.0298650.ref020"><label>20</label><mixed-citation publication-type="other">William L Hamilton, Jure Leskovec, and Dan Jurafsky. Diachronic word embeddings reveal statistical laws of semantic change. In <italic toggle="yes">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</italic>, pages 1489&#x02013;1501, 2016.</mixed-citation></ref><ref id="pone.0298650.ref021"><label>21</label><mixed-citation publication-type="other">Haim Dubossarsky, Daphna Weinshall, and Eitan Grossman. Outta control: Laws of semantic change and inherent biases in word representation models. In <italic toggle="yes">Proceedings of the 2017 conference on empirical methods in natural language processing</italic>, pages 1136&#x02013;1145, 2017.</mixed-citation></ref><ref id="pone.0298650.ref022"><label>22</label><mixed-citation publication-type="book">
<name><surname>Zipf</surname><given-names>George Kingsley</given-names></name>. <source><italic toggle="yes">Human behavior and the principle of least effort</italic></source>. <publisher-name>Addison-Wesley Press</publisher-name>, <year>1949</year>.</mixed-citation></ref><ref id="pone.0298650.ref023"><label>23</label><mixed-citation publication-type="journal">
<name><surname>Williams</surname><given-names>Jake Ryland</given-names></name>, <name><surname>Lessard</surname><given-names>Paul R</given-names></name>, <name><surname>Desu</surname><given-names>Suma</given-names></name>, <name><surname>Clark</surname><given-names>Eric M</given-names></name>, <name><surname>Bagrow</surname><given-names>James P</given-names></name>, <name><surname>Danforth</surname><given-names>Christopher M</given-names></name>, <etal>et al</etal>. <article-title>Zipf&#x02019;s law holds for phrases, not words</article-title>. <source><italic toggle="yes">Scientific reports</italic></source>, <volume>5</volume>(<issue>1</issue>):<fpage>1</fpage>&#x02013;<lpage>7</lpage>, <year>2015</year>.</mixed-citation></ref><ref id="pone.0298650.ref024"><label>24</label><mixed-citation publication-type="other">Robert Bamler and Stephan Mandt. Dynamic word embeddings. In <italic toggle="yes">International conference on Machine learning</italic>, pages 380&#x02013;389. PMLR, 2017.</mixed-citation></ref><ref id="pone.0298650.ref025"><label>25</label><mixed-citation publication-type="journal">
<name><surname>Dridi</surname><given-names>Amna</given-names></name>, <name><surname>Gaber</surname><given-names>Mohamed Medhat</given-names></name>, <name><surname>Azad</surname><given-names>Raja Muhammad Atif</given-names></name>, and <name><surname>Bhogal</surname><given-names>Jagdev</given-names></name>. <article-title>Vec2dynamics: A temporal word embedding approach to exploring the dynamics of scientific keywords&#x02014;machine learning as a case study</article-title>. <source><italic toggle="yes">Big Data and Cognitive Computing</italic></source>, <volume>6</volume>(<issue>1</issue>):<fpage>21</fpage>, <year>2022</year>. <comment>doi: </comment><pub-id pub-id-type="doi">10.3390/bdcc6010021</pub-id></mixed-citation></ref><ref id="pone.0298650.ref026"><label>26</label><mixed-citation publication-type="other">Zijun Yao, Yifan Sun, Weicong Ding, Nikhil Rao, and Hui Xiong. Dynamic word embeddings for evolving semantic discovery. In <italic toggle="yes">Proceedings of the eleventh acm international conference on web search and data mining</italic>, pages 673&#x02013;681, 2018.</mixed-citation></ref><ref id="pone.0298650.ref027"><label>27</label><mixed-citation publication-type="journal">
<name><surname>Bouchaud</surname><given-names>Jean-Philippe</given-names></name> and <name><surname>Georges</surname><given-names>Antoine</given-names></name>. <article-title>Anomalous diffusion in disordered media: statistical mechanisms, models and physical applications</article-title>. <source><italic toggle="yes">Physics reports</italic></source>, <volume>195</volume>(<issue>4-5</issue>):<fpage>127</fpage>&#x02013;<lpage>293</lpage>, <year>1990</year>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/0370-1573(90)90099-N</pub-id></mixed-citation></ref><ref id="pone.0298650.ref028"><label>28</label><mixed-citation publication-type="journal">
<name><surname>Metzler</surname><given-names>Ralf</given-names></name> and <name><surname>Klafter</surname><given-names>Joseph</given-names></name>. <article-title>The random walk&#x02019;s guide to anomalous diffusion: a fractional dynamics approach</article-title>. <source><italic toggle="yes">Physics reports</italic></source>, <volume>339</volume>(<issue>1</issue>):<fpage>1</fpage>&#x02013;<lpage>77</lpage>, <year>2000</year>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/S0370-1573(00)00070-3</pub-id></mixed-citation></ref><ref id="pone.0298650.ref029"><label>29</label><mixed-citation publication-type="book">
<name><surname>Klages</surname><given-names>Rainer</given-names></name>, <name><surname>Radons</surname><given-names>G&#x000fc;nter</given-names></name>, and <name><surname>Sokolov</surname><given-names>Igor Mihajlovi&#x0010d;</given-names></name>. <source><italic toggle="yes">Anomalous transport</italic></source>. <publisher-name>Wiley Online Library</publisher-name>, <year>2008</year>.</mixed-citation></ref><ref id="pone.0298650.ref030"><label>30</label><mixed-citation publication-type="journal">
<name><surname>Metzler</surname><given-names>Ralf</given-names></name>, <name><surname>Jeon</surname><given-names>Jae-Hyung</given-names></name>, <name><surname>Cherstvy</surname><given-names>Andrey G</given-names></name>, and <name><surname>Barkai</surname><given-names>Eli</given-names></name>. <article-title>Anomalous diffusion models and their properties: non-stationarity, non-ergodicity, and ageing at the centenary of single particle tracking</article-title>. <source><italic toggle="yes">Physical Chemistry Chemical Physics</italic></source>, <volume>16</volume>(<issue>44</issue>):<fpage>24128</fpage>&#x02013;<lpage>24164</lpage>, <year>2014</year>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1039/C4CP03465A</pub-id>
<?supplied-pmid 25297814?><pub-id pub-id-type="pmid">25297814</pub-id>
</mixed-citation></ref><ref id="pone.0298650.ref031"><label>31</label><mixed-citation publication-type="journal">
<name><surname>Golding</surname><given-names>Ido</given-names></name> and <name><surname>Cox</surname><given-names>Edward C</given-names></name>. <article-title>Physical nature of bacterial cytoplasm</article-title>. <source><italic toggle="yes">Physical review letters</italic></source>, <volume>96</volume>(<issue>9</issue>):<fpage>098102</fpage>, <year>2006</year>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1103/PhysRevLett.96.098102</pub-id>
<?supplied-pmid 16606319?><pub-id pub-id-type="pmid">16606319</pub-id>
</mixed-citation></ref><ref id="pone.0298650.ref032"><label>32</label><mixed-citation publication-type="journal">
<name><surname>Weigel</surname><given-names>Aubrey V</given-names></name>, <name><surname>Simon</surname><given-names>Blair</given-names></name>, <name><surname>Tamkun</surname><given-names>Michael M</given-names></name>, and <name><surname>Krapf</surname><given-names>Diego</given-names></name>. <article-title>Ergodic and nonergodic processes coexist in the plasma membrane as observed by single-molecule tracking</article-title>. <source><italic toggle="yes">Proceedings of the National Academy of Sciences</italic></source>, <volume>108</volume>(<issue>16</issue>):<fpage>6438</fpage>&#x02013;<lpage>6443</lpage>, <year>2011</year>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1073/pnas.1016325108</pub-id>
<?supplied-pmid 21464280?><pub-id pub-id-type="pmid">21464280</pub-id>
</mixed-citation></ref><ref id="pone.0298650.ref033"><label>33</label><mixed-citation publication-type="journal">
<name><surname>Bronstein</surname><given-names>Irena</given-names></name>, <name><surname>Israel</surname><given-names>Y</given-names></name>, <name><surname>Kepten</surname><given-names>Eldad</given-names></name>, <name><surname>Mai</surname><given-names>Sabine</given-names></name>, <name><surname>Shav-Tal</surname><given-names>Yaron</given-names></name>, <name><surname>Barkai</surname><given-names>E</given-names></name>, <etal>et al</etal>. <article-title>Transient anomalous diffusion of telomeres in the nucleus of mammalian cells</article-title>. <source><italic toggle="yes">Physical review letters</italic></source>, <volume>103</volume>(<issue>1</issue>):<fpage>018102</fpage>, <year>2009</year>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1103/PhysRevLett.103.018102</pub-id>
<?supplied-pmid 19659180?><pub-id pub-id-type="pmid">19659180</pub-id>
</mixed-citation></ref><ref id="pone.0298650.ref034"><label>34</label><mixed-citation publication-type="other">Ton Dieker. <italic toggle="yes">Simulation of fractional Brownian motion</italic>. PhD thesis, Masters Thesis, Department of Mathematical Sciences, University of Twente &#x02026;, 2004.</mixed-citation></ref><ref id="pone.0298650.ref035"><label>35</label><mixed-citation publication-type="journal">
<name><surname>Van der Maaten</surname><given-names>Laurens</given-names></name> and <name><surname>Hinton</surname><given-names>Geoffrey</given-names></name>. <article-title>Visualizing data using t-sne</article-title>. <source><italic toggle="yes">Journal of machine learning research</italic></source>, <volume>9</volume>(<issue>11</issue>), <year>2008</year>.</mixed-citation></ref><ref id="pone.0298650.ref036"><label>36</label><mixed-citation publication-type="other">Yuri Lin, Jean-Baptiste Michel, Erez Aiden Lieberman, Jon Orwant, Will Brockman, and Slav Petrov. Syntactic annotations for the Google Books NGram corpus. In <italic toggle="yes">Proceedings of the ACL 2012 System Demonstrations</italic>, pages 169&#x02013;174, Jeju Island, Korea, July 2012. Association for Computational Linguistics.</mixed-citation></ref><ref id="pone.0298650.ref037"><label>37</label><mixed-citation publication-type="journal">
<name><surname>Pechenick</surname><given-names>Eitan Adam</given-names></name>, <name><surname>Danforth</surname><given-names>Christopher M</given-names></name>, and <name><surname>Dodds</surname><given-names>Peter Sheridan</given-names></name>. <article-title>Characterizing the google books corpus: Strong limits to inferences of socio-cultural and linguistic evolution</article-title>. <source><italic toggle="yes">PloS one</italic></source>, <volume>10</volume>(<issue>10</issue>):<fpage>e0137041</fpage>, <year>2015</year>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1371/journal.pone.0137041</pub-id>
<?supplied-pmid 26445406?><pub-id pub-id-type="pmid">26445406</pub-id>
</mixed-citation></ref><ref id="pone.0298650.ref038"><label>38</label><mixed-citation publication-type="journal">
<name><surname>Koplenig</surname><given-names>Alexander</given-names></name>. <article-title>The impact of lacking metadata for the measurement of cultural and linguistic change using the google ngram data sets&#x02014;reconstructing the composition of the german corpus in times of wwii</article-title>. <source><italic toggle="yes">Digital Scholarship in the Humanities</italic></source>, <volume>32</volume>(<issue>1</issue>):<fpage>169</fpage>&#x02013;<lpage>188</lpage>, <year>2017</year>.</mixed-citation></ref><ref id="pone.0298650.ref039"><label>39</label><mixed-citation publication-type="book">
<name><surname>Bird</surname><given-names>Steven</given-names></name>, <name><surname>Klein</surname><given-names>Ewan</given-names></name>, and <name><surname>Loper</surname><given-names>Edward</given-names></name>. <source><italic toggle="yes">Natural language processing with Python: analyzing text with the natural language toolkit</italic></source>. &#x0201c;<publisher-name>O&#x02019;Reilly Media, Inc</publisher-name>.&#x0201d;, <year>2009</year>.</mixed-citation></ref><ref id="pone.0298650.ref040"><label>40</label><mixed-citation publication-type="book">
<name><surname>Sch&#x000fc;tze</surname><given-names>Hinrich</given-names></name>, <name><surname>Manning</surname><given-names>Christopher D</given-names></name>, and <name><surname>Raghavan</surname><given-names>Prabhakar</given-names></name>. <source><italic toggle="yes">Introduction to information retrieval</italic></source>, volume <volume>39</volume>. <publisher-name>Cambridge University Press</publisher-name>
<publisher-loc>Cambridge</publisher-loc>, <year>2008</year>.</mixed-citation></ref><ref id="pone.0298650.ref041"><label>41</label><mixed-citation publication-type="journal">
<name><surname>Milton</surname><given-names>James</given-names></name>, and <name><surname>Treffers-Daller</surname><given-names>Jeanine</given-names></name>. <article-title>Vocabulary size revisited: the link between vocabulary size and academic achievement</article-title>. <source><italic toggle="yes">Applied Linguistics Review</italic></source>, <volume>4</volume>(<issue>2</issue>):<fpage>151</fpage>&#x02013;<lpage>172</lpage>, <year>2013</year>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1515/applirev-2013-0007</pub-id></mixed-citation></ref><ref id="pone.0298650.ref042"><label>42</label><mixed-citation publication-type="other">Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. <italic toggle="yes">arXiv preprint arXiv:1301.3781</italic>, 2013.</mixed-citation></ref><ref id="pone.0298650.ref043"><label>43</label><mixed-citation publication-type="journal">
<name><surname>Sch&#x000f6;nemann</surname><given-names>Peter H</given-names></name>. <article-title>A generalized solution of the orthogonal procrustes problem</article-title>. <source><italic toggle="yes">Psychometrika</italic></source>, <volume>31</volume>(<issue>1</issue>):<fpage>1</fpage>&#x02013;<lpage>10</lpage>, <year>1966</year>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1007/BF02289451</pub-id></mixed-citation></ref><ref id="pone.0298650.ref044"><label>44</label><mixed-citation publication-type="journal">
<name><surname>Lenci</surname><given-names>Alessandro</given-names></name>. <article-title>Distributional semantics in linguistic and cognitive research</article-title>. <source><italic toggle="yes">Italian journal of linguistics</italic></source>, <volume>20</volume>(<issue>1</issue>):<fpage>1</fpage>&#x02013;<lpage>31</lpage>, <year>2008</year>.</mixed-citation></ref></ref-list></back><sub-article article-type="aggregated-review-documents" id="pone.0298650.r001" specific-use="decision-letter"><front-stub><article-id pub-id-type="doi">10.1371/journal.pone.0298650.r001</article-id><title-group><article-title>Decision Letter 0</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Ribeiro</surname><given-names>Haroldo V.</given-names></name><role>Academic Editor</role></contrib></contrib-group><permissions><copyright-statement>&#x000a9; 2024 Haroldo V. Ribeiro</copyright-statement><copyright-year>2024</copyright-year><copyright-holder>Haroldo V. Ribeiro</copyright-holder><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><related-article ext-link-type="doi" xlink:href="10.1371/journal.pone.0298650" id="rel-obj001" related-article-type="reviewed-article"/><custom-meta-group><custom-meta><meta-name>Submission Version</meta-name><meta-value>0</meta-value></custom-meta></custom-meta-group></front-stub><body><p>
<named-content content-type="letter-date">8 Aug 2023</named-content>
</p><p><!-- <div> -->PONE-D-23-19326<!-- </div> --><!-- <div> -->Subdiffusive semantic evolution in major Indo-European languages<!-- </div> --><!-- <div> -->PLOS ONE</p><p>Dear Dr. Cz&#x000e9;gel,</p><p>Thank you for submitting your manuscript to PLOS ONE. After careful consideration, we feel that it has merit but does not fully meet PLOS ONE&#x02019;s publication criteria as it currently stands. Therefore, we invite you to submit a revised version of the manuscript that addresses the points raised during the review process.</p><p>Please submit your revised manuscript by Sep 22 2023 11:59PM. If you will need more time than this to complete your revisions, please reply to this message or contact the journal office at&#x000a0;<email>plosone@plos.org</email>. When you're ready to submit your revision, log on to <ext-link xlink:href="https://www.editorialmanager.com/pone/" ext-link-type="uri">https://www.editorialmanager.com/pone/</ext-link> and select the 'Submissions Needing Revision' folder to locate your manuscript file.</p><p>Please include the following items when submitting your revised manuscript:<!-- </div> --><list list-type="bullet"><list-item><p>A rebuttal letter that responds to each point raised by the academic editor and reviewer(s). You should upload this letter as a separate file labeled 'Response to Reviewers'.</p></list-item><list-item><p>A marked-up copy of your manuscript that highlights changes made to the original version. You should upload this as a separate file labeled 'Revised Manuscript with Track Changes'.</p></list-item><list-item><p>An unmarked version of your revised paper without tracked changes. You should upload this as a separate file labeled 'Manuscript'.</p></list-item></list></p><p>If you would like to make changes to your financial disclosure, please include your updated statement in your cover letter. Guidelines for resubmitting your figure files are available below the reviewer comments at the end of this letter.</p><p>If applicable, we recommend that you deposit your laboratory protocols in protocols.io to enhance the reproducibility of your results. Protocols.io assigns your protocol its own identifier (DOI) so that it can be cited independently in the future. For instructions see: <ext-link xlink:href="https://journals.plos.org/plosone/s/submission-guidelines#loc-laboratory-protocols" ext-link-type="uri">https://journals.plos.org/plosone/s/submission-guidelines#loc-laboratory-protocols</ext-link>. Additionally, PLOS ONE offers an option for publishing peer-reviewed Lab Protocol articles, which describe protocols hosted on protocols.io. Read more information on sharing protocols at <ext-link xlink:href="https://plos.org/protocols?utm_medium=editorial-email&#x00026;utm_source=authorletters&#x00026;utm_campaign=protocols" ext-link-type="uri">https://plos.org/protocols?utm_medium=editorial-email&#x00026;utm_source=authorletters&#x00026;utm_campaign=protocols</ext-link>.</p><p>We look forward to receiving your revised manuscript.</p><p>Kind regards,</p><p>Haroldo V. Ribeiro</p><p>Academic Editor</p><p>PLOS ONE</p><p>Journal requirements:</p><p>When submitting your revision, we need you to address these additional requirements.</p><p>1. Please ensure that your manuscript meets PLOS ONE's style requirements, including those for file naming. The PLOS ONE style templates can be found at</p><p><ext-link xlink:href="https://journals.plos.org/plosone/s/file?id=wjVg/PLOSOne_formatting_sample_main_body.pdf" ext-link-type="uri">https://journals.plos.org/plosone/s/file?id=wjVg/PLOSOne_formatting_sample_main_body.pdf</ext-link> and</p><p>
<ext-link xlink:href="https://journals.plos.org/plosone/s/file?id=ba62/PLOSOne_formatting_sample_title_authors_affiliations.pdf" ext-link-type="uri">https://journals.plos.org/plosone/s/file?id=ba62/PLOSOne_formatting_sample_title_authors_affiliations.pdf</ext-link>
</p><p>2. Thank you for stating the following financial disclosure:</p><p>&#x0201c;This project has received funding from the European Union&#x02019;s Horizon 2020 research and innovation programme under grant agreement no. 101021607 and was partially supported by the National Research, Development and Innovation Office under grant no. K128780 and by the the European Union project RRF-2.3.1-21-2022-00004 within the framework of the Artificial Intelligence National Laborator.&#x0201d;</p><p>Please state what role the funders took in the study.&#x000a0; If the funders had no role, please state: "The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript."</p><p>If this statement is not correct you must amend it as needed.</p><p>Please include this amended Role of Funder statement in your cover letter; we will change the online submission form on your behalf.</p><p>3. Thank you for stating the following in the Acknowledgments Section of your manuscript:</p><p>&#x0201c;The authors thank Douglas Moore, Yanbo Zhang, Jake Hanson, Andr&#x000b4;as Sz&#x000b4;ant&#x000b4;o, J&#x000b4;ozsef 443</p><p>Venczeli, and P&#x000b4;eter Pollner for useful discussions at various stages of the project. This 444</p><p>project has received funding from the European Union&#x02019;s Horizon 2020 research and 445</p><p>innovation programme under grant agreement no. 101021607 and was partially 446</p><p>supported by the National Research, Development and Innovation Office under grant no. 447</p><p>K128780 and by the the European Union project RRF-2.3.1-21-2022-00004 within the 448</p><p>framework of the Artificial Intelligence National Laboratory and by the UNKP-21-3 &#x000b4; 449</p><p>New National Excellence Program of the Ministry for Innovation and Technology from 450</p><p>the source of the National Research, Development and Innovation Fund.&#x0201d;</p><p>We note that you have provided funding information that is currently declared in your Funding Statement. However, funding information should not appear in the Acknowledgments section or other areas of your manuscript. We will only publish funding information present in the Funding Statement section of the online submission form.</p><p>Please remove any funding-related text from the manuscript and let us know how you would like to update your Funding Statement. Currently, your Funding Statement reads as follows:</p><p>&#x0201c;This project has received funding from the European Union&#x02019;s Horizon 2020 research and innovation programme under grant agreement no. 101021607 and was partially supported by the National Research, Development and Innovation Office under grant no. K128780 and by the the European Union project RRF-2.3.1-21-2022-00004 within the framework of the Artificial Intelligence National Laborator.&#x0201d;</p><p>Please include your amended statements within your cover letter; we will change the online submission form on your behalf.</p><p>4. Please include captions for your Supporting Information files at the end of your manuscript, and update any in-text citations to match accordingly. Please see our Supporting Information guidelines for more information: <ext-link xlink:href="http://journals.plos.org/plosone/s/supporting-information" ext-link-type="uri">http://journals.plos.org/plosone/s/supporting-information</ext-link>.</p><p>[Note: HTML markup is below. Please do not edit.]</p><p>Reviewers' comments:</p><p>Reviewer's Responses to Questions</p><p>
<!-- <font color="black"> -->
<bold>Comments to the Author</bold>
</p><p>1. Is the manuscript technically sound, and do the data support the conclusions?</p><p>The manuscript must describe a technically sound piece of scientific research with data that supports the conclusions. Experiments must have been conducted rigorously, with appropriate controls, replication, and sample sizes. The conclusions must be drawn appropriately based on the data presented. <!-- </font> --></p><p>Reviewer #1:&#x000a0;Yes</p><p>Reviewer #2:&#x000a0;Partly</p><p>**********</p><p><!-- <font color="black"> -->2. Has the statistical analysis been performed appropriately and rigorously? <!-- </font> --></p><p>Reviewer #1:&#x000a0;Yes</p><p>Reviewer #2:&#x000a0;No</p><p>**********</p><p><!-- <font color="black"> -->3. Have the authors made all data underlying the findings in their manuscript fully available?</p><p>The <ext-link xlink:href="http://www.plosone.org/static/policies.action#sharing" ext-link-type="uri">PLOS Data policy</ext-link> requires authors to make all data underlying the findings described in their manuscript fully available without restriction, with rare exception (please refer to the Data Availability Statement in the manuscript PDF file). The data should be provided as part of the manuscript or its supporting information, or deposited to a public repository. For example, in addition to summary statistics, the data points behind means, medians and variance measures should be available. If there are restrictions on publicly sharing data&#x02014;e.g. participant privacy or use of data from a third party&#x02014;those must be specified.<!-- </font> --></p><p>Reviewer #1:&#x000a0;Yes</p><p>Reviewer #2:&#x000a0;Yes</p><p>**********</p><p><!-- <font color="black"> -->4. Is the manuscript presented in an intelligible fashion and written in standard English?</p><p>PLOS ONE does not copyedit accepted manuscripts, so the language in submitted articles must be clear, correct, and unambiguous. Any typographical or grammatical errors should be corrected at revision, so please note any specific errors here.<!-- </font> --></p><p>Reviewer #1:&#x000a0;Yes</p><p>Reviewer #2:&#x000a0;Yes</p><p>**********</p><p><!-- <font color="black"> -->5. Review Comments to the Author</p><p>Please use the space provided to explain your answers to the questions above. You may also include additional comments for the author, including concerns about dual publication, research ethics, or publication ethics. (Please upload your review as an attachment if it exceeds 20,000 characters)<!-- </font> --></p><p>Reviewer #1:&#x000a0;In the manuscript "Subdiffusive Semantic Evolution in Major Indo-European Languages," the authors investigate how words change their semantic meaning (diachronism). To do so, they create a vectorial space with an Euclidian 'semantic' measure of distance. The authors use this tool to study how the meanings of words change in five languages. The analysis is based on the estimation of the anomalous diffusion exponents. The main result is that word meanings do not randomly change (usual diffusion) but exhibit subdiffusion. The authors also use randomization methods to gain insight into the possible subdiffusive mechanisms.</p><p>The introduction presents the state-of-the-art of semantic change research. This section effectively argues the author's proposed method's significance compared to previous works. Moreover, the explanations about anomalous diffusion features are good, mainly describing the possible deviations from usual diffusion.</p><p>Construct trajectories in a Euclidian-like space facilitate the interpretation of the diffusive behaviors. When the 'distance' measures are topological, comparing the results with diffusion in physical systems is more challenging. Thus, the author's method provides a welcome framework for those with some anomalous diffusion background.</p><p>In general, the motivations, methods, and arguments are well explained. The results are essential for the area of anomalous diffusion research. They are related to sociocultural systems and highlight the general application of the anomalous diffusion concept.</p><p>Minor</p><p>1)The English must be revised. Some typos (lenght, patters, pyton, psycholiguistic) and other minor grammar issues exist.</p><p>2)Apparently, there needs to be an explicit explanation for why considering data from 1950.</p><p>Major (i.e., suggestions)</p><p>1) The authors present the ensemble-average anomalous diffusion exponent and the average anomalous diffusion exponent. Based on that parameters and in the randomization tests, the authors could mention/investigate the ergodic properties of the systems. The ergodicity concept may shed new light on the author's main arguments and provide a technical way to analyze their data.</p><p>2)The discussion section could better explain or explore the distinction between a usual random walk and the subdiffusion. One of the main results is that the meaning of the words does not change randomly (like usual diffusion) but is somehow correlated with its previous meaning. Such a result is a clear example of memory in diffusive systems. For a particle, we borrow the term "memory," but in the present work, the temporal correlations may be related to the memory (cultural memory). In other words (not a pun), people change the use of the words, not randomly, but considering more or less what they remember of the previous meaning.</p><p>3)Moreover, in the works of subdiffusion in biological systems, one of the causes of diffusion is the interactions in the systems (crowding, disorder, geometrical constraints, encounters of particles), and language exists for and because of interactions. The present work has an excellent opportunity to explore such a meaning of memory in anomalous diffusion, which sometimes is impossible to discuss deeply when one considers particles just colliding according to the laws of physics. In summary, the results could bring some insights into interpreting the memory concept (in models of anomalous diffusion).</p><p>Therefore, a discussion about these features (suggested) is welcome and could enhance the significance of the results in the anomalous diffusion community. The recommendation is accept with minor revisions.</p><p>Reviewer #2:&#x000a0;This manuscript proposes that the semantics of words can be viewed as a subdiffusive process. This is an interesting result and the authors did substantial work to support it, including the comparison to randomized processes. Still, the data analysis pipeline contains multiple non-trivial steps so that the crucial question is in which extent they affect (or drive) the observations of subdiffusion. I believe further analysis is required to support this main claim, as indicated below.</p><p>* Main points:</p><p>1) The plots showing evidence of subdiffusion in Fig. 3a show oscillating trajectories that, at best, show an average subdiffusive behaviour for 1 decade (from 2y to 20y). Even the averaged results for different languages in Fig. 3b show a systematic increase in the slope with time. This raises the question of the extent into which subdiffusion is a good description of the observations and, if so, what are the time scales in which it applies? The current evidence in support of the general applicability of a subdiffusive model and Eq. (1) is very weak and the conclusions would need to better reflect these limitations. Related questions to be clarified include:</p><p>1a) Why are the authors focusing only on the years from 1950? The co-occurence is built based on windows of size 2 only, so that information on the Google n-gram corpus should allow (e.g., taking n=5 in the n-gram data) studies going further into the past as there is abundant data up to 1800. This would allow one to clarify whether the scaling in (1) holds with a similar \\alpha through longer time scales (the essence of an anomalous diffusion approach). At least a clarification on this point would be important.</p><p>1b) The effect of the temporal sliding window in these plots is not clear. The size goes up to 10y and is thus comparable to the range in which a straight line is seen in the figures. Figure S3 shows that the effect of using the window changes significantly the value of the average exponent \\alpha, strongly suggesting that the straight line fit underlying the computation of individual \\alphas is not a good model for the data.</p><p>2) While the randomization provided are important, I recommend the authors to consider randomizations and data analysis performed at the original textual data so that the effect of the whole pipeline can be better evaluated (also in view of point 1. above). In particular, two simple tests of the results would be to reproduce Fig. 3a for:</p><p>2a) Stop words, i.e., maintaining some of the filtering words in step Fig. 2b and evaluate what would be the results for these words. It'd help to better evaluate whether the observed displacement can be indeed associated to semantic drift.</p><p>2b) Shuffling the years of each of the corpora, i.e., randomly attributing dates to each of the corpus. By simply changing the years associated to each corpus and reproducing all steps of the pipeline one should then obtain a good null model to compare the results against.</p><p>* Minor points:</p><p>3) Line 52. I found it puzzling that Ref. [23] is cite here in support of Zipf's law, as it claims that Zipf's law is not valid for words.</p><p>4) Methods section: is there any upper or lower bound on |\\Delta x| or on the vectors |v|? This would be important to evaluate limits on the subdiffusive behaviour.</p><p>5) Lines 189-191: the current wording suggest that Fig. 2f shows that subsample is needed to correct the sample size bias. In reality this is not the case and it remains unclear why equating the sample size is needed and whether Nc=10^7 provides an appropriate choice.</p><p>6) Line 363: could -&#x0003e; cloud.</p><p>7) Line 416-420: adding citations would be important to point the reader to possible mechanisms.</p><p>8) Line 438: The sentence is misleading as data is not available in the github repository. A link to Google n-gram should be added as well.</p><p>**********</p><p><!-- <font color="black"> -->6. PLOS authors have the option to publish the peer review history of their article (<ext-link xlink:href="https://journals.plos.org/plosone/s/editorial-and-peer-review-process#loc-peer-review-history" ext-link-type="uri">what does this mean?</ext-link>). If published, this will include your full peer review and any attached files.</p><p>If you choose &#x0201c;no&#x0201d;, your identity will remain anonymous but your review may still be made public.</p><p><bold>Do you want your identity to be public for this peer review?</bold> For information about this choice, including consent withdrawal, please see our <ext-link xlink:href="https://www.plos.org/privacy-policy" ext-link-type="uri">Privacy Policy</ext-link>.<!-- </font> --></p><p>Reviewer #1:&#x000a0;<bold>Yes:&#x000a0;</bold>Angel Akio Tateishi</p><p>Reviewer #2:&#x000a0;No</p><p>**********</p><p>[NOTE: If reviewer comments were submitted as an attachment file, they will be attached to this email and accessible via the submission site. Please log into your account, locate the manuscript record, and check for the action link "View Attachments". If this link does not appear, there are no attachment files.]</p><p>While revising your submission, please upload your figure files to the Preflight Analysis and Conversion Engine (PACE) digital diagnostic tool,&#x000a0;<ext-link xlink:href="https://pacev2.apexcovantage.com/" ext-link-type="uri">https://pacev2.apexcovantage.com/</ext-link>. PACE helps ensure that figures meet PLOS requirements. To use PACE, you must first register as a user. Registration is free. Then, login and navigate to the UPLOAD tab, where you will find detailed instructions on how to use the tool. If you encounter any issues or have any questions when using PACE, please email PLOS at&#x000a0;<email>figures@plos.org</email>. Please note that Supporting Information files do not need this step.</p></body></sub-article><sub-article article-type="author-comment" id="pone.0298650.r002"><front-stub><article-id pub-id-type="doi">10.1371/journal.pone.0298650.r002</article-id><title-group><article-title>Author response to Decision Letter 0</article-title></title-group><related-article ext-link-type="doi" xlink:href="10.1371/journal.pone.0298650" id="rel-obj002" related-article-type="editor-report"/><custom-meta-group><custom-meta><meta-name>Submission Version</meta-name><meta-value>1</meta-value></custom-meta></custom-meta-group></front-stub><body><p>
<named-content content-type="author-response-date">27 Oct 2023</named-content>
</p><p>D &#x000b4;aniel Cz &#x000b4;egel</p><p>Beyond Center for Fundamental Concepts in Science Arizona State University</p><p>Tempe, AZ 85287, USA</p><p>October 19, 2023</p><p>Academic Editor, PLOS ONE</p><p>Dear Haroldo V. Ribeiro,</p><p>thank you very much for your e-mail message of 8th of August 2023, related to the review of our manuscript with Submission ID PONE-D-23-19326 en- titled &#x0201d;Subdiffusive semantic evolution in major Indo-European languages&#x0201d; by Bogd &#x000b4;an Asztalos, Gergely Palla and D &#x000b4;aniel Cz &#x000b4;egel including two mainly positive reviews, suggesting the revision of our paper. Although the referees acknowledged the merits of our research, they have also raised fair criticism regarding certain aspects of our manuscript.</p><p>In addition to minor comments and adjustments, Reviewer 1 mainly raised conceptual and interpretive questions, while Reviewer 2 asked us to perform further analysis to support the main claims of the paper. Consequently, the main changes in our resubmitted manuscript are the following:</p><p>1. We have added more new sections to the Supporting information in which we place our argument in favour of the subdiffusion on a more stable basis.</p><p>2. We have explained the arbitrary methodological choices in more detail.</p><p>3. We have expanded the discussion trying to emphasise the relevance of subdiffusion, the conclusions that can be drawn from it, and high- lighting the analogies to natural sciences more explicitly.</p><p>4. During revision we noticed that the code calculating the standard de- viance for the exponents in S5 Fig (Figure S3 in the previous version) unfortunately took the square root of the variance twice, resulting larger deviance values compared to the true value. This is now cor- rected.</p><p>5. Adapting to the journal requirements, we have changed the numbering</p><p>Beyond Center for Fundamental Concepts in Science, Arizona State University</p><p>and the caption format of the figures in the Supporting information, added these captions to the end of the main paper and removed the founders from the Acknowledgments.</p><p>We thank all Referees for their outstanding work and the very valuable com- ments, making the revised version of the paper significantly better, which we hope is now suitable for publication. Our point-by-point response to the editorial board comments and the remarks by the Referees (reproduced in italics) are given below.</p><p>Response to Reviewer 1: Minor</p><p>1)The English must be revised. Some typos (lenght, patters, pyton, psy- choliguistic) and other minor grammar issues exist.</p><p>We have run a language checker on the entire manuscript to correct all the typos and minor grammar issues.</p><p>2)Apparently, there needs to be an explicit explanation for why considering data from 1950.</p><p>We focused on years after 1950 because taking earlier years into account would have decreased drastically the size of the vocabulary. We have added a few sentences to explain this to the section Temporal grouping in Methods.</p><p>Major (i.e., suggestions)</p><p>1) The authors present the ensemble-average anomalous diffusion exponent and the average anomalous diffusion exponent. Based on that parameters and in the randomization tests, the authors could mention/investigate the ergodic properties of the systems. The ergodicity concept may shed new light on the author&#x02019;s main arguments and provide a technical way to analyze their data.</p><p>Thank you for the suggestion. We have been considering it but in the end de- cided that robust claims about non-ergodicity would need both significantly more theory and also more analyses about the role of noise (the trajecto- ries are noisier than what typically is available in physical and biological systems). Furthermore, since this paper should address an interdisciplinary audience and mainly raise a question (why subdiffusion?), we suggest that the investigation of non-ergodic properties could be the topic of a follow-up study.</p><p>Beyond Center for Fundamental Concepts in Science, Arizona State University</p><p>2)The discussion section could better explain or explore the distinction be- tween a usual random walk and the subdiffusion. One of the main results is that the meaning of the words does not change randomly (like usual diffu- sion) but is somehow correlated with its previous meaning. Such a result is a clear example of memory in diffusive systems. For a particle, we borrow the term &#x0201d;memory,&#x0201d; but in the present work, the temporal correlations may be related to the memory (cultural memory). In other words (not a pun), people change the use of the words, not randomly, but considering more or less what they remember of the previous meaning.</p><p>This is also a very good point. One of the main issues is that autocorrelation of word trajectories is most likely a result of both collective phenomena and individual memory. For example, in the revised manuscript, we analyze the meaning change of stopwords and find that their average anomalous diffusion exponent is not significantly different than that of all words. This indicates that subdiffusion is primarily a collective phenomenon. As with the previous recommendation, this is a very important topic that deserves an in-depth analysis and is outside the scope of the current paper.</p><p>In order to clarify the scope of the current paper, we have expanded the last few paragraphs of Discussion to emphasise the relevance of subdiffusion and the conclusions that can be drawn from it.</p><p>3)Moreover, in the works of subdiffusion in biological systems, one of the causes of diffusion is the interactions in the systems (crowding, disorder, geometrical constraints, encounters of particles), and language exists for and because of interactions. The present work has an excellent opportunity to ex- plore such a meaning of memory in anomalous diffusion, which sometimes is impossible to discuss deeply when one considers particles just colliding according to the laws of physics. In summary, the results could bring some insights into interpreting the memory concept (in models of anomalous dif- fusion).</p><p>This was also our vision when we started to work on this subject, and we still hope our work can contribute to the evolution of how people view and interpret memory. However, we do not intend to go directly into this reinterpretation process in detail in this paper, instead just to present our results about subdiffusion that can be used in potential future work. In order to summarize the conclusion about the connection between cognitive memory and the dynamical memory concept in a more unequivocal way, we have rewritten the last paragraph of Discussion highlighting the analogies</p><p>Beyond Center for Fundamental Concepts in Science, Arizona State University</p><p>to natural sciences more explicitly. Response to Reviewer 2:</p><p>* Main points:</p><p>1) The plots showing evidence of subdiffusion in Fig. 3a show oscillating trajectories that, at best, show an average subdiffusive behaviour for 1 decade (from 2y to 20y). Even the averaged results for different languages in Fig. 3b show a systematic increase in the slope with time. This raises the question of the extent into which subdiffusion is a good description of the observations and, if so, what are the time scales in which it applies? The current evidence in support of the general applicability of a subdiffusive model and Eq. (1) is very weak and the conclusions would need to better reflect these limitations.</p><p>The individual trajectories corresponding to concrete words are very noisy indeed, but our scope was to describe their collective behavior through the ensemble average and the mean anomalous diffusion exponents. The quanti- tative analysis of the separate trajectories would not provide the right kind of information, because they can be viewed only in relation to the others. To make this clearer, we added a new paragraph to the subsection Mean versus ensemble average anomalous diffusion exponent in Methods, where we explain why collective phenomena should be studied instead of individ- ual trajectories. (The plotted trajectories in Fig 3a serve only visualization purposes: they illustrate what the blue curve is the average of, and to show that we can find certain words that change faster than others.)</p><p>To quantify how good model the subdiffusion is, we have calculated the R2 for each time dependence fit where it made sense (see Fig 3c, S3 Fig, and S7 Fig) and in the cases based on real data were very close to 1. Although there are indeed changes in the slope on the studied time scale these do not significantly degrade the subdiffusive model. Collecting data from longer time intervals may give more accurate results, but it cannot be increased in magnitudes even with an adequate amount of data, because the displacement cannot be arbitrarily large (see the answer for point 4) and will converge to 2 &#x000b7; (cloud radius)2 as the random walk curve does in Fig 3b. The anomalous diffusion exponent needs to be measured while this convergence is negligible i.e., just for a few decades. To study this phenomenon on a longer time scale, one must use a different methodology.</p><p>Related questions to be clarified include:</p><p>1a) Why are the authors focusing only on the years from 1950? The co-</p><p>Beyond Center for Fundamental Concepts in Science, Arizona State University</p><p>occurence is built based on windows of size 2 only, so that information on the Google n-gram corpus should allow (e.g., taking n = 5 in the n-gram data) studies going further into the past as there is abundant data up to 1800. This would allow one to clarify whether the scaling in (1) holds with a similar &#x003b1; through longer time scales (the essence of an anomalous diffusion approach). At least a clarification on this point would be important.</p><p>We focused on years after 1950 because taking earlier years into account would have decreased drastically the size of the vocabulary. We have added a few sentences to explain this to the section Temporal grouping in Methods.</p><p>1b) The effect of the temporal sliding window in these plots is not clear. The size goes up to 10y and is thus comparable to the range in which a straight line is seen in the figures. Figure S3 shows that the effect of using the window changes significantly the value of the average exponent &#x003b1;, strongly suggesting that the straight line fit underlying the computation of individual &#x003b1;s is not a good model for the data.</p><p>The exact value of the measured anomalous diffusion exponent depends on the size of the sliding window, but the subdiffusive behavior appears in the cases of shorter window sizes too, where it is not comparable to the range of the straight line (see the left column of S6 Fig). The main result of our paper is intended to be that the semantic evolution is subdiffusive, but since we had to make a lot of arbitrary parameter choices during the steps of the applied methodology, it is not surprising that the precise value of the measured exponent depends on these parameters. However, we ar- gue throughout most of the Supporting information that the subdiffusive behavior itself is a feature of the language, not the pipeline.</p><p>2) While the randomization provided are important, I recommend the authors to consider randomizations and data analysis performed at the original tex- tual data so that the effect of the whole pipeline can be better evaluated (also in view of point 1. above). In particular, two simple tests of the results would be to reproduce Fig. 3a for:</p><p>2a) Stop words, i.e., maintaining some of the filtering words in step Fig. 2b and evaluate what would be the results for these words. It&#x02019;d help to better evaluate whether the observed displacement can be indeed associated to semantic drift.</p><p>2b) Shuffling the years of each of the corpora, i.e., randomly attributing dates to each of the corpus. By simply changing the years associated to each</p><p>Beyond Center for Fundamental Concepts in Science, Arizona State University</p><p>corpus and reproducing all steps of the pipeline one should then obtain a good null model to compare the results against.</p><p>We have examined suggestions 2a) and 2b) and have included their result into the Supporting information (see sections S3, S6, and figures S2, and S5-S7). We have found no such result that suggests that we should dismiss the subdiffusive description of semantic evolution, and still think that Eq (1) is a good model for the observed data.</p><p>* Minor points:</p><p>3) Line 52. I found it puzzling that Ref. [23] is cite here in support of Zipf&#x02019;s law, as it claims that Zipf&#x02019;s law is not valid for words.</p><p>The sentence in line 52 states that Zipf&#x02019;s law causes a systematic bias due to the unequal distribution that can be found in language. Ref. [23] refor- mulates the statement of the original Zipf&#x02019;s law (formulated in Ref. [22]) from words to phrases. It does not invalidate the existence of Zipf&#x02019;s law nor the bias caused by it, just gives a more accurate form of it.</p><p>4) Methods section: is there any upper or lower bound on |&#x02206;x| or on the vectors |v|? This would be important to evaluate limits on the subdiffusive behaviour.</p><p>In theory, an individual word can go arbitrarily far from its initial position,</p><p>but the size of the whole word cloud, defined in Eq (11), is fixed by the</p><p>constant sample size Nc. This means that the number of words that sig-</p><p>nificantly moves away from the cloud is strictly limited, and our experience</p><p>is also that the length of individual displacement vectors are typically not</p><p>much larger than a few times of the cloud size, and their mean square would</p><p> converge to</p><p>&#x0221a;</p><p>2-times the cloud size after long time.</p><p>5) Lines 189-191: the current wording suggest that Fig. 2f shows that sub- sample is needed to correct the sample size bias. In reality this is not the case and it remains unclear why equating the sample size is needed and whether Nc = 107 provides an appropriate choice.</p><p>We have added a new section to the Supporting information where we ex- amine the effects of the arbitrary sample size and show that the sample size bias causes problems that should be corrected. We have also added another sentence after the mentioned part in the paper referring to this amendment and briefly explaining why this arbitrary sampling is needed.</p><p>Beyond Center for Fundamental Concepts in Science, Arizona State University</p><p>6) Line 363: could &#x02192; cloud.</p><p>We have reread the text to correct all similar typos.</p><p>7) Line 416-420: adding citations would be important to point the reader to possible mechanisms.</p><p>We have added citations to Refs. [27-30] who were cited already in Intro- duction.</p><p>8) Line 438: The sentence is misleading as data is not available in the github repository. A link to Google n-gram should be added as well.</p><p>We have changed the sentence, now we refer to both Google Ngrams database and our Github repository.</p><p>Yours Sincerely,</p><p>D &#x000b4;aniel Cz &#x000b4;egel</p><supplementary-material id="pone.0298650.s002" position="float" content-type="local-data"><label>Attachment</label><caption><p>Submitted filename: <named-content content-type="submitted-filename">response_letter.pdf</named-content></p></caption><media xlink:href="pone.0298650.s002.pdf"/></supplementary-material></body></sub-article><sub-article article-type="aggregated-review-documents" id="pone.0298650.r003" specific-use="decision-letter"><front-stub><article-id pub-id-type="doi">10.1371/journal.pone.0298650.r003</article-id><title-group><article-title>Decision Letter 1</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Ribeiro</surname><given-names>Haroldo V.</given-names></name><role>Academic Editor</role></contrib></contrib-group><permissions><copyright-statement>&#x000a9; 2024 Haroldo V. Ribeiro</copyright-statement><copyright-year>2024</copyright-year><copyright-holder>Haroldo V. Ribeiro</copyright-holder><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><related-article ext-link-type="doi" xlink:href="10.1371/journal.pone.0298650" id="rel-obj003" related-article-type="reviewed-article"/><custom-meta-group><custom-meta><meta-name>Submission Version</meta-name><meta-value>1</meta-value></custom-meta></custom-meta-group></front-stub><body><p>
<named-content content-type="letter-date">4 Dec 2023</named-content>
</p><p><!-- <div> -->PONE-D-23-19326R1<!-- </div> --><!-- <div> -->Subdiffusive semantic evolution in major Indo-European languages<!-- </div> --><!-- <div> -->PLOS ONE</p><p>Dear Dr. Cz&#x000e9;gel,</p><p>Thank you for submitting your manuscript to PLOS ONE. After careful consideration, we feel that it has merit but does not fully meet PLOS ONE&#x02019;s publication criteria as it currently stands. Therefore, we invite you to submit a revised version of the manuscript that addresses the points raised during the review process.</p><p>The same two previous reviewers have now reviewed the revised version of your manuscript. You will see that one of them has recommended the acceptance of your manuscript despite your avoidance of tracking the ergodicity problem. The other reviewer has, however, recommended rejection. In my opinion, his/her main argument is related to the comparison with the null models, especially regarding the results of SFig. 6, where your results indicate the existence of subdiffusion even after shuffling all time labels of the texts. Indeed, your results indicate an even more subdiffusive regime after shuffling the time label. This is a critical point for your study that needs to be seriously considered and better clarified to proceed further with the publication of your manuscript. It is not clear to me how you can possibly address this issue, but given your discussion in comparison with the usual diffusion, I am sure you also believe that this is a substantial limitation of your findings.</p><p>Please submit your revised manuscript by Jan 18 2024 11:59PM. If you will need more time than this to complete your revisions, please reply to this message or contact the journal office at&#x000a0;<email>plosone@plos.org</email>. When you're ready to submit your revision, log on to <ext-link xlink:href="https://www.editorialmanager.com/pone/" ext-link-type="uri">https://www.editorialmanager.com/pone/</ext-link> and select the 'Submissions Needing Revision' folder to locate your manuscript file.</p><p>Please include the following items when submitting your revised manuscript:<!-- </div> --><list list-type="bullet"><list-item><p>A rebuttal letter that responds to each point raised by the academic editor and reviewer(s). You should upload this letter as a separate file labeled 'Response to Reviewers'.</p></list-item><list-item><p>A marked-up copy of your manuscript that highlights changes made to the original version. You should upload this as a separate file labeled 'Revised Manuscript with Track Changes'.</p></list-item><list-item><p>An unmarked version of your revised paper without tracked changes. You should upload this as a separate file labeled 'Manuscript'.</p></list-item></list></p><p>If you would like to make changes to your financial disclosure, please include your updated statement in your cover letter. Guidelines for resubmitting your figure files are available below the reviewer comments at the end of this letter.</p><p>If applicable, we recommend that you deposit your laboratory protocols in protocols.io to enhance the reproducibility of your results. Protocols.io assigns your protocol its own identifier (DOI) so that it can be cited independently in the future. For instructions see: <ext-link xlink:href="https://journals.plos.org/plosone/s/submission-guidelines#loc-laboratory-protocols" ext-link-type="uri">https://journals.plos.org/plosone/s/submission-guidelines#loc-laboratory-protocols</ext-link>. Additionally, PLOS ONE offers an option for publishing peer-reviewed Lab Protocol articles, which describe protocols hosted on protocols.io. Read more information on sharing protocols at <ext-link xlink:href="https://plos.org/protocols?utm_medium=editorial-email&#x00026;utm_source=authorletters&#x00026;utm_campaign=protocols" ext-link-type="uri">https://plos.org/protocols?utm_medium=editorial-email&#x00026;utm_source=authorletters&#x00026;utm_campaign=protocols</ext-link>.</p><p>We look forward to receiving your revised manuscript.</p><p>Kind regards,</p><p>Haroldo V. Ribeiro</p><p>Academic Editor</p><p>PLOS ONE</p><p>[Note: HTML markup is below. Please do not edit.]</p><p>Reviewers' comments:</p><p>Reviewer's Responses to Questions</p><p>
<!-- <font color="black"> -->
<bold>Comments to the Author</bold>
</p><p>1. If the authors have adequately addressed your comments raised in a previous round of review and you feel that this manuscript is now acceptable for publication, you may indicate that here to bypass the &#x0201c;Comments to the Author&#x0201d; section, enter your conflict of interest statement in the &#x0201c;Confidential to Editor&#x0201d; section, and submit your "Accept" recommendation.<!-- </font> --></p><p>Reviewer #1:&#x000a0;All comments have been addressed</p><p>Reviewer #2:&#x000a0;(No Response)</p><p>**********</p><p><!-- <font color="black"> -->2. Is the manuscript technically sound, and do the data support the conclusions?</p><p>The manuscript must describe a technically sound piece of scientific research with data that supports the conclusions. Experiments must have been conducted rigorously, with appropriate controls, replication, and sample sizes. The conclusions must be drawn appropriately based on the data presented. <!-- </font> --></p><p>Reviewer #1:&#x000a0;Yes</p><p>Reviewer #2:&#x000a0;No</p><p>**********</p><p><!-- <font color="black"> -->3. Has the statistical analysis been performed appropriately and rigorously? <!-- </font> --></p><p>Reviewer #1:&#x000a0;Yes</p><p>Reviewer #2:&#x000a0;No</p><p>**********</p><p><!-- <font color="black"> -->4. Have the authors made all data underlying the findings in their manuscript fully available?</p><p>The <ext-link xlink:href="http://www.plosone.org/static/policies.action#sharing" ext-link-type="uri">PLOS Data policy</ext-link> requires authors to make all data underlying the findings described in their manuscript fully available without restriction, with rare exception (please refer to the Data Availability Statement in the manuscript PDF file). The data should be provided as part of the manuscript or its supporting information, or deposited to a public repository. For example, in addition to summary statistics, the data points behind means, medians and variance measures should be available. If there are restrictions on publicly sharing data&#x02014;e.g. participant privacy or use of data from a third party&#x02014;those must be specified.<!-- </font> --></p><p>Reviewer #1:&#x000a0;Yes</p><p>Reviewer #2:&#x000a0;Yes</p><p>**********</p><p><!-- <font color="black"> -->5. Is the manuscript presented in an intelligible fashion and written in standard English?</p><p>PLOS ONE does not copyedit accepted manuscripts, so the language in submitted articles must be clear, correct, and unambiguous. Any typographical or grammatical errors should be corrected at revision, so please note any specific errors here.<!-- </font> --></p><p>Reviewer #1:&#x000a0;Yes</p><p>Reviewer #2:&#x000a0;Yes</p><p>**********</p><p><!-- <font color="black"> -->6. Review Comments to the Author</p><p>Please use the space provided to explain your answers to the questions above. You may also include additional comments for the author, including concerns about dual publication, research ethics, or publication ethics. (Please upload your review as an attachment if it exceeds 20,000 characters)<!-- </font> --></p><p>Reviewer #1:&#x000a0;Although the authors did not accept the challenge of dealing with ergodic and memory mechanisms in semantic subdiffusive evolution, the paper has increased its overall quality after the revision. Based on the comments of the first revision and the author's replies, the recommendation is to accept. In summary, the work applies the concept of anomalous diffusion to social systems, showing the versatility of the tools of statistical mechanics. However, if the mechanisms of subdiffusion had been approached, the relevance of the work would be even higher.</p><p>Reviewer #2:&#x000a0;This manuscript claims that the process of evolution of the meaning of words is subdiffusive. The data analysis shows that (i) the characterization of subdiffusion is very limited and unreliable; and (ii) other effects not related to the semantic evolution in time lead essentially to the same observation (e.g., stop words or randomized temporal order lead to the same type of subdiffusion). Therefore, even if the manuscript contains detailed and through data analysis, I do not recommend publication in PLOS ONE because the claims are not supported by the analysis.</p><p>(i) The signature of anomalous diffusion is a non-linear growth of the mean-squared displacement over a variety of time scales. This manuscript estimates subdiffusion in only one order of magnitude and finds that that value of the exponent depends on the window size (against expectation from simple sudiffusive processes). Their choice for stopping the analysis at the year 1950 is not consistent. First, they argue that they can't go to larger time scales because they need around 10,000 words for the analysis, but at the same time the results in Fig. 1 show that the main observations are visible even for single words. Moreover, the choice of 10,000 words is justified by the vocabulary of a native speaker, a connection that seems irrelevant to this context.</p><p>(ii) The authors find subdiffusion in stop words (Figure S2), a case in which the "semantic evolution" is unclear or expected to behave differently. Subdiffusion is also found after shuffling all time labels of the texts (Figure S6). The authors then argue that the value of the exponent is different from their observations and that therefore their conclusions can remain unchanged. However, the significance of the exact value of the exponent is not relevant to their argument (and it is not reliable, as argue in point i above). More importantly, the conclusion about the subdiffusive nature of the semantic evolution would apply equally well for smaller alpha (in fact, it is even more sudiffusive?). In the abstract, the authors make a contrast to diffusing particles, but it is clear from their analysis that a normal diffusion is not what is observed in many null-models.</p><p>The detailed and careful data analysis present in this manuscript is of potential interest and should be published, but the presentation and interpretation given to them in this manuscript is not consistent with the data analysis reported by the authors.</p><p>**********</p><p><!-- <font color="black"> -->7. PLOS authors have the option to publish the peer review history of their article (<ext-link xlink:href="https://journals.plos.org/plosone/s/editorial-and-peer-review-process#loc-peer-review-history" ext-link-type="uri">what does this mean?</ext-link>). If published, this will include your full peer review and any attached files.</p><p>If you choose &#x0201c;no&#x0201d;, your identity will remain anonymous but your review may still be made public.</p><p><bold>Do you want your identity to be public for this peer review?</bold> For information about this choice, including consent withdrawal, please see our <ext-link xlink:href="https://www.plos.org/privacy-policy" ext-link-type="uri">Privacy Policy</ext-link>.<!-- </font> --></p><p>Reviewer #1:&#x000a0;No</p><p>Reviewer #2:&#x000a0;No</p><p>**********</p><p>[NOTE: If reviewer comments were submitted as an attachment file, they will be attached to this email and accessible via the submission site. Please log into your account, locate the manuscript record, and check for the action link "View Attachments". If this link does not appear, there are no attachment files.]</p><p>While revising your submission, please upload your figure files to the Preflight Analysis and Conversion Engine (PACE) digital diagnostic tool,&#x000a0;<ext-link xlink:href="https://pacev2.apexcovantage.com/" ext-link-type="uri">https://pacev2.apexcovantage.com/</ext-link>. PACE helps ensure that figures meet PLOS requirements. To use PACE, you must first register as a user. Registration is free. Then, login and navigate to the UPLOAD tab, where you will find detailed instructions on how to use the tool. If you encounter any issues or have any questions when using PACE, please email PLOS at&#x000a0;<email>figures@plos.org</email>. Please note that Supporting Information files do not need this step.</p></body></sub-article><sub-article article-type="author-comment" id="pone.0298650.r004"><front-stub><article-id pub-id-type="doi">10.1371/journal.pone.0298650.r004</article-id><title-group><article-title>Author response to Decision Letter 1</article-title></title-group><related-article ext-link-type="doi" xlink:href="10.1371/journal.pone.0298650" id="rel-obj004" related-article-type="editor-report"/><custom-meta-group><custom-meta><meta-name>Submission Version</meta-name><meta-value>2</meta-value></custom-meta></custom-meta-group></front-stub><body><p>
<named-content content-type="author-response-date">22 Jan 2024</named-content>
</p><p>Beyond Center for Fundamental Concepts in Science, Arizona State University</p><p> D &#x000b4;aniel Cz &#x000b4;egel</p><p>Beyond Center for Fundamental Concepts in Science Arizona State University</p><p>Tempe, AZ 85287, USA</p><p>January 22, 2024</p><p>Academic Editor, PLOS ONE</p><p>Dear Haroldo V. Ribeiro,</p><p>thank you very much for your e-mail message of 4th of December 2023, related to the review of our manuscript with Submission ID PONE-D-23- 19326R1 entitled &#x0201d;Subdiffusive semantic evolution in major Indo-European languages&#x0201d; by Bogd &#x000b4;an Asztalos, Gergely Palla and D &#x000b4;aniel Cz &#x000b4;egel, and we appreciate your decision to invite us to submit a revision of the manuscript, even though one of the reviewers recommended rejection.</p><p>Reviewer 2 raised some important questions which revealed that our ar- gument had logical gaps that we should explain in more detail, especially regarding limitations of our methodology and the clarity of the null model. Consequently, the main changes in our resubmitted manuscript are the fol- lowing:</p><p>1. We changed the title to &#x0201d;Anomalous diffusion analysis of semantic evo- lution in major Indo-European languages&#x0201d; to emphasize the method- ology employed rather than the specific outcomes obtained.</p><p>2. We have added a new paragraph before the end of Discussion dedi- cated to the summary of these limitations and removed overly direct statements about semantic evolution being subdiffusive.</p><p>3. We have expanded the explanation of the purpose and results of shuf- fled time labels in section S6.</p><p>We thank all Referees for their outstanding work and the very valuable com- ments, making the revised version of the paper significantly better, which we hope is now suitable for publication. Our point-by-point response to the editorial board comments and the remarks by the Referees (reproduced in italics) are given below.</p><p>Beyond Center for Fundamental Concepts in Science, Arizona State University</p><p>Response to the comment by the Editorial Board Member:</p><p>The same two previous reviewers have now reviewed the revised version of your manuscript. You will see that one of them has recommended the accep- tance of your manuscript despite your avoidance of tracking the ergodicity problem. The other reviewer has, however, recommended rejection. In my opinion, his/her main argument is related to the comparison with the null models, especially regarding the results of SFig. 6, where your results indi- cate the existence of subdiffusion even after shuffling all time labels of the texts. Indeed, your results indicate an even more subdiffusive regime after shuffling the time label. This is a critical point for your study that needs to be seriously considered and better clarified to proceed further with the publication of your manuscript. It is not clear to me how you can possibly address this issue, but given your discussion in comparison with the usual diffusion, I am sure you also believe that this is a substantial limitation of your findings.</p><p>We sincerely appreciate the detailed feedback in improving the manuscript. We have carefully considered your suggestions and have made significant revisions to the manuscript. As detailed in our response Reviewer 2, we consider diffusion as the theoretical null-model in our work, and our main claim is that semantic change deviates from it. Nevertheless, due to method- ological reasons, a measurable lower bound of &#x027e8;&#x003b1;&#x027e9; also has to be investigated, and this is what we do in section S6. Hence, the results of Fig S6 and Fig S8 does not indicate a &#x0201d;stronger subdiffusion&#x0201d;, but a baseline marked out by temporally uncorrelated time series. In order to make this argument clear, we have expanded the text of the manuscript, and section S6 in the Supporting information.</p><p>In sum, whenever semantic embedding methods are used in a diachronic context, alignment of embeddings is necessary. This introduces a similarity bias between subsequent timesteps that makes even totally independently sampled data dependent. In terms of the anomalous diffusion exponent, it introduces a non-zero &#x0201d;baseline&#x0201d; anomalous diffusion exponent that sets the lower bound for any semantically embedded and aligned time series. We observe a significant deviation from that baseline in the non-randomized (real) semantic trajectories in mean anomalous diffusion exponents as well as in the fluctuations of the trajectories.</p><p>We agree with Reviewer 2 and the Editor that our conclusions were too strong. We changed the title to &#x0201d;Anomalous diffusion analysis of seman-</p><p>Beyond Center for Fundamental Concepts in Science, Arizona State University</p><p>tic evolution in major Indo-European languages&#x0201d;. We also removed overly direct statements about semantic evolution being subdiffusive whenever it appeared without further explanation/context.</p><p>We also added a paragraph about limitations of this study, as we believe that the pipeline and its limitations are at least as informative of future work as the subdiffusive result itself.</p><p>Response to Reviewer 1:</p><p>Although the authors did not accept the challenge of dealing with ergodic and memory mechanisms in semantic subdiffusive evolution, the paper has increased its overall quality after the revision. Based on the comments of the first revision and the author&#x02019;s replies, the recommendation is to accept. In summary, the work applies the concept of anomalous diffusion to social systems, showing the versatility of the tools of statistical mechanics. How- ever, if the mechanisms of subdiffusion had been approached, the relevance of the work would be even higher.</p><p>Thank you for the feedback and the suggestions given in the previous re- vision. We believe these significantly helped to present our narrative in a scientifically more relevant way.</p><p>Response to Reviewer 2:</p><p>This manuscript claims that the process of evolution of the meaning of words is subdiffusive. The data analysis shows that (i) the characterization of sub- diffusion is very limited and unreliable; and (ii) other effects not related to the semantic evolution in time lead essentially to the same observation (e.g., stop words or randomized temporal order lead to the same type of subdiffu- sion). Therefore, even if the manuscript contains detailed and through data analysis, I do not recommend publication in PLOS ONE because the claims are not supported by the analysis.</p><p>Thank you for the thorough comments. We can see that a part of the claims in the previous version of our manuscripts did not adjust to the limitations of the characterization of subdiffusion and that our explanation needed some more details in a few places to be clear. Consequently, we have expanded the text with some more detailed explanation, and changed the title of the manuscript, to give our results a more accurate narrative.</p><p>(i) The signature of anomalous diffusion is a non-linear growth of the mean- squared displacement over a variety of time scales. This manuscript esti-</p><p>Beyond Center for Fundamental Concepts in Science, Arizona State University</p><p>mates subdiffusion in only one order of magnitude and finds that that value of the exponent depends on the window size (against expectation from sim- ple subdiffusive processes). Their choice for stopping the analysis at the year 1950 is not consistent. First, they argue that they can&#x02019;t go to larger time scales because they need around 10,000 words for the analysis, but at the same time the results in Fig. 1 show that the main observations are visible even for single words. Moreover, the choice of 10,000 words is justified by the vocabulary of a native speaker, a connection that seems irrelevant to this context.</p><p>We acknowledge that our results have their own limitations, so to facilitate comprehension for the reader, we have added a new paragraph before the end of Discussion dedicated to the summary of these limitations. (Specifi- cally, we have highlighted three primary factors: the unavoidable bias of the alignment, the lack of interpretability of individual word trajectories, and the length of the analyzed time period.) Also, we have rephrased some sen- tences at different points of the manuscripts which had suggested stronger conclusions than these limitations allow us to draw. Furthermore, we have changed the title of the manuscript to emphasize the methodology employed rather than the specific outcomes obtained.</p><p>The window size dependence of the anomalous diffusion exponent looks un- usual, but it can be attributed to the fact that we apply temporal grouping on data (panels d and e in Fig 2) before the embedding step (panel g in Fig 2), which is a non-linear operation. More specifically, word embedding is not even a data processing operation but the conversion of abstract linguis- tic information to a quantitative representation. This inevitably causes the window size to have a hard-to-investigate effect on the value of &#x000a1;alpha&#x000bf;, but as we point out, subdiffusive behavior remains, indicating that conditions i)-iv) are violated on a fundamental, linguistic level. Quantitative compari- son with simple subdiffusion models do not tell us much about our results, because these models (e.g. fractional Brownian motion or continuous-time random walk) are defined directly in geometric spaces, so such conversion is not needed, and simple linear temporal grouping would not affect the expo- nent in their case. (This is why we did not even detail it in our manuscript).</p><p>Our argument about the choice of 10,000 words in the previous version was a bit vague indeed, so we have rewritten the concerning paragraph in Temporal grouping in Methods. The objective was to ensure that the embedding model learns at least about as many words as a native speaker knows, because the constructed vector representation of an individual word</p><p>Beyond Center for Fundamental Concepts in Science, Arizona State University</p><p>makes sense only compared to the others. The observable results of single words shown in Fig. 1 are also interpretable only with a static background of other words. Also, Word2vec is typically used with vocabularies of at least tens of thousands of words (as we point out in line 38), and so its working process and hyperparameters are optimized for this amount of data.</p><p>(ii) The authors find subdiffusion in stop words (Figure S2), a case in which the &#x0201d;semantic evolution&#x0201d; is unclear or expected to behave differently. Sub- diffusion is also found after shuffling all time labels of the texts (Figure S6). The authors then argue that the value of the exponent is different from their observations and that therefore their conclusions can remain un- changed. However, the significance of the exact value of the exponent is not relevant to their argument (and it is not reliable, as argue in point i above). More importantly, the conclusion about the subdiffusive nature of the seman- tic evolution would apply equally well for smaller alpha (in fact, it is even more sudiffusive?). In the abstract, the authors make a contrast to diffusing particles, but it is clear from their analysis that a normal diffusion is not what is observed in many null-models.</p><p>Stopwords are usually filtered out before NLP investigations because fre- quent grammatical words with no semantic role would bias the statistics of words with real meaning. We argue in section S3 in Supporting Informa- tion that subdiffusion remains in the presence of stopwords, because they adapt to the subdiffusing environment due to the lack of semantic meaning. However, as we mention at the end of the section, one must be careful, be- cause this part of NLP is unclear (as the Reviewer also points out). Hence, we do not believe that stopwords not resolving subdiffusive behavior would invalidate our results.</p><p>In the case of shuffled time labels we expect no temporal correlations be- tween consecutive positions, so we expect &#x027e8;&#x003b1;&#x027e9; = 0 which is increased by the unavoidable alignment step. This non-zero baseline is intended to be determined in Fig S6 and the newly-added Fig S8, so their &#x0201d;small alpha&#x0201d; does not mean a &#x0201d;stronger subdiffusion&#x0201d;, but a lower bound for measurable alpha values. We have expanded the explanation in the first paragraph of section S6 to make this argument more explicit, also, we have added a new paragraph at the end of Results where we resume this comparison and its results.</p><p>Although the above comparison has important consequences from a tech- nical point of view, its main importance is to validate that the observed</p><p>Beyond Center for Fundamental Concepts in Science, Arizona State University</p><p>&#x027e8;&#x003b1;&#x027e9; &#x02248; 0.4 &#x02212; 0.5 is actually above the lower bound of measurable values. Our main claim in the manuscript however is that the analysis of semantic change reveals a significant deviation from diffusion that would be expected in case of any walking phenomenon fulfilling conditions i)-iv) detailed in the paragraph starting in line 80. Hence, diffusion can be considered as a theo- retical null-model in our work, and this is why we focus on it in the paper and highlight it in the abstract.</p><p>The detailed and careful data analysis present in this manuscript is of poten- tial interest and should be published, but the presentation and interpretation given to them in this manuscript is not consistent with the data analysis reported by the authors.</p><p>We hope the latest upgrades of the manuscript, and the above explanations have improved the presentation of the data analysis to be consistent with the claims made in the text.</p><p>Yours Sincerely,</p><p>D &#x000b4;aniel Cz &#x000b4;egel</p><supplementary-material id="pone.0298650.s003" position="float" content-type="local-data"><label>Attachment</label><caption><p>Submitted filename: <named-content content-type="submitted-filename">response_letter_r2.pdf</named-content></p></caption><media xlink:href="pone.0298650.s003.pdf"/></supplementary-material></body></sub-article><sub-article article-type="editor-report" id="pone.0298650.r005" specific-use="decision-letter"><front-stub><article-id pub-id-type="doi">10.1371/journal.pone.0298650.r005</article-id><title-group><article-title>Decision Letter 2</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Ribeiro</surname><given-names>Haroldo V.</given-names></name><role>Academic Editor</role></contrib></contrib-group><permissions><copyright-statement>&#x000a9; 2024 Haroldo V. Ribeiro</copyright-statement><copyright-year>2024</copyright-year><copyright-holder>Haroldo V. Ribeiro</copyright-holder><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><related-article ext-link-type="doi" xlink:href="10.1371/journal.pone.0298650" id="rel-obj005" related-article-type="reviewed-article"/><custom-meta-group><custom-meta><meta-name>Submission Version</meta-name><meta-value>2</meta-value></custom-meta></custom-meta-group></front-stub><body><p>
<named-content content-type="letter-date">30 Jan 2024</named-content>
</p><p>Anomalous diffusion analysis of semantic evolution in major Indo-European languages</p><p>PONE-D-23-19326R2</p><p>Dear Dr. Cz&#x000e9;gel,</p><p>We&#x02019;re pleased to inform you that your manuscript has been judged scientifically suitable for publication and will be formally accepted for publication once it meets all outstanding technical requirements.</p><p>Within one week, you&#x02019;ll receive an e-mail detailing the required amendments. When these have been addressed, you&#x02019;ll receive a formal acceptance letter and your manuscript will be scheduled for publication.</p><p>An invoice for payment will follow shortly after the formal acceptance. To ensure an efficient process, please log into Editorial Manager at <ext-link xlink:href="http://www.editorialmanager.com/pone/" ext-link-type="uri">http://www.editorialmanager.com/pone/</ext-link>, click the 'Update My Information' link at the top of the page, and double check that your user information is up-to-date. If you have any billing related questions, please contact our Author Billing department directly at <email>authorbilling@plos.org</email>.</p><p>If your institution or institutions have a press office, please notify them about your upcoming paper to help maximize its impact. If they&#x02019;ll be preparing press materials, please inform our press team as soon as possible -- no later than 48 hours after receiving the formal acceptance. Your manuscript will remain under strict press embargo until 2 pm Eastern Time on the date of publication. For more information, please contact <email>onepress@plos.org</email>.</p><p>Kind regards,</p><p>Haroldo V. Ribeiro</p><p>Academic Editor</p><p>PLOS ONE</p><p>Additional Editor Comments (optional):</p><p>I want to express my gratitude to the authors for submitting the revised version of their manuscript and for their diligent efforts in addressing the observations made by Reviewer #2. The inclusion of additional analysis in Section S6, along with the concluding paragraph in the results section, adeptly resolves the concerns regarding the presence of subdiffusion with shuffled time labels. Furthermore, the authors have commendably acknowledged the potential limitations and biases inherent in their research, which could potentially inspire further studies. In summary, from my perspective, the authors' innovative approach to conceptualizing semantic evolution as a diffusion problem is remarkable, and their findings will likely captivate a broad audience. Therefore, I do not need further review, and I congratulate the authors for their excellent work.</p></body></sub-article><sub-article article-type="editor-report" id="pone.0298650.r006" specific-use="acceptance-letter"><front-stub><article-id pub-id-type="doi">10.1371/journal.pone.0298650.r006</article-id><title-group><article-title>Acceptance letter</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Ribeiro</surname><given-names>Haroldo V.</given-names></name><role>Academic Editor</role></contrib></contrib-group><permissions><copyright-statement>&#x000a9; 2024 Haroldo V. Ribeiro</copyright-statement><copyright-year>2024</copyright-year><copyright-holder>Haroldo V. Ribeiro</copyright-holder><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><related-article ext-link-type="doi" xlink:href="10.1371/journal.pone.0298650" id="rel-obj006" related-article-type="reviewed-article"/></front-stub><body><p>
<named-content content-type="letter-date">15 Mar 2024</named-content>
</p><p>PONE-D-23-19326R2 </p><p>PLOS ONE</p><p>Dear Dr. Cz&#x000e9;gel, </p><p>I'm pleased to inform you that your manuscript has been deemed suitable for publication in PLOS ONE. Congratulations! Your manuscript is now being handed over to our production team.</p><p>At this stage, our production department will prepare your paper for publication. This includes ensuring the following:</p><p>* All references, tables, and figures are properly cited</p><p>* All relevant supporting information is included in the manuscript submission,</p><p>* There are no issues that prevent the paper from being properly typeset</p><p>If revisions are needed, the production department will contact you directly to resolve them. If no revisions are needed, you will receive an email when the publication date has been set. At this time, we do not offer pre-publication proofs to authors during production of the accepted work. Please keep in mind that we are working through a large volume of accepted articles, so please give us a few weeks to review your paper and let you know the next and final steps. </p><p>Lastly, if your institution or institutions have a press office, please let them know about your upcoming paper now to help maximize its impact. If they'll be preparing press materials, please inform our press team within the next 48 hours. Your manuscript will remain under strict press embargo until 2 pm Eastern Time on the date of publication. For more information, please contact <email>onepress@plos.org</email>.</p><p>If we can help with anything else, please email us at <email>customercare@plos.org</email>.</p><p>Thank you for submitting your work to PLOS ONE and supporting open access. </p><p>Kind regards, </p><p>PLOS ONE Editorial Office Staff</p><p>on behalf of</p><p>Dr. Haroldo V. Ribeiro </p><p>Academic Editor</p><p>PLOS ONE</p></body></sub-article></article>