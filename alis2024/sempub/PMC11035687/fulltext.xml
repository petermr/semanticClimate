<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd"> 
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><?properties open_access?><?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?><?DTDIdentifier.IdentifierType public?><?SourceDTD.DTDName A++V2.4.dtd?><?SourceDTD.Version 2.4?><?ConverterInfo.XSLTName springer2nlmx2.xsl?><?ConverterInfo.Version 1?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Commun Biol</journal-id><journal-id journal-id-type="iso-abbrev">Commun Biol</journal-id><journal-title-group><journal-title>Communications Biology</journal-title></journal-title-group><issn pub-type="epub">2399-3642</issn><publisher><publisher-name>Nature Publishing Group UK</publisher-name><publisher-loc>London</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmcid">11035687</article-id><article-id pub-id-type="publisher-id">6162</article-id><article-id pub-id-type="doi">10.1038/s42003-024-06162-0</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Revealing the mechanisms of semantic satiation with deep learning models</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Zhang</surname><given-names>Xinyu</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author"><name><surname>Lian</surname><given-names>Jing</given-names></name><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author"><name><surname>Yu</surname><given-names>Zhaofei</given-names></name><xref ref-type="aff" rid="Aff3">3</xref><xref ref-type="aff" rid="Aff4">4</xref></contrib><contrib contrib-type="author"><name><surname>Tang</surname><given-names>Huajin</given-names></name><xref ref-type="aff" rid="Aff5">5</xref><xref ref-type="aff" rid="Aff6">6</xref></contrib><contrib contrib-type="author"><name><surname>Liang</surname><given-names>Dong</given-names></name><xref ref-type="aff" rid="Aff7">7</xref></contrib><contrib contrib-type="author" corresp="yes"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-8303-2619</contrib-id><name><surname>Liu</surname><given-names>Jizhao</given-names></name><address><email>liujz@lzu.edu.cn</email></address><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author" corresp="yes"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-5391-7213</contrib-id><name><surname>Liu</surname><given-names>Jian K.</given-names></name><address><email>j.liu.22@bham.ac.uk</email></address><xref ref-type="aff" rid="Aff8">8</xref></contrib><aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/01mkqqe32</institution-id><institution-id institution-id-type="GRID">grid.32566.34</institution-id><institution-id institution-id-type="ISNI">0000 0000 8571 0482</institution-id><institution>School of Information Science and Engineering, </institution><institution>Lanzhou University, </institution></institution-wrap>Lanzhou, 730000 Gansu China </aff><aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/03144pv92</institution-id><institution-id institution-id-type="GRID">grid.411290.f</institution-id><institution-id institution-id-type="ISNI">0000 0000 9533 0029</institution-id><institution>School of Electronics and Information Engineering, </institution><institution>Lanzhou Jiaotong University, </institution></institution-wrap>Lanzhou, 730070 Gansu China </aff><aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/02v51f717</institution-id><institution-id institution-id-type="GRID">grid.11135.37</institution-id><institution-id institution-id-type="ISNI">0000 0001 2256 9319</institution-id><institution>School of Computer Science, </institution><institution>Peking University, Beijing, </institution></institution-wrap>100871 Beijing, China </aff><aff id="Aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/02v51f717</institution-id><institution-id institution-id-type="GRID">grid.11135.37</institution-id><institution-id institution-id-type="ISNI">0000 0001 2256 9319</institution-id><institution>Institute for Artificial Intelligence, </institution><institution>Peking University, Beijing, </institution></institution-wrap>100871 Beijing, China </aff><aff id="Aff5"><label>5</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/00a2xv884</institution-id><institution-id institution-id-type="GRID">grid.13402.34</institution-id><institution-id institution-id-type="ISNI">0000 0004 1759 700X</institution-id><institution>The State Key Lab of Brain-Machine Intelligence, </institution><institution>Zhejiang University, </institution></institution-wrap>Hangzhou, 310027 Zhejiang China </aff><aff id="Aff6"><label>6</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/00a2xv884</institution-id><institution-id institution-id-type="GRID">grid.13402.34</institution-id><institution-id institution-id-type="ISNI">0000 0004 1759 700X</institution-id><institution>The MOE Frontier Science Center for Brain Science and Brain-Machine Integration, </institution><institution>Zhejiang University, </institution></institution-wrap>Hangzhou, 310027 Zhejiang China </aff><aff id="Aff7"><label>7</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/01scyh794</institution-id><institution-id institution-id-type="GRID">grid.64938.30</institution-id><institution-id institution-id-type="ISNI">0000 0000 9558 9911</institution-id><institution>Department of Computer Science and Technology, </institution><institution>Nanjing University of Aeronautics and Astronautics, </institution></institution-wrap>Nanjing, 211106 Jiangsu China </aff><aff id="Aff8"><label>8</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/03angcq70</institution-id><institution-id institution-id-type="GRID">grid.6572.6</institution-id><institution-id institution-id-type="ISNI">0000 0004 1936 7486</institution-id><institution>School of Computer Science, Centre for Human Brain Health, </institution><institution>University of Birmingham, </institution></institution-wrap>Birmingham, B15 2TT UK </aff></contrib-group><pub-date pub-type="epub"><day>22</day><month>4</month><year>2024</year></pub-date><pub-date pub-type="pmc-release"><day>22</day><month>4</month><year>2024</year></pub-date><pub-date pub-type="collection"><year>2024</year></pub-date><volume>7</volume><elocation-id>487</elocation-id><history><date date-type="received"><day>3</day><month>11</month><year>2023</year></date><date date-type="accepted"><day>8</day><month>4</month><year>2024</year></date></history><permissions><copyright-statement>&#x000a9; The Author(s) 2024</copyright-statement><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article&#x02019;s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article&#x02019;s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p></license></permissions><abstract id="Abs1"><p id="Par1">The phenomenon of semantic satiation, which refers to the loss of meaning of a word or phrase after being repeated many times, is a well-known psychological phenomenon. However, the microscopic neural computational principles responsible for these mechanisms remain unknown. In this study, we use a deep learning model of continuous coupled neural networks to investigate the mechanism underlying semantic satiation and precisely describe this process with neuronal components. Our results suggest that, from a mesoscopic perspective, semantic satiation may be a bottom-up process. Unlike existing macroscopic psychological studies that suggest that semantic satiation is a top-down process, our simulations use a similar experimental paradigm as classical psychology experiments and observe similar results. Satiation of semantic objectives, similar to the learning process of our network model used for object recognition, relies on continuous learning and switching between objects. The underlying neural coupling strengthens or weakens satiation. Taken together, both neural and network mechanisms play a role in controlling semantic satiation.</p></abstract><abstract id="Abs2" abstract-type="web-summary"><p id="Par2">This study, utilizing deep learning to model semantic saturation, shows its tie to repetitive visual stimuli processing by the primary visual cortex, suggesting a bottom-up neurocognitive process.</p></abstract><kwd-group kwd-group-type="npg-subject"><title>Subject terms</title><kwd>Perception</kwd><kwd>Neural encoding</kwd><kwd>Visual system</kwd></kwd-group><funding-group><award-group><funding-source><institution-wrap><institution-id institution-id-type="FundRef">501100009620</institution-id><institution>Gansu Science and Technology Department (Science and Technology Department of Gansu Province)</institution></institution-wrap></funding-source><award-id>21JR7RA510</award-id></award-group></funding-group><funding-group><award-group><funding-source><institution-wrap><institution-id institution-id-type="FundRef">100012899</institution-id><institution>Lanzhou University (LZU)</institution></institution-wrap></funding-source></award-group></funding-group><funding-group><award-group><funding-source><institution-wrap><institution-id institution-id-type="FundRef">501100001809</institution-id><institution>National Natural Science Foundation of China (National Science Foundation of China)</institution></institution-wrap></funding-source><award-id>62236007</award-id></award-group></funding-group><funding-group><award-group><funding-source><institution-wrap><institution-id institution-id-type="FundRef">501100001809</institution-id><institution>National Natural Science Foundation of China (National Science Foundation of China)</institution></institution-wrap></funding-source><award-id>62176003</award-id></award-group></funding-group><funding-group><award-group><funding-source><institution-wrap><institution-id institution-id-type="FundRef">501100005090</institution-id><institution>Beijing Nova Program</institution></institution-wrap></funding-source><award-id>20230484362</award-id></award-group></funding-group><funding-group><award-group><funding-source><institution>Natural Science Foundation of Gansu Province (No. 21JR7RA510) Talent Scientific Fund of Lanzhou University Supercomputing Center of Lanzhou University</institution></funding-source></award-group></funding-group><custom-meta-group><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#x000a9; Springer Nature Limited 2024</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1" sec-type="introduction"><title>Introduction</title><p id="Par3">Have you ever engaged in prolonged contemplation of a linguistic entity to the extent that its semantic essence begins to elude you? Consider, for instance, the word &#x0201c;cat&#x0201d; Prolonged and unwavering fixation on this linguistic symbol may evoke an eerie sense of detachment. Abruptly, the very word &#x0201c;cat&#x0201d;, conventionally evoking imagery of endearing domesticated felines, appears to undergo a peculiar transmutation, losing its inherent signification, and even forfeiting its status as a recognizable linguistic construct. Iterative inscriptions or readings of &#x0201c;cat&#x0201d; yield an analogous outcome. The ceaseless recurrence of a single lexical unit or phrase can culminate in its ephemeral deprivation of semantic meaning, an intriguing psychological phenomenon referred to as &#x0201c;semantic satiation&#x0201d;<sup><xref ref-type="bibr" rid="CR1">1</xref>&#x02013;<xref ref-type="bibr" rid="CR6">6</xref></sup>. It is imperative to note that these enigmatic encounters are not restricted solely to the domain of language but extend to encompass non-verbal entities that have been studied in both humans and animals with different experimental protocols and techniques<sup><xref ref-type="bibr" rid="CR7">7</xref>&#x02013;<xref ref-type="bibr" rid="CR15">15</xref></sup>. With the more advanced methods recently developed, this phenomenon has been continuously investigated with newly identified biomarkers<sup><xref ref-type="bibr" rid="CR16">16</xref>&#x02013;<xref ref-type="bibr" rid="CR23">23</xref></sup>.</p><p id="Par4">In the realm of neuroscience, despite the invaluable advantages it offers, research in the field of semantic satiation has predominantly operated at a macro level, thus far constrained in its ability to establish pertinent links between macro-level satiation phenomena and the intricacies of micro-level neural activity<sup><xref ref-type="bibr" rid="CR15">15</xref>,<xref ref-type="bibr" rid="CR24">24</xref></sup>. Neurophysiological experiments, by necessity, entail the examination of neural responses under stimulus conditions during the deep sleep of animals to mitigate the influence of extraneous variables<sup><xref ref-type="bibr" rid="CR25">25</xref>&#x02013;<xref ref-type="bibr" rid="CR27">27</xref></sup>. However, the contention arises that the assessment of semantic satiation should ideally transpire under conditions where animals are in an awakened state. This suggestion presents a conundrum when contemplating the integration of neuroscience experimental methodologies into the ambit of psychological research<sup><xref ref-type="bibr" rid="CR28">28</xref></sup>. Consequently, while significant strides have been taken in deciphering the mechanistic underpinnings of semantic satiation, the intricate dynamics of this phenomenon continue to elude comprehensive understanding. This challenge stems from the innate susceptibility of participant performance to the nuances of experimental paradigms and tasks. Thus, the proposition of developing a mesoscopic model emerges as an attractive pathway for research endeavors, one that endeavors to bridge the chasm between micro-level neural activity and the macro-level manifestation of semantic satiation<sup><xref ref-type="bibr" rid="CR29">29</xref></sup>.</p><p id="Par5">In light of these challenges, here we endeavor to employ a novel approach, utilizing a continuous coupled neural network<sup><xref ref-type="bibr" rid="CR30">30</xref></sup> and a fully connected layer to construct an artificial neural network based on deep learning that simulates the cognitive mechanisms underpinning semantic satiation. The CCNN, inspired by the dynamics of primary visual cortex<sup><xref ref-type="bibr" rid="CR31">31</xref></sup>, exhibits commensurate static and dynamic properties with real neurons. Previous research has demonstrated that, by appropriately configuring the parameters of the CCNN, model complexity can be reduced while faithfully replicating the electrophysiological signals of actual neurons<sup><xref ref-type="bibr" rid="CR32">32</xref>&#x02013;<xref ref-type="bibr" rid="CR34">34</xref></sup>. Consequently, the CCNN holds the promise of facilitating large-scale simulation computations while mirroring the dynamical attributes of real neurons<sup><xref ref-type="bibr" rid="CR35">35</xref></sup>. This approach is poised to facilitate efficient and accurate observations and quantification of experimental results pertaining to semantic satiation, thus advancing our comprehension of this enigmatic phenomenon.</p><p id="Par6">Our findings serve to elucidate the temporal dynamics of the proposed ventral pathway model&#x02019;s image classification accuracy using a deep learning approach, revealing a distinct pattern over time. Specifically, we observe an initial augmentation in classification accuracy as time progresses, reaching an apex, followed by a subsequent decline. This pattern of performance closely mirrors the phenomenon of semantic satiation, a well-documented phenomenon associated with fluctuations in human classification ability<sup><xref ref-type="bibr" rid="CR36">36</xref>,<xref ref-type="bibr" rid="CR37">37</xref></sup>. As a repeated stimulus persists, information processing within the primary visual cortex becomes increasingly enigmatic. This underscores the pivotal role played by the primary visual cortex in information processing and suggests a link between the occurrence of semantic satiation and the underlying mechanisms operating within this brain region. Consequently, our observations imply that semantic satiation may be characterized as a bottom-up process, with the primary visual cortex being a key player in this cognitive phenomenon.</p></sec><sec id="Sec2" sec-type="results"><title>Results</title><sec id="Sec3"><title>Semantic satiation as a neural network model via deep learning</title><p id="Par7">Figure&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>a provides a depiction of the veritable architecture of the ventral visual pathway within the visual cortex<sup><xref ref-type="bibr" rid="CR38">38</xref></sup>. This neural pathway encompasses the sequential processing of light signals, commencing with their capture by the retina, followed by transmission through lateral geniculate nucleus (LGN) cells, further passage to the primary visual cortex (V1), and subsequent progression through secondary visual regions, including V2, V4, and the inferior temporal cortex (IT cortex)<sup><xref ref-type="bibr" rid="CR39">39</xref>&#x02013;<xref ref-type="bibr" rid="CR43">43</xref></sup>. It is noteworthy that neurons located within V1 and V2 are characterized by smaller spatiotemporal integration receptive fields, allowing them to effectively process localized visual information<sup><xref ref-type="bibr" rid="CR44">44</xref>,<xref ref-type="bibr" rid="CR45">45</xref></sup>. In contrast, V4 neurons feature larger receptive fields, rendering them suitable for the integration of visual information within a broader visual field, while the IT cortex is primarily responsible for object recognition and higher-order cognitive functions.<fig id="Fig1"><label>Fig. 1</label><caption><title>Comparison of ventral pathway and artificial ventral pathway framework.</title><p><bold>a</bold> The ventral visual pathway and neuronal connections in mammals. <bold>b</bold> Comparison of changing trends between our ANN primary visual cortex framework output and EEG signal. <bold>c</bold> Experimental paradigm of semantic satiation in psychology. <bold>d</bold> Artificial neural network semantic satiation framework based on CCNN.</p></caption><graphic xlink:href="42003_2024_6162_Fig1_HTML" id="d33e450"/></fig></p><p id="Par8">Figure&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>c portrays the semantic satiation paradigm in psychology, focusing on speed classification. This experimental design typically encompasses two critical processes: first, the repetitive presentation of identical stimuli to participants; and second, the participants&#x02019; subsequent judgments and responses, designed to gauge the impact of stimulus repetition on the accuracy of their classifications. In the realm of empirical studies on semantic satiation, a variety of experimental paradigms have been employed. Nonetheless, a common thread among these paradigms is the categorization task, which recurrently presents a target stimulus and requires participants to assess the category to which the target word belongs. Subsequently, the time required for categorization or the accuracy of judgment is assessed to explore the phenomenon. In the present study, we conceive of the primary visual cortex as a CCNN model that emulates the repetitive processing of input stimuli, with a fully connected layer simulating the recognition function of more advanced visual regions<sup><xref ref-type="bibr" rid="CR46">46</xref>,<xref ref-type="bibr" rid="CR47">47</xref></sup>. This model incorporates both inter-layer and intra-layer connections, mirroring the intricacies of the actual visual system, as portrayed in Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>d.</p><p id="Par9">The architecture and procedures of our network draw inspiration from classic semantic satiation experiments conducted by numerous psychologists. In this context, the accuracy of network-based classifications emerges as a pivotal metric for discerning the occurrence of semantic satiation. Furthermore, the average output of CCNN neural clusters serves as a representative indicator of the level of activity within the corresponding brain areas, facilitating comparisons with results from Electroencephalography (EEG) experiments<sup><xref ref-type="bibr" rid="CR20">20</xref>&#x02013;<xref ref-type="bibr" rid="CR22">22</xref></sup> as shown in Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>b.</p><p id="Par10">This proposed framework thus offers a relevant explanation for a multitude of intriguing findings in the domains of neuroscience and psychology, particularly those pertaining to semantic satiation. To evaluate the credibility of this model, we conduct a series of classification tasks that parallel psychology experiments. Our simulation framework, as delineated in Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>c, endeavors to replicate various neuron behaviors observed in real-world experiments. Semantic satiation experiments in psychology, akin to the classification tasks in our study, involve the repetitive presentation of the same stimulus to participants<sup><xref ref-type="bibr" rid="CR48">48</xref></sup>. Participants subsequently classify the presented stimulus after varying durations of exposure<sup><xref ref-type="bibr" rid="CR49">49</xref></sup>, with the time required for classification serving as the determinant criterion for ascertaining the occurrence of semantic satiation.</p><p id="Par11">As illustrated in Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>d, our model framework is comprised of a network model, commensurate in size with the input image, designed to establish an Artificial Neural Network (ANN) simulating the primary visual cortex. The CCNN neurons are interlinked with neighboring neurons through linking matrices, with the dimensions of these matrices signifying the receptive field&#x02019;s size. Subsequent to this initial stage, a fully connected layer, mirroring the functioning of the Inferior Temporal (IT) areas, is deployed to conduct the classification of the processing results generated by the ANN primary visual cortex. Our experimental protocol entails the repeated input of an identical image into the CCNN network layer for processing, with the number of iterations mirroring the stimulus presentation duration. The classification accuracy, a salient parameter, serves as a metric for appraising the network&#x02019;s classification proficiency.</p><p id="Par12">In the ensuing series of experiments, we employ two datasets, the MNIST dataset and the Fashion-MNIST dataset, to facilitate comprehensive simulations<sup><xref ref-type="bibr" rid="CR50">50</xref>,<xref ref-type="bibr" rid="CR51">51</xref></sup>. These datasets encompass both verbal and non-verbal images, affording us the capability to investigate the phenomenon of semantic satiation in both verbal and non-verbal contexts. The images serve as input stimuli, while the classification results constitute the model&#x02019;s output. The number of repetitions corresponds to the duration during which primary visual cortex neurons process information and is consequently denoted as model time. This meticulous alignment with the experimental protocols in psychological studies enables a precise emulation of semantic satiation.</p><p id="Par13">The primary objectives of the ensuing experiments are twofold: (1) To elucidate the emergence of visual-related semantic satiation at the mesoscopic level and ascertain whether it manifests as a bottom-up or top-down cognitive process. (2) To scrutinize the extent to which visual information undergoes modification within the primary visual cortex (V1) before reaching the inferior temporal cortex (IT) region, thus unraveling the intricate interplay between visual processing mechanisms and the phenomenon of semantic satiation.</p></sec><sec id="Sec4"><title>Semantic satiation caused by same repeated stimulus</title><p id="Par14">Semantic satiation, a cognitive phenomenon, manifests when individuals are subjected to prolonged exposure to the same stimulus. Classic experiments in the realm of psychology have traditionally illuminated this phenomenon by repetitively presenting subjects with identical stimuli. The assessment of semantic satiation in such experiments typically hinges on quantifying the duration of stimulus presentation and the duration required for participants to reach a decision. Nonetheless, in conventional psychological experimentation, the precise quantification of participants&#x02019; response times in the context of varying stimulus presentation durations proves to be a formidable challenge. As a result, researchers often obtain only qualitative results, discerning whether the presentation time of the same stimulus is relatively shorter or longer. For instance, when the same target word is reiterated thirty times compared to three times, participants invariably exhibit extended decision-making times. In essence, excessive repetition of stimuli correlates with lengthened response times.</p><p id="Par15">In our study, we emulate the experimental process in psychology, as delineated in Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref>a. The discernible trajectory of participants&#x02019; classification proficiency in psychological experiments is depicted in Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref>b. Our proposed Artificial Neural Network (ANN) framework is anchored in the same experimental paradigm. The results showcased in Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref> indicate that the model&#x02019;s accuracy initially ascends and subsequently declines with increasing model time. This trajectory mirrors the trends observed in psychological experiments (Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref>c, d). Notably, variations in the size of the receptive field do not fundamentally alter the overarching pattern. This underscores the efficacy of the ANN framework in faithfully simulating and reproducing the intricate process of semantic satiation. Significantly, the phenomenon of satiation is not confined solely to verbal stimuli but also extends to other non-verbal images, encompassing vision-related semantic satiation phenomena. This observation aligns seamlessly with the findings documented by psychologists.<fig id="Fig2"><label>Fig. 2</label><caption><title>Semantic satiation caused by the same stimulus.</title><p><bold>a</bold> Input stimulus sequences in the experiment. The input stimulus remains the same during the model time process. <bold>b</bold> The trend of classification ability of participants in psychology experiments. The judgment ability of the participants first increases and then decreases as the model time increases. <bold>c</bold>, <bold>d</bold> The variation of MNIST and Fashion-MNIST classification accuracy with the model time under different receptive field sizes. The colored lines represent the size of receptive fields. The accuracy curve in various situations has the same trend as real psychological experiments. Receptive fields of different sizes show the same trend.</p></caption><graphic xlink:href="42003_2024_6162_Fig2_HTML" id="d33e544"/></fig></p></sec><sec id="Sec5"><title>Semantic satiation caused by similar repeated stimulus</title><p id="Par16">In classification tasks, showing a related word before the target word requires more time for the participant to make a classification decision<sup><xref ref-type="bibr" rid="CR14">14</xref></sup>. For example, repeating &#x0201c;apple&#x0201d; thirty times before asking participants to classify &#x0201c;banana&#x0201d; or &#x0201c;chair&#x0201d; as fruit will significantly increase the time required to make a judgment for &#x0201c;banana&#x0201d; and result in more errors in classification. However, this effect does not occur when classifying &#x0201c;chair&#x0201d;. This suggests that stimuli related to the initial repeated stimulus are also affected by the satiation effect.</p><p id="Par17">The aim of this experiment is to explore how neurons react to images with high relevance, low relevance, or relatedness after an extended period of stimulus. In section, Semantic satiation as a neural network model via deep learning, the size of the receptive field doesn&#x02019;t have a significant impact on the experimental results. Therefore, in this experiment, the receptive field is set to 3&#x02009;&#x000d7;&#x02009;3. The experimental input sequence is shown in Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3</xref>a, c. Firstly, a prime stimulus is input as input-1, and then after several repetitions of input-1, the target stimulus to be classified is input as input-2. By analyzing the correlation between input-2 and input-1 and the classification accuracy of input-2, the effect of the similarity between the different categories on the degree of satiation can be exposed through input-1 and input-2. Both datasets show the same phenomenon. There are different correlations between the different categories. For instance, some numbers may have higher relevance to the number &#x0201c;1&#x0201d; (e.g., &#x0201c;7&#x0201d;), while others may be less relevant (e.g., &#x0201c;9&#x0201d;), or unrelated (e.g., &#x0201c;3&#x0201d;). People tend to perceive more similarities between &#x0201c;1&#x0201d; and &#x0201c;7&#x0201d;, such as the shape of a straight line while finding a little resemblance between &#x0201c;1&#x0201d; and &#x0201c;3&#x0201d;. The Structural Similarity Index (SSIM) is used to measure the degree of similarity between different categories. In Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3</xref>b, the SSIM value for &#x0201c;1-3&#x0201d; is 0.203, the SSIM value for &#x0201c;1-7&#x0201d; is 0.250, and the SSIM value for &#x0201c;1-9&#x0201d; is 0.243. In Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3</xref>d, the SSIM value for &#x0201c;Sandal-Bag&#x0201d; is 0.045, the SSIM value for &#x0201c;Sandal-Pullover&#x0201d; is 0.067, and the SSIM value for &#x0201c;Sandal-Ankle boot&#x0201d; is 0.161. SSIM values between all categories in the two datasets are shown in Fig.&#x000a0;<xref rid="MOESM1" ref-type="media">S1</xref>. The subsequent study involves inputting one type of number image into the single-layer CCNN framework repeatedly and then inputting a different type of number image to measure the recognition accuracy of MNIST (Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3</xref>a). The extent of the decrease varies according to the relevance of each number to input-1 as shown in Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3</xref>b. The same results are obtained in the Fashion-MNIST dataset experiment as shown in Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3</xref>c, d. Psychological study has shown that participants require more time to recognize the numbers &#x0201c;1&#x0201d; or &#x0201c;7&#x0201d;, as verified by EEG-based experiments<sup><xref ref-type="bibr" rid="CR21">21</xref></sup>. Therefore, the results of this study are consistent with those of actual psychological experiments.<fig id="Fig3"><label>Fig. 3</label><caption><title>Semantic satiation caused by similar stimulus.</title><p><bold>a</bold>, <bold>c</bold> Similar input sequences from the MNIST and Fashion-MNIST datasets. The input is divided into two parts, starting with a stimulus input that is repeated 5 or 10 times before inputting the target stimulus. The participants show a higher classification ability of input-2 on the class with a lower relevance. <bold>b</bold>, <bold>d</bold> The trend of the accuracy of the target input (input-2). The left and right graphs show the cases where input-1 is repeated 5 and 10 times, respectively. The colored lines represent different categories of input-2. Darker colored lines represent higher category similarity and lower accuracy. The higher the similarity with input-1, the lower the accuracy of input-2. The results demonstrate that the proposed ANN framework is able to imitate the characteristics of semantic satiation in humans.</p></caption><graphic xlink:href="42003_2024_6162_Fig3_HTML" id="d33e600"/></fig></p></sec><sec id="Sec6"><title>Visualization of visual information processing&#x02019;s intermediate state</title><p id="Par18">The experiments above demonstrate that the model is capable of simulating semantic satiation from multiple perspectives. However, the evolution of semantic or visual signals over time remains unclear. The output feature of the CCNN layer at individual time points and the outputs of the fully connected layer are plotted to study the evolution of semantics in the primary visual cortex and IT area. The visualization includes the following parts:</p><p id="Par19">Figures&#x000a0;<xref rid="Fig4" ref-type="fig">4</xref> and <xref rid="Fig5" ref-type="fig">5</xref> show the changes in the image in our ANN primary visual cortex. Figure&#x000a0;<xref rid="Fig4" ref-type="fig">4</xref> displays the images processed by the ANN network at various model times, corresponding to section, Semantic satiation as a neural network model via deep learning. In both datasets, the image signals gradually become more obscure and the main parts, which refer to the parts related to semantics, become larger and more unclear (Fig.&#x000a0;<xref rid="Fig4" ref-type="fig">4</xref>a, b). Figure&#x000a0;<xref rid="Fig5" ref-type="fig">5</xref> shows the changes in a similar stimulus experiment, corresponding to section, Semantic satiation caused by same repeated stimulus. The effect of continuous pre-stimulus persists for some time after the stimulus stops. The accuracy of other input1 and input2 situations can be found in Fig.&#x000a0;<xref rid="MOESM1" ref-type="media">S2</xref>. The residual image of the processing result of the first image signal appears on the processing result of the other subsequent pictures for some time before gradually fading away.<fig id="Fig4"><label>Fig. 4</label><caption><title>Visualization of the results of the processing of the CCNN at different time points.</title><p><bold>a</bold> Repeat processing results for number &#x0201c;7&#x0201d; in CCNN. <bold>b</bold> Repeat processing results for &#x0201c;pullover&#x0201d; in CCNN. Both datasets show that the images are changing as the model time lasts. This may make it more difficult to classify images for the model.</p></caption><graphic xlink:href="42003_2024_6162_Fig4_HTML" id="d33e639"/></fig><fig id="Fig5"><label>Fig. 5</label><caption><title>Visualization of semantic satiation caused by similar input.</title><p><bold>a</bold> The process of experiment on MNIST with the five repetitions of input-1. <bold>b</bold> The process of experiment on Fashion-MNIST with the repetitions of input-1. <bold>c</bold> The process of experiment on MNIST with the ten repetitions of input-1. <bold>d</bold> The process of experiment on Fashion-MNIST with the ten repetitions of input-1. In fig. (<bold>a</bold>, <bold>c</bold>), &#x0201c;1&#x0201d; repeats 10 times has a greater impact on the following images than 5 times. The same result can be seen on Fashion-MNIST in fig. (<bold>b</bold>, <bold>d</bold>). Continuous repetition will make it more difficult for the model to classify similar content.</p></caption><graphic xlink:href="42003_2024_6162_Fig5_HTML" id="d33e672"/></fig></p><p id="Par20">Owing to the existence of coupling connections, the firing information of neurons propagates through the coupling links to neighboring neurons, giving rise to an automatic wave effect. As model time progresses, this automatic wave effect becomes susceptible to interference from high-intensity waves present in the darker regions of the image. Consequently, this interference disrupts the state of neurons in the darker areas and distorts the features at the image&#x02019;s edges. The temporal evolution of several intermediate states of the CCNN neuron was visualized in Fig.&#x000a0;<xref rid="MOESM1" ref-type="media">S3</xref>. The five main parts are feeding input F, couple linking L, modulation product U, dynamic activity E, and continuous output Y. Due to the presence of coupling connections, noise gradually spreads towards the surrounding dark areas.</p><p id="Par21">Figures&#x000a0;<xref rid="Fig6" ref-type="fig">6</xref> and <xref rid="Fig7" ref-type="fig">7</xref> show the variation in processing results for the manual IT area. The fully connected layer abstracts each image into a ten-dimensional vector in Fig.&#x000a0;<xref rid="Fig6" ref-type="fig">6</xref>a. Then, the distance between the mean values of different categories of vectors can be calculated. This simulation uses abstract digital vectors to represent semantic segments. The inter-class distance&#x02019;s variation trend is consistent with the accuracy, but the intra-class distance&#x02019;s trend does not move in the opposite direction as expected as shown in Fig.&#x000a0;<xref rid="Fig6" ref-type="fig">6</xref>b, c<sup><xref ref-type="bibr" rid="CR52">52</xref></sup>. Figure&#x000a0;<xref rid="Fig7" ref-type="fig">7</xref> shows the vector distribution after t-distributed stochastic neighbor embedding (t-SNE). This indicates that in the semantic satiation phenomenon, the semantics of different classes become more dispersed, making classification more challenging to complete and even leading to errors.<fig id="Fig6"><label>Fig. 6</label><caption><title>Intra-class and inter-class distance of semantic vectors.</title><p><bold>a</bold> The details of the fully connected layer. This layer extracts 784 elements into a 10-dimension vector. <bold>b</bold> The inter-class distance of MNIST and Fashion-MNIST datasets. <bold>c</bold> The intra-class distance of MNIST and Fashion-MNIST datasets. In both datasets, the inter-class distance shows a trend of first rising and then falling. From the results, for the model, the larger the inter-class distance, the easier the classification will be. However, the intra-class distance shows a more complicated situation.</p></caption><graphic xlink:href="42003_2024_6162_Fig6_HTML" id="d33e716"/></fig><fig id="Fig7"><label>Fig. 7</label><caption><title>Semantic distance for different categories.</title><p><bold>a</bold> Two-dimensional vectors of the semantics of MNIST. <bold>b</bold> Two-dimensional vectors of the semantics of Fashion-MNIST. The distribution of semantics shows a trend of being gradually disordered.</p></caption><graphic xlink:href="42003_2024_6162_Fig7_HTML" id="d33e730"/></fig></p><p id="Par22">In summary, semantic satiation is closely related to the working mechanisms of the primary visual cortex. Due to the connection between neurons, the continuous stimulus of a single neuron will impact the action potential and signal transmission process of other surrounding neurons, which consequently alters the information processed by all neurons. This explains why the persistent input signal to the brain may lead to the attenuation of definition. From Fig.&#x000a0;<xref rid="Fig5" ref-type="fig">5</xref>, it can be inferred that neuronal responses exhibit a certain time delay compared to stimulus signal input. In Fig.&#x000a0;<xref rid="Fig5" ref-type="fig">5</xref>, the residual image of the previous input signal is still retained in the output even after the signal has ceased for a while. This delay prevents neurons from swiftly returning to the initial state and adapting promptly to new tasks. And it is able to explain well the changes in the semantic distribution of the IT area.</p></sec><sec id="Sec7"><title>Comparison experiment of framework and EEG based cognitive study</title><p id="Par23">Psychological perspectives have utilized the N400 component of event-related potentials (ERPs) to investigate the phenomenon of semantic satiation. Researchers observe the amplitude changes of the N400 component before and after semantic satiation<sup><xref ref-type="bibr" rid="CR20">20</xref>,<xref ref-type="bibr" rid="CR21">21</xref>,<xref ref-type="bibr" rid="CR53">53</xref></sup>. When two completely unrelated stimuli are presented, significant differences in the amplitude of the N400 component will occur. The difference in N400 of population activities before and after satiation can be used to measure the degree of satiation as shown in Fig.&#x000a0;<xref rid="Fig8" ref-type="fig">8</xref>b<sup><xref ref-type="bibr" rid="CR54">54</xref>&#x02013;<xref ref-type="bibr" rid="CR57">57</xref></sup>. When the difference is larger, the degree of satiation is generally believed to be deeper.<fig id="Fig8"><label>Fig. 8</label><caption><title>Comparison with the trend of N400 composition.</title><p><bold>a</bold> Schematic diagram of the experimental process in psychology and ANN model. The change of them has the same trend. <bold>b</bold> The changing trend of N400 before and after satiation. Semantic satiation makes the N400&#x02019;s Amplitude become lower. <bold>c</bold> Trend of the average output (<italic>Y</italic>) difference (D-value) of different categories. The left plot shows the average output of &#x0201c;3&#x0201d; and &#x0201c;7&#x0201d;. The middle plot shows the average output of &#x0201c;9&#x0201d; and &#x0201c;7&#x0201d;. The right one shows the <italic>D</italic>-value of the output <italic>Y</italic>: &#x0201c;3&#x02009;&#x02212;&#x02009;7&#x0201d; and &#x0201c;9&#x02009;&#x02212;&#x02009;7&#x0201d;.</p></caption><graphic xlink:href="42003_2024_6162_Fig8_HTML" id="d33e788"/></fig></p><p id="Par24">Parallels have been drawn between the neuronal action potentials generated by the CCNN model and the N400 component observed in EEG signals due to their shared manifestation as electrical signal characteristics indicative of neural activity. EEG signals serve as a proxy for neuronal activity fluctuations transmitted by electrical signals across the brain. Similarly, the output Y from the CCNN model&#x02019;s neurons, representing action potentials, reflects the underlying neuronal activities. N400 typically manifests itself in the central and upper regions of the brain<sup><xref ref-type="bibr" rid="CR58">58</xref></sup>. For a comprehensive understanding of the spatial distribution of the N400 effect and the interaction between brain regions, multiple electrodes are typically employed for recording<sup><xref ref-type="bibr" rid="CR59">59</xref></sup>. This facilitates the analysis of N400 waveform distribution across the entire scalp and the exploration of spatiotemporal relationships between different brain regions. Drawing from psychological experiments on N400, researchers primarily focus on changes in N400 amplitudes before and after semantic satiation<sup><xref ref-type="bibr" rid="CR53">53</xref></sup>. When two entirely unrelated stimuli are presented, significant differences in N400 component amplitudes arise. The disparity in the amplitudes of the N400 components before and after satiation serves as a measure of the degree of satiation<sup><xref ref-type="bibr" rid="CR54">54</xref></sup>.</p><p id="Par25">In our model, the average output of neurons in the ANN primary visual cortex represents the level of neuron activity. By comparing the output difference of the visual cortex before and after semantic satiation, it can be determined whether there is a satiation phenomenon in this stage. As shown in Fig.&#x000a0;<xref rid="Fig8" ref-type="fig">8</xref>a, the output of our model has the same trend as the change in N400. Our experiment yielded a similar conclusion: once semantic satiation sets in, the difference in the mean output of the neurons significantly reduces. The experiments compared the average output of our model&#x02019;s neurons at input-2 from the experiments in Section, Semantic satiation caused by same repeated stimulus, as shown in Fig.&#x000a0;<xref rid="Fig8" ref-type="fig">8</xref>c. The difference in output between the numbers &#x0201c;7&#x0201d; and &#x0201c;3&#x0201d; is significantly higher than the difference in output between the numbers &#x0201c;7&#x0201d; and &#x0201c;9&#x0201d;. The disparity gradually decreased following the onset of semantic satiation. Our findings resemble those of N400 experiments conducted by cognitive psychologists, however, our conclusions differ. Our experimental results show that semantic satiation emerges in the primary visual cortex. This difference may be attributed to the spatial scale of our respective investigations. While existing cognitive research based on EEG characterizes cognitive processes by employing the average activity of numerous neurons in the brain region, our research examines cognitive processes using mesoscopic neural networks. Hence, this study presents a different landscape.</p></sec></sec><sec id="Sec8" sec-type="discussion"><title>Discussion</title><p id="Par26">Early studies in the field of psychology coined the term &#x0201c;Semantic Satiation&#x0201d;<sup><xref ref-type="bibr" rid="CR1">1</xref></sup> to delineate the phenomenon wherein the repetition of a stimulus or context precipitates a waning of the affective response. The conceptual underpinning of &#x0201c;satiation&#x0201d; first materialized in the context of a comprehensive meta-analysis of research endeavors related to maze-running experiments in the context of rodent behavior<sup><xref ref-type="bibr" rid="CR7">7</xref></sup>. Divergent from response inhibition, &#x0201c;satiation&#x0201d; pertains to a form of aversion related to the continued processing of a recurrent stimulus or context, specifically denominated as &#x0201c;processing satiation&#x0201d;<sup><xref ref-type="bibr" rid="CR8">8</xref></sup>. The exploration of semantic satiation has piqued the interest of numerous psychologists, engendering a proliferation of research methodologies, thereby yielding a kaleidoscope of distinctive theories<sup><xref ref-type="bibr" rid="CR1">1</xref>,<xref ref-type="bibr" rid="CR3">3</xref>,<xref ref-type="bibr" rid="CR4">4</xref></sup>. However, hitherto, the collective scientific endeavor has not succeeded in providing a definitive elucidation concerning the precise neural mechanisms and locales implicated in the manifestation of this intriguing phenomenon.</p><p id="Par27">The inquiry into the phenomenon of semantic satiation has embarked upon a protracted and intricate journey, marked by methodological evolution and refinement<sup><xref ref-type="bibr" rid="CR9">9</xref></sup>. In its nascent stages, research predominantly leaned on introspective methods, which necessitated participants to expound upon their own cognitive experiences, encompassing thoughts, emotions, and consciousness<sup><xref ref-type="bibr" rid="CR10">10</xref>&#x02013;<xref ref-type="bibr" rid="CR12">12</xref></sup>. Of notable significance, Lambert&#x02019;s seminal investigation entailed participants evaluating their affective responses to repeatedly presented words<sup><xref ref-type="bibr" rid="CR1">1</xref></sup>. The findings of this study revealed a perceptible attenuation in participants&#x02019; emotional responses following the repetition of identical words, thus manifesting a marked semantic erosion. However, by employing a closely analogous experimental approach, a congruent phenomenon was intriguingly failed to observe<sup><xref ref-type="bibr" rid="CR13">13</xref></sup>. The inherent limitation of introspective methods lies in their challenge in quantifying and standardizing participants&#x02019; subjective experiences<sup><xref ref-type="bibr" rid="CR14">14</xref></sup>, thus yielding inconsistent findings that have hindered the advancement of semantic satiation research. Consequently, introspective methods prove insufficient for discerning the intricate stages underpinning the emergence of semantic satiation<sup><xref ref-type="bibr" rid="CR15">15</xref></sup>.</p><p id="Par28">Subsequent to this nascent phase, the burgeoning fields of cognitive psychology and cognitive neuroscience ushered in a plethora of diverse experimental paradigms and neurophysiological techniques, marking a new era of objective and quantifiable approaches to investigating semantic satiation<sup><xref ref-type="bibr" rid="CR16">16</xref>&#x02013;<xref ref-type="bibr" rid="CR19">19</xref></sup>. Notably, the N400 component emerged as a prominent tool in probing the processes underlying semantic satiation<sup><xref ref-type="bibr" rid="CR20">20</xref>&#x02013;<xref ref-type="bibr" rid="CR22">22</xref></sup>. This component is commonly employed to measure semantic processing, with greater amplitude observed in cases of semantic or contextual incongruence compared to congruence. While experimental outcomes have not been universally consistent, they collectively support the conclusion that semantic satiation transpires at the semantic level, rather than being a mere offshoot of perceptual decay. Notably, magnetoencephalography (MEG) was used to discern that satiation materializes at the juncture where perception and semantics converge<sup><xref ref-type="bibr" rid="CR23">23</xref></sup>. Not with standing the advantages afforded by neuroscience, research in this domain has primarily operated at a macro level, with a limited capacity to establish links between macro satiation phenomena and micro-level neural activity. Neurophysiological experiments necessitate the examination of neural responses under stimulus conditions during the deep sleep of animals to mitigate extraneous interference. However, it is suggested that the evaluation of semantic satiation should be conducted under conditions where animals are awake, presenting a potential challenge when integrating neuroscience experimental methodologies into psychological research.</p><p id="Par29">Therefore, while several strides have been made in elucidating the mechanism of semantic satiation, the intricacies of the phenomenon continue to elude complete understanding due to the susceptibility of participant performance to the intricacies of experimental paradigms and tasks. Hence, the development of a mesoscopic model that bridges micro-level neural activity with the macro-level manifestation of semantic satiation appears to be a promising avenue. In this study, drawing from the intricate physiological underpinnings of neurons within the visual cortex, we have engineered an ANN semantic satiation framework rooted in the CCNN. This innovative framework endeavors to improve the inherent limitations of real-life psychological experiments, which are susceptible to disruptions and possess complexities that challenge their reliability. Our empirical findings resonate with previous studies, conclusively establishing that semantic satiation transpires upon the recurrent presentation of identical or closely akin stimuli. Yet, our mesoscopic-scale investigation introduces an intriguing perspective, postulating that semantic satiation could be characterized as a bottom-up cognitive process. This novel perspective holds the potential to furnish the realm of psychological research with numerical benchmarks and methodological paradigms. However, owing to the constraints of our present experimental conditions, we are currently unable to validate our simulation results on actual neurons. Nevertheless, our research proffers novel insights and avenues for future exploration, thereby extending an invitation for subsequent research endeavors to substantiate our discoveries.</p><p id="Par30">In particular, these implications appear to diverge from the prevalent psychological research, which generally posits semantic satiation as a top-down process<sup><xref ref-type="bibr" rid="CR22">22</xref>,<xref ref-type="bibr" rid="CR60">60</xref>,<xref ref-type="bibr" rid="CR61">61</xref></sup>. Although our study aligns with psychological research in its choice of experimental paradigm, discrepant views may be attributable to discrepancies in the scales of observation. It is imperative to underscore that our experimental results are underpinned by careful examination, and we note a congruence between the overall neural output levels in the CCNN model and the observed trends in the N400 signal as documented in classical experiments. Both the N400 components and the CCNN model&#x02019;s output exhibit a discernible decline in the context of semantic satiation prompted by heightened image similarity. Furthermore, our real-time monitoring of individual neuron activity unveils a distinct neural landscape, potentially affording novel insights and a mesoscopic perspective for the continued exploration of the intricate phenomenon of semantic satiation. This holistic approach, bridging computational modeling and neural activity analysis, provides a promising avenue for further unraveling the cognitive intricacies of this phenomenon. Notably, in psychological experiments, the process typically involves two main stages: presenting prime words (input-1) and target words (input-2), with a deliberate pause, referred to as a &#x0201c;blank wait&#x0201d; period, between each stimulus presentation<sup><xref ref-type="bibr" rid="CR53">53</xref></sup>. This pause is crucial for ensuring that participants&#x02019; attention remains squarely focused on the visual stimuli. In our model, unlike the human brain, there isn&#x02019;t an element of uncontrolled distraction; consequently, the &#x0201c;blank wait&#x0201d; phase was not initially incorporated. Therefore, integrating distraction and attentional focus in future research could significantly enrich our understanding and the realism of our model.</p><p id="Par31">Moreover, our model proposes broader inquiries that may be interesting in the domain of psychological experimental research. What are the advantageous aspects of this operational mechanism within the human brain? Can it be deemed the optimal mode of information processing in the cerebral apparatus, and how might we harness or circumvent this phenomenon? These intriguing questions, grounded in our computer simulation outcomes, eagerly await corroboration through dedicated biological and psychological experiments. It is possible that these information-processing mechanisms could serve as sources of inspiration for brain-inspired computing. In general, our results provide the potential to study semantic satiation using neural network models, which may spark the emergence of novel research questions that span the domains of brain-inspired computing and psychology.</p></sec><sec id="Sec9"><title>Methods</title><p id="Par32">In this study, all simulations employ a uniform model structure and utilize the MNIST and Fashion-MNIST datasets as inputs. Each experiment adopts different input, analysis, and statistical techniques. The architecture mainly comprises two components: the ANN primary visual cortex layer and the artificial IT layer, as illustrated in Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>. The following parts will outline the principles of the two layers and explicate the specific stages and intricacies of network training and testing.</p><sec id="Sec10"><title>The ANN primary visual cortex layer</title><p id="Par33">The ANN primary visual cortex layer is composed of the CCNN network. In the realm of modeling the visual pathway, Convolutional Neural Networks (CNNs) have been widely adopted due to their efficacy in emulating the functioning of the visual cortex. However, a notable limitation of traditional CNNs is their lack of capacity to incorporate temporal dynamics, synchronous oscillations, and refractory periods, which are intrinsic to the behavior of real neurons. This disparity prompted exploration into alternative models offering a closer approximation to the physiological processes of the human visual system. The Pulse-Coupled Neural Network (PCNN), inspired by the groundbreaking work of Eckhorn<sup><xref ref-type="bibr" rid="CR62">62</xref></sup> and further elaborated by Rangnanath<sup><xref ref-type="bibr" rid="CR63">63</xref></sup>, is a model with high biological rationality. PCNNs, with their feedback mechanism and spike coding, provide a more physiologically aligned model. The equations of PCNN are as follows:<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\left\{\begin{array}{l}{F}_{ij}(n)={e}^{-{\alpha }_{f}}{F}_{ij}(n-1)+{V}_{F}{M}_{ijkl}{Y}_{kl}(n-1)+{S}_{ij}\quad \\ {L}_{ij}(n)={e}^{-{\alpha }_{l}}{L}_{ij}(n-1)+{V}_{L}{W}_{ijkl}{Y}_{kl}(n-1)\hfill \quad \\ {U}_{ij}(n)={F}_{ij}(n)(1+\beta {L}_{ij}(n))\hfill \quad \\ {Y}_{ij}(n)=\left\{\begin{array}{ll}1,\quad &#x00026;if\quad {U}_{ij}(n)\, &#x0003e; \,{E}_{ij}(n)\\ 0,\quad &#x00026;otherwise\end{array}\right.\hfill \quad \\ {E}_{ij}(n)={e}^{-{\alpha }_{e}}{E}_{ij}(n-1)+{V}_{E}{Y}_{ij}(n-1)\quad \hfill \end{array}\right.$$\end{document}</tex-math><mml:math id="M2"><mml:mfenced open="{"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mrow><mml:mi>&#x003b1;</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi><mml:mi>k</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mrow><mml:mi>&#x003b1;</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi><mml:mi>k</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:msub><mml:mrow><mml:mi>U</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>&#x003b2;</mml:mi><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfenced open="{"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mn>1</mml:mn><mml:mo>,</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mspace width="1.0em"/><mml:msub><mml:mrow><mml:mi>U</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="0.25em"/><mml:mo>&#x0003e;</mml:mo><mml:mspace width="0.25em"/><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>w</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mrow><mml:mi>&#x003b1;</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:math><graphic xlink:href="42003_2024_6162_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula>Where (<italic>i</italic>, <italic>j</italic>) is the (<italic>i</italic><sub><italic>t</italic><italic>h</italic></sub>, <italic>j</italic><sub><italic>t</italic><italic>h</italic></sub>) neuron, and (<italic>k</italic>, <italic>l</italic>) is the (<italic>k</italic><sub><italic>t</italic><italic>h</italic></sub>, <italic>l</italic><sub><italic>t</italic><italic>h</italic></sub>) neuron. The five main parts are couple linking <italic>L</italic><sub><italic>i</italic><italic>j</italic></sub>(<italic>n</italic>), feeding input <italic>F</italic><sub><italic>i</italic><italic>j</italic></sub>(<italic>n</italic>), modulation product <italic>U</italic><sub><italic>i</italic><italic>j</italic></sub>(<italic>n</italic>), dynamic activity <italic>E</italic><sub><italic>i</italic><italic>j</italic></sub>(<italic>n</italic>) and output <italic>Y</italic><sub><italic>i</italic><italic>j</italic></sub>(<italic>n</italic>). <italic>S</italic><sub><italic>i</italic><italic>j</italic></sub> represents the external feeding input obtained by the receptive fields. <italic>&#x003b1;</italic><sub><italic>f</italic></sub>, <italic>&#x003b1;</italic><sub><italic>l</italic></sub> and <italic>&#x003b1;</italic><sub><italic>e</italic></sub> are exponential decay factors. <italic>V</italic><sub><italic>F</italic></sub> and <italic>V</italic><sub><italic>L</italic></sub> denote weighting factors. Moreover, <italic>W</italic><sub><italic>i</italic><italic>j</italic><italic>k</italic><italic>l</italic></sub> and <italic>M</italic><sub><italic>i</italic><italic>j</italic><italic>k</italic><italic>l</italic></sub> indicate feeding and linking synaptic weights, respectively, and <italic>&#x003b2;</italic> indicates the linking strength, which directly determines <italic>L</italic><sub><italic>i</italic><italic>j</italic></sub>(<italic>n</italic>) in the modulation product <italic>U</italic><sub><italic>i</italic><italic>j</italic></sub>(<italic>n</italic>). <italic>S</italic><sub><italic>i</italic><italic>j</italic></sub> is the external feeding input, <italic>&#x003b1;</italic><sub><italic>f</italic></sub>, <italic>&#x003b1;</italic><sub><italic>l</italic></sub>, <italic>&#x003b1;</italic><sub><italic>e</italic></sub> are exponential decay factors of the five main parts.</p><p id="Par34">However, a critical gap in the PCNN model&#x02019;s capacity to replicate the dynamic responses of neurons to external periodic signals was identified by Siegel&#x02019;s experiments<sup><xref ref-type="bibr" rid="CR64">64</xref></sup> with real primary visual cortex neurons. This limitation motivated the development of the (CCNN)<sup><xref ref-type="bibr" rid="CR30">30</xref></sup>, which was introduced to address the shortcomings of the PCNN in capturing the complex nonlinear dynamics of neuron activity under dynamic stimuli. The CCNN comprises five components: coupling linkage, feeding input, modulation product, dynamic activity, and continuous output. The CCNN equations are as follows:<disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\left\{\begin{array}{l}{F}_{ij}(n)={e}^{-{\alpha }_{f}}{F}_{ij}(n-1)+{V}_{F}{M}_{ijkl}{Y}_{kl}(n-1)+{S}_{ij}\quad \\ {L}_{ij}(n)={e}^{-{\alpha }_{l}}{L}_{ij}(n-1)+{V}_{L}{W}_{ijkl}{Y}_{kl}(n-1)\hfill \quad \\ {U}_{ij}(n)={F}_{ij}(n)(1+\beta {L}_{ij}(n))\hfill \quad \\ {Y}_{ij}(n)=\frac{1}{1+{e}^{-({U}_{ij}(n)-{E}_{ij}(n))}}\hfill \quad \\ {E}_{ij}(n)={e}^{-{\alpha }_{e}}{E}_{ij}(n-1)+{V}_{E}{Y}_{ij}(n-1)\hfill \quad \end{array}\right.$$\end{document}</tex-math><mml:math id="M4"><mml:mfenced open="{"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mrow><mml:mi>&#x003b1;</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi><mml:mi>k</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mrow><mml:mi>&#x003b1;</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi><mml:mi>k</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:msub><mml:mrow><mml:mi>U</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>&#x003b2;</mml:mi><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>U</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mrow><mml:mi>&#x003b1;</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:math><graphic xlink:href="42003_2024_6162_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula>Where, the output <italic>Y</italic><sub><italic>i</italic><italic>j</italic></sub>(<italic>n</italic>) is changed from a pulse signal to a continuous value. The CCNN model distinguishes itself by generating continuous output values, <italic>Y</italic><sub><italic>i</italic><italic>j</italic></sub>(<italic>n</italic>), and demonstrates behavior that closely mirrors the dynamic of chaotic activity observed in real V1 neurons when exposed to periodic stimuli. This adaptation enables the CCNN to better simulate the intricate dynamics of neuron interactions within the primary visual cortex, offering a model that strikes an optimal balance between biological fidelity and the complexity of neuronal behavior.</p><p id="Par35">In experiments, all neurons are in a static state at the beginning, then all parameters in the matrices <italic>F</italic>, <italic>L</italic>, <italic>U</italic>, <italic>Y</italic> and <italic>E</italic> are zero in the initial state. The parameter settings in all experiments are as follows: <italic>&#x003b1;</italic><sub><italic>f</italic></sub>&#x02009;=&#x02009;0.1,&#x02009;<italic>&#x003b1;</italic><sub><italic>e</italic></sub>&#x02009;=&#x02009;1,&#x02009;<italic>&#x003b1;</italic><sub><italic>l</italic></sub>&#x02009;=&#x02009;0.1, and <italic>&#x003b2;</italic>&#x02009;=&#x02009;0.5. The impact analysis of these parameter settings on the model is detailed in Fig.&#x000a0;<xref rid="MOESM1" ref-type="media">S4</xref>(see&#x000a0;<xref rid="MOESM1" ref-type="media">Supplementary Figures)</xref>.</p><p id="Par36">The ANN model of the primary visual cortex encompasses neurons of identical size as the input images, totaling 784 (28&#x02009;&#x000d7;&#x02009;28) neurons. The size of the receptive field is set to 3&#x02009;&#x000d7;&#x02009;3 and 5&#x02009;&#x000d7;&#x02009;5 for simulation experiments. In the specific computation, we realize this calculation by convolution without bias.</p></sec><sec id="Sec11"><title>The ANN model of IT area</title><p id="Par37">This model&#x02019;s IT area employs the fully connected layer to reduce the dimensionality and classify the information processed by CCNN as shown in Fig.&#x000a0;<xref rid="Fig6" ref-type="fig">6</xref>a<sup><xref ref-type="bibr" rid="CR65">65</xref>,<xref ref-type="bibr" rid="CR66">66</xref></sup>. Following the iterative processing of CCNN, the output is a 28&#x02009;&#x000d7;&#x02009;28 feature. Then it is flattened into a one-dimensional vector with a size of 784, which is then input into the fully connected layer. As both datasets contain 10 classes, the number of neurons in the fully connected layer is set to 10 to match the simulation task.</p></sec><sec id="Sec12"><title>Training and testing</title><p id="Par38">The MNIST and Fashion-MNIST datasets are utilized for training and testing. During training, the aim is to learn the content of the two linking matrices <italic>M</italic> and <italic>W</italic>. Specifically, in this study, the models for training and testing are nonidentical. During training, the best repetition of the input stimulus is set as <italic>n</italic>. The model is only allowed to learn the situation where the stimulus is repeated <italic>n</italic> times. <italic>n</italic> is set to 4 in this research. The CCNN layer accepts the input signal, repeats it <italic>n</italic> times, and then inputs it into the fully connected layer. Finally, the output of the fully connected layer gets the classification probability of the training object through the activation function. The network adjusts the values of the matrices <italic>M</italic> and <italic>W</italic>, as well as the weight in the fully connected layer.</p><p id="Par39">During testing, it is necessary to evaluate the recognition performance of each model time. When testing, the CCNN Module is repeated 100 times, and the output from each model time is fed into the fully connected layer for classification. The main difference is that for the output of the CCNN network, the training network only requires the <italic>n</italic><sub><italic>t</italic><italic>h</italic></sub> model time&#x02019;s output as the target, whereas in the testing network, the output results of each model time need to be classified and counted<sup><xref ref-type="bibr" rid="CR67">67</xref></sup>. Thus, the output of the CCNN network needs to be transmitted to the fully connected layer for classification after each model time. The loss function is the cross-entropy loss of the model&#x02019;s predicted results and the image&#x02019;s label.</p><p id="Par40">For the simulation of semantic satiation caused by the same or similar repeated stimulus, the same training results are used as model parameters. The primary difference lies in whether the inputs are changed after 5 or 10 repetitions. In the visualization part, the output <italic>Y</italic> of each model time is converted into a grayscale image and displayed for observation. Additionally, the concepts of intra-class distance and inter-class distance are used to measure the division of semantics in the brain. The distance of the same semantics is represented by the intra-class distance. The inter-class distance is utilized to measure the difference between different semantics. Both are measured using the Euclidean distance.</p><p id="Par41">The model is trained with a batch size of 200 and Adam with a learning rate of 0.001. The maximum number of epochs is set to 100. The model is implemented by Pytorch and trained on a NVIDIA GeForce GTX 1660.</p></sec><sec id="Sec13"><title>Reporting summary</title><p id="Par42">Further information on research design is available in the&#x000a0;<xref rid="MOESM4" ref-type="media">Nature Portfolio Reporting Summary</xref> linked to this article.</p></sec></sec><sec sec-type="supplementary-material"><sec id="Sec14"><title>Supplementary information</title><p>
<supplementary-material content-type="local-data" id="MOESM1"><media xlink:href="42003_2024_6162_MOESM1_ESM.pdf"><caption><p>Supplementary Information</p></caption></media></supplementary-material>
<supplementary-material content-type="local-data" id="MOESM2"><media xlink:href="42003_2024_6162_MOESM2_ESM.pdf"><caption><p>Description of Additional Supplementary Files</p></caption></media></supplementary-material>
<supplementary-material content-type="local-data" id="MOESM3"><media xlink:href="42003_2024_6162_MOESM3_ESM.xlsx"><caption><p>Supplementary Data</p></caption></media></supplementary-material>
<supplementary-material content-type="local-data" id="MOESM4"><media xlink:href="42003_2024_6162_MOESM4_ESM.pdf"><caption><p>Reporting Summary</p></caption></media></supplementary-material>
</p></sec></sec></body><back><fn-group><fn><p><bold>Publisher&#x02019;s note</bold> Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></fn></fn-group><sec><title>Supplementary information</title><p>The online version contains supplementary material available at 10.1038/s42003-024-06162-0.</p></sec><ack><title>Acknowledgements</title><p>This study is supported by the Natural Science Foundation of Gansu Province (No. 21JR7RA510); the Talent Scientific Fund of Lanzhou University; the National Natural Science Foundation of China(Nos. 62236007, 62176003, 62088102), and Beijing Nova Program (20230484362). Some experiments are supported by the Supercomputing Center of Lanzhou University.</p></ack><notes notes-type="author-contribution"><title>Author contributions</title><p>Xinyu Zhang and Jizhao Liu conceived the presented idea. Xinyu Zhang and Jing Lian carried out the experiment. Zhaofei Yu, Huajin Tang and Dong Liang developed the theory. Xinyu Zhang took the lead in writing the manuscript. Jizhao Liu and Jian K. Liu revised the manuscript. Jizhao Liu and Jian K. Liu supervised the findings of this work. Jizhao Liu, Zhaofei Yu and Huajin Tang provided the financial support.</p></notes><notes notes-type="peer-review"><title>Peer review</title><sec id="FPar1"><title>Peer review information</title><p id="Par43"><italic>Communications Biology</italic> thanks Zhongqing Jiang and the other, anonymous, reviewer(s) for their contribution to the peer review of this work. Primary Handling Editor: George Inglis.</p></sec></notes><notes notes-type="data-availability"><title>Data availability</title><p>Publicly available datasets were used in this study. These datasets can be found at the following sites: <ext-link ext-link-type="uri" xlink:href="http://yann.lecun.com/exdb/mnist/">http://yann.lecun.com/exdb/mnist/</ext-link>(MNIST), <ext-link ext-link-type="uri" xlink:href="http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/">http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/</ext-link>(Fashion MNIST). The source data for all figures of fluorescence is available in&#x000a0;<xref rid="MOESM3" ref-type="media">Supplementary Data</xref>. All figures are open on figshare (10.6084/m9.figshare.25507406.v2). Any further data not included in the text will be made available upon request.</p></notes><notes notes-type="data-availability"><title>Code availability</title><p>The code used in this study is available at <ext-link ext-link-type="uri" xlink:href="https://zenodo.org/records/10894595">https://zenodo.org/records/10894595</ext-link>.</p></notes><notes id="FPar2" notes-type="COI-statement"><title>Competing interests</title><p id="Par44">The authors declare no competing interests.</p></notes><ref-list id="Bib1"><title>References</title><ref id="CR1"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lambert</surname><given-names>WE</given-names></name><name><surname>Jakobovits</surname><given-names>LA</given-names></name></person-group><article-title>Verbal satiation and changes in the intensity of meaning</article-title><source>J. Exp. Psychol.</source><year>1960</year><volume>60</volume><fpage>376</fpage><pub-id pub-id-type="doi">10.1037/h0045624</pub-id><?supplied-pmid 13758466?><pub-id pub-id-type="pmid">13758466</pub-id>
</element-citation></ref><ref id="CR2"><label>2.</label><mixed-citation publication-type="other">Das, J. P. <italic>Verbal conditioning and behaviour</italic> (Elsevier, 2014).</mixed-citation></ref><ref id="CR3"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bassett</surname><given-names>M</given-names></name><name><surname>Warne</surname><given-names>C</given-names></name></person-group><article-title>On the lapse of verbal meaning with repetition</article-title><source>A. J. Psychol.</source><year>1919</year><volume>30</volume><fpage>415</fpage><lpage>418</lpage><pub-id pub-id-type="doi">10.2307/1413679</pub-id></element-citation></ref><ref id="CR4"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Esposito</surname><given-names>NJ</given-names></name><name><surname>Pelton</surname><given-names>LH</given-names></name></person-group><article-title>Review of the measurement of semantic satiation</article-title><source>Psychol. Bull.</source><year>1971</year><volume>75</volume><fpage>330</fpage><pub-id pub-id-type="doi">10.1037/h0031001</pub-id></element-citation></ref><ref id="CR5"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Di Bona</surname><given-names>E</given-names></name></person-group><article-title>On hearing meanings. reflections on the method of contrast, adaptational effects, and semantic satiation</article-title><source>Rivista di filosofia</source><year>2020</year><volume>111</volume><fpage>215</fpage><lpage>237</lpage></element-citation></ref><ref id="CR6"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Black</surname><given-names>SR</given-names></name><name><surname>Wood</surname><given-names>MM</given-names></name><name><surname>Choi</surname><given-names>J</given-names></name><name><surname>Jackson</surname><given-names>B-S</given-names></name><name><surname>Evans</surname><given-names>TZ</given-names></name></person-group><article-title>Adult age differences in sensitivity to semantic satiation</article-title><source>Exp. Aging Res.</source><year>2023</year><volume>49</volume><fpage>152</fpage><lpage>172</lpage><pub-id pub-id-type="doi">10.1080/0361073X.2022.2048585</pub-id><?supplied-pmid 35287550?><pub-id pub-id-type="pmid">35287550</pub-id>
</element-citation></ref><ref id="CR7"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Glanzer</surname><given-names>M</given-names></name></person-group><article-title>Stimulus satiation: an explanation of spontaneous alternation and related phenomena</article-title><source>Psychol. Rev.</source><year>1953</year><volume>60</volume><fpage>257</fpage><pub-id pub-id-type="doi">10.1037/h0062718</pub-id><?supplied-pmid 13089004?><pub-id pub-id-type="pmid">13089004</pub-id>
</element-citation></ref><ref id="CR8"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Glanzer</surname><given-names>M</given-names></name></person-group><article-title>Curiosity, exploratory drive, and stimulus satiation</article-title><source>Psychol. Bull.</source><year>1958</year><volume>55</volume><fpage>302</fpage><pub-id pub-id-type="doi">10.1037/h0044731</pub-id><?supplied-pmid 13591450?><pub-id pub-id-type="pmid">13591450</pub-id>
</element-citation></ref><ref id="CR9"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Popham</surname><given-names>SF</given-names></name><etal/></person-group><article-title>Visual and linguistic semantic representations are aligned at the border of human visual cortex</article-title><source>Nat. Neurosci.</source><year>2021</year><volume>24</volume><fpage>1628</fpage><lpage>1636</lpage><pub-id pub-id-type="doi">10.1038/s41593-021-00921-6</pub-id><?supplied-pmid 34711960?><pub-id pub-id-type="pmid">34711960</pub-id>
</element-citation></ref><ref id="CR10"><label>10.</label><mixed-citation publication-type="other">Severance, E. &#x00026; Washburn, M. F. The loss of associative power in words after long fixation. <italic>Am J. Psychol.</italic><bold>18</bold>, 182&#x02013;186 (1907).</mixed-citation></ref><ref id="CR11"><label>11.</label><mixed-citation publication-type="other">Osgood, C. E., Suci, G. J. &#x00026; Tannenbaum, P. H.<italic>The measurement of meaning</italic>. 47 (University of Illinois Press, 1957).</mixed-citation></ref><ref id="CR12"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>DE</given-names></name><name><surname>Raygor</surname><given-names>AL</given-names></name></person-group><article-title>Verbal satiation and personality</article-title><source>J. Abnormal Soc. Psychol.</source><year>1956</year><volume>52</volume><fpage>323</fpage><pub-id pub-id-type="doi">10.1037/h0041334</pub-id></element-citation></ref><ref id="CR13"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yelen</surname><given-names>DR</given-names></name><name><surname>Schulz</surname><given-names>RW</given-names></name></person-group><article-title>Verbal satiation?</article-title><source>J. Verb. Learning Verbal Behav.</source><year>1963</year><volume>1</volume><fpage>372</fpage><lpage>377</lpage><pub-id pub-id-type="doi">10.1016/S0022-5371(63)80020-1</pub-id></element-citation></ref><ref id="CR14"><label>14.</label><mixed-citation publication-type="other">Neely, J. H. The effects of visual and verbal satiation on a lexical decision task. <italic>Am. J. Psychol.</italic><bold>90</bold>, 447&#x02013;459 (1977).</mixed-citation></ref><ref id="CR15"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jakobovits</surname><given-names>LA</given-names></name></person-group><article-title>Semantic satiation in concept formation</article-title><source>Psychol. Rep.</source><year>1965</year><volume>17</volume><fpage>113</fpage><lpage>114</lpage><pub-id pub-id-type="doi">10.2466/pr0.1965.17.1.113</pub-id><?supplied-pmid 5826453?><pub-id pub-id-type="pmid">5826453</pub-id>
</element-citation></ref><ref id="CR16"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fischler</surname><given-names>I</given-names></name><name><surname>Bloom</surname><given-names>PA</given-names></name><name><surname>Childers</surname><given-names>DG</given-names></name><name><surname>Roucos</surname><given-names>SE</given-names></name><name><surname>Perry Jr</surname><given-names>NW</given-names></name></person-group><article-title>Brain potentials related to stages of sentence verification</article-title><source>Psychophysiology</source><year>1983</year><volume>20</volume><fpage>400</fpage><lpage>409</lpage><pub-id pub-id-type="doi">10.1111/j.1469-8986.1983.tb00920.x</pub-id><?supplied-pmid 6356204?><pub-id pub-id-type="pmid">6356204</pub-id>
</element-citation></ref><ref id="CR17"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Holcomb</surname><given-names>PJ</given-names></name><name><surname>Neville</surname><given-names>HJ</given-names></name></person-group><article-title>Auditory and visual semantic priming in lexical decision: A comparison using event-related brain potentials</article-title><source>Lang. Cogn. Processes</source><year>1990</year><volume>5</volume><fpage>281</fpage><lpage>312</lpage><pub-id pub-id-type="doi">10.1080/01690969008407065</pub-id></element-citation></ref><ref id="CR18"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kutas</surname><given-names>M</given-names></name><name><surname>Hillyard</surname><given-names>SA</given-names></name></person-group><article-title>An electrophysiological probe of incidental semantic association</article-title><source>J. Cogn. Neurosci.</source><year>1989</year><volume>1</volume><fpage>38</fpage><lpage>49</lpage><pub-id pub-id-type="doi">10.1162/jocn.1989.1.1.38</pub-id><?supplied-pmid 23968409?><pub-id pub-id-type="pmid">23968409</pub-id>
</element-citation></ref><ref id="CR19"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tian</surname><given-names>X</given-names></name><name><surname>Huber</surname><given-names>DE</given-names></name></person-group><article-title>Testing an associative account of semantic satiation</article-title><source>Cogn. Psychol.</source><year>2010</year><volume>60</volume><fpage>267</fpage><lpage>290</lpage><pub-id pub-id-type="doi">10.1016/j.cogpsych.2010.01.003</pub-id><?supplied-pmid 20156620?><pub-id pub-id-type="pmid">20156620</pub-id>
</element-citation></ref><ref id="CR20"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frenck-Mestre</surname><given-names>C</given-names></name><name><surname>Besson</surname><given-names>M</given-names></name><name><surname>Pynte</surname><given-names>J</given-names></name></person-group><article-title>Finding the locus of semantic satiation: an electrophysiological attempt</article-title><source>Brain Lang.</source><year>1997</year><volume>57</volume><fpage>406</fpage><lpage>422</lpage><pub-id pub-id-type="doi">10.1006/brln.1997.1756</pub-id><?supplied-pmid 9126424?><pub-id pub-id-type="pmid">9126424</pub-id>
</element-citation></ref><ref id="CR21"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kounios</surname><given-names>J</given-names></name><name><surname>Kotz</surname><given-names>SA</given-names></name><name><surname>Holcomb</surname><given-names>PJ</given-names></name></person-group><article-title>On the locus of the semantic satiation effect: Evidence from event-related brain potentials</article-title><source>Memory Cogn.</source><year>2000</year><volume>28</volume><fpage>1366</fpage><lpage>1377</lpage><pub-id pub-id-type="doi">10.3758/BF03211837</pub-id></element-citation></ref><ref id="CR22"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Black</surname><given-names>SR</given-names></name></person-group><article-title>Review of semantic satiation</article-title><source>Adv. Psychol. Res.</source><year>2003</year><volume>26</volume><fpage>63</fpage><lpage>74</lpage></element-citation></ref><ref id="CR23"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tian</surname><given-names>X</given-names></name><name><surname>Poeppel</surname><given-names>D</given-names></name><name><surname>Huber</surname><given-names>DE</given-names></name></person-group><article-title>Topotoolbox: using sensor topography to calculate psychologically meaningful measures from event-related eeg/meg</article-title><source>Comput. Intelligence Neurosci.</source><year>2011</year><volume>2011</volume><fpage>1</fpage><lpage>8</lpage><pub-id pub-id-type="doi">10.1155/2011/674605</pub-id></element-citation></ref><ref id="CR24"><label>24.</label><mixed-citation publication-type="other">Jakobovits, L. et al. <italic>Effects of Repeated Stimulation on Cognitive Aspects of Behavior: Some Experiments on the Phenomenon of Semantic Satiation.</italic> (McGill University, 1962).</mixed-citation></ref><ref id="CR25"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Seth</surname><given-names>AK</given-names></name><name><surname>Dienes</surname><given-names>Z</given-names></name><name><surname>Cleeremans</surname><given-names>A</given-names></name><name><surname>Overgaard</surname><given-names>M</given-names></name><name><surname>Pessoa</surname><given-names>L</given-names></name></person-group><article-title>Measuring consciousness: relating behavioural and neurophysiological approaches</article-title><source>Trends Cogn. Sci.</source><year>2008</year><volume>12</volume><fpage>314</fpage><lpage>321</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2008.04.008</pub-id><?supplied-pmid 18606562?><pub-id pub-id-type="pmid">18606562</pub-id>
</element-citation></ref><ref id="CR26"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kristj&#x000e1;nsson</surname><given-names>&#x000c1;</given-names></name><name><surname>Egeth</surname><given-names>H</given-names></name></person-group><article-title>How feature integration theory integrated cognitive psychology, neurophysiology, and psychophysics</article-title><source>Attention, Percep, Psychophys.</source><year>2020</year><volume>82</volume><fpage>7</fpage><lpage>23</lpage><pub-id pub-id-type="doi">10.3758/s13414-019-01803-7</pub-id></element-citation></ref><ref id="CR27"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Darvishi</surname><given-names>A</given-names></name><name><surname>Khosravi</surname><given-names>H</given-names></name><name><surname>Sadiq</surname><given-names>S</given-names></name><name><surname>Weber</surname><given-names>B</given-names></name></person-group><article-title>Neurophysiological measurements in higher education: A systematic literature review</article-title><source>Int. J. Artificial Intelligence Edu.</source><year>2022</year><volume>32</volume><fpage>413</fpage><lpage>453</lpage><pub-id pub-id-type="doi">10.1007/s40593-021-00256-0</pub-id></element-citation></ref><ref id="CR28"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buzs&#x000e1;ki</surname><given-names>G</given-names></name><name><surname>McKenzie</surname><given-names>S</given-names></name><name><surname>Davachi</surname><given-names>L</given-names></name></person-group><article-title>Neurophysiology of remembering</article-title><source>Ann. Rev. Psychol.</source><year>2022</year><volume>73</volume><fpage>187</fpage><lpage>215</lpage><pub-id pub-id-type="doi">10.1146/annurev-psych-021721-110002</pub-id><pub-id pub-id-type="pmid">34535061</pub-id>
</element-citation></ref><ref id="CR29"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maest&#x000fa;</surname><given-names>F</given-names></name><name><surname>de Haan</surname><given-names>W</given-names></name><name><surname>Busche</surname><given-names>MA</given-names></name><name><surname>DeFelipe</surname><given-names>J</given-names></name></person-group><article-title>Neuronal excitation/inhibition imbalance: core element of a translational perspective on Alzheimer pathophysiology</article-title><source>Ageing Res. Rev.</source><year>2021</year><volume>69</volume><fpage>101372</fpage><pub-id pub-id-type="doi">10.1016/j.arr.2021.101372</pub-id><?supplied-pmid 34029743?><pub-id pub-id-type="pmid">34029743</pub-id>
</element-citation></ref><ref id="CR30"><label>30.</label><mixed-citation publication-type="other">Liu, J., Lian, J., Sprott, J. C., Liu, Q. &#x00026; Ma, Y. The butterfly effect in primary visual cortex. In <italic>IEEE Transactions on Computers</italic>, Vol. 71, 2803&#x02013;2815 (IEEE, 2022).</mixed-citation></ref><ref id="CR31"><label>31.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Im</surname><given-names>HY</given-names></name><name><surname>Cushing</surname><given-names>CA</given-names></name><name><surname>Ward</surname><given-names>N</given-names></name><name><surname>Kveraga</surname><given-names>K</given-names></name></person-group><article-title>Differential neurodynamics and connectivity in the dorsal and ventral visual pathways during perception of emotional crowds and individuals: a meg study</article-title><source>Cogn., Affective, Behav. Neurosci.</source><year>2021</year><volume>21</volume><fpage>776</fpage><lpage>792</lpage><pub-id pub-id-type="doi">10.3758/s13415-021-00880-2</pub-id></element-citation></ref><ref id="CR32"><label>32.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Johnson</surname><given-names>JL</given-names></name><name><surname>Padgett</surname><given-names>ML</given-names></name></person-group><article-title>Pcnn models and applications</article-title><source>IEEE Trans. Neural Netw.</source><year>1999</year><volume>10</volume><fpage>480</fpage><lpage>498</lpage><pub-id pub-id-type="doi">10.1109/72.761706</pub-id><?supplied-pmid 18252547?><pub-id pub-id-type="pmid">18252547</pub-id>
</element-citation></ref><ref id="CR33"><label>33.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Homann</surname><given-names>J</given-names></name><name><surname>Koay</surname><given-names>SA</given-names></name><name><surname>Chen</surname><given-names>KS</given-names></name><name><surname>Tank</surname><given-names>DW</given-names></name><name><surname>Berry</surname><given-names>MJ</given-names></name></person-group><article-title>Novel stimuli evoke excess activity in the mouse primary visual cortex</article-title><source>Proc. Natl. Acad. Sci.</source><year>2022</year><volume>119</volume><fpage>e2108882119</fpage><pub-id pub-id-type="doi">10.1073/pnas.2108882119</pub-id><?supplied-pmid 35101916?><pub-id pub-id-type="pmid">35101916</pub-id>
</element-citation></ref><ref id="CR34"><label>34.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Himmelberg</surname><given-names>MM</given-names></name><name><surname>Winawer</surname><given-names>J</given-names></name><name><surname>Carrasco</surname><given-names>M</given-names></name></person-group><article-title>Linking individual differences in human primary visual cortex to contrast sensitivity around the visual field</article-title><source>Nat. Commun.</source><year>2022</year><volume>13</volume><fpage>3309</fpage><pub-id pub-id-type="doi">10.1038/s41467-022-31041-9</pub-id><?supplied-pmid 35697680?><pub-id pub-id-type="pmid">35697680</pub-id>
</element-citation></ref><ref id="CR35"><label>35.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sadaghiani</surname><given-names>S</given-names></name><name><surname>Brookes</surname><given-names>MJ</given-names></name><name><surname>Baillet</surname><given-names>S</given-names></name></person-group><article-title>Connectomics of human electrophysiology</article-title><source>NeuroImage</source><year>2022</year><volume>247</volume><fpage>118788</fpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2021.118788</pub-id><?supplied-pmid 34906715?><pub-id pub-id-type="pmid">34906715</pub-id>
</element-citation></ref><ref id="CR36"><label>36.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Woolnough</surname><given-names>O</given-names></name><etal/></person-group><article-title>Spatiotemporal dynamics of orthographic and lexical processing in the ventral visual pathway</article-title><source>Nat. Hum. Behav.</source><year>2021</year><volume>5</volume><fpage>389</fpage><lpage>398</lpage><pub-id pub-id-type="doi">10.1038/s41562-020-00982-w</pub-id><?supplied-pmid 33257877?><pub-id pub-id-type="pmid">33257877</pub-id>
</element-citation></ref><ref id="CR37"><label>37.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ayzenberg</surname><given-names>V</given-names></name><name><surname>Behrmann</surname><given-names>M</given-names></name></person-group><article-title>Does the brain&#x02019;s ventral visual pathway compute object shape?</article-title><source>Trends Cogn. Sci.</source><year>2022</year><volume>26</volume><fpage>1119</fpage><lpage>1132</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2022.09.019</pub-id><?supplied-pmid 36272937?><pub-id pub-id-type="pmid">36272937</pub-id>
</element-citation></ref><ref id="CR38"><label>38.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tong</surname><given-names>F</given-names></name></person-group><article-title>Primary visual cortex and visual awareness</article-title><source>Nat. Rev. Neurosci.</source><year>2003</year><volume>4</volume><fpage>219</fpage><lpage>229</lpage><pub-id pub-id-type="doi">10.1038/nrn1055</pub-id><?supplied-pmid 12612634?><pub-id pub-id-type="pmid">12612634</pub-id>
</element-citation></ref><ref id="CR39"><label>39.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marchesi</surname><given-names>N</given-names></name><name><surname>Fahmideh</surname><given-names>F</given-names></name><name><surname>Boschi</surname><given-names>F</given-names></name><name><surname>Pascale</surname><given-names>A</given-names></name><name><surname>Barbieri</surname><given-names>A</given-names></name></person-group><article-title>Ocular neurodegenerative diseases: interconnection between retina and cortical areas</article-title><source>Cells</source><year>2021</year><volume>10</volume><fpage>2394</fpage><pub-id pub-id-type="doi">10.3390/cells10092394</pub-id><?supplied-pmid 34572041?><pub-id pub-id-type="pmid">34572041</pub-id>
</element-citation></ref><ref id="CR40"><label>40.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Masland</surname><given-names>RH</given-names></name></person-group><article-title>The neuronal organization of the retina</article-title><source>Neuron</source><year>2012</year><volume>76</volume><fpage>266</fpage><lpage>280</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.10.002</pub-id><?supplied-pmid 23083731?><pub-id pub-id-type="pmid">23083731</pub-id>
</element-citation></ref><ref id="CR41"><label>41.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O&#x02019;Connor</surname><given-names>DH</given-names></name><name><surname>Fukui</surname><given-names>MM</given-names></name><name><surname>Pinsk</surname><given-names>MA</given-names></name><name><surname>Kastner</surname><given-names>S</given-names></name></person-group><article-title>Attention modulates responses in the human lateral geniculate nucleus</article-title><source>Nat. Neurosci.</source><year>2002</year><volume>5</volume><fpage>1203</fpage><lpage>1209</lpage><pub-id pub-id-type="doi">10.1038/nn957</pub-id><?supplied-pmid 12379861?><pub-id pub-id-type="pmid">12379861</pub-id>
</element-citation></ref><ref id="CR42"><label>42.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ta&#x0015f;c&#x00131;</surname><given-names>B</given-names></name><etal/></person-group><article-title>A new lateral geniculate nucleus pattern-based environmental sound classification using a new large sound dataset</article-title><source>Appl. Acoustics</source><year>2022</year><volume>196</volume><fpage>108897</fpage><pub-id pub-id-type="doi">10.1016/j.apacoust.2022.108897</pub-id></element-citation></ref><ref id="CR43"><label>43.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lafer-Sousa</surname><given-names>R</given-names></name><etal/></person-group><article-title>Behavioral detectability of optogenetic stimulation of inferior temporal cortex varies with the size of concurrently viewed objects</article-title><source>Curr. Res. Neurobiol.</source><year>2023</year><volume>4</volume><fpage>100063</fpage><pub-id pub-id-type="doi">10.1016/j.crneur.2022.100063</pub-id><?supplied-pmid 36578652?><pub-id pub-id-type="pmid">36578652</pub-id>
</element-citation></ref><ref id="CR44"><label>44.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Belliveau</surname><given-names>J</given-names></name><etal/></person-group><article-title>Functional mapping of the human visual cortex by magnetic resonance imaging</article-title><source>Science</source><year>1991</year><volume>254</volume><fpage>716</fpage><lpage>719</lpage><pub-id pub-id-type="doi">10.1126/science.1948051</pub-id><?supplied-pmid 1948051?><pub-id pub-id-type="pmid">1948051</pub-id>
</element-citation></ref><ref id="CR45"><label>45.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Azadi</surname><given-names>R</given-names></name><etal/></person-group><article-title>Image-dependence of the detectability of optogenetic stimulation in macaque inferotemporal cortex</article-title><source>Curr. Biol.</source><year>2023</year><volume>33</volume><fpage>581</fpage><lpage>588</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2022.12.021</pub-id><?supplied-pmid 36610394?><pub-id pub-id-type="pmid">36610394</pub-id>
</element-citation></ref><ref id="CR46"><label>46.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Knudsen</surname><given-names>EI</given-names></name></person-group><article-title>Evolution of neural processing for visual perception in vertebrates</article-title><source>J. Comparat. Neurol.</source><year>2020</year><volume>528</volume><fpage>2888</fpage><lpage>2901</lpage><pub-id pub-id-type="doi">10.1002/cne.24871</pub-id></element-citation></ref><ref id="CR47"><label>47.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>David</surname><given-names>SV</given-names></name><name><surname>Hayden</surname><given-names>BY</given-names></name><name><surname>Gallant</surname><given-names>JL</given-names></name></person-group><article-title>Spectral receptive field properties explain shape selectivity in area v4</article-title><source>J. Neurophysiol.</source><year>2006</year><volume>96</volume><fpage>3492</fpage><lpage>3505</lpage><pub-id pub-id-type="doi">10.1152/jn.00575.2006</pub-id><?supplied-pmid 16987926?><pub-id pub-id-type="pmid">16987926</pub-id>
</element-citation></ref><ref id="CR48"><label>48.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Parmentier</surname><given-names>FB</given-names></name><name><surname>Pacheco-Unguetti</surname><given-names>AP</given-names></name><name><surname>Valero</surname><given-names>S</given-names></name></person-group><article-title>Food words distract the hungry: Evidence of involuntary semantic processing of task-irrelevant but biologically-relevant unexpected auditory words</article-title><source>PLoS One</source><year>2018</year><volume>13</volume><fpage>e0190644</fpage><pub-id pub-id-type="doi">10.1371/journal.pone.0190644</pub-id><?supplied-pmid 29300763?><pub-id pub-id-type="pmid">29300763</pub-id>
</element-citation></ref><ref id="CR49"><label>49.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>LC</given-names></name></person-group><article-title>Semantic satiation affects category membership decision time but not lexical priming</article-title><source>Memory Cogn.</source><year>1984</year><volume>12</volume><fpage>483</fpage><lpage>488</lpage><pub-id pub-id-type="doi">10.3758/BF03198310</pub-id></element-citation></ref><ref id="CR50"><label>50.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Deng</surname><given-names>L</given-names></name></person-group><article-title>The mnist database of handwritten digit images for machine learning research</article-title><source>IEEE Sig. Proc. Magaz.</source><year>2012</year><volume>29</volume><fpage>141</fpage><lpage>142</lpage><pub-id pub-id-type="doi">10.1109/MSP.2012.2211477</pub-id></element-citation></ref><ref id="CR51"><label>51.</label><mixed-citation publication-type="other">Xiao, H., Rasul, K. &#x00026; Vollgraf, R. Fashion-MNIST: a novel image dataset for benchmarking machine learning algorithms. Preprint at <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1708.07747">https://arxiv.org/abs/1708.07747</ext-link> (2017).</mixed-citation></ref><ref id="CR52"><label>52.</label><mixed-citation publication-type="other">Zhong, Y. et al. Intraq: Learning synthetic images with intra-class heterogeneity for zero-shot network quantization. In <italic>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</italic>, 12339&#x02013;12348 (2022).</mixed-citation></ref><ref id="CR53"><label>53.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Str&#x000f6;berg</surname><given-names>K</given-names></name><name><surname>Andersen</surname><given-names>LM</given-names></name><name><surname>Wiens</surname><given-names>S</given-names></name></person-group><article-title>Electrocortical n400 effects of semantic satiation</article-title><source>Front. Psychol.</source><year>2017</year><volume>8</volume><fpage>2117</fpage><pub-id pub-id-type="doi">10.3389/fpsyg.2017.02117</pub-id><?supplied-pmid 29375411?><pub-id pub-id-type="pmid">29375411</pub-id>
</element-citation></ref><ref id="CR54"><label>54.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kutas</surname><given-names>M</given-names></name><name><surname>Federmeier</surname><given-names>KD</given-names></name></person-group><article-title>Thirty years and counting: finding meaning in the n400 component of the event-related brain potential (erp)</article-title><source>Ann. Rev. Psychol.</source><year>2011</year><volume>62</volume><fpage>621</fpage><lpage>647</lpage><pub-id pub-id-type="doi">10.1146/annurev.psych.093008.131123</pub-id><pub-id pub-id-type="pmid">20809790</pub-id>
</element-citation></ref><ref id="CR55"><label>55.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Duncan</surname><given-names>CC</given-names></name><etal/></person-group><article-title>Event-related potentials in clinical research: guidelines for eliciting, recording, and quantifying mismatch negativity, p300, and n400</article-title><source>Clin. Neurophysiol.</source><year>2009</year><volume>120</volume><fpage>1883</fpage><lpage>1908</lpage><pub-id pub-id-type="doi">10.1016/j.clinph.2009.07.045</pub-id><?supplied-pmid 19796989?><pub-id pub-id-type="pmid">19796989</pub-id>
</element-citation></ref><ref id="CR56"><label>56.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Calzavarini</surname><given-names>F</given-names></name></person-group><article-title>The empirical status of semantic perceptualism</article-title><source>Mind Lang.</source><year>2023</year><volume>38</volume><fpage>1000</fpage><lpage>1020</lpage><pub-id pub-id-type="doi">10.1111/mila.12444</pub-id></element-citation></ref><ref id="CR57"><label>57.</label><mixed-citation publication-type="other">Xu, Q., Wang, W., Yang, Y. &#x00026; Li, W. Effects of emotion words activation and satiation on facial expression perception: evidence from behavioral and ERP investigations. <italic>Front. Psychiatry</italic><bold>14</bold>, 1192450 (2023).</mixed-citation></ref><ref id="CR58"><label>58.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Amoruso</surname><given-names>L</given-names></name><etal/></person-group><article-title>N400 erps for actions: building meaning in context</article-title><source>Front. Hum. Neurosci.</source><year>2013</year><volume>7</volume><fpage>57</fpage><pub-id pub-id-type="doi">10.3389/fnhum.2013.00057</pub-id><?supplied-pmid 23459873?><pub-id pub-id-type="pmid">23459873</pub-id>
</element-citation></ref><ref id="CR59"><label>59.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lau</surname><given-names>EF</given-names></name><name><surname>Phillips</surname><given-names>C</given-names></name><name><surname>Poeppel</surname><given-names>D</given-names></name></person-group><article-title>A cortical network for semantics:(de) constructing the n400</article-title><source>Nat. Rev. Neurosci.</source><year>2008</year><volume>9</volume><fpage>920</fpage><lpage>933</lpage><pub-id pub-id-type="doi">10.1038/nrn2532</pub-id><?supplied-pmid 19020511?><pub-id pub-id-type="pmid">19020511</pub-id>
</element-citation></ref><ref id="CR60"><label>60.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>L</given-names></name><name><surname>Klein</surname><given-names>R</given-names></name></person-group><article-title>Evidence for semantic satiation: Repeating a category slows subsequent semantic processing</article-title><source>J. Exp. Psychol.: Learning, Memory, and Cognition</source><year>1990</year><volume>16</volume><fpage>852</fpage></element-citation></ref><ref id="CR61"><label>61.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Collins</surname><given-names>AM</given-names></name><name><surname>Quillian</surname><given-names>MR</given-names></name></person-group><article-title>Retrieval time from semantic memory</article-title><source>J. Verbal Learning Verbal Behav.</source><year>1969</year><volume>8</volume><fpage>240</fpage><lpage>247</lpage><pub-id pub-id-type="doi">10.1016/S0022-5371(69)80069-1</pub-id></element-citation></ref><ref id="CR62"><label>62.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eckhorn</surname><given-names>R</given-names></name><name><surname>Reitboeck</surname><given-names>HJ</given-names></name><name><surname>Arndt</surname><given-names>M</given-names></name><name><surname>Dicke</surname><given-names>P</given-names></name></person-group><article-title>Feature linking via synchronization among distributed assemblies: Simulations of results from cat visual cortex</article-title><source>Neural Comput.</source><year>1990</year><volume>2</volume><fpage>293</fpage><lpage>307</lpage><pub-id pub-id-type="doi">10.1162/neco.1990.2.3.293</pub-id></element-citation></ref><ref id="CR63"><label>63.</label><mixed-citation publication-type="other">Ranganath, H., Kuntimad, G. &#x00026; Johnson, J. Pulse coupled neural networks for image processing. In <italic>Proceedings IEEE Southeastcon&#x02019;95. Visualize the Future</italic>, <ext-link ext-link-type="uri" xlink:href="https://ieeexplore.ieee.org/abstract/document/513053">https://ieeexplore.ieee.org/abstract/document/513053</ext-link> (IEEE, 1995).</mixed-citation></ref><ref id="CR64"><label>64.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Siegel</surname><given-names>R</given-names></name></person-group><article-title>Non-linear dynamical system theory and primary visual cortical processing</article-title><source>Physica D: Nonlinear Phenomena</source><year>1990</year><volume>42</volume><fpage>385</fpage><lpage>395</lpage><pub-id pub-id-type="doi">10.1016/0167-2789(90)90090-C</pub-id></element-citation></ref><ref id="CR65"><label>65.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Basha</surname><given-names>SS</given-names></name><name><surname>Dubey</surname><given-names>SR</given-names></name><name><surname>Pulabaigari</surname><given-names>V</given-names></name><name><surname>Mukherjee</surname><given-names>S</given-names></name></person-group><article-title>Impact of fully connected layers on performance of convolutional neural networks for image classification</article-title><source>Neurocomputing</source><year>2020</year><volume>378</volume><fpage>112</fpage><lpage>119</lpage><pub-id pub-id-type="doi">10.1016/j.neucom.2019.10.008</pub-id></element-citation></ref><ref id="CR66"><label>66.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yeh</surname><given-names>C-H</given-names></name><name><surname>Lin</surname><given-names>M-H</given-names></name><name><surname>Chang</surname><given-names>P-C</given-names></name><name><surname>Kang</surname><given-names>L-W</given-names></name></person-group><article-title>Enhanced visual attention-guided deep neural networks for image classification</article-title><source>IEEE Access</source><year>2020</year><volume>8</volume><fpage>163447</fpage><lpage>163457</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2020.3021729</pub-id></element-citation></ref><ref id="CR67"><label>67.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>R&#x000e1;cz</surname><given-names>A</given-names></name><name><surname>Bajusz</surname><given-names>D</given-names></name><name><surname>H&#x000e9;berger</surname><given-names>K</given-names></name></person-group><article-title>Effect of dataset size and train/test split ratios in qsar/qspr multiclass classification</article-title><source>Molecules</source><year>2021</year><volume>26</volume><fpage>1111</fpage><pub-id pub-id-type="doi">10.3390/molecules26041111</pub-id><?supplied-pmid 33669834?><pub-id pub-id-type="pmid">33669834</pub-id>
</element-citation></ref></ref-list></back></article>