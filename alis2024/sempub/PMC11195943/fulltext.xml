<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd"> 
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN?><?DTDIdentifier.IdentifierType public?><?SourceDTD.DTDName JATS-journalpublishing1.dtd?><?SourceDTD.Version 39.96?><?ConverterInfo.XSLTName jats2jats3.xsl?><?ConverterInfo.Version 1?><?subarticle pone.0299623.r001?><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">PLoS One</journal-id><journal-id journal-id-type="iso-abbrev">PLoS One</journal-id><journal-id journal-id-type="publisher-id">plos</journal-id><journal-title-group><journal-title>PLOS ONE</journal-title></journal-title-group><issn pub-type="epub">1932-6203</issn><publisher><publisher-name>Public Library of Science</publisher-name><publisher-loc>San Francisco, CA USA</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmcid">11195943</article-id><article-id pub-id-type="doi">10.1371/journal.pone.0299623</article-id><article-id pub-id-type="publisher-id">PONE-D-24-05839</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and Information Sciences</subject><subj-group><subject>Software Engineering</subject><subj-group><subject>Preprocessing</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Engineering and Technology</subject><subj-group><subject>Software Engineering</subject><subj-group><subject>Preprocessing</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and Analysis Methods</subject><subj-group><subject>Imaging Techniques</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and Health Sciences</subject><subj-group><subject>Diagnostic Medicine</subject><subj-group><subject>Diagnostic Radiology</subject><subj-group><subject>Bone Imaging</subject><subj-group><subject>X-Ray Radiography</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and Analysis Methods</subject><subj-group><subject>Imaging Techniques</subject><subj-group><subject>Diagnostic Radiology</subject><subj-group><subject>Bone Imaging</subject><subj-group><subject>X-Ray Radiography</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and Health Sciences</subject><subj-group><subject>Radiology and Imaging</subject><subj-group><subject>Diagnostic Radiology</subject><subj-group><subject>Bone Imaging</subject><subj-group><subject>X-Ray Radiography</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and Health Sciences</subject><subj-group><subject>Diagnostic Medicine</subject><subj-group><subject>Diagnostic Radiology</subject><subj-group><subject>X-Ray Radiography</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and Analysis Methods</subject><subj-group><subject>Imaging Techniques</subject><subj-group><subject>Diagnostic Radiology</subject><subj-group><subject>X-Ray Radiography</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and Health Sciences</subject><subj-group><subject>Radiology and Imaging</subject><subj-group><subject>Diagnostic Radiology</subject><subj-group><subject>X-Ray Radiography</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Cardiovascular Anatomy</subject><subj-group><subject>Heart</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and Health Sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Cardiovascular Anatomy</subject><subj-group><subject>Heart</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical Sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Applied Mathematics</subject><subj-group><subject>Algorithms</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and Analysis Methods</subject><subj-group><subject>Simulation and Modeling</subject><subj-group><subject>Algorithms</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and Health Sciences</subject><subj-group><subject>Diagnostic Medicine</subject><subj-group><subject>Diagnostic Radiology</subject><subj-group><subject>Pulmonary Imaging</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and Analysis Methods</subject><subj-group><subject>Imaging Techniques</subject><subj-group><subject>Diagnostic Radiology</subject><subj-group><subject>Pulmonary Imaging</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and Health Sciences</subject><subj-group><subject>Radiology and Imaging</subject><subj-group><subject>Diagnostic Radiology</subject><subj-group><subject>Pulmonary Imaging</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical Sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Optimization</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical Sciences</subject><subj-group><subject>Physics</subject><subj-group><subject>Thermodynamics</subject><subj-group><subject>Entropy</subject></subj-group></subj-group></subj-group></subj-group></article-categories><title-group><article-title>Enhancing semantic segmentation in chest X-ray images through image preprocessing: ps-KDE for pixel-wise substitution by kernel density estimation</article-title><alt-title alt-title-type="running-head">ps-KDE and segmentation</alt-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Wang</surname><given-names>Yuanchen</given-names></name><role content-type="http://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role content-type="http://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role content-type="http://credit.niso.org/contributor-roles/methodology/">Methodology</role><role content-type="http://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#x02013; original draft</role><role content-type="http://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><xref rid="aff001" ref-type="aff"/></contrib><contrib contrib-type="author"><name><surname>Guo</surname><given-names>Yujie</given-names></name><role content-type="http://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role content-type="http://credit.niso.org/contributor-roles/resources/">Resources</role><role content-type="http://credit.niso.org/contributor-roles/visualization/">Visualization</role><role content-type="http://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#x02013; original draft</role><xref rid="aff001" ref-type="aff"/></contrib><contrib contrib-type="author"><name><surname>Wang</surname><given-names>Ziqi</given-names></name><role content-type="http://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role content-type="http://credit.niso.org/contributor-roles/resources/">Resources</role><role content-type="http://credit.niso.org/contributor-roles/visualization/">Visualization</role><role content-type="http://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#x02013; original draft</role><xref rid="aff001" ref-type="aff"/></contrib><contrib contrib-type="author"><name><surname>Yu</surname><given-names>Linzi</given-names></name><role content-type="http://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role content-type="http://credit.niso.org/contributor-roles/resources/">Resources</role><role content-type="http://credit.niso.org/contributor-roles/visualization/">Visualization</role><role content-type="http://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#x02013; original draft</role><xref rid="aff001" ref-type="aff"/></contrib><contrib contrib-type="author"><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-9455-8106</contrib-id><name><surname>Yan</surname><given-names>Yujie</given-names></name><role content-type="http://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role content-type="http://credit.niso.org/contributor-roles/software/">Software</role><role content-type="http://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><xref rid="aff001" ref-type="aff"/></contrib><contrib contrib-type="author" corresp="yes"><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-8024-2629</contrib-id><name><surname>Gu</surname><given-names>Zifan</given-names></name><role content-type="http://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role content-type="http://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role content-type="http://credit.niso.org/contributor-roles/methodology/">Methodology</role><role content-type="http://credit.niso.org/contributor-roles/project-administration/">Project administration</role><role content-type="http://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#x02013; original draft</role><role content-type="http://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><xref rid="cor001" ref-type="corresp">*</xref><xref rid="aff001" ref-type="aff"/></contrib></contrib-group><aff id="aff001">
<addr-line>Department of Biomedical Informatics, Harvard Medical School, Boston, Massachusetts, United States of America</addr-line>
</aff><contrib-group><contrib contrib-type="editor"><name><surname>Shaikh</surname><given-names>Asadullah</given-names></name><role>Editor</role><xref rid="edit1" ref-type="aff"/></contrib></contrib-group><aff id="edit1">
<addr-line>Najran University College of Computer Science and Information Systems, SAUDI ARABIA</addr-line>
</aff><author-notes><fn fn-type="COI-statement" id="coi001"><p><bold>Competing Interests: </bold>The authors have declared that no competing interests exist.</p></fn><corresp id="cor001">* E-mail: <email>zifan.gu@utsouthwestern.edu</email></corresp></author-notes><pub-date pub-type="epub"><day>24</day><month>6</month><year>2024</year></pub-date><pub-date pub-type="collection"><year>2024</year></pub-date><volume>19</volume><issue>6</issue><elocation-id>e0299623</elocation-id><history><date date-type="received"><day>13</day><month>2</month><year>2024</year></date><date date-type="accepted"><day>8</day><month>5</month><year>2024</year></date></history><permissions><copyright-statement>&#x000a9; 2024 Wang et al</copyright-statement><copyright-year>2024</copyright-year><copyright-holder>Wang et al</copyright-holder><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="pone.0299623.pdf"/><abstract><sec id="sec001"><title>Background</title><p>In medical imaging, the integration of deep-learning-based semantic segmentation algorithms with preprocessing techniques can reduce the need for human annotation and advance disease classification. Among established preprocessing techniques, Contrast Limited Adaptive Histogram Equalization (CLAHE) has demonstrated efficacy in improving segmentation algorithms across various modalities, such as X-rays and CT. However, there remains a demand for improved contrast enhancement methods considering the heterogeneity of datasets and the various contrasts across different anatomic structures.</p></sec><sec id="sec002"><title>Method</title><p>This study proposes a novel preprocessing technique, ps-KDE, to investigate its impact on deep learning algorithms to segment major organs in posterior-anterior chest X-rays. Ps-KDE augments image contrast by substituting pixel values based on their normalized frequency across all images. We evaluate our approach on a U-Net architecture with ResNet34 backbone pre-trained on ImageNet. Five separate models are trained to segment the heart, left lung, right lung, left clavicle, and right clavicle.</p></sec><sec id="sec003"><title>Results</title><p>The model trained to segment the left lung using ps-KDE achieved a Dice score of 0.780 (SD = 0.13), while that of trained on CLAHE achieved a Dice score of 0.717 (SD = 0.19), <italic toggle="yes">p</italic>&#x0003c;0.01. ps-KDE also appears to be more robust as CLAHE-based models misclassified right lungs in select test images for the left lung model. The algorithm for performing ps-KDE is available at <ext-link xlink:href="https://github.com/wyc79/ps-KDE" ext-link-type="uri">https://github.com/wyc79/ps-KDE</ext-link>.</p></sec><sec id="sec004"><title>Discussion</title><p>Our results suggest that ps-KDE offers advantages over current preprocessing techniques when segmenting certain lung regions. This could be beneficial in subsequent analyses such as disease classification and risk stratification.</p></sec></abstract><funding-group><funding-statement>The author(s) received no specific funding for this work.</funding-statement></funding-group><counts><fig-count count="7"/><table-count count="3"/><page-count count="17"/></counts><custom-meta-group><custom-meta id="data-availability"><meta-name>Data Availability</meta-name><meta-value>All radiographs are held in a public repository from the Japanese Society of Radiological Technology (JSRT) database (<ext-link xlink:href="http://db.jsrt.or.jp/eng.php" ext-link-type="uri">http://db.jsrt.or.jp/eng.php</ext-link>).</meta-value></custom-meta></custom-meta-group></article-meta><notes><title>Data Availability</title><p>All radiographs are held in a public repository from the Japanese Society of Radiological Technology (JSRT) database (<ext-link xlink:href="http://db.jsrt.or.jp/eng.php" ext-link-type="uri">http://db.jsrt.or.jp/eng.php</ext-link>).</p></notes></front><body><sec sec-type="intro" id="sec005"><title>Introduction</title><sec sec-type="intro" id="sec006"><title>Background</title><p>With recent advances in artificial intelligence, deep learning (DL) has emerged as a leading machine-learning technique in medical imaging analysis, playing a transformative role in tasks such as image segmentation [<xref rid="pone.0299623.ref001" ref-type="bibr">1</xref>&#x02013;<xref rid="pone.0299623.ref003" ref-type="bibr">3</xref>]. This capability extends to various applications, including the segmentation of breast lesions [<xref rid="pone.0299623.ref004" ref-type="bibr">4</xref>, <xref rid="pone.0299623.ref005" ref-type="bibr">5</xref>], classification of pulmonary cancer stages [<xref rid="pone.0299623.ref006" ref-type="bibr">6</xref>], tissue characterization [<xref rid="pone.0299623.ref007" ref-type="bibr">7</xref>], detection of cardiomegaly [<xref rid="pone.0299623.ref008" ref-type="bibr">8</xref>], and many more. The improved performance for these intricate tasks suggests the potential of computer-aided techniques to improve diagnosis via segmentation.</p><p>Within radiology, the segmentation of organs and tumors in medical images holds promise for disease diagnosis and treatment [<xref rid="pone.0299623.ref007" ref-type="bibr">7</xref>]. One approach was the fully convolutional network (FCN), pioneering pixel-to-pixel semantic segmentation [<xref rid="pone.0299623.ref009" ref-type="bibr">9</xref>]. FCN&#x02019;s innovation lies in replacing the last fully connected layer with a deconvolutional layer. Building upon this foundation, the U-Net model as a modification of FCN increases the number of deconvolutional layers and therefore effectively captures more context while requiring smaller training samples [<xref rid="pone.0299623.ref010" ref-type="bibr">10</xref>]. Notably, U-Net has found a widespread application in segmenting medical images across various modalities, including X-rays, Magnetic Resonance Imaging (MRI), Computed Tomography (CT), and histopathology [<xref rid="pone.0299623.ref003" ref-type="bibr">3</xref>, <xref rid="pone.0299623.ref011" ref-type="bibr">11</xref>, <xref rid="pone.0299623.ref012" ref-type="bibr">12</xref>].</p><p>Current research has focused much on the development of pipelines for the automatic segmentation of medical images, leveraging both preprocessing techniques and the U-Net architecture. A popular generalizable segmentation tool, nnU-Net which was ranked 1st place in Medical Segmentation Decathlon, demonstrated the importance of preprocessing to model performance [<xref rid="pone.0299623.ref013" ref-type="bibr">13</xref>]. Remarkably, even the other simpler U-Net architectures with self-configured preprocessing procedures outperformed more intricate model architectures [<xref rid="pone.0299623.ref014" ref-type="bibr">14</xref>]. Contrast enhancement (i.e., enhancing brightness difference between objects and backgrounds), as a pivotal step for X-ray and CT preprocessing, plays a crucial role in providing human viewers and computer-aided algorithms with crucial features to facilitate analysis [<xref rid="pone.0299623.ref015" ref-type="bibr">15</xref>].</p></sec><sec id="sec007"><title>Related works in contrast enhancement</title><sec id="sec008"><title>Histogram equalization</title><p>Histogram equalization (HE) is a widely used digital image processing method to enhance the contrast of images. It expands an image&#x02019;s distribution range, as some images might only occupy a small portion of the entire value range. The resulting distribution of the pixel value would become more similar to a uniform distribution. However, since most images usually use the whole range of intensity (for instance, 0&#x02013;255 for a standard RGB image), the HE method would not have much impact on those images [<xref rid="pone.0299623.ref016" ref-type="bibr">16</xref>].</p></sec><sec id="sec009"><title>Adaptive Histogram Equalization (AHE)</title><p>The AHE method, as a result, was developed to address this limitation [<xref rid="pone.0299623.ref016" ref-type="bibr">16</xref>]. In AHE, images are divided into subsections, and each subsection is equalized separately. Compared to regular HE, AHE enhances local contrast but with the risk of over-amplifying noise in some regions. Nonetheless, AHE emerged as a popular image preprocessing method in medical imaging applications [<xref rid="pone.0299623.ref015" ref-type="bibr">15</xref>, <xref rid="pone.0299623.ref017" ref-type="bibr">17</xref>, <xref rid="pone.0299623.ref018" ref-type="bibr">18</xref>].</p></sec><sec id="sec010"><title>Constrast Limited Adaptive Historgam Equalization (CLAHE)</title><p>An enhancement upon traditional AHE methods, CLAHE, was introduced by clipping histograms to constrain the contrast [<xref rid="pone.0299623.ref016" ref-type="bibr">16</xref>]. It clips the outliers in histograms and redistributes the values across the value range [<xref rid="pone.0299623.ref016" ref-type="bibr">16</xref>]. Recently, several studies have shown the advantages of CLAHE on DL-related tasks, such as predicting five stages of diabetic retinopathy [<xref rid="pone.0299623.ref019" ref-type="bibr">19</xref>], segmenting temporomandibular joint articular disks from MRI [<xref rid="pone.0299623.ref020" ref-type="bibr">20</xref>], and classification of COVID-19 and other pneumonia cases [<xref rid="pone.0299623.ref021" ref-type="bibr">21</xref>].</p></sec><sec id="sec011"><title>Deep-Learning-Based contrast enhancement</title><p>Neural-network-based image enhancement has emerged in recent years. Anand et al. introduced a contrast diffusion model that learned different contrast levels from low- and high-contrast CXR images [<xref rid="pone.0299623.ref022" ref-type="bibr">22</xref>]. Wei et al. proposed an unsupervised, deep Retinex model for low-light image enhancement via a Decom-Net for decomposition and an Enhance-Net for illumination adjustment [<xref rid="pone.0299623.ref023" ref-type="bibr">23</xref>].</p><p>While current methods do exist, there remains a need to pioneer more efficient contrast-enhancement techniques with adequate interpretability. Furthermore, the scarcity of large datasets in real clinical environments poses a challenge to the development of deep-learning-based contrast-enhancement methods that can be generalized effectively.</p><p>In this study, we propose a novel, histogram-based, contrast-enhancing method termed ps-KDE. We apply this contrast-enhancement method along with deep-learning segmentation algorithms (e.g., U-Net with ResNet backbones) to the various anatomic structures in a small dataset of X-ray images. We then assess its impact on the performance of deep-learning segmentation based on multiple commonly used evaluation metrics, including Dice/F1-score, Intersection of Union (IoU), recall, and precision. ps-KDE brings three notable contributions: 1) it presents an end-to-end data enhancement method characterized by its simplicity of implementation and adaptability for fine-tuning to accommodate diverse datasets; 2) it demonstrates the efficacy of a density-based augmentation method in segmenting vital organs in chest X-rays; and 3) it establishes the robustness of segmentation algorithms through the interpretation of heatmaps generated by the model.</p></sec></sec></sec><sec sec-type="materials|methods" id="sec012"><title>Materials and methods</title><p>We employed an openly accessible dataset of chest radiographs for our study. The dataset was split equally into a training and a testing set. The images in the training set were augmented through randomized data augmentation and resized to the same resolution for preparation. Then, we performed hyperparameter optimization to identify the optimal parameter configuration. These parameters were integral to the training of our models. After the training phase, the models&#x02019; performance was evaluated using the test set. <xref rid="pone.0299623.s001" ref-type="supplementary-material">S1 Fig</xref> illustrates the project&#x02019;s overview in a flowchart format.</p><sec id="sec013"><title>Data</title><p>We used a publicly available dataset with 247 posterior-anterior (PA) chest radiographs collected from 13 institutions in Japan and one in the United States. The original radiographs are provided by the Japanese Society of Radiological Technology (JSRT) Database [<xref rid="pone.0299623.ref024" ref-type="bibr">24</xref>] and the manual mask annotations are provided by the Segmentation in Chest Radiology (SCR) Database [<xref rid="pone.0299623.ref025" ref-type="bibr">25</xref>]. The chest radiographs are in PNG format, and the labels are in the form of binary masks. Each image in the database was scanned from film to a size of 2048*2048. Among the 247 images, 154 of them showed solitary pulmonary lung nodules, while the remaining 93 images exhibited no signs of lung nodules. The ethnic representation is unknown.</p><p>Among the subset of patients with nodules, gender distribution was observed as 68 males and 86 females. In contrast, among patients without nodules, the gender distribution consisted of 51 males and 42 females. The mean age for patients with nodules is 60 years old. Each image has five matching masks generated manually by expert radiologists. Each binary mask delineates the boundary of one of the five anatomical structures: heart, left lung, right lung, left clavicle, and right clavicle. Since the original images are in grayscale, with only one color channel, we replicate this single channel to create three channels. This adjustment is necessary to meet the requirement of our deep learning model, which expects inputs to have three color channels.</p><p>This study utilizes exclusively publicly available data and thus does not require the Institutional Review Board (IRB) review per regulations set by the Office for Human Research Protections (OHRP) within the U.S. Department of Health and Human Services. The data was accessed on the third day of April 2022. The authors had no access to information that could identify individual participants during or after data collection.</p></sec><sec id="sec014"><title>Data augmentation</title><p>Large quantities of data are often needed to train most deep-learning algorithms successfully. Data augmentation is crucial when large datasets are not feasible in order to prevent overfitting and increase model performance. Five types of augmentation were simultaneously applied to each of the original images and its corresponding mask so that the masks correctly represent the anatomical structures on the augmented images. Augmentations include rotation, horizontal flip, vertical flip, a range for image zooms, and rescale. The rotation can occur between 90 degrees clockwise and counterclockwise of the original orientation. Horizontal and vertical flips occur at a probability of 0.5. The range of zoom is between 0.5 and 1.5 for the original images. All images are then rescaled from the red-green-blue scale [0, 255] to [0,1] and resized to 256x256 pixels to help the predictive models achieve faster convergence and higher stability. <xref rid="pone.0299623.g001" ref-type="fig">Fig 1</xref> contains examples of augmentation. Data augmentation was implemented with <italic toggle="yes">ImageDataGenerator</italic> from <italic toggle="yes">TensorFlow</italic> [<xref rid="pone.0299623.ref026" ref-type="bibr">26</xref>].</p><fig position="float" id="pone.0299623.g001"><object-id pub-id-type="doi">10.1371/journal.pone.0299623.g001</object-id><label>Fig 1</label><caption><title>Augmented images of three distinct individuals in the training set.</title></caption><graphic xlink:href="pone.0299623.g001" position="float"/></fig></sec><sec id="sec015"><title>Train/Validation split</title><p>We randomly split our dataset into training and validation sets. The training set contains 50% of the radiographs (n = 124), and the validation contains the other 50% (n = 123). Having executed five distinct augmentation techniques individually, we expanded our initial training set to five times its original size, leading to an allocation of 80% for training (n = 620) and 20% for validation (n = 123).</p></sec><sec id="sec016"><title>Image preprocessing</title><sec id="sec017"><title>Contrast Limited Adaptive Histogram Equalization (CLAHE)</title><p>We applied CLAHE to our data. An example of chest X-rays preprocessing with CLAHE is shown in <xref rid="pone.0299623.g002" ref-type="fig">Fig 2a and 2b</xref>. The equalization of histograms can be visualized in <xref rid="pone.0299623.g003" ref-type="fig">Fig 3a and 3b</xref>. The distribution of pixel values became more uniform after CLAHE.</p><fig position="float" id="pone.0299623.g002"><object-id pub-id-type="doi">10.1371/journal.pone.0299623.g002</object-id><label>Fig 2</label><caption><title>An example of an X-ray image being processed.</title><p>(a) Original chest X-ray image. (b) CLAHE processed chest X-ray image. (c) ps-KDE processed chest X-ray image (location: heart). CLAHE: Contrast Limited Adaptive Histogram Equalization.</p></caption><graphic xlink:href="pone.0299623.g002" position="float"/></fig><fig position="float" id="pone.0299623.g003"><object-id pub-id-type="doi">10.1371/journal.pone.0299623.g003</object-id><label>Fig 3</label><caption><title>Normalized distribution of pixel values in X-ray images.</title><p>(a) Histogram of original images. (b) Histogram of CLAHE-processed images. (c) KDE of pixel values in different organs. CLAHE: Contrast Limited Adaptive Histogram Equalization; KDE: Kernel density estimation.</p></caption><graphic xlink:href="pone.0299623.g003" position="float"/></fig></sec><sec id="sec018"><title>Pixel-wise substitution by Kernel Density Estimation (ps-KDE)</title><p>During the initial exploration of the data, we observed that the distribution of pixel values appeared to be different from organ to organ. We generated histograms of pixel values in different organs to validate our initial observation. We then performed kernel density estimation (KDE) to get a probability density function (PDF) for each organ (Figs <xref rid="pone.0299623.g003" ref-type="fig">3c</xref> and <xref rid="pone.0299623.g004" ref-type="fig">4a</xref>). The PDFs were calculated based on the training set and were stored as prior knowledge. For each image, we substitute each pixel with the density of that pixel value (<xref rid="pone.0299623.g004" ref-type="fig">Fig 4b</xref>). The image would then be mapped to a 0&#x02013;1 range to ensure consistency among images. In other words, our proposed ps-KDE substitutes pixel value for frequency, so that more frequently occurring pixel values in an organ would have a higher value in the resulting plot. Similar to CLAHE, the results were visually appealing (<xref rid="pone.0299623.g002" ref-type="fig">Fig 2c</xref>).</p><fig position="float" id="pone.0299623.g004"><object-id pub-id-type="doi">10.1371/journal.pone.0299623.g004</object-id><label>Fig 4</label><caption><title>Flowchart for performing ps-KDE on images.</title><p>Codes for performing ps-KDE is available at <ext-link xlink:href="https://github.com/wyc79/ps-KDE" ext-link-type="uri">https://github.com/wyc79/ps-KDE</ext-link>. (a) Algorithm for calculating density using kernel density estimation based on the training set. (b) Algorithm for substituting pixel value with density.</p></caption><graphic xlink:href="pone.0299623.g004" position="float"/></fig></sec></sec><sec id="sec019"><title>Model development</title><p>We employed a deep learning method for the semantic segmentation of chest radiographs, leveraging the <italic toggle="yes">U-Net</italic> neural network with ResNet backbone designed for segmentation tasks [<xref rid="pone.0299623.ref010" ref-type="bibr">10</xref>].</p><sec id="sec020"><title>Network architecture and implementation</title><p>The network architecture consists of a contracting path and an expansive path. The original design for the contracting path consists of unpadded convolutions with size 3x3, followed by rectified linear units with a 2x2 max-pooling layer, whereas the expansive path applies upsampling for each feature map from the contracting path to restore the original input size. The final layer maps the feature vector to the number of classes.</p><p>Implementation-wise, we used the Python package <italic toggle="yes">segmentation_models</italic> (Yakubovskiy, 2019, v1.0.1) with a <italic toggle="yes">ResNet34</italic> backbone featuring pre-trained weights from <italic toggle="yes">ImageNet</italic> (<xref rid="pone.0299623.g005" ref-type="fig">Fig 5</xref>). <italic toggle="yes">ResNet</italic> won the <italic toggle="yes">ImageNet</italic> Large Scale Visual Recognition Challenge 2015 with a top-five test error of 3.567 percent in the image classification category [<xref rid="pone.0299623.ref027" ref-type="bibr">27</xref>]. With a network depth of 152, <italic toggle="yes">ResNet</italic> surpasses <italic toggle="yes">VGGNet</italic> in depth by eight times [<xref rid="pone.0299623.ref028" ref-type="bibr">28</xref>]. Referred to as the <italic toggle="yes">ResNetUnet</italic> model in our paper, this amalgamation of <italic toggle="yes">U-Net</italic> and <italic toggle="yes">ResNet34</italic> structures incorporates additional enhancements such as batch normalization and zero padding to complement the original design.</p><fig position="float" id="pone.0299623.g005"><object-id pub-id-type="doi">10.1371/journal.pone.0299623.g005</object-id><label>Fig 5</label><caption><title><italic toggle="yes">ResNetUnet</italic> model.</title><p><italic toggle="yes">U-Net</italic> Model Architecture Implemented with <italic toggle="yes">ResNet34</italic> Backbone.</p></caption><graphic xlink:href="pone.0299623.g005" position="float"/></fig></sec><sec id="sec021"><title>Loss functions</title><p>A loss function is needed for machine learning models to learn through propagation. Multiple loss functions could be used for image segmentation tasks. For example, three loss functions were proposed to have good performances: binary cross entropy (BCE), binary cross entropy with Jaccard loss (BCE+JCD), and Dice loss (DL). BCE is one of the most commonly used loss functions for machine learning in binary classification tasks. For current work, the mask of each location is a zero or one matrix, which makes the task similar to a pixel-wise binary class classification. Therefore, BCE would be an appropriate loss function to use. The formula for BCE is shown below, considering the ground truth mask <italic toggle="yes">gt</italic> and the model predicted mask <italic toggle="yes">pr</italic>:
<disp-formula id="pone.0299623.e001">
<alternatives><graphic xlink:href="pone.0299623.e001.jpg" id="pone.0299623.e001g" position="anchor"/><mml:math id="M1" display="block" overflow="scroll"><mml:mrow><mml:mi>B</mml:mi><mml:mi>C</mml:mi><mml:mi>E</mml:mi><mml:mfenced><mml:mrow><mml:mi>g</mml:mi><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mspace width="2pt"/><mml:mi>p</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mi>g</mml:mi><mml:mi>t</mml:mi><mml:mspace width="2pt"/><mml:mo>&#x000b7;</mml:mo><mml:mspace width="2pt"/><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mfenced><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:mfenced><mml:mo>&#x02212;</mml:mo><mml:mfenced><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mi>g</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mspace width="2pt"/><mml:mo>&#x000b7;</mml:mo><mml:mspace width="2pt"/><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mfenced><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mi>p</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:math></alternatives>
<label>(1)</label>
</disp-formula></p><p>Another widely used loss function in segmentation tasks is the numeric sum of binary cross entropy and IoU score (Jaccard loss).</p><disp-formula id="pone.0299623.e002">
<alternatives><graphic xlink:href="pone.0299623.e002.jpg" id="pone.0299623.e002g" position="anchor"/><mml:math id="M2" display="block" overflow="scroll"><mml:mrow><mml:mi>B</mml:mi><mml:mi>C</mml:mi><mml:mi>E</mml:mi><mml:mo>+</mml:mo><mml:mi>J</mml:mi><mml:mi>C</mml:mi><mml:mi>D</mml:mi><mml:mfenced><mml:mrow><mml:mi>g</mml:mi><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>p</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>B</mml:mi><mml:mi>C</mml:mi><mml:mi>E</mml:mi><mml:mfenced><mml:mrow><mml:mi>g</mml:mi><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>p</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:mi>U</mml:mi><mml:mfenced><mml:mrow><mml:mi>g</mml:mi><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>p</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:math></alternatives>
<label>(2)</label>
</disp-formula><p>The Dice coefficient (DC) is a commonly used metric to calculate similarities between images. The Dice coefficient is defined similarly as IoU:
<disp-formula id="pone.0299623.e003">
<alternatives><graphic xlink:href="pone.0299623.e003.jpg" id="pone.0299623.e003g" position="anchor"/><mml:math id="M3" display="block" overflow="scroll"><mml:mrow><mml:mi>D</mml:mi><mml:mi>C</mml:mi><mml:mfenced><mml:mrow><mml:mi>g</mml:mi><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>p</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>B</mml:mi><mml:mi>C</mml:mi><mml:mi>E</mml:mi><mml:mfenced><mml:mrow><mml:mi>g</mml:mi><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>p</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:mi>U</mml:mi><mml:mfenced><mml:mrow><mml:mi>g</mml:mi><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>p</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:math></alternatives>
<label>(3)</label>
</disp-formula></p></sec><sec id="sec022"><title>Model training and hyperparameter optimization</title><p>For optimizing the hyperparameters, we used five-fold cross-validation with all possible combinations of hyperparameters, including the optimizer, loss function, batch size, and learning rate. The list of tuning spaces for each hyper-parameter is shown in <xref rid="pone.0299623.t001" ref-type="table">Table 1</xref>. To search through the proposed space of hyper-parameters, we used a Bayesian optimization process through the <italic toggle="yes">scikit-optimize</italic> package. We first defined an objective function that took instances of hyper-parameters, trained the model, and returned the cross-validation scores (CV scores). We then passed the scores to the optimization function of the package. The optimization process assumed the objective function results to follow a multivariate Gaussian distribution. It would take all observed scores until the current iteration, calculate a posterior distribution and sample the next set of hyper-parameters instances out of the posterior distribution. The best combination of hyper-parameters is chosen for the final model training.</p><table-wrap position="float" id="pone.0299623.t001"><object-id pub-id-type="doi">10.1371/journal.pone.0299623.t001</object-id><label>Table 1</label><caption><title>Search space for hyper-parameters.</title></caption><alternatives><graphic xlink:href="pone.0299623.t001" id="pone.0299623.t001g" position="float"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" rowspan="1" colspan="1">Hyper-parameters</th><th align="left" rowspan="1" colspan="1">Tuning Space</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">Optimizer</td><td align="left" rowspan="1" colspan="1">RMSprop, Adam, SGD</td></tr><tr><td align="left" rowspan="1" colspan="1">Loss Function</td><td align="left" rowspan="1" colspan="1">BCE, BCE+JCD, DL</td></tr><tr><td align="left" rowspan="1" colspan="1">Batch Size</td><td align="left" rowspan="1" colspan="1">Integer [1, 6]</td></tr><tr><td align="left" rowspan="1" colspan="1">Learning Rate</td><td align="left" rowspan="1" colspan="1">Real [1 &#x000d7; 10<sup>&#x02212;5</sup>]</td></tr></tbody></table></alternatives><table-wrap-foot><fn id="t001fn001"><p>The categorical and integer variables (optimizer, loss function, and batch size) were initialized to have a uniform prior probability; the learning rate was initialized to have uniform prior distribution in log space (log-uniform). <italic toggle="yes">RMSprop</italic>: <italic toggle="yes">root mean squared propagation; Adam</italic>: <italic toggle="yes">adaptive Moment Estimation; SGD</italic>: <italic toggle="yes">stochastic gradient descent; BCE</italic>: <italic toggle="yes">binary cross entropy; BCE+JCD</italic>: <italic toggle="yes">binary cross entropy with Jaccard loss; DL</italic>: <italic toggle="yes">Dice loss</italic>.</p></fn></table-wrap-foot></table-wrap><p>After obtaining the optimized hyperparameters, we fitted models using the original images and two distinct pre-processing techniques (i.e. CLAHE, and ps-KDE) onto the five anatomic structures (i.e., heart, left lung, right lung, left clavicle, right clavicle) with the corresponding best-performing hyperparameters for that task, for a total of 15 models. Our predictive models were then trained with 50 epochs, with <inline-formula id="pone.0299623.e004"><alternatives><graphic xlink:href="pone.0299623.e004.jpg" id="pone.0299623.e004g" position="anchor"/><mml:math id="M4" display="inline" overflow="scroll"><mml:mrow><mml:mfrac><mml:mrow><mml:mo>#</mml:mo><mml:mspace width="2pt"/><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mspace width="2pt"/><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi><mml:mspace width="2pt"/><mml:mi>s</mml:mi><mml:mi>a</mml:mi><mml:mi>m</mml:mi><mml:mi>p</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mo>*</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mo>_</mml:mo><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>z</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math></alternatives></inline-formula> samples in each step.</p></sec></sec><sec id="sec023"><title>Model evaluation and interpretability</title><sec id="sec024"><title>Evaluation metrics</title><p>We used intersection over union (IoU) and the Dice coefficient (i.e., F-score, Dice score) to evaluate our models. IoU, also known as Jaccard loss, is a commonly used metric in image segmentation tasks. Consider the ground truth mask <italic toggle="yes">gt</italic> and the model predicted mask <italic toggle="yes">pr</italic>:
<disp-formula id="pone.0299623.e005">
<alternatives><graphic xlink:href="pone.0299623.e005.jpg" id="pone.0299623.e005g" position="anchor"/><mml:math id="M5" display="block" overflow="scroll"><mml:mrow><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:mi>U</mml:mi><mml:mfenced><mml:mrow><mml:mi>g</mml:mi><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>p</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>A</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mfenced><mml:mrow><mml:mi>g</mml:mi><mml:mi>t</mml:mi><mml:mo>&#x02229;</mml:mo><mml:mi>p</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mfenced><mml:mrow><mml:mi>g</mml:mi><mml:mi>t</mml:mi><mml:mo>&#x0222a;</mml:mo><mml:mi>p</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac></mml:mrow></mml:math></alternatives>
<label>(4)</label>
</disp-formula></p><p>We assume that both masks are image matrices of 0&#x02019;s and 1&#x02019;s. Therefore, the area of the mask would be a count of 1&#x02019;s in the corresponding pixel matrix. A high IoU score indicates that more pixels are predicted correctly (more true positives) while fewer pixels are missed (less false negatives and false positives).</p><p>F-score, on the other hand, represents a weighted average between precision and recall. In this study, we will report the F1/Dice score. Specifically,
<disp-formula id="pone.0299623.e006">
<alternatives><graphic xlink:href="pone.0299623.e006.jpg" id="pone.0299623.e006g" position="anchor"/><mml:math id="M6" display="block" overflow="scroll"><mml:mrow><mml:mi>F</mml:mi><mml:mfenced><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mo>*</mml:mo><mml:mfrac><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>*</mml:mo><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math></alternatives>
<label>(5)</label>
</disp-formula></p><p>We evaluated models on the validation set and reported the mean and standard deviation for each evaluation metric. We performed the independent samples t-test assuming no equal variance to compare the distributions of the Dice scoring metrics between two preprocessing methods using R (version 4.2.3). The significance level (p = 0.01) was not corrected for multiple comparisons as none of the comparisons was tested more than once. No significance test was performed on precision, recall, IoU, or accuracy.</p></sec><sec id="sec025"><title>Generation of constrast-enhaned images and probability heatmaps</title><p>To understand our models&#x02019; classification, we randomly chose subjects and obtained the probability of each pixel being classified into the organs or clavicle. A heatmap was produced based on the probabilities using <italic toggle="yes">Matplotlib</italic>. In addition, we overlapped the model&#x02019;s prediction with the original chest X-ray image to evaluate whether the segmentation has clinical merits.</p></sec></sec></sec><sec id="sec026"><title>Result</title><sec id="sec027"><title>Model evaluation</title><p><xref rid="pone.0299623.t002" ref-type="table">Table 2</xref> demonstrates the results of model optimization based on the cross-validation scheme. The best loss function for all five locations was BCE+JCD, which considers pixel-wise information and intersection maximization.</p><table-wrap position="float" id="pone.0299623.t002"><object-id pub-id-type="doi">10.1371/journal.pone.0299623.t002</object-id><label>Table 2</label><caption><title>Optimization results.</title></caption><alternatives><graphic xlink:href="pone.0299623.t002" id="pone.0299623.t002g" position="float"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" rowspan="1" colspan="1">Region</th><th align="left" rowspan="1" colspan="1">Optimizer</th><th align="left" rowspan="1" colspan="1">Loss Function</th><th align="left" rowspan="1" colspan="1">Batch Size</th><th align="left" rowspan="1" colspan="1">Learning Rate</th><th align="left" rowspan="1" colspan="1">CV Score</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">Heart</td><td align="left" rowspan="1" colspan="1">SGD</td><td align="left" rowspan="1" colspan="1">BCE+JCD</td><td align="left" rowspan="1" colspan="1">1</td><td align="char" char="." rowspan="1" colspan="1">0.07879</td><td align="char" char="." rowspan="1" colspan="1">0.63121</td></tr><tr><td align="left" rowspan="1" colspan="1">Left Clavicle</td><td align="left" rowspan="1" colspan="1">SGD</td><td align="left" rowspan="1" colspan="1">BCE+JCD</td><td align="left" rowspan="1" colspan="1">1</td><td align="char" char="." rowspan="1" colspan="1">0.10000</td><td align="char" char="." rowspan="1" colspan="1">0.33311</td></tr><tr><td align="left" rowspan="1" colspan="1">Left Lung</td><td align="left" rowspan="1" colspan="1">SGD</td><td align="left" rowspan="1" colspan="1">BCE+JCD</td><td align="left" rowspan="1" colspan="1">1</td><td align="char" char="." rowspan="1" colspan="1">0.00358</td><td align="char" char="." rowspan="1" colspan="1">0.65930</td></tr><tr><td align="left" rowspan="1" colspan="1">Right Clavicle</td><td align="left" rowspan="1" colspan="1">RMSprop</td><td align="left" rowspan="1" colspan="1">BCE+JCD</td><td align="left" rowspan="1" colspan="1">1</td><td align="char" char="." rowspan="1" colspan="1">0.00035</td><td align="char" char="." rowspan="1" colspan="1">0.37285</td></tr><tr><td align="left" rowspan="1" colspan="1">Right Lung</td><td align="left" rowspan="1" colspan="1">SGD</td><td align="left" rowspan="1" colspan="1">BCE+JCD</td><td align="left" rowspan="1" colspan="1">1</td><td align="char" char="." rowspan="1" colspan="1">0.00139</td><td align="char" char="." rowspan="1" colspan="1">0.73626</td></tr></tbody></table></alternatives><table-wrap-foot><fn id="t002fn001"><p>The best combination for each location is shown in the table. Note that for batch size, the actual batch size used in cross-validation and training models was the above batch size x 5. This multiplier was a result of the data augmentation, as we are loading the original images and augmented images all at the same time.</p></fn></table-wrap-foot></table-wrap><p><xref rid="pone.0299623.t003" ref-type="table">Table 3</xref> illustrates the evaluation results for various anatomic structures utilizing three distinct image processing techniques. In terms of technique-specific model performance, our analysis revealed that when using the original images (i.e., without CLAHE or ps-KDE transformation), the heart demonstrated the highest segmentation performance, whereas the left clavicle exhibited the least favorable performance based on IoU and Dice scores. With CLAHE transformations, the heart model maintained its superior performance, albeit with the right clavicle registering the lowest scores. With ps-KDE transformation, the five models achieved a mean IoU ranging from 0.577 (SD = 0.06) in the right clavicle to 0.927 (SD = 0.05) in the heart, with Dice scores ranging from 0.275 (SD = 0.17) in the right clavicle to 0.926 (SD = 0.070) in the heart (<xref rid="pone.0299623.g006" ref-type="fig">Fig 6</xref>). Across all three techniques, it is noteworthy that the best-performing model differed significantly from the worst-performing one (p &#x0003c; 2.2 &#x000d7; 10<sup>&#x02212;16</sup>) when assessed by the Dice score. Precision and recall closely mirrored the ranking pattern observed in the Dice score, as anticipated. Accuracy is the highest-performing metric for all three image processing techniques.</p><fig position="float" id="pone.0299623.g006"><object-id pub-id-type="doi">10.1371/journal.pone.0299623.g006</object-id><label>Fig 6</label><caption><title>Model performance represented by violin plots for each anatomical structure.</title><p>a) IoU b) Dice score.</p></caption><graphic xlink:href="pone.0299623.g006" position="float"/></fig><table-wrap position="float" id="pone.0299623.t003"><object-id pub-id-type="doi">10.1371/journal.pone.0299623.t003</object-id><label>Table 3</label><caption><title>Model performance after applying preprocessing methods (CLAHE and ps-KDE) evaluated by IoU and Dice scores.</title></caption><alternatives><graphic xlink:href="pone.0299623.t003" id="pone.0299623.t003g" position="float"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="center" rowspan="1" colspan="1"/><th align="center" rowspan="1" colspan="1">Region</th><th align="center" rowspan="1" colspan="1">Original</th><th align="center" rowspan="1" colspan="1">CLAHE</th><th align="center" rowspan="1" colspan="1">ps-KDE</th></tr></thead><tbody><tr><td align="center" rowspan="5" colspan="1">IoU</td><td align="center" rowspan="1" colspan="1">Heart</td><td align="center" rowspan="1" colspan="1">0.925 (0.07)</td><td align="center" rowspan="1" colspan="1">0.921 (0.08)</td><td align="center" rowspan="1" colspan="1">0.927 (0.05)</td></tr><tr><td align="center" rowspan="1" colspan="1">Left Clavicle</td><td align="center" rowspan="1" colspan="1">0.703 (0.12)</td><td align="center" rowspan="1" colspan="1">0.778 (0.14)</td><td align="center" rowspan="1" colspan="1">0.706 (0.12)</td></tr><tr><td align="center" rowspan="1" colspan="1">Left Lung</td><td align="center" rowspan="1" colspan="1">0.848 (0.11)</td><td align="center" rowspan="1" colspan="1">0.752 (0.12)</td><td align="center" rowspan="1" colspan="1">0.799 (0.09)</td></tr><tr><td align="center" rowspan="1" colspan="1">Right Clavicle</td><td align="center" rowspan="1" colspan="1">0.716 (0.09)</td><td align="center" rowspan="1" colspan="1">0.666 (0.14)</td><td align="center" rowspan="1" colspan="1">0.577 (0.06)</td></tr><tr><td align="center" rowspan="1" colspan="1">Right Lung</td><td align="center" rowspan="1" colspan="1">0.863 (0.10)</td><td align="center" rowspan="1" colspan="1">0.885 (0.05)</td><td align="center" rowspan="1" colspan="1">0.848 (0.09)</td></tr><tr><td align="center" rowspan="5" colspan="1">Dice Score</td><td align="center" rowspan="1" colspan="1">Heart</td><td align="center" rowspan="1" colspan="1">0.918 (0.13)</td><td align="center" rowspan="1" colspan="1">0.911 (0.14)</td><td align="center" rowspan="1" colspan="1">0.926 (0.07)</td></tr><tr><td align="center" rowspan="1" colspan="1">Left Clavicle</td><td align="center" rowspan="1" colspan="1">0.537 (0.28)</td><td align="center" rowspan="1" colspan="1">0.667 (0.30)<xref rid="t003fn002" ref-type="table-fn">*</xref></td><td align="center" rowspan="1" colspan="1">0.545 (0.28)<xref rid="t003fn002" ref-type="table-fn">*</xref></td></tr><tr><td align="center" rowspan="1" colspan="1">Left Lung</td><td align="center" rowspan="1" colspan="1">0.839 (0.16)</td><td align="center" rowspan="1" colspan="1">
<bold>0.717 (0.19)</bold>
<xref rid="t003fn002" ref-type="table-fn">*</xref>
</td><td align="center" rowspan="1" colspan="1">
<bold>0.780 (0.13)</bold>
<xref rid="t003fn002" ref-type="table-fn">*</xref>
</td></tr><tr><td align="center" rowspan="1" colspan="1">Right Clavicle</td><td align="center" rowspan="1" colspan="1">0.597 (0.17)</td><td align="center" rowspan="1" colspan="1">0.440 (0.33)<xref rid="t003fn002" ref-type="table-fn">*</xref></td><td align="center" rowspan="1" colspan="1">0.275 (0.17)<xref rid="t003fn002" ref-type="table-fn">*</xref></td></tr><tr><td align="center" rowspan="1" colspan="1">Right Lung</td><td align="center" rowspan="1" colspan="1">0.855 (0.17)</td><td align="center" rowspan="1" colspan="1">0.894 (0.06)<xref rid="t003fn002" ref-type="table-fn">*</xref></td><td align="center" rowspan="1" colspan="1">0.850 (0.12)<xref rid="t003fn002" ref-type="table-fn">*</xref></td></tr><tr><td align="center" rowspan="5" colspan="1">Precision</td><td align="center" rowspan="1" colspan="1">Heart</td><td align="center" rowspan="1" colspan="1">0.954 (0.09)</td><td align="center" rowspan="1" colspan="1">0.947 (0.13)</td><td align="center" rowspan="1" colspan="1">0.963 (0.04)</td></tr><tr><td align="center" rowspan="1" colspan="1">Left Clavicle</td><td align="center" rowspan="1" colspan="1">0.628 (0.32)</td><td align="center" rowspan="1" colspan="1">0.781 (0.29)</td><td align="center" rowspan="1" colspan="1">0.571 (0.28)</td></tr><tr><td align="center" rowspan="1" colspan="1">Left Lung</td><td align="center" rowspan="1" colspan="1">0.855 (0.19)</td><td align="center" rowspan="1" colspan="1">0.781 (0.21)</td><td align="center" rowspan="1" colspan="1">0.927 (0.11)</td></tr><tr><td align="center" rowspan="1" colspan="1">Right Clavicle</td><td align="center" rowspan="1" colspan="1">0.479 (0.20)</td><td align="center" rowspan="1" colspan="1">0.446 (0.33)</td><td align="center" rowspan="1" colspan="1">0.233 (0.15)</td></tr><tr><td align="center" rowspan="1" colspan="1">Right Lung</td><td align="center" rowspan="1" colspan="1">0.946 (0.13)</td><td align="center" rowspan="1" colspan="1">0.903 (0.08)</td><td align="center" rowspan="1" colspan="1">0.871 (0.10)</td></tr><tr><td align="center" rowspan="5" colspan="1">Recall</td><td align="center" rowspan="1" colspan="1">Heart</td><td align="center" rowspan="1" colspan="1">0.899 (0.15)</td><td align="center" rowspan="1" colspan="1">0.889 (0.16)</td><td align="center" rowspan="1" colspan="1">0.901 (0.10)</td></tr><tr><td align="center" rowspan="1" colspan="1">Left Clavicle</td><td align="center" rowspan="1" colspan="1">0.567 (0.35)</td><td align="center" rowspan="1" colspan="1">0.628 (0.32)</td><td align="center" rowspan="1" colspan="1">0.613 (0.35)</td></tr><tr><td align="center" rowspan="1" colspan="1">Left Lung</td><td align="center" rowspan="1" colspan="1">0.862 (0.15)</td><td align="center" rowspan="1" colspan="1">0.720 (0.21)</td><td align="center" rowspan="1" colspan="1">0.702 (0.17)</td></tr><tr><td align="center" rowspan="1" colspan="1">Right Clavicle</td><td align="center" rowspan="1" colspan="1">0.885 (0.15)</td><td align="center" rowspan="1" colspan="1">0.496 (0.40)</td><td align="center" rowspan="1" colspan="1">0.394 (0.28)</td></tr><tr><td align="center" rowspan="1" colspan="1">Right Lung</td><td align="center" rowspan="1" colspan="1">0.798 (0.18)</td><td align="center" rowspan="1" colspan="1">0.895 (0.08)</td><td align="center" rowspan="1" colspan="1">0.856 (0.15)</td></tr><tr><td align="center" rowspan="5" colspan="1">Accuracy</td><td align="center" rowspan="1" colspan="1">Heart</td><td align="center" rowspan="1" colspan="1">0.987 (0.01)</td><td align="center" rowspan="1" colspan="1">0.986 (0.01)</td><td align="center" rowspan="1" colspan="1">0.987 (0.01)</td></tr><tr><td align="center" rowspan="1" colspan="1">Left Clavicle</td><td align="center" rowspan="1" colspan="1">0.993 (0.00)</td><td align="center" rowspan="1" colspan="1">0.996 (0.00)</td><td align="center" rowspan="1" colspan="1">0.993 (0.00)</td></tr><tr><td align="center" rowspan="1" colspan="1">Left Lung</td><td align="center" rowspan="1" colspan="1">0.954 (0.04)</td><td align="center" rowspan="1" colspan="1">0.924 (0.05)</td><td align="center" rowspan="1" colspan="1">0.949 (0.03)</td></tr><tr><td align="center" rowspan="1" colspan="1">Right Clavicle</td><td align="center" rowspan="1" colspan="1">0.987 (0.01)</td><td align="center" rowspan="1" colspan="1">0.992 (0.00)</td><td align="center" rowspan="1" colspan="1">0.982 (0.01)</td></tr><tr><td align="center" rowspan="1" colspan="1">Right Lung</td><td align="center" rowspan="1" colspan="1">0.959 (0.03)</td><td align="center" rowspan="1" colspan="1">0.964 (0.02)</td><td align="center" rowspan="1" colspan="1">0.950 (0.03)</td></tr></tbody></table></alternatives><table-wrap-foot><fn id="t003fn001"><p>IoU and Dice scores are shown as mean (SD). CLAHE: Contrast Limited Adaptive Histogram Equalization; Ps-KDE: Pixel-wise substitution by Kernel Density Estimation; IoU: Intersection over Union.</p></fn><fn id="t003fn002"><p>*:p&#x0003c;0.01 in model performance when comparing between CLAHE and ps-KDE for each segmentation region pair.</p></fn></table-wrap-foot></table-wrap><p>Since there is no difference in the ranked order of model performances between the Dice score and mean IoU metrics we will exclusively present the Dice score to assess organ-specific model performances. This decision is made as the five models achieved a lower performance compared to that measured by mean IoU, providing a conservative estimate of the effectiveness of ps-KDE. Notably, significant differences in model performance were observed between the regions classified using CLAHE and ps-KDE. Specifically, in the left lung region, CLAHE had a Dice score of 0.717 (SD = 0.19), and ps-KDE had a Dice score of 0.780 (SD = 0.13), <italic toggle="yes">p</italic> = 0.0026 (<xref rid="pone.0299623.t003" ref-type="table">Table 3</xref>). We observed no differences between the two datasets in heart segmentation. CLAHE transformation achieved a significant result than ps-KDE in the left clavicle, right clavicle, and right lung.</p></sec><sec id="sec028"><title>Model interpretation</title><p>Examples of model predictions with both processing techniques (CLAHE and ps-KDE) are shown in <xref rid="pone.0299623.g007" ref-type="fig">Fig 7</xref>. The probability heatmaps showed a decrease in confidence around the edges of the segmentation object. This is more prevalent in the heart and the left clavicle model. Visually, the overlap of the predicted segmentation from ps-KDE and the original x-ray pinpoints the regions that radiologists typically focus on. The partial misclassification in the right lung from the CLAHE technique is discussed in later sections.</p><fig position="float" id="pone.0299623.g007"><object-id pub-id-type="doi">10.1371/journal.pone.0299623.g007</object-id><label>Fig 7</label><caption><title>Prediction of a randomly selected subject.</title><p>From left to right, the input of the model, the ground truth, the predicted segmentation overlap with the original x-ray, and the heatmap of the predicted probability. A) CLAHE processed, B) ps-KDE processed.</p></caption><graphic xlink:href="pone.0299623.g007" position="float"/></fig></sec></sec><sec sec-type="conclusions" id="sec029"><title>Discussion</title><p>In this study, we proposed a novel method, ps-KDE, to substitute the pixel value based on a normalized histogram distribution. Our investigation focused on evaluating the performance of the <italic toggle="yes">ResNetUnet</italic> architecture in the context of segmentation tasks, specifically applied to 247 chest X-rays with PA projection. We assessed each model&#x02019;s segmentation capabilities across five distinct anatomic structures, considering the impact of preprocessing techniques such as ps-KDE and CLAHE. We present ps-KDE as an end-to-end data augmentation method, which transforms raw X-ray images into augmented versions. As an overview, implementing ps-KDE involves traversing a representative image pool to calculate the frequency and density of each pixel value, which is then stored as prior knowledge. Both frequency and density computations can be achieved through a single programming language function call. Subsequently, users only need to assign each pixel value to its corresponding density, making the implementation available to most researchers with minimal programming experience. Crucially, this adaptability allows fine-tuning of ps-KDE to match the representation nuances of various datasets, whether at departmental, institutional, or national scales.</p><p>We first compared results within each technique (i.e. original, CLAHE, and ps-KDE), revealing a substantial gap between the highest and lowest Dice scores. These fluctuations should be concerning as test images came from the same dataset. This suggests that although <italic toggle="yes">U-Net</italic> is supposedly designed for end-to-end biomedical image segmentation with very few samples, this algorithm with validation on electronic microscopy image stacks may not generalize to radiographs [<xref rid="pone.0299623.ref010" ref-type="bibr">10</xref>]. We found that, in general, the model predicting lung regions and heart has the highest Dice scores, whereas in the clavicle regions, the Dice Score may drop below 0.6. The higher performance in large regions suggests the model could recognize larger patterns but fell short of smaller ones within the X-Ray.</p><p>We then evaluated the efficacy across the three processing techniques on models targeting the same organ. We observed that models preprocessed with CLAHE have higher IoU and Dice scores (<xref rid="pone.0299623.g006" ref-type="fig">Fig 6a and 6b</xref>) in the left clavicle regions compared to the original image models. The ps-KDE method, on the other hand, showed better performance in the left lung model than CLAHE. This means the combined use of both preprocessing techniques through a dynamic voting algorithm could be useful by harnessing the advantage of CLAHE in smaller regions and that of ps-KDE in larger regions. The novelty of the ps-KDE method lies within utilizing histogram values not only to generate density estimations but also to execute substitutions. Therefore, such combination allows the pixel substitution to benefit from CLAHE which has a more uniform overall distribution. By enabling accurate and consistent identification of anatomical structures, our proposed technique stands to enhance the precision of subsequent disease detection algorithms. Furthermore, given its demonstrated superior performance in segmenting specific anatomical structures in chest X-rays, we hypothesize that more advanced imaging techniques, such as CT scans, could potentially benefit from a similar approach.</p><p>While ps-KDE showed superior performance in some regions, it is also essential to examine why it may have underperformed in others. Specifically, in the right clavicle region, there is a notable difference in dice score between ps-KDE and CLAHE. Given that the dice score combines precision and recall, examining both metrics reveals that precision shows roughly double the difference compared to recall between ps-KDE and CLAHE. Lower precision indicates a reduced ability to distinguish between true and false positives. Considering that the clavicles are much smaller anatomical structures compared to others, the algorithm might overly contour these regions, leading to reduced precision. This further suggests that ps-KDE may only be suitable for segmenting larger areas. As a potential improvement, it might be worth considering incorporating the density distribution of both the left and right clavicles to double the number of points used for estimating density distribution. Evaluation of performance in the right lung region shows comparable results in terms of precision, recall, and accuracy. However, the reason for the observed superior performance in the left lung but not the right remains unclear. Future investigations could explore how density distributions might adversely affect neural network models [<xref rid="pone.0299623.ref029" ref-type="bibr">29</xref>], potentially guiding improvements to the ps-KDE method by incorporating additional smoothing and density estimation techniques.</p><p>We would also like to factor in computational efficiency given its potential impact on the integration of algorithms into clinical practice [<xref rid="pone.0299623.ref030" ref-type="bibr">30</xref>, <xref rid="pone.0299623.ref031" ref-type="bibr">31</xref>]. CLAHE operates by dividing the input image into tiles and applying histogram equalization to each, thereby obviating the need to modify all pixel values throughout the image [<xref rid="pone.0299623.ref016" ref-type="bibr">16</xref>]. Conversely, ps-KDE relies on a pre-defined frequency table of pixel values, thus necessitating only the referencing of corresponding frequency values for all pixels&#x02014;a computationally trivial step. While it is worth noting that CLAHE exhibits slightly faster processing times at 0.05(0.01)s and ps-KDE at 0.20(0.11)s, in the context of clinical practice, such discrepancies in turnaround time for radiologists may be negligible. This implies that ps-KDE, alongside other existing contrast enhancement methods offering rapid, on-demand processing speeds, holds promise for seamless integration into existing imaging systems [<xref rid="pone.0299623.ref032" ref-type="bibr">32</xref>], benefiting both clinicians and patients. Moreover, through the establishment of robust and representative pixel frequency tables, this preprocessing method could potentially mitigate systematic biases related to demographics, disease representation, and data management [<xref rid="pone.0299623.ref033" ref-type="bibr">33</xref>, <xref rid="pone.0299623.ref034" ref-type="bibr">34</xref>].</p><p>The incorporation of heatmaps offers invaluable insights into areas of interest and uncertainty during the segmentation process. Notably, we observed a consistent decrease in probability around object edges in the majority of images. This gradual phasing out of probability as the model progresses into negative pixels is ideal, as models exhibiting abrupt switches between high and low confidence levels may lack stability. The visualization of heatmaps also serves to pinpoint regions requiring further investigation. For instance, in CLAHE models, a few misclassifications of the right lung were observed when predicting left lung regions (<xref rid="pone.0299623.g007" ref-type="fig">Fig 7</xref>). This may be attributed to image augmentation techniques such as horizontal flips and rotation ranges applied before inputting the images. We hypothesize that, given the small size of our dataset, the spatial distribution of the ground truth significantly influences segmentation outcomes. This suggests that ps-KDE may exhibit greater robustness against substantial image augmentation and small datasets. Future studies could investigate the potential of applying transfer learning to <italic toggle="yes">ResNetUnet</italic> to mitigate the unintended impacts of augmentation [<xref rid="pone.0299623.ref035" ref-type="bibr">35</xref>, <xref rid="pone.0299623.ref036" ref-type="bibr">36</xref>].</p><p>It&#x02019;s worth noting that the predicted lungs still adhere to the clinical expectation that the left lung is narrow and long. Even in cases of misclassification, we can still observe that the model accurately outlines the shape and conforms to the expected characteristics of the right lung. Heatmaps present clinicians with a valuable tool, providing visual assessments of segmentation accuracy and quality. This facilitates interpretation and enables informed clinical decision-making.</p></sec><sec id="sec030"><title>Limitation</title><p>Our current dataset contains exclusively PNG images, whereas clinical practices heavily rely on the DICOM format for medical image analysis. While PNG is suitable for research and imaging information in DICOM can be easily converted to PNG format, it lacks the crucial metadata and standardized structure that DICOM would offer. This disconnection hinders the model&#x02019;s direct applicability in clinical settings where DICOM&#x02019;s comprehensive patient information and imaging details are essential.</p><p>To mitigate this limitation, the model needs further adaptation for DICOM data format. This involves adjusting the data processing pipeline to handle DICOM images and accounting for metadata intricacies. The model&#x02019;s effectiveness must be re-validated using DICOM data to ensure its reliability in clinical workflows. Addressing this constraint is vital to bridge the gap between research-oriented PNG images and the practical demands of medical professionals who predominantly rely on DICOM for accurate diagnosis and treatment.</p><p>We also recognize that the size of our dataset is small for a deep learning algorithm. We also only trained <italic toggle="yes">ResNetUnet</italic> on 50 epochs because of computing resource constraints. Higher performance may be achieved in larger epochs. In addition, the smoothed histogram takes account of only the pixel distribution for this dataset. An additional limitation of our study is the absence of external validation for our models. From a dataset perspective, it remains uncertain how effectively the smoothed histograms can extend to external radiographs, especially those with low quality and contrast. Moreover, there is a potential for another enhanced <italic toggle="yes">U-Net</italic> architecture [<xref rid="pone.0299623.ref037" ref-type="bibr">37</xref>] to provide further validation regarding the applicability of the ps-KDE technique across various model architectures.</p></sec><sec sec-type="conclusions" id="sec031"><title>Conclusion</title><p>In conclusion, we significantly improved semantic segmentation of the left lung in chest radiographs using ps-KDE. ps-KDE is easy to implement, adaptive across diverse datasets, and enhances the robustness of segmentation algorithms. The introduction of the ps-KDE preprocessing technique contributes to the available image contrasting methods for segmentation but should be treated with caution and further validations.</p></sec><sec id="sec032" sec-type="supplementary-material"><title>Supporting information</title><supplementary-material id="pone.0299623.s001" position="float" content-type="local-data"><label>S1 Fig</label><caption><title>Overall workflow of the project.</title><p>(PDF)</p></caption><media xlink:href="pone.0299623.s001.pdf"/></supplementary-material></sec></body><back><ack><p>We thank Kun-Hsing Yu, MD, Ph.D. for assistance with the segmentation algorithm, and Andrew Beam, Ph.D. for comments that greatly improved the experiment design. This work is inspired by the course BMI 707: Deep Learning for Biomedical Data. <ext-link xlink:href="https://hms-dbmi.github.io/BMI_707/" ext-link-type="uri">https://hms-dbmi.github.io/BMI_707/</ext-link>.</p></ack><ref-list><title>References</title><ref id="pone.0299623.ref001"><label>1</label><mixed-citation publication-type="journal"><name><surname>Chan</surname><given-names>HP</given-names></name>, <name><surname>Samala</surname><given-names>RK</given-names></name>, <name><surname>Hadjiiski</surname><given-names>LM</given-names></name>, <name><surname>Zhou</surname><given-names>C</given-names></name>. <article-title>Deep Learning in Medical Image Analysis</article-title>. <source>Adv Exp Med Biol</source>. <year>2020</year>;<volume>1213</volume>:<fpage>3</fpage>&#x02013;<lpage>21</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1007/978-3-030-33128-3_1</pub-id> .<?supplied-pmid 32030660?><pub-id pub-id-type="pmid">32030660</pub-id>
</mixed-citation></ref><ref id="pone.0299623.ref002"><label>2</label><mixed-citation publication-type="journal"><name><surname>Currie</surname><given-names>G</given-names></name>, <name><surname>Hawk</surname><given-names>KE</given-names></name>, <name><surname>Rohren</surname><given-names>E</given-names></name>, <name><surname>Vial</surname><given-names>A</given-names></name>, <name><surname>Klein</surname><given-names>R</given-names></name>. <article-title>Machine Learning and Deep Learning in Medical Imaging: Intelligent Imaging</article-title>. <source>J Med Imaging Radiat Sci</source>. <year>2019</year>;<volume>50</volume>(<issue>4</issue>):<fpage>477</fpage>&#x02013;<lpage>87</lpage>. Epub 20191007. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.jmir.2019.09.005</pub-id> .<?supplied-pmid 31601480?><pub-id pub-id-type="pmid">31601480</pub-id>
</mixed-citation></ref><ref id="pone.0299623.ref003"><label>3</label><mixed-citation publication-type="journal"><name><surname>Chen</surname><given-names>X</given-names></name>, <name><surname>Wang</surname><given-names>X</given-names></name>, <name><surname>Zhang</surname><given-names>K</given-names></name>, <name><surname>Fung</surname><given-names>KM</given-names></name>, <name><surname>Thai</surname><given-names>TC</given-names></name>, <name><surname>Moore</surname><given-names>K</given-names></name>, <etal>et al</etal>. <article-title>Recent advances and clinical applications of deep learning in medical image analysis</article-title>. <source>Med Image Anal</source>. <year>2022</year>;<volume>79</volume>:<fpage>102444</fpage>. Epub 20220404. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.media.2022.102444</pub-id> .<?supplied-pmid 35472844?><pub-id pub-id-type="pmid">35472844</pub-id>
</mixed-citation></ref><ref id="pone.0299623.ref004"><label>4</label><mixed-citation publication-type="journal"><name><surname>Al-Antari</surname><given-names>MA</given-names></name>, <name><surname>Al-Masni</surname><given-names>MA</given-names></name>, <name><surname>Choi</surname><given-names>MT</given-names></name>, <name><surname>Han</surname><given-names>SM</given-names></name>, <name><surname>Kim</surname><given-names>TS</given-names></name>. <article-title>A fully integrated computer-aided diagnosis system for digital X-ray mammograms via deep learning detection, segmentation, and classification</article-title>. <source>Int J Med Inform</source>. <year>2018</year>;<volume>117</volume>:<fpage>44</fpage>&#x02013;<lpage>54</lpage>. Epub 20180618. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.ijmedinf.2018.06.003</pub-id> .<?supplied-pmid 30032964?><pub-id pub-id-type="pmid">30032964</pub-id>
</mixed-citation></ref><ref id="pone.0299623.ref005"><label>5</label><mixed-citation publication-type="journal"><name><surname>Balkenende</surname><given-names>L</given-names></name>, <name><surname>Teuwen</surname><given-names>J</given-names></name>, <name><surname>Mann</surname><given-names>RM</given-names></name>. <article-title>Application of Deep Learning in Breast Cancer Imaging</article-title>. <source>Semin Nucl Med</source>. <year>2022</year>;<volume>52</volume>(<issue>5</issue>):<fpage>584</fpage>&#x02013;<lpage>96</lpage>. Epub 20220324. <comment>doi: </comment><pub-id pub-id-type="doi">10.1053/j.semnuclmed.2022.02.003</pub-id> .<?supplied-pmid 35339259?><pub-id pub-id-type="pmid">35339259</pub-id>
</mixed-citation></ref><ref id="pone.0299623.ref006"><label>6</label><mixed-citation publication-type="journal"><name><surname>Masood</surname><given-names>A</given-names></name>, <name><surname>Sheng</surname><given-names>B</given-names></name>, <name><surname>Li</surname><given-names>P</given-names></name>, <name><surname>Hou</surname><given-names>X</given-names></name>, <name><surname>Wei</surname><given-names>X</given-names></name>, <name><surname>Qin</surname><given-names>J</given-names></name>, <etal>et al</etal>. <article-title>Computer-Assisted Decision Support System in Pulmonary Cancer detection and stage classification on CT images</article-title>. <source>J Biomed Inform</source>. <year>2018</year>;<volume>79</volume>:<fpage>117</fpage>&#x02013;<lpage>28</lpage>. Epub 20180131. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.jbi.2018.01.005</pub-id> .<?supplied-pmid 29366586?><pub-id pub-id-type="pmid">29366586</pub-id>
</mixed-citation></ref><ref id="pone.0299623.ref007"><label>7</label><mixed-citation publication-type="journal"><name><surname>Sahiner</surname><given-names>B</given-names></name>, <name><surname>Pezeshk</surname><given-names>A</given-names></name>, <name><surname>Hadjiiski</surname><given-names>LM</given-names></name>, <name><surname>Wang</surname><given-names>X</given-names></name>, <name><surname>Drukker</surname><given-names>K</given-names></name>, <name><surname>Cha</surname><given-names>KH</given-names></name>, <etal>et al</etal>. <article-title>Deep learning in medical imaging and radiation therapy</article-title>. <source>Med Phys</source>. <year>2019</year>;<volume>46</volume>(<issue>1</issue>):<fpage>e1</fpage>&#x02013;<lpage>e36</lpage>. Epub 20181120. <comment>doi: </comment><pub-id pub-id-type="doi">10.1002/mp.13264</pub-id> .<?supplied-pmid 30367497?><pub-id pub-id-type="pmid">30367497</pub-id>
</mixed-citation></ref><ref id="pone.0299623.ref008"><label>8</label><mixed-citation publication-type="journal"><name><surname>Li</surname><given-names>Z</given-names></name>, <name><surname>Hou</surname><given-names>Z</given-names></name>, <name><surname>Chen</surname><given-names>C</given-names></name>, <name><surname>Hao</surname><given-names>Z</given-names></name>, <name><surname>An</surname><given-names>Y</given-names></name>, <name><surname>Liang</surname><given-names>S</given-names></name>, <etal>et al</etal>. <article-title>Automatic cardiothoracic ratio calculation with deep learning</article-title>. <source>IEEE Access</source>. <year>2019</year>;<volume>7</volume>:<fpage>37749</fpage>&#x02013;<lpage>56</lpage>.</mixed-citation></ref><ref id="pone.0299623.ref009"><label>9</label><mixed-citation publication-type="other">Long J, Shelhamer E, Darrell T, editors. Fully convolutional networks for semantic segmentation. Proceedings of the IEEE conference on computer vision and pattern recognition; 2015.</mixed-citation></ref><ref id="pone.0299623.ref010"><label>10</label><mixed-citation publication-type="other">Ronneberger O, Fischer P, Brox T, editors. U-net: Convolutional networks for biomedical image segmentation. Medical Image Computing and Computer-Assisted Intervention&#x02013;MICCAI 2015: 18th International Conference, Munich, Germany, October 5&#x02013;9, 2015, Proceedings, Part III 18; 2015: Springer.</mixed-citation></ref><ref id="pone.0299623.ref011"><label>11</label><mixed-citation publication-type="journal"><name><surname>Krithika Alias AnbuDevi</surname><given-names>M</given-names></name>, <name><surname>Suganthi</surname><given-names>K</given-names></name>. <article-title>Review of Semantic Segmentation of Medical Images Using Modified Architectures of UNET</article-title>. <source>Diagnostics (Basel)</source>. <year>2022</year>;<volume>12</volume>(<issue>12</issue>). Epub 20221206. <comment>doi: </comment><pub-id pub-id-type="doi">10.3390/diagnostics12123064</pub-id> .<?supplied-pmid 36553071?><pub-id pub-id-type="pmid">36553071</pub-id>
</mixed-citation></ref><ref id="pone.0299623.ref012"><label>12</label><mixed-citation publication-type="journal"><name><surname>Wang</surname><given-names>S</given-names></name>, <name><surname>Yang</surname><given-names>DM</given-names></name>, <name><surname>Rong</surname><given-names>R</given-names></name>, <name><surname>Zhan</surname><given-names>X</given-names></name>, <name><surname>Xiao</surname><given-names>G</given-names></name>. <article-title>Pathology Image Analysis Using Segmentation Deep Learning Algorithms</article-title>. <source>Am J Pathol</source>. <year>2019</year>;<volume>189</volume>(<issue>9</issue>):<fpage>1686</fpage>&#x02013;<lpage>98</lpage>. Epub 20190611. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.ajpath.2019.05.007</pub-id> .<?supplied-pmid 31199919?><pub-id pub-id-type="pmid">31199919</pub-id>
</mixed-citation></ref><ref id="pone.0299623.ref013"><label>13</label><mixed-citation publication-type="journal"><name><surname>Isensee</surname><given-names>F</given-names></name>, <name><surname>Jaeger</surname><given-names>PF</given-names></name>, <name><surname>Kohl</surname><given-names>SAA</given-names></name>, <name><surname>Petersen</surname><given-names>J</given-names></name>, <name><surname>Maier-Hein</surname><given-names>KH</given-names></name>. <article-title>nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation</article-title>. <source>Nat Methods</source>. <year>2021</year>;<volume>18</volume>(<issue>2</issue>):<fpage>203</fpage>&#x02013;<lpage>11</lpage>. Epub 20201207. <comment>doi: </comment><pub-id pub-id-type="doi">10.1038/s41592-020-01008-z</pub-id> .<?supplied-pmid 33288961?><pub-id pub-id-type="pmid">33288961</pub-id>
</mixed-citation></ref><ref id="pone.0299623.ref014"><label>14</label><mixed-citation publication-type="journal"><name><surname>Antonelli</surname><given-names>M</given-names></name>, <name><surname>Reinke</surname><given-names>A</given-names></name>, <name><surname>Bakas</surname><given-names>S</given-names></name>, <name><surname>Farahani</surname><given-names>K</given-names></name>, <name><surname>Kopp-Schneider</surname><given-names>A</given-names></name>, <name><surname>Landman</surname><given-names>BA</given-names></name>, <etal>et al</etal>. <article-title>The Medical Segmentation Decathlon</article-title>. <source>Nat Commun</source>. <year>2022</year>;<volume>13</volume>(<issue>1</issue>):<fpage>4128</fpage>. Epub 20220715. <comment>doi: </comment><pub-id pub-id-type="doi">10.1038/s41467-022-30695-9</pub-id> .<?supplied-pmid 35840566?><pub-id pub-id-type="pmid">35840566</pub-id>
</mixed-citation></ref><ref id="pone.0299623.ref015"><label>15</label><mixed-citation publication-type="journal"><name><surname>Lee</surname><given-names>J</given-names></name>, <name><surname>Pant</surname><given-names>SR</given-names></name>, <name><surname>Lee H-</surname><given-names>S</given-names></name>. <article-title>An adaptive histogram equalization based local technique for contrast preserving image enhancement</article-title>. <source>International Journal of Fuzzy Logic and Intelligent Systems</source>. <year>2015</year>;<volume>15</volume>(<issue>1</issue>):<fpage>35</fpage>&#x02013;<lpage>44</lpage>.</mixed-citation></ref><ref id="pone.0299623.ref016"><label>16</label><mixed-citation publication-type="journal"><name><surname>Pizer</surname><given-names>SM</given-names></name>, <name><surname>Johnston</surname><given-names>RE</given-names></name>, <name><surname>Ericksen</surname><given-names>JP</given-names></name>, <name><surname>Yankaskas</surname><given-names>BC</given-names></name>, <name><surname>Muller</surname><given-names>KE</given-names></name>. <source>Contrast-limited adaptive histogram equalization: speed and effectiveness</source>. <year>1990</year>. p. <fpage>337</fpage>&#x02013;<lpage>45</lpage>.</mixed-citation></ref><ref id="pone.0299623.ref017"><label>17</label><mixed-citation publication-type="other">Li Y, Wang W, Yu D, editors. Application of adaptive histogram equalization to x-ray chest images. Second International Conference on Optoelectronic Science and Engineering&#x02019;94; 1994: Spie.</mixed-citation></ref><ref id="pone.0299623.ref018"><label>18</label><mixed-citation publication-type="journal"><name><surname>Zimmerman</surname><given-names>JB</given-names></name>, <name><surname>Pizer</surname><given-names>SM</given-names></name>, <name><surname>Staab</surname><given-names>EV</given-names></name>, <name><surname>Perry</surname><given-names>JR</given-names></name>, <name><surname>McCartney</surname><given-names>W</given-names></name>, <name><surname>Brenton</surname><given-names>BC</given-names></name>. <article-title>An evaluation of the effectiveness of adaptive histogram equalization for contrast enhancement</article-title>. <source>IEEE Transactions on Medical Imaging</source>. <year>1988</year>;<volume>7</volume>(<issue>4</issue>):<fpage>304</fpage>&#x02013;<lpage>12</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/42.14513</pub-id>
<?supplied-pmid 18230483?><pub-id pub-id-type="pmid">18230483</pub-id>
</mixed-citation></ref><ref id="pone.0299623.ref019"><label>19</label><mixed-citation publication-type="journal"><name><surname>Alwakid</surname><given-names>G</given-names></name>, <name><surname>Gouda</surname><given-names>W</given-names></name>, <name><surname>Humayun</surname><given-names>M</given-names></name>. <article-title>Deep Learning-Based Prediction of Diabetic Retinopathy Using CLAHE and ESRGAN for Enhancement</article-title>. <source>Healthcare (Basel)</source>. <year>2023</year>;<volume>11</volume>(<issue>6</issue>). Epub 20230315. <comment>doi: </comment><pub-id pub-id-type="doi">10.3390/healthcare11060863</pub-id> .<?supplied-pmid 36981520?><pub-id pub-id-type="pmid">36981520</pub-id>
</mixed-citation></ref><ref id="pone.0299623.ref020"><label>20</label><mixed-citation publication-type="journal"><name><surname>Yoshimi</surname><given-names>Y</given-names></name>, <name><surname>Mine</surname><given-names>Y</given-names></name>, <name><surname>Ito</surname><given-names>S</given-names></name>, <name><surname>Takeda</surname><given-names>S</given-names></name>, <name><surname>Okazaki</surname><given-names>S</given-names></name>, <name><surname>Nakamoto</surname><given-names>T</given-names></name>, <etal>et al</etal>. <article-title>Image preprocessing with contrast-limited adaptive histogram equalization improves the segmentation performance of deep learning for the articular disk of the temporomandibular joint on magnetic resonance images</article-title>. <source>Oral Surg Oral Med Oral Pathol Oral Radiol</source>. <year>2023</year>. Epub 20230414. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.oooo.2023.01.016</pub-id> .<?supplied-pmid 37263812?><pub-id pub-id-type="pmid">37263812</pub-id>
</mixed-citation></ref><ref id="pone.0299623.ref021"><label>21</label><mixed-citation publication-type="other">Tjoa EA, Suparta IPYN, Magdalena R, CP NK, editors. The use of CLAHE for improving an accuracy of CNN architecture for detecting pneumonia. SHS Web of Conferences; 2022: EDP Sciences.</mixed-citation></ref><ref id="pone.0299623.ref022"><label>22</label><mixed-citation publication-type="journal"><name><surname>Anand</surname><given-names>S</given-names></name>, <name><surname>Roshan</surname><given-names>R</given-names></name>. <article-title>Chest X ray image enhancement using deep contrast diffusion learning</article-title>. <source>Optik</source>. <year>2023</year>;<volume>279</volume>:<fpage>170751</fpage>.</mixed-citation></ref><ref id="pone.0299623.ref023"><label>23</label><mixed-citation publication-type="other">Wei C, Wang W, Yang W, Liu J. Deep retinex decomposition for low-light enhancement. arXiv preprint arXiv:180804560. 2018.</mixed-citation></ref><ref id="pone.0299623.ref024"><label>24</label><mixed-citation publication-type="journal"><name><surname>Shiraishi</surname><given-names>J</given-names></name>, <name><surname>Katsuragawa</surname><given-names>S</given-names></name>, <name><surname>Ikezoe</surname><given-names>J</given-names></name>, <name><surname>Matsumoto</surname><given-names>T</given-names></name>, <name><surname>Kobayashi</surname><given-names>T</given-names></name>, <name><surname>Komatsu</surname><given-names>K</given-names></name>, <etal>et al</etal>. <article-title>Development of a digital image database for chest radiographs with and without a lung nodule: receiver operating characteristic analysis of radiologists&#x02019; detection of pulmonary nodules</article-title>. <source>AJR Am J Roentgenol</source>. <year>2000</year>;<volume>174</volume>(<issue>1</issue>):<fpage>71</fpage>&#x02013;<lpage>4</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.2214/ajr.174.1.1740071</pub-id> .<?supplied-pmid 10628457?><pub-id pub-id-type="pmid">10628457</pub-id>
</mixed-citation></ref><ref id="pone.0299623.ref025"><label>25</label><mixed-citation publication-type="journal"><name><surname>van Ginneken</surname><given-names>B</given-names></name>, <name><surname>Stegmann</surname><given-names>MB</given-names></name>, <name><surname>Loog</surname><given-names>M</given-names></name>. <article-title>Segmentation of anatomical structures in chest radiographs using supervised methods: a comparative study on a public database</article-title>. <source>Med Image Anal</source>. <year>2006</year>;<volume>10</volume>(<issue>1</issue>):<fpage>19</fpage>&#x02013;<lpage>40</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.media.2005.02.002</pub-id> .<?supplied-pmid 15919232?><pub-id pub-id-type="pmid">15919232</pub-id>
</mixed-citation></ref><ref id="pone.0299623.ref026"><label>26</label><mixed-citation publication-type="other">Abadi M, Agarwal A, Barham P, Brevdo E, Chen Z, Citro C, et al. Tensorflow: Large-scale machine learning on heterogeneous distributed systems. arXiv preprint arXiv:160304467. 2016.</mixed-citation></ref><ref id="pone.0299623.ref027"><label>27</label><mixed-citation publication-type="other">He K, Zhang X, Ren S, Sun J, editors. Deep residual learning for image recognition. Proceedings of the IEEE conference on computer vision and pattern recognition; 2016.</mixed-citation></ref><ref id="pone.0299623.ref028"><label>28</label><mixed-citation publication-type="other">Liu S, Deng W, editors. Very deep convolutional neural network based image classification using small training sample size. 2015 3rd IAPR Asian conference on pattern recognition (ACPR); 2015: IEEE.</mixed-citation></ref><ref id="pone.0299623.ref029"><label>29</label><mixed-citation publication-type="other">Basri R, Galun M, Geifman A, Jacobs D, Kasten Y, Kritchman S, editors. Frequency bias in neural networks for input of non-uniform density. International Conference on Machine Learning; 2020: PMLR.</mixed-citation></ref><ref id="pone.0299623.ref030"><label>30</label><mixed-citation publication-type="journal"><name><surname>Yan</surname><given-names>C</given-names></name>, <name><surname>Combine</surname><given-names>AG</given-names></name>, <name><surname>Bednarz</surname><given-names>G</given-names></name>, <name><surname>Lalonde</surname><given-names>RJ</given-names></name>, <name><surname>Hu</surname><given-names>B</given-names></name>, <name><surname>Dickens</surname><given-names>K</given-names></name>, <etal>et al</etal>. <article-title>Clinical implementation and evaluation of the Acuros dose calculation algorithm</article-title>. <source>J Appl Clin Med Phys</source>. <year>2017</year>;<volume>18</volume>(<issue>5</issue>):<fpage>195</fpage>&#x02013;<lpage>209</lpage>. Epub 20170820. <comment>doi: </comment><pub-id pub-id-type="doi">10.1002/acm2.12149</pub-id> .<?supplied-pmid 28834214?><pub-id pub-id-type="pmid">28834214</pub-id>
</mixed-citation></ref><ref id="pone.0299623.ref031"><label>31</label><mixed-citation publication-type="journal"><name><surname>Sharp</surname><given-names>G</given-names></name>, <name><surname>Fritscher</surname><given-names>KD</given-names></name>, <name><surname>Pekar</surname><given-names>V</given-names></name>, <name><surname>Peroni</surname><given-names>M</given-names></name>, <name><surname>Shusharina</surname><given-names>N</given-names></name>, <name><surname>Veeraraghavan</surname><given-names>H</given-names></name>, <etal>et al</etal>. <article-title>Vision 20/20: perspectives on automated image segmentation for radiotherapy</article-title>. <source>Med Phys</source>. <year>2014</year>;<volume>41</volume>(<issue>5</issue>):<fpage>050902</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1118/1.4871620</pub-id> .<?supplied-pmid 24784366?><pub-id pub-id-type="pmid">24784366</pub-id>
</mixed-citation></ref><ref id="pone.0299623.ref032"><label>32</label><mixed-citation publication-type="journal"><name><surname>Qiu</surname><given-names>J</given-names></name>, <name><surname>Harold Li</surname><given-names>H</given-names></name>, <name><surname>Zhang</surname><given-names>T</given-names></name>, <name><surname>Ma</surname><given-names>F</given-names></name>, <name><surname>Yang</surname><given-names>D</given-names></name>. <article-title>Automatic x-ray image contrast enhancement based on parameter auto-optimization</article-title>. <source>J Appl Clin Med Phys</source>. <year>2017</year>;<volume>18</volume>(<issue>6</issue>):<fpage>218</fpage>&#x02013;<lpage>23</lpage>. Epub 20170906. <comment>doi: </comment><pub-id pub-id-type="doi">10.1002/acm2.12172</pub-id> .<?supplied-pmid 28875594?><pub-id pub-id-type="pmid">28875594</pub-id>
</mixed-citation></ref><ref id="pone.0299623.ref033"><label>33</label><mixed-citation publication-type="journal"><name><surname>Rouzrokh</surname><given-names>P</given-names></name>, <name><surname>Khosravi</surname><given-names>B</given-names></name>, <name><surname>Faghani</surname><given-names>S</given-names></name>, <name><surname>Moassefi</surname><given-names>M</given-names></name>, <name><surname>Vera Garcia</surname><given-names>DV</given-names></name>, <name><surname>Singh</surname><given-names>Y</given-names></name>, <etal>et al</etal>. <article-title>Mitigating Bias in Radiology Machine Learning: 1. Data Handling</article-title>. <source>Radiol Artif Intell</source>. <year>2022</year>;<volume>4</volume>(<issue>5</issue>):<fpage>e210290</fpage>. Epub 20220824. <comment>doi: </comment><pub-id pub-id-type="doi">10.1148/ryai.210290</pub-id> .<?supplied-pmid 36204544?><pub-id pub-id-type="pmid">36204544</pub-id>
</mixed-citation></ref><ref id="pone.0299623.ref034"><label>34</label><mixed-citation publication-type="journal"><name><surname>Tripathi</surname><given-names>S</given-names></name>, <name><surname>Gabriel</surname><given-names>K</given-names></name>, <name><surname>Dheer</surname><given-names>S</given-names></name>, <name><surname>Parajuli</surname><given-names>A</given-names></name>, <name><surname>Augustin</surname><given-names>AI</given-names></name>, <name><surname>Elahi</surname><given-names>A</given-names></name>, <etal>et al</etal>. <article-title>Understanding Biases and Disparities in Radiology AI Datasets: A Review</article-title>. <source>J Am Coll Radiol</source>. <year>2023</year>;<volume>20</volume>(<issue>9</issue>):<fpage>836</fpage>&#x02013;<lpage>41</lpage>. Epub 20230716. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.jacr.2023.06.015</pub-id> .<?supplied-pmid 37454752?><pub-id pub-id-type="pmid">37454752</pub-id>
</mixed-citation></ref><ref id="pone.0299623.ref035"><label>35</label><mixed-citation publication-type="other">Ganin Y, Lempitsky V, editors. Unsupervised domain adaptation by backpropagation. International conference on machine learning; 2015: PMLR.</mixed-citation></ref><ref id="pone.0299623.ref036"><label>36</label><mixed-citation publication-type="journal"><name><surname>Wang</surname><given-names>S</given-names></name>, <name><surname>Rong</surname><given-names>R</given-names></name>, <name><surname>Gu</surname><given-names>Z</given-names></name>, <name><surname>Fujimoto</surname><given-names>J</given-names></name>, <name><surname>Zhan</surname><given-names>X</given-names></name>, <name><surname>Xie</surname><given-names>Y</given-names></name>, <etal>et al</etal>. <article-title>Unsupervised domain adaptation for nuclei segmentation: Adapting from hematoxylin &#x00026; eosin stained slides to immunohistochemistry stained slides using a curriculum approach</article-title>. <source>Comput Methods Programs Biomed</source>. <year>2023</year>;<volume>241</volume>:<fpage>107768</fpage>. Epub 20230819. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.cmpb.2023.107768</pub-id> .<?supplied-pmid 37619429?><pub-id pub-id-type="pmid">37619429</pub-id>
</mixed-citation></ref><ref id="pone.0299623.ref037"><label>37</label><mixed-citation publication-type="journal"><name><surname>Liu</surname><given-names>W</given-names></name>, <name><surname>Luo</surname><given-names>J</given-names></name>, <name><surname>Yang</surname><given-names>Y</given-names></name>, <name><surname>Wang</surname><given-names>W</given-names></name>, <name><surname>Deng</surname><given-names>J</given-names></name>, <name><surname>Yu</surname><given-names>L</given-names></name>. <article-title>Automatic lung segmentation in chest X-ray images using improved U-Net</article-title>. <source>Sci Rep</source>. <year>2022</year>;<volume>12</volume>(<issue>1</issue>):<fpage>8649</fpage>. Epub 20220523. <comment>doi: </comment><pub-id pub-id-type="doi">10.1038/s41598-022-12743-y</pub-id> .<?supplied-pmid 35606509?><pub-id pub-id-type="pmid">35606509</pub-id>
</mixed-citation></ref></ref-list></back><sub-article article-type="aggregated-review-documents" id="pone.0299623.r001" specific-use="decision-letter"><front-stub><article-id pub-id-type="doi">10.1371/journal.pone.0299623.r001</article-id><title-group><article-title>Decision Letter 0</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Shaikh</surname><given-names>Asadullah</given-names></name><role>Academic Editor</role></contrib></contrib-group><permissions><copyright-statement>&#x000a9; 2024 Asadullah Shaikh</copyright-statement><copyright-year>2024</copyright-year><copyright-holder>Asadullah Shaikh</copyright-holder><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><related-article ext-link-type="doi" xlink:href="10.1371/journal.pone.0299623" id="rel-obj001" related-article-type="reviewed-article"/><custom-meta-group><custom-meta><meta-name>Submission Version</meta-name><meta-value>0</meta-value></custom-meta></custom-meta-group></front-stub><body><p>
<named-content content-type="letter-date">5 Mar 2024</named-content>
</p><p><!-- <div> -->PONE-D-24-05839<!-- </div> --><!-- <div> -->Enhancing Semantic Segmentation in Chest X-Ray Images through Image Preprocessing: ps-KDE for Pixel-wise Substitution by Kernel Density Estimation<!-- </div> --><!-- <div> -->PLOS ONE</p><p>Dear Dr. Gu,</p><p>Thank you for submitting your manuscript to PLOS ONE. After careful consideration, we feel that it has merit but does not fully meet PLOS ONE&#x02019;s publication criteria as it currently stands. Therefore, we invite you to submit a revised version of the manuscript that addresses the points raised during the review process.</p><p>Please submit your revised manuscript by Apr 19 2024 11:59PM. If you will need more time than this to complete your revisions, please reply to this message or contact the journal office at&#x000a0;<email>plosone@plos.org</email>. When you're ready to submit your revision, log on to <ext-link xlink:href="https://www.editorialmanager.com/pone/" ext-link-type="uri">https://www.editorialmanager.com/pone/</ext-link> and select the 'Submissions Needing Revision' folder to locate your manuscript file.</p><p>Please include the following items when submitting your revised manuscript:<!-- </div> --><list list-type="bullet"><list-item><p>A rebuttal letter that responds to each point raised by the academic editor and reviewer(s). You should upload this letter as a separate file labeled 'Response to Reviewers'.</p></list-item><list-item><p>A marked-up copy of your manuscript that highlights changes made to the original version. You should upload this as a separate file labeled 'Revised Manuscript with Track Changes'.</p></list-item><list-item><p>An unmarked version of your revised paper without tracked changes. You should upload this as a separate file labeled 'Manuscript'.</p></list-item></list></p><p>If you would like to make changes to your financial disclosure, please include your updated statement in your cover letter. Guidelines for resubmitting your figure files are available below the reviewer comments at the end of this letter.</p><p>If applicable, we recommend that you deposit your laboratory protocols in protocols.io to enhance the reproducibility of your results. Protocols.io assigns your protocol its own identifier (DOI) so that it can be cited independently in the future. For instructions see: <ext-link xlink:href="https://journals.plos.org/plosone/s/submission-guidelines#loc-laboratory-protocols" ext-link-type="uri">https://journals.plos.org/plosone/s/submission-guidelines#loc-laboratory-protocols</ext-link>. Additionally, PLOS ONE offers an option for publishing peer-reviewed Lab Protocol articles, which describe protocols hosted on protocols.io. Read more information on sharing protocols at <ext-link xlink:href="https://plos.org/protocols?utm_medium=editorial-email&#x00026;utm_source=authorletters&#x00026;utm_campaign=protocols" ext-link-type="uri">https://plos.org/protocols?utm_medium=editorial-email&#x00026;utm_source=authorletters&#x00026;utm_campaign=protocols</ext-link>.</p><p>We look forward to receiving your revised manuscript.</p><p>Kind regards,</p><p>Asadullah Shaikh, Ph.D.</p><p>Academic Editor</p><p>PLOS ONE</p><p>Journal Requirements:</p><p>When submitting your revision, we need you to address these additional requirements.</p><p>1. Please ensure that your manuscript meets PLOS ONE's style requirements, including those for file naming. The PLOS ONE style templates can be found at&#x000a0;</p><p><ext-link xlink:href="https://journals.plos.org/plosone/s/file?id=wjVg/PLOSOne_formatting_sample_main_body.pdf" ext-link-type="uri">https://journals.plos.org/plosone/s/file?id=wjVg/PLOSOne_formatting_sample_main_body.pdf</ext-link> and&#x000a0;</p><p>
<ext-link xlink:href="https://journals.plos.org/plosone/s/file?id=ba62/PLOSOne_formatting_sample_title_authors_affiliations.pdf" ext-link-type="uri">https://journals.plos.org/plosone/s/file?id=ba62/PLOSOne_formatting_sample_title_authors_affiliations.pdf</ext-link>
</p><p>2. Please note that PLOS ONE has specific guidelines on code sharing for submissions in which author-generated code underpins the findings in the manuscript. In these cases, all author-generated code must be made available without restrictions upon publication of the work. Please review our guidelines at <ext-link xlink:href="https://journals.plos.org/plosone/s/materials-and-software-sharing#loc-sharing-code" ext-link-type="uri">https://journals.plos.org/plosone/s/materials-and-software-sharing#loc-sharing-code</ext-link> and ensure that your code is shared in a way that follows best practice and facilitates reproducibility and reuse.</p><p>[Note: HTML markup is below. Please do not edit.]</p><p>Reviewers' comments:</p><p>Reviewer's Responses to Questions</p><p>
<!-- <font color="black"> -->
<bold>Comments to the Author</bold>
</p><p>1. Is the manuscript technically sound, and do the data support the conclusions?</p><p>The manuscript must describe a technically sound piece of scientific research with data that supports the conclusions. Experiments must have been conducted rigorously, with appropriate controls, replication, and sample sizes. The conclusions must be drawn appropriately based on the data presented. <!-- </font> --></p><p>Reviewer #1:&#x000a0;Yes</p><p>Reviewer #2:&#x000a0;Partly</p><p>Reviewer #3:&#x000a0;Yes</p><p>**********</p><p><!-- <font color="black"> -->2. Has the statistical analysis been performed appropriately and rigorously? <!-- </font> --></p><p>Reviewer #1:&#x000a0;No</p><p>Reviewer #2:&#x000a0;I Don't Know</p><p>Reviewer #3:&#x000a0;Yes</p><p>**********</p><p><!-- <font color="black"> -->3. Have the authors made all data underlying the findings in their manuscript fully available?</p><p>The <ext-link xlink:href="http://www.plosone.org/static/policies.action#sharing" ext-link-type="uri">PLOS Data policy</ext-link> requires authors to make all data underlying the findings described in their manuscript fully available without restriction, with rare exception (please refer to the Data Availability Statement in the manuscript PDF file). The data should be provided as part of the manuscript or its supporting information, or deposited to a public repository. For example, in addition to summary statistics, the data points behind means, medians and variance measures should be available. If there are restrictions on publicly sharing data&#x02014;e.g. participant privacy or use of data from a third party&#x02014;those must be specified.<!-- </font> --></p><p>Reviewer #1:&#x000a0;Yes</p><p>Reviewer #2:&#x000a0;Yes</p><p>Reviewer #3:&#x000a0;Yes</p><p>**********</p><p><!-- <font color="black"> -->4. Is the manuscript presented in an intelligible fashion and written in standard English?</p><p>PLOS ONE does not copyedit accepted manuscripts, so the language in submitted articles must be clear, correct, and unambiguous. Any typographical or grammatical errors should be corrected at revision, so please note any specific errors here.<!-- </font> --></p><p>Reviewer #1:&#x000a0;Yes</p><p>Reviewer #2:&#x000a0;No</p><p>Reviewer #3:&#x000a0;Yes</p><p>**********</p><p><!-- <font color="black"> -->5. Review Comments to the Author</p><p>Please use the space provided to explain your answers to the questions above. You may also include additional comments for the author, including concerns about dual publication, research ethics, or publication ethics. (Please upload your review as an attachment if it exceeds 20,000 characters)<!-- </font> --></p><p>Reviewer #1:&#x000a0;This is a good manuscript that only needs to undergo a few minor changes:</p><p>1-The paper should undergo language editing before it can be published.</p><p>2-A literature review section is required.</p><p>3-The evaluation should be for the whole model in terms of loss, accuracy, intersection over union (IoU), dice, sensitivity, specificity, specificity, recall, and precision.</p><p>4-The pseudocode should be used to write algorithms 1 and 2. Furthermore, a flowchart will be good if included with algorithms.</p><p>5-To improve the readability of the paper, I suggest dividing the analysis into several subsections.</p><p>6-The authors should explain the dataset split (Training and Testing) rate (90% and 10% or 80% and 20% or 70% and 30%)</p><p>7-The author should provide evidence to support that the suggested model is better than the existing models.</p><p>8-The discussion and conclusions need to be rewritten to highlight the key contributions of this manuscript.</p><p>9-A flowchart for the suggested model is required to show the process steps.</p><p>Reviewer #2:&#x000a0;The authors submitted a manuscript entitled &#x0201c;Enhancing Semantic Segmentation in Chest X-Ray Images through Image Preprocessing: ps-KDE for Pixel-wise Substitution by Kernel Density Estimation&#x0201d;. The manuscript is well written and have the scope to be published in PLOS ONE, however, it&#x02019;s current version needs improvement, and there are some concerns that must be addressed in resubmission. The comments are:</p><p>1. Define MRI, CT, and other abbreviation in its first place in the document.</p><p>2. It would be good to present both of the algorithms in its standard form, not in image or screenshot.</p><p>3. There are advance versions of ResNet like ResNet50 etc , why the authors used ResNet34?</p><p>4. Result section is short and poor, please improve.</p><p>5. In introduction, add more work as the authors did not include literature as a separate section so the introduction part must be longer.</p><p>Reviewer #3:&#x000a0;The paper hasn&#x02019;t mentioned the dataset splitting (train, validation, test), this could give the reader in depth knowledge about the experiment.</p><p>The paper sorted the limitation while discussing the results, however, for regions where ps-KDE underperformed (such as the Right Clavicle and Right Lung), I recommend providing a more detailed analysis or hypothesis explaining why this might be the case as well as adding more evaluation metrics which help to give wider look in the performance. Including such a discussion would help in understanding the limitations of ps-KDE and in guiding future improvements.</p><p>I also recommend expanding the limitation and discussion to discuss the computational efficiency of ps-KDE compared to other methods, especially if the approach is to be recommended for clinical applications where processing speed can be a critical factor.</p><p>**********</p><p><!-- <font color="black"> -->6. PLOS authors have the option to publish the peer review history of their article (<ext-link xlink:href="https://journals.plos.org/plosone/s/editorial-and-peer-review-process#loc-peer-review-history" ext-link-type="uri">what does this mean?</ext-link>). If published, this will include your full peer review and any attached files.</p><p>If you choose &#x0201c;no&#x0201d;, your identity will remain anonymous but your review may still be made public.</p><p><bold>Do you want your identity to be public for this peer review?</bold> For information about this choice, including consent withdrawal, please see our <ext-link xlink:href="https://www.plos.org/privacy-policy" ext-link-type="uri">Privacy Policy</ext-link>.<!-- </font> --></p><p>Reviewer #1:&#x000a0;No</p><p>Reviewer #2:&#x000a0;No</p><p>Reviewer #3:&#x000a0;No</p><p>**********</p><p>[NOTE: If reviewer comments were submitted as an attachment file, they will be attached to this email and accessible via the submission site. Please log into your account, locate the manuscript record, and check for the action link "View Attachments". If this link does not appear, there are no attachment files.]</p><p>While revising your submission, please upload your figure files to the Preflight Analysis and Conversion Engine (PACE) digital diagnostic tool,&#x000a0;<ext-link xlink:href="https://pacev2.apexcovantage.com/" ext-link-type="uri">https://pacev2.apexcovantage.com/</ext-link>. PACE helps ensure that figures meet PLOS requirements. To use PACE, you must first register as a user. Registration is free. Then, login and navigate to the UPLOAD tab, where you will find detailed instructions on how to use the tool. If you encounter any issues or have any questions when using PACE, please email PLOS at&#x000a0;<email>figures@plos.org</email>. Please note that Supporting Information files do not need this step.</p></body></sub-article><sub-article article-type="author-comment" id="pone.0299623.r002"><front-stub><article-id pub-id-type="doi">10.1371/journal.pone.0299623.r002</article-id><title-group><article-title>Author response to Decision Letter 0</article-title></title-group><related-article ext-link-type="doi" xlink:href="10.1371/journal.pone.0299623" id="rel-obj002" related-article-type="editor-report"/><custom-meta-group><custom-meta><meta-name>Submission Version</meta-name><meta-value>1</meta-value></custom-meta></custom-meta-group></front-stub><body><p>
<named-content content-type="author-response-date">17 Apr 2024</named-content>
</p><p>Please see attached file.</p><supplementary-material id="pone.0299623.s002" position="float" content-type="local-data"><label>Attachment</label><caption><p>Submitted filename: <named-content content-type="submitted-filename">PLOS-ONE reviewer-response-Wang.docx</named-content></p></caption><media xlink:href="pone.0299623.s002.docx"/></supplementary-material></body></sub-article><sub-article article-type="aggregated-review-documents" id="pone.0299623.r003" specific-use="decision-letter"><front-stub><article-id pub-id-type="doi">10.1371/journal.pone.0299623.r003</article-id><title-group><article-title>Decision Letter 1</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Shaikh</surname><given-names>Asadullah</given-names></name><role>Academic Editor</role></contrib></contrib-group><permissions><copyright-statement>&#x000a9; 2024 Asadullah Shaikh</copyright-statement><copyright-year>2024</copyright-year><copyright-holder>Asadullah Shaikh</copyright-holder><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><related-article ext-link-type="doi" xlink:href="10.1371/journal.pone.0299623" id="rel-obj003" related-article-type="reviewed-article"/><custom-meta-group><custom-meta><meta-name>Submission Version</meta-name><meta-value>1</meta-value></custom-meta></custom-meta-group></front-stub><body><p>
<named-content content-type="letter-date">9 May 2024</named-content>
</p><p>Enhancing Semantic Segmentation in Chest X-Ray Images through Image Preprocessing: ps-KDE for Pixel-wise Substitution by Kernel Density Estimation</p><p>PONE-D-24-05839R1</p><p>Dear Dr. Gu,</p><p>We&#x02019;re pleased to inform you that your manuscript has been judged scientifically suitable for publication and will be formally accepted for publication once it meets all outstanding technical requirements.</p><p>Within one week, you&#x02019;ll receive an e-mail detailing the required amendments. When these have been addressed, you&#x02019;ll receive a formal acceptance letter and your manuscript will be scheduled for publication.</p><p>An invoice will be generated when your article is formally accepted. Please note, if your institution has a publishing partnership with PLOS and your article meets the relevant criteria, all or part of your publication costs will be covered. Please make sure your user information is up-to-date by logging into Editorial Manager at <ext-link xlink:href="https://www.editorialmanager.com/pone/" ext-link-type="uri">Editorial Manager&#x000ae;</ext-link>&#x000a0;and clicking the &#x02018;Update My Information' link at the top of the page. If you have any questions relating to publication charges, please contact our Author Billing department directly at <email>authorbilling@plos.org</email>.</p><p>If your institution or institutions have a press office, please notify them about your upcoming paper to help maximize its impact. If they&#x02019;ll be preparing press materials, please inform our press team as soon as possible -- no later than 48 hours after receiving the formal acceptance. Your manuscript will remain under strict press embargo until 2 pm Eastern Time on the date of publication. For more information, please contact <email>onepress@plos.org</email>.</p><p>Kind regards,</p><p>Asadullah Shaikh, Ph.D.</p><p>Academic Editor</p><p>PLOS ONE</p><p>Additional Editor Comments (optional):</p><p>Reviewers' comments:</p><p>Reviewer's Responses to Questions</p><p>
<!-- <font color="black"> -->
<bold>Comments to the Author</bold>
</p><p>1. If the authors have adequately addressed your comments raised in a previous round of review and you feel that this manuscript is now acceptable for publication, you may indicate that here to bypass the &#x0201c;Comments to the Author&#x0201d; section, enter your conflict of interest statement in the &#x0201c;Confidential to Editor&#x0201d; section, and submit your "Accept" recommendation.<!-- </font> --></p><p>Reviewer #1:&#x000a0;All comments have been addressed</p><p>Reviewer #3:&#x000a0;All comments have been addressed</p><p>**********</p><p><!-- <font color="black"> -->2. Is the manuscript technically sound, and do the data support the conclusions?</p><p>The manuscript must describe a technically sound piece of scientific research with data that supports the conclusions. Experiments must have been conducted rigorously, with appropriate controls, replication, and sample sizes. The conclusions must be drawn appropriately based on the data presented. <!-- </font> --></p><p>Reviewer #1:&#x000a0;Yes</p><p>Reviewer #3:&#x000a0;Yes</p><p>**********</p><p><!-- <font color="black"> -->3. Has the statistical analysis been performed appropriately and rigorously? <!-- </font> --></p><p>Reviewer #1:&#x000a0;Yes</p><p>Reviewer #3:&#x000a0;Yes</p><p>**********</p><p><!-- <font color="black"> -->4. Have the authors made all data underlying the findings in their manuscript fully available?</p><p>The <ext-link xlink:href="http://www.plosone.org/static/policies.action#sharing" ext-link-type="uri">PLOS Data policy</ext-link> requires authors to make all data underlying the findings described in their manuscript fully available without restriction, with rare exception (please refer to the Data Availability Statement in the manuscript PDF file). The data should be provided as part of the manuscript or its supporting information, or deposited to a public repository. For example, in addition to summary statistics, the data points behind means, medians and variance measures should be available. If there are restrictions on publicly sharing data&#x02014;e.g. participant privacy or use of data from a third party&#x02014;those must be specified.<!-- </font> --></p><p>Reviewer #1:&#x000a0;Yes</p><p>Reviewer #3:&#x000a0;Yes</p><p>**********</p><p><!-- <font color="black"> -->5. Is the manuscript presented in an intelligible fashion and written in standard English?</p><p>PLOS ONE does not copyedit accepted manuscripts, so the language in submitted articles must be clear, correct, and unambiguous. Any typographical or grammatical errors should be corrected at revision, so please note any specific errors here.<!-- </font> --></p><p>Reviewer #1:&#x000a0;Yes</p><p>Reviewer #3:&#x000a0;Yes</p><p>**********</p><p><!-- <font color="black"> -->6. Review Comments to the Author</p><p>Please use the space provided to explain your answers to the questions above. You may also include additional comments for the author, including concerns about dual publication, research ethics, or publication ethics. (Please upload your review as an attachment if it exceeds 20,000 characters)<!-- </font> --></p><p>Reviewer #1:&#x000a0;Dear Author</p><p>Thank you for your response and for taking all the comments in your consideration.</p><p>Kind Regards</p><p>Reviewer #3:&#x000a0;(No Response)</p><p>**********</p><p><!-- <font color="black"> -->7. PLOS authors have the option to publish the peer review history of their article (<ext-link xlink:href="https://journals.plos.org/plosone/s/editorial-and-peer-review-process#loc-peer-review-history" ext-link-type="uri">what does this mean?</ext-link>). If published, this will include your full peer review and any attached files.</p><p>If you choose &#x0201c;no&#x0201d;, your identity will remain anonymous but your review may still be made public.</p><p><bold>Do you want your identity to be public for this peer review?</bold> For information about this choice, including consent withdrawal, please see our <ext-link xlink:href="https://www.plos.org/privacy-policy" ext-link-type="uri">Privacy Policy</ext-link>.<!-- </font> --></p><p>Reviewer #1:&#x000a0;No</p><p>Reviewer #3:&#x000a0;No</p><p>**********</p></body></sub-article><sub-article article-type="editor-report" id="pone.0299623.r004" specific-use="acceptance-letter"><front-stub><article-id pub-id-type="doi">10.1371/journal.pone.0299623.r004</article-id><title-group><article-title>Acceptance letter</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Shaikh</surname><given-names>Asadullah</given-names></name><role>Academic Editor</role></contrib></contrib-group><permissions><copyright-statement>&#x000a9; 2024 Asadullah Shaikh</copyright-statement><copyright-year>2024</copyright-year><copyright-holder>Asadullah Shaikh</copyright-holder><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><related-article ext-link-type="doi" xlink:href="10.1371/journal.pone.0299623" id="rel-obj004" related-article-type="reviewed-article"/></front-stub><body><p>
<named-content content-type="letter-date">14 May 2024</named-content>
</p><p>PONE-D-24-05839R1 </p><p>PLOS ONE</p><p>Dear Dr. Gu, </p><p>I'm pleased to inform you that your manuscript has been deemed suitable for publication in PLOS ONE. Congratulations! Your manuscript is now being handed over to our production team.</p><p>At this stage, our production department will prepare your paper for publication. This includes ensuring the following:</p><p>* All references, tables, and figures are properly cited</p><p>* All relevant supporting information is included in the manuscript submission,</p><p>* There are no issues that prevent the paper from being properly typeset</p><p>If revisions are needed, the production department will contact you directly to resolve them. If no revisions are needed, you will receive an email when the publication date has been set. At this time, we do not offer pre-publication proofs to authors during production of the accepted work. Please keep in mind that we are working through a large volume of accepted articles, so please give us a few weeks to review your paper and let you know the next and final steps. </p><p>Lastly, if your institution or institutions have a press office, please let them know about your upcoming paper now to help maximize its impact. If they'll be preparing press materials, please inform our press team within the next 48 hours. Your manuscript will remain under strict press embargo until 2 pm Eastern Time on the date of publication. For more information, please contact <email>onepress@plos.org</email>.</p><p>If we can help with anything else, please email us at <email>customercare@plos.org</email>.</p><p>Thank you for submitting your work to PLOS ONE and supporting open access. </p><p>Kind regards, </p><p>PLOS ONE Editorial Office Staff</p><p>on behalf of</p><p>Prof. Asadullah Shaikh </p><p>Academic Editor</p><p>PLOS ONE</p></body></sub-article></article>