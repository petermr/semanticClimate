<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd"> 
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><?properties open_access?><?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?><?DTDIdentifier.IdentifierType public?><?SourceDTD.DTDName A++V2.4.dtd?><?SourceDTD.Version 2.4?><?ConverterInfo.XSLTName springer2nlmx2.xsl?><?ConverterInfo.Version 1?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sci Rep</journal-id><journal-id journal-id-type="iso-abbrev">Sci Rep</journal-id><journal-title-group><journal-title>Scientific Reports</journal-title></journal-title-group><issn pub-type="epub">2045-2322</issn><publisher><publisher-name>Nature Publishing Group UK</publisher-name><publisher-loc>London</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmcid">11059262</article-id><article-id pub-id-type="pmid">38684904</article-id>
<article-id pub-id-type="publisher-id">60668</article-id><article-id pub-id-type="doi">10.1038/s41598-024-60668-5</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Deep learning-aided 3D proxy-bridged region-growing framework for multi-organ segmentation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Chen</surname><given-names>Zhihong</given-names></name><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author"><name><surname>Yao</surname><given-names>Lisha</given-names></name><xref ref-type="aff" rid="Aff2">2</xref><xref ref-type="aff" rid="Aff3">3</xref></contrib><contrib contrib-type="author"><name><surname>Liu</surname><given-names>Yue</given-names></name><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff4">4</xref></contrib><contrib contrib-type="author"><name><surname>Han</surname><given-names>Xiaorui</given-names></name><xref ref-type="aff" rid="Aff5">5</xref></contrib><contrib contrib-type="author"><name><surname>Gong</surname><given-names>Zhengze</given-names></name><xref ref-type="aff" rid="Aff6">6</xref></contrib><contrib contrib-type="author"><name><surname>Luo</surname><given-names>Jichao</given-names></name><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author"><name><surname>Zhao</surname><given-names>Jietong</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Fang</surname><given-names>Gang</given-names></name><address><email>gangf@gzhu.edu.cn</email></address><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff2">2</xref></contrib><aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/05ar8rn06</institution-id><institution-id institution-id-type="GRID">grid.411863.9</institution-id><institution-id institution-id-type="ISNI">0000 0001 0067 3588</institution-id><institution>Institute of Computing Science and Technology, </institution><institution>Guangzhou University, </institution></institution-wrap>Guangzhou, 510006 China </aff><aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="GRID">grid.484195.5</institution-id><institution>Guangdong Provincial Key Laboratory of Artificial Intelligence in Medical Image Analysis and Application, </institution></institution-wrap>Guangzhou, 510080 China </aff><aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/0530pts50</institution-id><institution-id institution-id-type="GRID">grid.79703.3a</institution-id><institution-id institution-id-type="ISNI">0000 0004 1764 3838</institution-id><institution>School of Medicine, </institution><institution>South China University of Technology, </institution></institution-wrap>Guangzhou, 510180 China </aff><aff id="Aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/05d5kcc69</institution-id><institution-id institution-id-type="GRID">grid.496827.6</institution-id><institution-id institution-id-type="ISNI">0000 0004 1761 5802</institution-id><institution>School of Information Engineering, </institution><institution>Jiangxi College of Applied Technology, </institution></institution-wrap>Ganzhou, 341000 China </aff><aff id="Aff5"><label>5</label>Department of Radiology, School of Medicine, Guangzhou First People&#x02019;s Hospital, South China University of Technology, Guangzhou, 510180 China </aff><aff id="Aff6"><label>6</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/02bwytq13</institution-id><institution-id institution-id-type="GRID">grid.413432.3</institution-id><institution-id institution-id-type="ISNI">0000 0004 1798 5993</institution-id><institution>Information and Data Centre, School of Medicine, Guangzhou First People&#x02019;s Hospital, </institution><institution>South China University of Technology Guangdong, </institution></institution-wrap>Guangzhou, 510180 China </aff></contrib-group><pub-date pub-type="epub"><day>29</day><month>4</month><year>2024</year></pub-date><pub-date pub-type="pmc-release"><day>29</day><month>4</month><year>2024</year></pub-date><pub-date pub-type="collection"><year>2024</year></pub-date><volume>14</volume><elocation-id>9784</elocation-id><history><date date-type="received"><day>13</day><month>5</month><year>2023</year></date><date date-type="accepted"><day>25</day><month>4</month><year>2024</year></date></history><permissions><copyright-statement>&#x000a9; The Author(s) 2024</copyright-statement><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p></license></permissions><abstract id="Abs1"><p id="Par1">Accurate multi-organ segmentation&#x000a0;in 3D CT images is imperative for enhancing computer-aided diagnosis and radiotherapy planning. However, current deep learning-based methods for 3D multi-organ segmentation face challenges such as the need for labor-intensive manual pixel-level annotations and high hardware resource demands, especially regarding GPU resources. To address these issues, we propose a 3D proxy-bridged region-growing framework specifically designed for the segmentation of the liver and spleen. Specifically, a key slice is selected from each 3D volume according to the corresponding intensity histogram. Subsequently, a deep learning model is employed to pinpoint the semantic central patch on this key slice, to calculate the growing seed. To counteract the impact of noise, segmentation of the liver and spleen is conducted on superpixel images created through proxy-bridging strategy. The segmentation process is then extended to adjacent slices by applying the same methodology iteratively, culminating in the comprehensive segmentation results. Experimental results demonstrate that the proposed framework accomplishes segmentation of the liver and spleen with an average Dice Similarity Coefficient of approximately 0.93 and a Jaccard Similarity Coefficient of around 0.88. These outcomes substantiate the framework's capability to achieve performance on par with that of deep learning methods, albeit requiring less guidance information and lower GPU resources.</p></abstract><kwd-group xml:lang="en"><title>Keywords</title><kwd>Multi-organ segmentation</kwd><kwd>3D CT image</kwd><kwd>Region-growing</kwd><kwd>Deep learning</kwd><kwd>Proxy-bridging</kwd></kwd-group><kwd-group kwd-group-type="npg-subject"><title>Subject terms</title><kwd>Computational biology and bioinformatics</kwd><kwd>Engineering</kwd></kwd-group><funding-group><award-group><funding-source><institution>Guangdong Provincial Key Laboratory of Artificial Intelligence in Medical Image Analysis and Application</institution></funding-source><award-id>2022B1212010011</award-id><principal-award-recipient><name><surname>Yao</surname><given-names>Lisha</given-names></name></principal-award-recipient></award-group></funding-group><funding-group><award-group><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001809</institution-id><institution>National Natural Science Foundation of China</institution></institution-wrap></funding-source><award-id>61972107</award-id><principal-award-recipient><name><surname>Fang</surname><given-names>Gang</given-names></name></principal-award-recipient></award-group></funding-group><custom-meta-group><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#x000a9; Springer Nature Limited 2024</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1"><title>Introduction</title><p id="Par2">Multi-organ segmentation in 3D abdominal computed tomography (CT) images is a pivotal task for computer-aided diagnosis, radiotherapy, and surgical planning<sup><xref ref-type="bibr" rid="CR1">1</xref></sup>. Accurate segmentation results can provide valuable information, including organ location, size, and boundary, which are crucial for clinical diagnosis and the subsequent clinical workflow<sup><xref ref-type="bibr" rid="CR2">2</xref></sup>. However, manual annotation of organs slice-by-slice is tedious and often yields low reproducibility, attributed to the low contrast in images and the extensive number of CT slices involved<sup><xref ref-type="bibr" rid="CR3">3</xref></sup>.</p><p id="Par3">To achieve automatic multi-organ segmentation, various methods have been proposed, which can be broadly categorized into traditional segmentation methods and deep learning segmentation methods. Traditional segmentation methods include region-growing<sup><xref ref-type="bibr" rid="CR4">4</xref></sup>, statistical shape model<sup><xref ref-type="bibr" rid="CR5">5</xref></sup> and structured random forest (SRF)<sup><xref ref-type="bibr" rid="CR6">6</xref></sup>. These methods exploit inherent intensity differences to design various features<sup><xref ref-type="bibr" rid="CR7">7</xref>,<xref ref-type="bibr" rid="CR8">8</xref></sup>. For instance, region-growing methods use defined growing seeds and growing conditions for multi-organ segmentation<sup><xref ref-type="bibr" rid="CR7">7</xref></sup>. However, selecting these features requires human intervention. Furthermore, since traditional segmentation methods rely on intensity values, their performance is inevitably compromised by inhomogeneous intensity and noise.</p><p id="Par4">Compared to traditional methods, deep learning methods have garnered increased attention for multi-organ segmentation<sup><xref ref-type="bibr" rid="CR2">2</xref>,<xref ref-type="bibr" rid="CR9">9</xref>,<xref ref-type="bibr" rid="CR10">10</xref></sup>. These methods automatically extract high-level and low-level features and encode them for segmentation. A typical segmentation network is the U-shaped network, which features an encoder-decoder structure<sup><xref ref-type="bibr" rid="CR11">11</xref></sup>. Although they achieve great success, these methods require extensive pixel-by-pixel annotations. Moreover, as deep learning models become larger and more complex, many traditional forms of computational power provision have struggled to meet their demands<sup><xref ref-type="bibr" rid="CR12">12</xref>,<xref ref-type="bibr" rid="CR13">13</xref></sup>. Simultaneously, it is crucial to seriously address accompanying problems, such as overfitting<sup><xref ref-type="bibr" rid="CR14">14</xref></sup>.</p><p id="Par5">Inspired by these works, we propose an automatic 3D hierarchical framework that combines the advantages of traditional methods and deep learning to achieve multi-organ segmentation in abdominal CT images. Specifically, we first select a key slice of each 3D volume based on the intensity histogram statistics. Then, we train a supervised vision transformer (ViT) model<sup><xref ref-type="bibr" rid="CR15">15</xref></sup> to automatically locate the semantic central region for calculating seed points. To mitigate the effect of noise, we employ a proxy-bridged strategy that transforms the original CT images into superpixel images for multi-organ segmentation. After obtaining the segmentation results of the current slice, we calculate its centroids and use these as the growing seeds for neighboring slices, repeating the process until completion. Notably, we design three iterative termination conditions based on the morphological and density properties of the organs to ensure the iterative segmentation process terminates at the appropriate CT slice. Furthermore, the only guidance information required for the deep learning module throughout the process is the index of the semantic central patch on key slices, manually selected with a single click, thus eliminating the need for pixel-level annotations.</p><p id="Par6">The main contributions of this work can be summarized as follows:<list list-type="order"><list-item><p id="Par7">We propose a deep learning-aided framework for 3D abdominal multi-organ segmentation that requires only a single manual click as guidance, yet achieves segmentation performance comparable to the deep learning methods that need pixel-level annotations.</p></list-item><list-item><p id="Par8">We propose a novel key semantic slice selection strategy that constructs an intensity histogram based on prior information and uses the intensity distribution to calculate and select key semantic slices.</p></list-item><list-item><p id="Par9">We propose an innovative 3D proxy-bridged region-growing segmentation method that enhances image representation through proxy-bridging of input CT images, followed by segmentation leveraging the characteristics of the region-growing algorithm.</p></list-item><list-item><p id="Par10">We propose the idea of using the region-growing algorithm to analyze 2D data for achieving 3D organ segmentation, an approach that can be generalized to other scenarios.</p></list-item></list></p></sec><sec id="Sec2"><title>Related works</title><sec id="Sec3"><title>Segmentation with conventional methods</title><p id="Par11">Threshold-based methods are popular techniques for organ segmentation, which subdivide the image into several cohesive regions based on the intensity of the pixels<sup><xref ref-type="bibr" rid="CR16">16</xref>,<xref ref-type="bibr" rid="CR17">17</xref></sup>. Numerous algorithms have been proposed in this direction over recent years, including grayscale threshold<sup><xref ref-type="bibr" rid="CR18">18</xref></sup>, interactive pixel classification<sup><xref ref-type="bibr" rid="CR19">19</xref></sup>, and fuzzy rule algorithms<sup><xref ref-type="bibr" rid="CR20">20</xref></sup>. Although such methods perform well and have fast computational speed for simple tasks, they fail to take consideration of the spatial correlation information between voxels and are highly influenced by external disturbances, such as noise.</p><p id="Par12">Region-growing algorithm has been well-applied in organ segmentation researches based on the high similarity in voxel grayscale intensity within the intra-organ voxels in medical images<sup><xref ref-type="bibr" rid="CR4">4</xref>,<xref ref-type="bibr" rid="CR7">7</xref></sup>. The growth of regions relies on the connectivity of growing seeds with adjacent voxels, which depends on predefined growth conditions or similarity criteria according to grayscale intensity or color. Statistical information and prior knowledge assimilated in algorithm to make it adaptive. The algorithm also has some limitations: (1) the segmentation results depend on the selection of growing seeds and growing conditions, which requires human intervention, (2) it works poorly for images with a large overlap of grayscale ranges, and (3) the pattern of region-growing is also sensitive to noise<sup><xref ref-type="bibr" rid="CR21">21</xref></sup>. Therefore, region-growing is rarely used alone, many existing studies have combined it with other methods in order to achieve satisfactory performance<sup><xref ref-type="bibr" rid="CR22">22</xref>,<xref ref-type="bibr" rid="CR23">23</xref></sup>.</p><p id="Par13">Conventional methods often exhibit limited noise resistance. Lei et al.<sup><xref ref-type="bibr" rid="CR24">24</xref></sup> introduced FRFCM, an enhanced fuzzy c-means (FCM) algorithm using morphological reconstruction and membership filtering to integrate local spatial information and improve anti-noise capabilities. However, it tends to overly smooth clustering outcomes, resulting in lost edge details. The RSFCM algorithm<sup><xref ref-type="bibr" rid="CR25">25</xref></sup> emerged as a solution to enhance image pixel relationship analysis, employing spatial correlations and a reliability indicator alongside local similarity metrics to further improve noise resistance. Most discussions surrounding noise-resistant segmentation algorithms focus on FCM, underscoring its significance. However, a major drawback exists in that the method's reliance on randomly selected initial clustering centers can detrimentally affect its efficiency if these centers are inappropriately chosen. Moreover, FCM struggles with segmentation tasks in complex scenarios, such as abdominal CT images.</p></sec><sec id="Sec4"><title>Segmentation with deep learning</title><p id="Par14">Although conventional methods have been broadly utilized for segmentation tasks in biomedical imaging over the last decade, they cannot always achieve acceptable results compared to current advanced artificial intelligence (AI) techniques. Recently, deep learning has made satisfactory progress in organ segmentation<sup><xref ref-type="bibr" rid="CR2">2</xref>,<xref ref-type="bibr" rid="CR10">10</xref></sup>. In deep learning models, data must be organized such that the machine can clearly decipher the information. However, pixel-level annotations are very costly and time-consuming to obtain. Additionally, supervised approaches may exhibit poor performances due to over-fitting (with excessive data load) or under-fitting (with insufficient data)<sup><xref ref-type="bibr" rid="CR26">26</xref></sup>. Therefore, deep learning methods may not always be feasible for medical image segmentation tasks, not only due to the amount of task-specific data but also the high costs associated with pixel-level annotation and the required expertise of annotators.</p><p id="Par15">The high annotation cost problem can be somewhat alleviated through semi-supervised learning (SSL)<sup><xref ref-type="bibr" rid="CR10">10</xref></sup>, wherein the model is iteratively retrained on the training set. This process involves data augmentation by adding unlabeled data and corresponding model predictions, termed pseudo-labels. In this vein, Li et al.<sup><xref ref-type="bibr" rid="CR27">27</xref></sup> introduced a semi-supervised approach for medical image segmentation, employing self-loop uncertainty as a novel pseudo-label to improve accuracy and efficiency with limited labeled data. MC-Net&#x02009;+&#x02009;<sup><xref ref-type="bibr" rid="CR28">28</xref></sup>, a novel semi-supervised network, utilizes a shared encoder and multiple decoders, enhanced by mutual consistency constraints, to refine the segmentation of indistinct regions. Recognizing the significant impact of pseudo-label quality on the model is important, as low-quality pseudo-labels may heighten the risk of judgment errors within the model.</p><p id="Par16">Weakly supervised learning (WSL) is considered another solution to this problem<sup><xref ref-type="bibr" rid="CR29">29</xref></sup>. These techniques require only weaker forms of training label annotation<sup><xref ref-type="bibr" rid="CR30">30</xref>,<xref ref-type="bibr" rid="CR31">31</xref></sup>, such as in the field of image segmentation, which ranges from weak to strong levels of supervision: 1) image tags, 2) size information of segmented objects, 3) points or curves labeling, and 4) bounding boxes of segmented objects. CAMEL<sup><xref ref-type="bibr" rid="CR32">32</xref></sup> utilized image-level labels and employed a Multiple Instance Learning method for label enrichment, enabling the automatic generation of instance- and pixel-level labels, thereby facilitating the training of segmentation models with performance comparable to that of fully supervised methods. C-CAM<sup><xref ref-type="bibr" rid="CR33">33</xref></sup> employed cause-effect chains to address the challenges of unclear object foreground boundaries and severe co-occurrence phenomena, generating superior pseudo masks and achieving enhanced segmentation performance. The weaker level of tagging enables models to be effectively trained, circumventing the risk of overfitting, while the information on labels that is easier to learn allows for reduced training time and lower data costs. However, WSL also presents significant challenges, including model fitting, prediction precision, and the complexities of formulating and optimizing loss functions, all of which demand considerable attention.</p></sec><sec id="Sec5"><title>Proxy-bridging strategy</title><p id="Par17">Proxy-bridging results in data smoothing and feature enhancement, which can eliminate the data's individual characteristics and enhance the model's generalization ability to some extent. Different proxy types, such as edge enhancement and image smoothing, can be selected according to the research purposes.</p><p id="Par18">Recently, as a typical proxy-bridged strategy, superpixels provide over-segmentation of an image by grouping pixels into homogeneous clusters based on intensity, texture, and other features. This approach represents image features with a small number of superpixels instead of a large number of pixels, significantly reducing the complexity of image post-processing<sup><xref ref-type="bibr" rid="CR34">34</xref>,<xref ref-type="bibr" rid="CR35">35</xref></sup>. For example,<sup><xref ref-type="bibr" rid="CR36">36</xref></sup> mitigates the identity mapping problem by using superpixel images as an intermediate proxy to bridge the input image and the reconstructed image.<sup><xref ref-type="bibr" rid="CR37">37</xref></sup> generates a salient map based on superpixel images to assist in breast lesion segmentation. Furthermore, incorporating information about the gradient change between each pixel and its adjacent ones<sup><xref ref-type="bibr" rid="CR38">38</xref></sup> can more effectively reveal the edge status of the tissue. Additionally, providing absolute position information assists in determining the distance between pixels, leading to more accurate and reliable segmentation<sup><xref ref-type="bibr" rid="CR39">39</xref></sup>.</p></sec></sec><sec id="Sec6"><title>Methods</title><sec id="Sec7"><title>Overview of framework</title><p id="Par19">Figure&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref> illustrates the overview of the proposed framework, which consists of five steps: (1) key slice selection, (2) proxy-bridging images generation, (3) deep learning based semantic central patch prediction for key slice, (4) key slice segmentation, and (5) hierarchical segmentation. The framework utilizes manual labeling information only during the first step (key slice selection) and the third step (semantic central patch prediction). In the first step, a minimal amount of pixel-level segmentation target annotation is required for histogram analysis. Additionally, the third step required a minimal amount of supervised information for segmenting key slices, which served as prior information for propagation to both upper and lower slices. This strategy culminates in achieving end-to-end 3D segmentation without the need for extensive manual labor.<fig id="Fig1"><label>Figure 1</label><caption><p>The overview of the proposed framework, which consists of five steps with different color markings: (<bold>a</bold>) key slice selection, (<bold>b</bold>) proxy-bridging images generation, (<bold>c</bold>) deep learning based semantic central patch prediction for key slice, (<bold>d</bold>) key slice segmentation, and (<bold>e</bold>) hierarchical segmentation.</p></caption><graphic xlink:href="41598_2024_60668_Fig1_HTML" id="MO1"/></fig></p></sec><sec id="Sec8"><title>Key slice selection</title><p id="Par20">Slices with larger target organ regions (also called key slices) are easier to segment, and the confidence in the segmentation results is higher. The idea of starting segmentation from key slice and iteratively propagating the result to neighboring slices for auxiliary segmentation has been widely adopted<sup><xref ref-type="bibr" rid="CR40">40</xref>,<xref ref-type="bibr" rid="CR41">41</xref></sup>.</p><p id="Par21">As shown in Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>a, we select the key slices of the liver and spleen in 3D CT images. The specific workflow is shown in Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref>. First, we randomly select 10 CT images and manually annotate the liver and spleen. We then construct histograms for the liver and spleen areas, respectively. Finally, for each histogram, we define the range of the gray values within the top 50% probabilities as a key interval, and select the slice that includes the most pixels within this key interval as the key slice.<fig id="Fig2"><label>Figure 2</label><caption><p>The workflow of gray histogram statistical analysis, where <inline-formula id="IEq44"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$n=10$$\end{document}</tex-math><mml:math id="M2"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_60668_Article_IEq44.gif"/></alternatives></inline-formula> in this study, and the interval of key voxels is calculated based on the top 50% of semantic gray values.</p></caption><graphic xlink:href="41598_2024_60668_Fig2_HTML" id="MO2"/></fig></p></sec><sec id="Sec9"><title>Proxy-bridged method</title><p id="Par22">In this study, we acquire the superpixels from CT images using the SLIC algorithm<sup><xref ref-type="bibr" rid="CR42">42</xref></sup>, which groups meaningful pixels into a superpixel by combining adjacent pixels spatially, and each superpixel is colored by the average gray value of the pixels within it.</p><p id="Par23">Specifically, SLIC is a methodology based on the idea of fuzzy C-means (FCM) clustering, which requires only one parameter k, the number of superpixels expected to be derived from segmentation. Suppose the image has <inline-formula id="IEq1"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$N$$\end{document}</tex-math><mml:math id="M4"><mml:mi>N</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_60668_Article_IEq1.gif"/></alternatives></inline-formula> pixels in total, then the average size of each superpixel is <inline-formula id="IEq2"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$N/k$$\end{document}</tex-math><mml:math id="M6"><mml:mrow><mml:mi>N</mml:mi><mml:mo stretchy="false">/</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_60668_Article_IEq2.gif"/></alternatives></inline-formula>. Therefore, the distance between the centers of adjacent superpixels is approximated as <inline-formula id="IEq3"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$S=\sqrt{N/k}$$\end{document}</tex-math><mml:math id="M8"><mml:mrow><mml:mi>S</mml:mi><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:mi>N</mml:mi><mml:mo stretchy="false">/</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msqrt></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_60668_Article_IEq3.gif"/></alternatives></inline-formula>. To avoid placing the initial center on edge or a noisy pixel, the center is moved to the lowest gradient position in its 3&#x02009;&#x000d7;&#x02009;3 neighborhood. The ensuing iterative process clusters each pixel by distance <inline-formula id="IEq4"><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$D$$\end{document}</tex-math><mml:math id="M10"><mml:mi>D</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_60668_Article_IEq4.gif"/></alternatives></inline-formula>, which consists of color distance <inline-formula id="IEq5"><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${d}_{c}$$\end{document}</tex-math><mml:math id="M12"><mml:msub><mml:mi>d</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_60668_Article_IEq5.gif"/></alternatives></inline-formula> and space distance <inline-formula id="IEq6"><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${d}_{s}$$\end{document}</tex-math><mml:math id="M14"><mml:msub><mml:mi>d</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_60668_Article_IEq6.gif"/></alternatives></inline-formula>, and the detailed formulations are as follows:<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${d}_{c}=\sqrt{{\left({l}_{j}-{l}_{i}\right)}^{2}+{\left({a}_{j}-{a}_{i}\right)}^{2}+{\left({b}_{j}-{b}_{i}\right)}^{2}}$$\end{document}</tex-math><mml:math id="M16" display="block"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:msup><mml:mrow><mml:mfenced close=")" open="("><mml:msub><mml:mi>l</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>l</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mfenced></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mfenced close=")" open="("><mml:msub><mml:mi>a</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mfenced></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mfenced close=")" open="("><mml:msub><mml:mi>b</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mfenced></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msqrt></mml:mrow></mml:math><graphic xlink:href="41598_2024_60668_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${d}_{s}=\sqrt{{\left({x}_{j}-{x}_{i}\right)}^{2}+{\left({y}_{j}-{y}_{i}\right)}^{2}}$$\end{document}</tex-math><mml:math id="M18" display="block"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:msup><mml:mrow><mml:mfenced close=")" open="("><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mfenced></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mfenced close=")" open="("><mml:msub><mml:mi>y</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mfenced></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msqrt></mml:mrow></mml:math><graphic xlink:href="41598_2024_60668_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$D=\sqrt{{\left(\frac{{d}_{c}}{m}\right)}^{2}+{\left(\frac{{d}_{s}}{S}\right)}^{2}}$$\end{document}</tex-math><mml:math id="M20" display="block"><mml:mrow><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:msup><mml:mrow><mml:mfenced close=")" open="("><mml:mfrac><mml:msub><mml:mi>d</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mi>m</mml:mi></mml:mfrac></mml:mfenced></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mfenced close=")" open="("><mml:mfrac><mml:msub><mml:mi>d</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mi>S</mml:mi></mml:mfrac></mml:mfenced></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msqrt></mml:mrow></mml:math><graphic xlink:href="41598_2024_60668_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula>where the metric of <inline-formula id="IEq7"><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${d}_{c}$$\end{document}</tex-math><mml:math id="M22"><mml:msub><mml:mi>d</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_60668_Article_IEq7.gif"/></alternatives></inline-formula> and <inline-formula id="IEq8"><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${d}_{s}$$\end{document}</tex-math><mml:math id="M24"><mml:msub><mml:mi>d</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_60668_Article_IEq8.gif"/></alternatives></inline-formula> are the L1 parametrization in the Lab color space and the coordinates in the image, respectively. In the Lab color space, the component <inline-formula id="IEq9"><alternatives><tex-math id="M25">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$l$$\end{document}</tex-math><mml:math id="M26"><mml:mi>l</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_60668_Article_IEq9.gif"/></alternatives></inline-formula> represents luminance, and the components <inline-formula id="IEq10"><alternatives><tex-math id="M27">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$a$$\end{document}</tex-math><mml:math id="M28"><mml:mi>a</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_60668_Article_IEq10.gif"/></alternatives></inline-formula> and <inline-formula id="IEq11"><alternatives><tex-math id="M29">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$b$$\end{document}</tex-math><mml:math id="M30"><mml:mi>b</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_60668_Article_IEq11.gif"/></alternatives></inline-formula> represent the relative color positions (<inline-formula id="IEq12"><alternatives><tex-math id="M31">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$a$$\end{document}</tex-math><mml:math id="M32"><mml:mi>a</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_60668_Article_IEq12.gif"/></alternatives></inline-formula>: red-green, <inline-formula id="IEq13"><alternatives><tex-math id="M33">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$b$$\end{document}</tex-math><mml:math id="M34"><mml:mi>b</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_60668_Article_IEq13.gif"/></alternatives></inline-formula>: yellow-blue). For image coordinates, <inline-formula id="IEq14"><alternatives><tex-math id="M35">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$x$$\end{document}</tex-math><mml:math id="M36"><mml:mi>x</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_60668_Article_IEq14.gif"/></alternatives></inline-formula> and <inline-formula id="IEq15"><alternatives><tex-math id="M37">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$y$$\end{document}</tex-math><mml:math id="M38"><mml:mi>y</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_60668_Article_IEq15.gif"/></alternatives></inline-formula> denote the position of the current pixel in the 2D key slice. For aggregate distance <inline-formula id="IEq16"><alternatives><tex-math id="M39">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$D$$\end{document}</tex-math><mml:math id="M40"><mml:mi>D</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_60668_Article_IEq16.gif"/></alternatives></inline-formula>, <inline-formula id="IEq17"><alternatives><tex-math id="M41">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$m$$\end{document}</tex-math><mml:math id="M42"><mml:mi>m</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_60668_Article_IEq17.gif"/></alternatives></inline-formula> represents the maximum color distance, controlling the compactness of the superpixels.</p><p id="Par24">The iterative process of SLIC can be considered a type of local FCM clustering, differing from standard FCM clustering in the area of pixels searched for each cluster center. Figure&#x000a0;<xref rid="Fig3" ref-type="fig">3</xref> illustrates the search area of each cluster center in the standard FCM clustering and local FCM clustering in SLIC. The search area of each cluster center in standard FCM clustering is the whole image, which requires computing the distance from each cluster center to each pixel within the image. In SLIC, however, the search space for cluster centers is restricted to a local <inline-formula id="IEq18"><alternatives><tex-math id="M43">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$2S\times 2S$$\end{document}</tex-math><mml:math id="M44"><mml:mrow><mml:mn>2</mml:mn><mml:mi>S</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mn>2</mml:mn><mml:mi>S</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_60668_Article_IEq18.gif"/></alternatives></inline-formula> square region.<fig id="Fig3"><label>Figure 3</label><caption><p>Illustrations of search areas of cluster center in standard FCM and local FCM in SLIC algorithm, in which sampling interval of cluster centers represents the average side length of square superpixels or the average distance between the centers of adjacent superpixels.</p></caption><graphic xlink:href="41598_2024_60668_Fig3_HTML" id="MO3"/></fig></p><p id="Par25">Additionally, to facilitate better data-model fit, we integrate extra information about the position in the slice and gradient with the surrounding eight voxels. Figure&#x000a0;<xref rid="Fig4" ref-type="fig">4</xref>a and b give examples of the superpixel strategy and the additional information respectively. It is noteworthy that the proxy-bridged method proposed in this study can minimize the data variability caused by the different CT scanners and CT acquisition protocols. Simultaneously, it enhances the edge information between image tissues, facilitating easier data segmentation.<fig id="Fig4"><label>Figure 4</label><caption><p>An example of proxy-bridged image. (<bold>a</bold>) Superpixel strategy, which the superpixel-image means the image consists of superpixels that colored with the average gray value in each superpixel. (<bold>b</bold>) Additional information addition, which adds the positional information of each pixel and the gradient information with its surrounding voxels to the original data.</p></caption><graphic xlink:href="41598_2024_60668_Fig4_HTML" id="MO4"/></fig></p></sec><sec id="Sec10"><title>Deep learning based semantic central patch prediction</title><p id="Par26">To overcome the limitation of the region-growing algorithm, which requires manual selection of growing seeds, we utilize a deep learning method to automate this process and achieve adaptive segmentation. Given that the computational complexity of the Transformer is quadratic in relation to the number of tokens (i.e., sequence length), it is impractical to directly flatten the input image into a sequence for the Transformer. Therefore, as illustrated in Fig.&#x000a0;<xref rid="Fig5" ref-type="fig">5</xref>, the ViT divides the image data into patches of fixed size and models their correlations as a sequence using a Transformer encoder. Additionally, the supervised signal (label) utilized in this step is the index of the patch containing the centroid. In the testing phase, once the patch containing the semantic center of the key slice has been localized, we calculate the coordinates of the initial growing seed based on the histogram statistical analysis as detailed in the &#x02018;Key slice selection&#x02019; subsection.<fig id="Fig5"><label>Figure 5</label><caption><p>The architecture of ViT. Norm, normalization; MLP, multilayer perceptron. The slice was split into fixed-size patches, each patch was linearly embedded and added with positional information to obtain a new sequence of vectors, which was then fed to a Transformer encoder. In order to achieve the central region location, an extra learnable &#x02018;location token&#x02019; was added to the sequence header.</p></caption><graphic xlink:href="41598_2024_60668_Fig5_HTML" id="MO5"/></fig></p><p id="Par27">Unlike existing 3D deep learning methods that require pixel-level organ annotations, the supervision signal (manual label) needed at this stage can be efficiently obtained with just a single click at the centroid of a key slice to generate the patch index. Furthermore, these key slices can be automatically identified using the &#x02018;Key slice selection&#x02019; strategy. This implies that the time required to annotate a single conventional 3D CT image of an organ (e.g., liver) at the pixel level could, in this step, be leveraged to generate supervision signals for hundreds of CT images.</p><p id="Par28">Assume that the input image is <inline-formula id="IEq19"><alternatives><tex-math id="M45">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$x\in {\mathbb{R}}^{H\times W}$$\end{document}</tex-math><mml:math id="M46"><mml:mrow><mml:mi>x</mml:mi><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_60668_Article_IEq19.gif"/></alternatives></inline-formula>, it is reshaped into a sequence of flattened patches <inline-formula id="IEq20"><alternatives><tex-math id="M47">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${x}_{p}\in {\mathbb{R}}^{N\times {P}^{2}}$$\end{document}</tex-math><mml:math id="M48"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:msup><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_60668_Article_IEq20.gif"/></alternatives></inline-formula>. Here, <inline-formula id="IEq21"><alternatives><tex-math id="M49">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$H\times W$$\end{document}</tex-math><mml:math id="M50"><mml:mrow><mml:mi>H</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_60668_Article_IEq21.gif"/></alternatives></inline-formula> denotes the shape of original image, <inline-formula id="IEq22"><alternatives><tex-math id="M51">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${P}^{2}$$\end{document}</tex-math><mml:math id="M52"><mml:msup><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:math><inline-graphic xlink:href="41598_2024_60668_Article_IEq22.gif"/></alternatives></inline-formula> represents the size of each split patch, and <inline-formula id="IEq23"><alternatives><tex-math id="M53">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$N=HW/{P}^{2}$$\end{document}</tex-math><mml:math id="M54"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mi>H</mml:mi><mml:mi>W</mml:mi><mml:mo stretchy="false">/</mml:mo><mml:msup><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_60668_Article_IEq23.gif"/></alternatives></inline-formula> indicates the total number of patches. Initially, patch embeddings <inline-formula id="IEq24"><alternatives><tex-math id="M55">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${E}_{patch}$$\end{document}</tex-math><mml:math id="M56"><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi mathvariant="italic">patch</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_60668_Article_IEq24.gif"/></alternatives></inline-formula> are obtained by projecting <inline-formula id="IEq25"><alternatives><tex-math id="M57">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${x}_{p}$$\end{document}</tex-math><mml:math id="M58"><mml:msub><mml:mi>x</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_60668_Article_IEq25.gif"/></alternatives></inline-formula> to <inline-formula id="IEq26"><alternatives><tex-math id="M59">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$D$$\end{document}</tex-math><mml:math id="M60"><mml:mi>D</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_60668_Article_IEq26.gif"/></alternatives></inline-formula> dimensions using a trainable linear projection. Subsequently, position embeddings <inline-formula id="IEq27"><alternatives><tex-math id="M61">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${E}_{pos}$$\end{document}</tex-math><mml:math id="M62"><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi mathvariant="italic">pos</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_60668_Article_IEq27.gif"/></alternatives></inline-formula> are added to the patch embeddings to preserve positional information. It is noteworthy that an additional learnable &#x02018;location token&#x02019; (<inline-formula id="IEq28"><alternatives><tex-math id="M63">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${Z}_{0}^{0}={x}_{loc}$$\end{document}</tex-math><mml:math id="M64"><mml:mrow><mml:msubsup><mml:mi>Z</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mn>0</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi mathvariant="italic">loc</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_60668_Article_IEq28.gif"/></alternatives></inline-formula>) is introduced to pinpoint the central region location, with the state at the output of the Transformer encoder (<inline-formula id="IEq29"><alternatives><tex-math id="M65">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${Z}_{0}^{L}$$\end{document}</tex-math><mml:math id="M66"><mml:msubsup><mml:mi>Z</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mi>L</mml:mi></mml:msubsup></mml:math><inline-graphic xlink:href="41598_2024_60668_Article_IEq29.gif"/></alternatives></inline-formula>) serving as the regional representation. The detailed calculation of embeddings is specified as follows:<disp-formula id="Equ4"><label>4</label><alternatives><tex-math id="M67">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${Z}_{0}=\left[{x}_{loc};{x}_{p}^{1}E;{x}_{p}^{2}E;\dots ;{x}_{p}^{N}E\right]+{E}_{pos},E\in {\mathbb{R}}^{{P}^{2}\times D},{E}_{pos}\in {\mathbb{R}}^{\left(N+1\right)\times D}$$\end{document}</tex-math><mml:math id="M68" display="block"><mml:mrow><mml:msub><mml:mi>Z</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mfenced close="]" open="["><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi mathvariant="italic">loc</mml:mi></mml:mrow></mml:msub><mml:mo>&#x0037e;</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msubsup><mml:mi>E</mml:mi><mml:mo>&#x0037e;</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mi>E</mml:mi><mml:mo>&#x0037e;</mml:mo><mml:mo>&#x022ef;</mml:mo><mml:mo>&#x0037e;</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mi>N</mml:mi></mml:msubsup><mml:mi>E</mml:mi></mml:mfenced><mml:mo>+</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi mathvariant="italic">pos</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>E</mml:mi><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>&#x000d7;</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi mathvariant="italic">pos</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mfenced close=")" open="("><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mfenced><mml:mo>&#x000d7;</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math><graphic xlink:href="41598_2024_60668_Article_Equ4.gif" position="anchor"/></alternatives></disp-formula></p><p id="Par29">And then, <inline-formula id="IEq30"><alternatives><tex-math id="M69">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${Z}_{0}$$\end{document}</tex-math><mml:math id="M70"><mml:msub><mml:mi>Z</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_60668_Article_IEq30.gif"/></alternatives></inline-formula> is fed into Transformer encoder<sup><xref ref-type="bibr" rid="CR43">43</xref></sup>, comprising <inline-formula id="IEq31"><alternatives><tex-math id="M71">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$L$$\end{document}</tex-math><mml:math id="M72"><mml:mi>L</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_60668_Article_IEq31.gif"/></alternatives></inline-formula> layers that each consist of multi-head self-Attention (MSA) and multi-layer perceptron (MLP) blocks. Layer normalization (LN) is applied before each block, and residual connections are implemented after each one<sup><xref ref-type="bibr" rid="CR44">44</xref>,<xref ref-type="bibr" rid="CR45">45</xref></sup>. This process can be described by the following formulas:<disp-formula id="Equ5"><label>5</label><alternatives><tex-math id="M73">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${Z}_{l}{\prime}=MSA\left(LN\left({Z}_{l-1}\right)\right)+{Z}_{l-1},l=\mathrm{1,2},\dots ,L$$\end{document}</tex-math><mml:math id="M74" display="block"><mml:mrow><mml:msub><mml:mi>Z</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo>&#x02032;</mml:mo><mml:mo>=</mml:mo><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:mi>A</mml:mi><mml:mfenced close=")" open="("><mml:mi>L</mml:mi><mml:mi>N</mml:mi><mml:mfenced close=")" open="("><mml:msub><mml:mi>Z</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mfenced></mml:mfenced><mml:mo>+</mml:mo><mml:msub><mml:mi>Z</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mo>&#x022ef;</mml:mo><mml:mo>,</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:math><graphic xlink:href="41598_2024_60668_Article_Equ5.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ6"><label>6</label><alternatives><tex-math id="M75">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${Z}_{l}=MLP\left(LN\left({Z}_{l}{\prime}\right)\right)+{Z}_{l}{\prime},l=\mathrm{1,2},\dots ,L$$\end{document}</tex-math><mml:math id="M76" display="block"><mml:mrow><mml:msub><mml:mi>Z</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>M</mml:mi><mml:mi>L</mml:mi><mml:mi>P</mml:mi><mml:mfenced close=")" open="("><mml:mi>L</mml:mi><mml:mi>N</mml:mi><mml:mfenced close=")" open="("><mml:msub><mml:mi>Z</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo>&#x02032;</mml:mo></mml:mfenced></mml:mfenced><mml:mo>+</mml:mo><mml:msub><mml:mi>Z</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo>&#x02032;</mml:mo><mml:mo>,</mml:mo><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mo>&#x022ef;</mml:mo><mml:mo>,</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:math><graphic xlink:href="41598_2024_60668_Article_Equ6.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ7"><label>7</label><alternatives><tex-math id="M77">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$y=LN\left({Z}_{l}^{0}\right)$$\end{document}</tex-math><mml:math id="M78" display="block"><mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mi>L</mml:mi><mml:mi>N</mml:mi><mml:mfenced close=")" open="("><mml:msubsup><mml:mi>Z</mml:mi><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mn>0</mml:mn></mml:msubsup></mml:mfenced></mml:mrow></mml:math><graphic xlink:href="41598_2024_60668_Article_Equ7.gif" position="anchor"/></alternatives></disp-formula></p><p id="Par30">After the Transformer Encoder, the output <inline-formula id="IEq32"><alternatives><tex-math id="M79">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$y$$\end{document}</tex-math><mml:math id="M80"><mml:mi>y</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_60668_Article_IEq32.gif"/></alternatives></inline-formula> is reshaped to a size of <inline-formula id="IEq33"><alternatives><tex-math id="M81">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\left(1, {N}_{region}\right)$$\end{document}</tex-math><mml:math id="M82"><mml:mfenced close=")" open="("><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi mathvariant="italic">region</mml:mi></mml:mrow></mml:msub></mml:mfenced></mml:math><inline-graphic xlink:href="41598_2024_60668_Article_IEq33.gif"/></alternatives></inline-formula> and subsequently subjected to a softmax activation function to determine the index of the central region. The network is trained using the classical cross-entropy loss <inline-formula id="IEq34"><alternatives><tex-math id="M83">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${L}_{CE}$$\end{document}</tex-math><mml:math id="M84"><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi mathvariant="italic">CE</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_60668_Article_IEq34.gif"/></alternatives></inline-formula>, as defined in Eq.&#x000a0;(<xref rid="Equ8" ref-type="disp-formula">8</xref>):<disp-formula id="Equ8"><label>8</label><alternatives><tex-math id="M85">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${L}_{CE}=\frac{1}{N}\sum_{i=1}^{{N}_{region}}{Y}_{i}log{y}_{i}+\left(1-{Y}_{i}\right)log\left(1-{y}_{i}\right)$$\end{document}</tex-math><mml:math id="M86" display="block"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi mathvariant="italic">CE</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi mathvariant="italic">region</mml:mi></mml:mrow></mml:msub></mml:munderover><mml:msub><mml:mi>Y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mfenced close=")" open="("><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mfenced><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mfenced close=")" open="("><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mfenced></mml:mrow></mml:math><graphic xlink:href="41598_2024_60668_Article_Equ8.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq35"><alternatives><tex-math id="M87">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${Y}_{i}$$\end{document}</tex-math><mml:math id="M88"><mml:msub><mml:mi>Y</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_60668_Article_IEq35.gif"/></alternatives></inline-formula> represents whether the i-th patch corresponds to the central region (e.g., 1 yes, 0 no). Finally, by leveraging key interval, the key voxels within the identified central region are determined. The coordinates of these voxels are then averaged to calculate the growing seed for the subsequent region-growing step.</p></sec><sec id="Sec11"><title>Hierarchical region-growing segmentation</title><p id="Par31">We propose a hierarchical segmentation strategy for multi-organ segmentation, as shown in Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref> (d) and (e). Initially, we combine the proxy-bridged key slice with a growing seed for segmentation. Subsequently, we initialize two lists <inline-formula id="IEq36"><alternatives><tex-math id="M89">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${L}_{global}$$\end{document}</tex-math><mml:math id="M90"><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi mathvariant="italic">global</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_60668_Article_IEq36.gif"/></alternatives></inline-formula> and <inline-formula id="IEq37"><alternatives><tex-math id="M91">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${L}_{local}$$\end{document}</tex-math><mml:math id="M92"><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi mathvariant="italic">local</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_60668_Article_IEq37.gif"/></alternatives></inline-formula> to store global seeds and local seeds, aimed at segmenting adjacent slices. As shown in Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>d, global seeds represent the centers of the organ across all slices, while local seeds correspond to three centers (global, upper, and lower parts) in the current slice. Introducing local seeds effectively reduces the risk of incomplete segmentation that occurs when the global semantic center is located at noisy voxels, such as vessels or lesions. During the region-growing segmentation, <inline-formula id="IEq38"><alternatives><tex-math id="M93">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${L}_{global}$$\end{document}</tex-math><mml:math id="M94"><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi mathvariant="italic">global</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_60668_Article_IEq38.gif"/></alternatives></inline-formula> and <inline-formula id="IEq39"><alternatives><tex-math id="M95">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${L}_{local}$$\end{document}</tex-math><mml:math id="M96"><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi mathvariant="italic">local</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_60668_Article_IEq39.gif"/></alternatives></inline-formula> are employed to calculate the mean and variance, respectively, determining the gray value interval for segmenting each slice. In this way, in terms of the segmentation interval in each slice, the global seeds can be used to ensure the medial axis, and the local seeds information can be used to calculate the upper and lower limits. Finally, we continue the process until the end of segmentation. To be mentioned, the segmentation results of each slice are performed a 2D closing operation to ensure the integrity of the semantic parts, and a 3D closing operation at the end of the hierarchical segmentation to ensure the coherence and consistency of semantic parts between the slices. This slice-by-slice parameter determination approach exhibits adaptive control and generalization capabilities, effectively mitigating over- and under-segmentation issues caused by disparate categorical assignments of identical intensity levels across different samples or slices. Additionally, when over- or under-segmentation occurs in a particular layer, its impact on the region-growing of the subsequent slice is relatively minor.</p><p id="Par32">To decide whether to continue with the hierarchical iterative segmentation, we have experimentally defined three termination conditions, as illustrated in Fig.&#x000a0;<xref rid="Fig6" ref-type="fig">6</xref>: (1) All growing seeds of the current slice fall outside of the key voxel gray scale range, (2) The number of semantic voxels resulting from the current slice&#x02019;s segmentation falls below 10, and (3) The global semantic center of the current slice is excessively distant from the adjacent 10 layers, with a maximum Euclidean distance greater than 50 units between the global centers of each slice.<fig id="Fig6"><label>Figure 6</label><caption><p>Iteration termination conditions in hierarchical segmentation, where the iteration ceases when one of them is satisfied. The gray range in condition 1 is obtained by histogram analysis in Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref>.</p></caption><graphic xlink:href="41598_2024_60668_Fig6_HTML" id="MO6"/></fig></p><p id="Par33">The comprehensive algorithm for the Hierarchical Region-growing Segmentation Algorithm is outlined in Algorithm 1.<fig position="anchor" id="Figa"><label>Algorithm 1</label><caption><p>Hierarchical region growing segmentation algorithm</p></caption><graphic position="anchor" xlink:href="41598_2024_60668_Figa_HTML" id="MO7"/></fig></p></sec><sec id="Sec12"><title>Evaluation metrics</title><p id="Par34">Two metrics were introduced to evaluate the properties of the growing seed localization model in this paper, accuracy (ACC) and error Euclidean distance (EED). Where ACC denotes the model's success rate in localizing the growing seed's region, whereas EED measures the distance between the localized region and the ground truth when localization at the patch level is incorrect (e.g., edge-adjacent is 1, vertex-adjacent is <inline-formula id="IEq40"><alternatives><tex-math id="M97">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\sqrt{1+1}=\sqrt{2}$$\end{document}</tex-math><mml:math id="M98"><mml:mrow><mml:msqrt><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msqrt><mml:mo>=</mml:mo><mml:msqrt><mml:mn>2</mml:mn></mml:msqrt></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_60668_Article_IEq40.gif"/></alternatives></inline-formula>, with the measurement unit being the patch).</p><p id="Par35">Furthermore, to assess the segmentation performance of the proposed framework, six evaluation metrics were employed in this study. The Dice Similarity Coefficient (DSC) assesses the accuracy of model segmentation, calculated as the intersection of two masks divided by the total area of both masks, with Eq.&#x000a0;(<xref rid="Equ9" ref-type="disp-formula">9</xref>) detailing this calculation. The Eq.&#x000a0;(<xref rid="Equ10" ref-type="disp-formula">10</xref>) represents the Jaccard Similarity Coefficient (JSC)<sup><xref ref-type="bibr" rid="CR46">46</xref></sup>, which imposes stricter penalties on over- and under-segmentation compared to the DSC.<disp-formula id="Equ9"><label>9</label><alternatives><tex-math id="M99">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$DSC=\frac{2TP}{FP+2TP+FN}$$\end{document}</tex-math><mml:math id="M100" display="block"><mml:mrow><mml:mi>D</mml:mi><mml:mi>S</mml:mi><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math><graphic xlink:href="41598_2024_60668_Article_Equ9.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ10"><label>10</label><alternatives><tex-math id="M101">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$Jaccard=\frac{TP}{FP+TP+FN}$$\end{document}</tex-math><mml:math id="M102" display="block"><mml:mrow><mml:mi>J</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="italic">TP</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math><graphic xlink:href="41598_2024_60668_Article_Equ10.gif" position="anchor"/></alternatives></disp-formula>where TP (True Positive) and FP (False Positive) represent semantic voxels that are correctly and incorrectly classified, respectively. Similarly, TN (True Negative) denotes correctly classified background voxels, while FN (False Negative) indicates background voxels that are incorrectly classified.</p><p id="Par36">Three additional metrics recall, specificity, and precision are routinely adopted to evaluate the segmented result. Recall, also known as sensitivity, focuses on the model's ability to detect true positives. Specificity, also referred to as the true negative rate, measures the proportion of background voxels that are correctly segmented. These three metrics are formulated as Eqs.&#x000a0;<xref rid="Equ11" ref-type="disp-formula">11</xref>, <xref rid="Equ12" ref-type="disp-formula">12</xref>, and <xref rid="Equ13" ref-type="disp-formula">13</xref>, respectively.<disp-formula id="Equ11"><label>11</label><alternatives><tex-math id="M103">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$Recall=\frac{TP}{TP+FN}$$\end{document}</tex-math><mml:math id="M104" display="block"><mml:mrow><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="italic">TP</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math><graphic xlink:href="41598_2024_60668_Article_Equ11.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ12"><label>12</label><alternatives><tex-math id="M105">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$Specificity=\frac{TN}{TN+FP}$$\end{document}</tex-math><mml:math id="M106" display="block"><mml:mrow><mml:mi>S</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="italic">TN</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math><graphic xlink:href="41598_2024_60668_Article_Equ12.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ13"><label>13</label><alternatives><tex-math id="M107">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$Precision=\frac{TP}{TP+FP}$$\end{document}</tex-math><mml:math id="M108" display="block"><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="italic">TP</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math><graphic xlink:href="41598_2024_60668_Article_Equ13.gif" position="anchor"/></alternatives></disp-formula></p><p id="Par37">The above metrics are focus on the internal voxel composition of the segmented mask, and in order to evaluate the model more comprehensively, this study incorporates Hausdorff Distance 95 (HD95) for a more holistic evaluation, specifically to assess boundary similarity with the ground truth. Defined in Eq.&#x000a0;(<xref rid="Equ14" ref-type="disp-formula">14</xref>), HD95 calculates the minimum distance for each voxel in set <inline-formula id="IEq41"><alternatives><tex-math id="M109">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$x (or y)$$\end{document}</tex-math><mml:math id="M110"><mml:mrow><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_60668_Article_IEq41.gif"/></alternatives></inline-formula> to set <inline-formula id="IEq42"><alternatives><tex-math id="M111">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$y (or x)$$\end{document}</tex-math><mml:math id="M112"><mml:mrow><mml:mi>y</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_60668_Article_IEq42.gif"/></alternatives></inline-formula>, subsequently adjusting the maximum of these distances by 95% to mitigate the effect of outliers<sup><xref ref-type="bibr" rid="CR47">47</xref></sup>.<disp-formula id="Equ14"><label>14</label><alternatives><tex-math id="M113">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$HD95={max}_{95\%}\left\{\genfrac{}{}{0pt}{}{max}{x\in X}\genfrac{}{}{0pt}{}{min}{y\in Y}d\left(x,y\right),\genfrac{}{}{0pt}{}{max}{y\in Y}\genfrac{}{}{0pt}{}{min}{x\in X}d\left(x,y\right)\right\}$$\end{document}</tex-math><mml:math id="M114" display="block"><mml:mrow><mml:mi>H</mml:mi><mml:mi>D</mml:mi><mml:mn>95</mml:mn><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="italic">max</mml:mi></mml:mrow><mml:mrow><mml:mn>95</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:msub><mml:mfenced close="}" open="{"><mml:mfrac linethickness="0.0pt"><mml:mrow><mml:mi mathvariant="italic">max</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi>X</mml:mi></mml:mrow></mml:mfrac><mml:mfrac linethickness="0.0pt"><mml:mrow><mml:mi mathvariant="italic">min</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi>Y</mml:mi></mml:mrow></mml:mfrac><mml:mi>d</mml:mi><mml:mfenced close=")" open="("><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mfenced><mml:mo>,</mml:mo><mml:mfrac linethickness="0.0pt"><mml:mrow><mml:mi mathvariant="italic">max</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi>Y</mml:mi></mml:mrow></mml:mfrac><mml:mfrac linethickness="0.0pt"><mml:mrow><mml:mi mathvariant="italic">min</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi>X</mml:mi></mml:mrow></mml:mfrac><mml:mi>d</mml:mi><mml:mfenced close=")" open="("><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mfenced></mml:mfenced></mml:mrow></mml:math><graphic xlink:href="41598_2024_60668_Article_Equ14.gif" position="anchor"/></alternatives></disp-formula></p></sec></sec><sec id="Sec13"><title>Results</title><sec id="Sec14"><title>Dataset and Preprocessing</title><p id="Par38">In this study, the proposed method was validated on a mixed dataset<sup><xref ref-type="bibr" rid="CR48">48</xref></sup>, which collected from MSD<sup><xref ref-type="bibr" rid="CR49">49</xref></sup>, NIH<sup><xref ref-type="bibr" rid="CR50">50</xref></sup>, KiTS<sup><xref ref-type="bibr" rid="CR51">51</xref></sup>, and LiTS<sup><xref ref-type="bibr" rid="CR52">52</xref></sup>. 330 contrasted CT volumes with complete annotations of liver and spleen were selected, the resolution of all CT volumes is 512&#x02009;&#x000d7;&#x02009;512 and the slice thickness ranges from 1.25 to 5&#x000a0;mm. All of them were rescaled within the range of [-240, 360] and normalized to zero mean and unit variance. In addition, a median filter with neighborhood size of 3&#x02009;&#x000d7;&#x02009;3 is used for spatial smoothing.</p><p id="Par39">During the &#x02018;semantic central patch prediction&#x02019;, for the training of the ViT model, data were allocated into training, testing, and validation sets with the ratio of 9:1:1 (270: 30: 30). To better harness the value of the data and increase the quantity, data augmentation was performed by selecting three of the key slices, instead of only the single key slice, from each dataset under the &#x02018;Key Slice Selection&#x02019; strategy. Consequently, a total of 990 slices (810: 90: 90) were utilized for the training of the localization model.</p></sec><sec id="Sec15"><title>Implementation details</title><p id="Par40">During the deep learning model training in the subsection &#x02018;Deep learning based semantic central patch prediction&#x02019; in &#x02018;Methods&#x02019;, the optimizer is the Adam optimizer<sup><xref ref-type="bibr" rid="CR53">53</xref></sup>, the training epoch is set as 500, the learning rate is set as 0.001, and the others are the default settings of the native ViT. The scale of each slice in both the initial inputs 3D CT data and the proxy image is 512&#x02009;&#x000d7;&#x02009;512, and the size of patches in ViT is 32&#x02009;&#x000d7;&#x02009;32. In the segmentation of the liver and spleen, the simple linear iterative clustering (SLIC) algorithm generates 800 and 1300 superpixels, respectively. The hyperparameter <inline-formula id="IEq43"><alternatives><tex-math id="M115">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\alpha$$\end{document}</tex-math><mml:math id="M116"><mml:mi>&#x003b1;</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_60668_Article_IEq43.gif"/></alternatives></inline-formula> in Algorithm 1 is adjusted to 3, dictating the degree of interval amplification.</p></sec><sec id="Sec16"><title>Segmentation performance</title><sec id="Sec17"><title>Growing seed localization</title><p id="Par41">Noting that after proxy-bridging, region-growing from any voxel inside the semantic scope can achieve satisfactory segmentation results in single key slice. Therefore, as shown in Fig.&#x000a0;<xref rid="Fig7" ref-type="fig">7</xref>, to further articulate the necessity of employing a deep learning-aided method that involves GPU resources in this step, we compared it with both a random strategy and a histogram-based localization strategy. In the random strategy, due to the difficulty of successfully randomizing the growth seed onto the segmentation target in a single attempt for key slice (with an average success rate of 10.32% for the liver and 2.21% for the spleen), the experiment continuously generates random seeds until one lands on a liver voxel before initiating the experiment. In the histogram-based localization strategy, the initial growth seed is determined by calculating the centroid of the key voxels selected through histogram analysis on the current key slice. Due to the overlapping key intervals of CT values for liver and spleen voxels, and the substantially larger size of the liver compared to the spleen, it is challenging to select growth seeds for the spleen using this strategy. Therefore, the experiment primarily focuses on the analysis of liver segmentation.<fig id="Fig7"><label>Figure 7</label><caption><p>Seed localization and segmentation results in the liver using different methods. The random strategy involves multiple selections at random until a liver voxel is hit, while in the histogram strategy, the blue voxels represent the key liver voxels obtained. The red dots in the first row indicate the positions of the initial seeds. The axial view results in the second row show the segmentation results of the key slices, and the coronal view in the third row illustrates the stopping positions of iterative segmentation.</p></caption><graphic xlink:href="41598_2024_60668_Fig7_HTML" id="MO8"/></fig></p><p id="Par42">It can be observed that the growing seed obtained through the deep learning-aided method not only play a crucial role in segmenting key slice but also in controlling the termination of iterative segmentation. This is attributed to the fact that, as indicated by the third condition in Fig.&#x000a0;<xref rid="Fig6" ref-type="fig">6</xref>, the termination of the subsequent iterative slice-by-slice segmentation is contingent upon the coherence of the growing seeds in each slice, and the selection of seeds situated too far from the semantic center may precipitate the premature and abnormal termination of the iterative process. Furthermore, within the improved region-growing algorithm that follows, the semantic center voxel of each layer collaboratively establishes the medial axis of the gray value interval. An unstable selection strategy for growth seeds may result in significant fluctuations within the gray value interval, thus adversely affecting segmentation efficacy.</p><p id="Par43">To evaluate the performance of the growing seed localization model in this study, we compared it with two others common models (VGG16<sup><xref ref-type="bibr" rid="CR54">54</xref></sup> and ResNet50<sup><xref ref-type="bibr" rid="CR55">55</xref></sup>), and these models are evaluated using 90 slices (3 slices are selected for each testing 3D CT data). Table <xref rid="Tab1" ref-type="table">1</xref> displays the average values of ACC and ED of the three models. The model used in this study demonstrates superior performance compared to the other two, achieving an ACC of over 0.93 and an EED is merely 1.1391 patches.<table-wrap id="Tab1"><label>Table 1</label><caption><p>The evaluation metrics (ACC and EED) results of three growing seed localization models,&#x000a0;the&#x000a0;improved ViT used in this&#x000a0;study achieved the optimal performance (bold shown).</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"/><th align="left">ACC</th><th align="left">EED</th></tr></thead><tbody><tr><td align="left">VGG16</td><td char="." align="char">0.8222</td><td char="." align="char">1.2803</td></tr><tr><td align="left">ResNet50</td><td char="." align="char">0.8556</td><td char="." align="char">1.1725</td></tr><tr><td align="left">This Study (ViT)</td><td char="." align="char"><bold>0.9333</bold></td><td char="." align="char"><bold>1.1391</bold></td></tr></tbody></table></table-wrap></p></sec><sec id="Sec18"><title>Comparison with other segmentation methods</title><p id="Par44">In this study, as the proposed segmentation framework combines traditional statistical methods with deep learning, we compare the segmentation performance of the proposed method with four typical segmentation methods: SRF<sup><xref ref-type="bibr" rid="CR6">6</xref></sup>, 3D-UNet (3DU)<sup><xref ref-type="bibr" rid="CR11">11</xref></sup>, 3D-Attention UNet (AttnUNet)<sup><xref ref-type="bibr" rid="CR56">56</xref></sup>, and UNETR<sup><xref ref-type="bibr" rid="CR57">57</xref></sup>. SRF utilizes ensemble learning to segment volumetric data effectively. 3DU is known for its strong generalization ability and simple structure. AttnUNet emphasizes important features using attention mechanisms, enhancing its performance in complex tasks. UNETR, a pure Transformer architecture, handles diverse datasets without traditional convolutional layers.</p><p id="Par45">Tables <xref rid="Tab2" ref-type="table">2</xref> and <xref rid="Tab3" ref-type="table">3</xref> list the quantitative results of liver and spleen, respectively. From the experimental results, the performance of the proposed framework in this study can be compared to deep learning methods and much higher than traditional method (SRF) overall. In liver segmentation, the proposed framework underperforms compared to AttnUNet and 3DU in terms of DSC, JSC, and Recall, yet it surpasses UNETR. This indicates that for liver with significant adhesion to surrounding tissues, the framework can more accurately differentiate liver voxels from other voxels, achieving a level of performance comparable to deep learning methods in this task. For the spleen, which is comparatively more autonomous, of a more regular shape, and exhibits lesser adhesion to surrounding tissues, the proposed framework outperforms in five metrics (except Recall). This demonstrates that for targets that are both more regularly shaped and more spatially independent, the framework enhances the efficacy of region-growing algorithms. By analyzing voxel intensities and their relative differences across layers, it achieves a finer distinction between the target and the background, thereby securing superior outcomes. In addition, it is obvious that the framework has superior potency in the HD95 (8.24 in liver and 2.26 in spleen), thanks to the control of segmentation by connectivity and gray scale interval in hierarchical segmentation.<table-wrap id="Tab2"><label>Table 2</label><caption><p>The segmentation performance of three different methods based on liver.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"/><th align="left">DSC</th><th align="left">JSC</th><th align="left">Recall</th><th align="left">Specificity</th><th align="left">Precision</th><th align="left">HD95</th></tr></thead><tbody><tr><td align="left">SRF</td><td char="." align="char">0.7820</td><td char="." align="char">0.6420</td><td char="." align="char">0.7325</td><td char="." align="char">0.9915</td><td char="." align="char">0.8387</td><td char="." align="char">15.4271</td></tr><tr><td align="left">3DU</td><td char="." align="char">0.9405</td><td char="." align="char">0.8876</td><td char="." align="char">0.9712</td><td char="." align="char">0.9954</td><td char="." align="char">0.9117</td><td char="." align="char">9.7741</td></tr><tr><td align="left">AttnUNet</td><td char="." align="char">0.9441</td><td char="." align="char">0.8962</td><td char="." align="char">0.9806</td><td char="." align="char">0.9957</td><td char="." align="char">0.9160</td><td char="." align="char">9.5898</td></tr><tr><td align="left">UNETR</td><td char="." align="char">0.9209</td><td char="." align="char">0.8398</td><td char="." align="char">0.9389</td><td char="." align="char">0.9937</td><td char="." align="char">0.8980</td><td char="." align="char">13.7268</td></tr><tr><td align="left">Ours</td><td char="." align="char">0.9343</td><td char="." align="char">0.8772</td><td char="." align="char">0.9513</td><td char="." align="char">0.9966</td><td char="." align="char">0.9192</td><td char="." align="char">8.2401</td></tr></tbody></table></table-wrap><table-wrap id="Tab3"><label>Table 3</label><caption><p>The segmentation performance of three different methods based on spleen.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"/><th align="left">DSC</th><th align="left">JSC</th><th align="left">Recall</th><th align="left">Specificity</th><th align="left">Precision</th><th align="left">HD95</th></tr></thead><tbody><tr><td align="left">SRF</td><td char="." align="char">0.7978</td><td char="." align="char">0.6636</td><td char="." align="char">0.7665</td><td char="." align="char">0.9989</td><td char="." align="char">0.8319</td><td char="." align="char">8.7047</td></tr><tr><td align="left">3DU</td><td char="." align="char">0.9260</td><td char="." align="char">0.8623</td><td char="." align="char">0.9195</td><td char="." align="char">0.9995</td><td char="." align="char">0.9327</td><td char="." align="char">4.9305</td></tr><tr><td align="left">AttnUNet</td><td char="." align="char">0.9313</td><td char="." align="char">0.8755</td><td char="." align="char">0.9153</td><td char="." align="char">0.9996</td><td char="." align="char">0.9514</td><td char="." align="char">4.1720</td></tr><tr><td align="left">UNETR</td><td char="." align="char">0.9281</td><td char="." align="char">0.8691</td><td char="." align="char">0.9063</td><td char="." align="char">0.9992</td><td char="." align="char">0.9407</td><td char="." align="char">7.7884</td></tr><tr><td align="left">Ours</td><td char="." align="char">0.9359</td><td char="." align="char">0.8800</td><td char="." align="char">0.9111</td><td char="." align="char">0.9998</td><td char="." align="char">0.9635</td><td char="." align="char">2.2608</td></tr></tbody></table></table-wrap></p><p id="Par46">Figure&#x000a0;<xref rid="Fig8" ref-type="fig">8</xref> shows the visualization of segmentation results of these methods, with cases 1&#x02013;3 primarily focusing on the segmentation of the liver, and cases 4&#x02013;5 on the segmentation of the spleen. Overall, the segmentation performance of the proposed framework significantly surpasses that of the traditional method SRF, and it demonstrates superior capabilities in handling specific scenarios compared to deep learning methods. Specifically, case 1 is characterized by a close adhesion between the liver and the stomach, with similar CT values, where 3DU and AttnUNet failed to accurately detect and segment the liver. UNETR, on the other hand, mistakenly included the intervening venous vessels as part of the segmentation target. The proposed framework, however, is capable of accurately and completely segmenting the liver through voxel intensity and connectivity analysis. Case 2 presents a similar situation, where all comparative methods mistakenly identified adjacent venous vessels as part of the liver, with only the proposed method being able to accurately discern it. Case 3 exhibits a small protrusion of liver tissue near the aorta that the three deep learning comparison methods failed to detect and segment. Although SRF managed to segment this area, it also resulted in severe over-segmentation. The proposed method, however, successfully segmented this area through connectivity and intensity assessments based on region growing. For case 4, 3DU showed under-segmentation of the spleen, while UNETR incorrectly included a portion of the splenic vein in its segmentation target. The proposed framework displayed stable performance, achieving segmentation results on par with AttnUNet. In case 5, there is a lymph node adjacent to the spleen with similar CT values, which all comparison methods misidentified as part of the spleen. However, the proposed method was able to exclude it, achieving a more precise segmentation.<fig id="Fig8"><label>Figure 8</label><caption><p>Visualization of the segmentation results of the proposed method and four other comparative methods, including SRF, 3DU, AttnUNet, and UNETR. The enhanced focus area signifies the effectiveness region of the proposed method, with Cases 1&#x02013;3 primarily discussing liver segmentation, while Cases 4&#x02013;5 are devoted to spleen segmentation.</p></caption><graphic xlink:href="41598_2024_60668_Fig8_HTML" id="MO9"/></fig></p></sec><sec id="Sec19"><title>Statistical analysis on significance</title><p id="Par47">To determine the significance of segmentation results and reveal genuine differences between the proposed and comparative methods, we employed Student's t-tests to compute p-values for both DSC and HD95 metrics, as depicted in Table <xref rid="Tab4" ref-type="table">4</xref>. DSC indicates the overlap between segmentation results and ground truth, while HD95 reflects the consistency of segmentation results with boundaries of ground truth when compared to other methods. It is observed that compared to all comparative methods, we consistently achieve <italic>p</italic>&#x02009;&#x0003c;&#x02009;0.05, indicating a statistically significant difference with our approach.<table-wrap id="Tab4"><label>Table 4</label><caption><p>P-values for statistical analysis between all comparative methods and proposed method.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="2">Method</th><th align="left" colspan="2">Liver</th><th align="left" colspan="2">Spleen</th></tr><tr><th align="left">DSC</th><th align="left">HD95</th><th align="left">DSC</th><th align="left">HD95</th></tr></thead><tbody><tr><td align="left">SRF</td><td align="left">5.21&#x02009;&#x000d7;&#x02009;10<sup>&#x02013;34</sup></td><td align="left">2.46&#x02009;&#x000d7;&#x02009;10<sup>&#x02013;5</sup></td><td align="left">1.22&#x02009;&#x000d7;&#x02009;10<sup>&#x02013;26</sup></td><td align="left">4.82&#x02009;&#x000d7;&#x02009;10<sup>&#x02013;8</sup></td></tr><tr><td align="left">3DU</td><td align="left">1.06&#x02009;&#x000d7;&#x02009;10<sup>&#x02013;6</sup></td><td align="left">2.75&#x02009;&#x000d7;&#x02009;10<sup>&#x02013;6</sup></td><td align="left">1.57&#x02009;&#x000d7;&#x02009;10<sup>&#x02013;2</sup></td><td align="left">1.49&#x02009;&#x000d7;&#x02009;10<sup>&#x02013;10</sup></td></tr><tr><td align="left">AttnUNet</td><td align="left">2.37&#x02009;&#x000d7;&#x02009;10<sup>&#x02013;21</sup></td><td align="left">4.62&#x02009;&#x000d7;&#x02009;10<sup>&#x02013;4</sup></td><td align="left">2.90&#x02009;&#x000d7;&#x02009;10<sup>&#x02013;2</sup></td><td align="left">4.23&#x02009;&#x000d7;&#x02009;10<sup>&#x02013;16</sup></td></tr><tr><td align="left">UNETR</td><td align="left">9.20&#x02009;&#x000d7;&#x02009;10<sup>&#x02013;6</sup></td><td align="left">2.80&#x02009;&#x000d7;&#x02009;10<sup>&#x02013;6</sup></td><td align="left">1.30&#x02009;&#x000d7;&#x02009;10<sup>&#x02013;3</sup></td><td align="left">5.59&#x02009;&#x000d7;&#x02009;10<sup>&#x02013;16</sup></td></tr></tbody></table></table-wrap></p></sec><sec id="Sec20"><title>Computational resource consumption analysis</title><p id="Par48">The consumption of computational resources also serves as a standard for gauging model performance. In both academic and industrial settings, minimizing computational resource use without sacrificing model efficacy is crucial. Noting that the SRF model, along with the iterative region-growing segmentation within our framework, does not involve GPU parallel computing nor does it incurs GPU memory consumption. Consequently, the SRF model is not considered in the calculation of resource consumption, and the discussion regarding the proposed framework is centered on the 2D ViT aspect. Table <xref rid="Tab5" ref-type="table">5</xref> compares the resource consumption of the proposed framework with that of three deep learning methods, focusing on parameter count (PARAMs) and floating-point operations per second (FLOPs).<table-wrap id="Tab5"><label>Table 5</label><caption><p>Comparison results of required computational resources in liver segmentation,&#x000a0;the proposed framework has the lowest required FLOPs&#x000a0;and PARAMs&#x000a0;(bold shown).</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Method</th><th align="left">FLOPs (G)</th><th align="left">PARAMs (M)</th></tr></thead><tbody><tr><td align="left">3DU</td><td char="." align="char">138.94</td><td char="." align="char">38.12</td></tr><tr><td align="left">AttnUNet</td><td char="." align="char">602.39</td><td char="." align="char">47.01</td></tr><tr><td align="left">UNETR</td><td char="." align="char">770.65</td><td char="." align="char">111.95</td></tr><tr><td align="left"><bold>Ours (2D ViT)</bold></td><td char="." align="char"><bold>2.04</bold></td><td char="." align="char"><bold>8.68</bold></td></tr></tbody></table></table-wrap></p><p id="Par49">It can be observed that the FLOPs required by the proposed framework amount to only about 1/68 of those for 3DU and approximately 1/378 of those required by UNETR, which similarly employs the Transformer architecture. Simultaneously, when compared to CNN-based models, the PARAMs needed by this framework amount to merely 22.77% for 3DU, 18.46% for AttnUNet, and just 7.75% for the Transformer-based architecture UNETR. This is attributed to deep learning methods requiring the analysis and judgment of every voxel within 3D feature maps, while this framework only needs to locate the Patch index of the semantic center in key slices. Consequently, with the premise of ensuring end-to-end 3D segmentation performance, the resource consumption required by this framework is significantly lower than that of existing deep learning methods.</p></sec></sec><sec id="Sec21"><title>Ablation studies</title><p id="Par50">A series of ablation experiments were conducted to validate the effectiveness of each component within the framework, with Table <xref rid="Tab6" ref-type="table">6</xref> reporting the results (DCS metric) of these ablation experiments. (a) The effectiveness of proxy-bridging method: This involved introducing (1) a superpixel image (SI) and (2) adding gradient and positional information (GPI) to each pixel, enhancing data representation and smoothing. From exp. 1&#x02013;3, we can observe that the segmentation framework performs better with proxy-bridging than without, which demonstrates that the introduction of SI can eliminate the noises, and GPI can further strengthen the boundaries to achieve better segmentation performance to a certain extent. (b) The impact of the number of superpixels: This number influences the size and the boundary pattern of each aggregation cluster. The results of exp. 4&#x02013;7 show that too much or too few numbers of superpixels cannot achieve the optimal segmentation performance, and 1300 is the preferred parameter in this study. (c) The performance of the proposed termination conditions (as shown in Fig.&#x000a0;<xref rid="Fig6" ref-type="fig">6</xref>): Exp. 8&#x02013;11 indicate that all three conditions enhance segmentation performance, with their combination yielding the best results. (d) The effectiveness of morphological closing operation (post-processing method): The results of exp 12&#x02013;14 demonstrate that closing operation could be an effective tool for correcting the segmentation results and getting better segmentation. (e) The necessity of local seeds in slice-wise segmentation: Exp. 15&#x02013;17 demonstrate how introducing local seeds, based on global seeds, influences segmentation results. Here, &#x02018;2-local seeds&#x02019; refers to the center voxels of the upper and lower parts, while &#x02018;4-local seeds&#x02019; refers to the center voxels of the top-left, top-right, bottom-left, and bottom-right parts. Results show that the selection of &#x02018;2-local seeds&#x02019; performs better than &#x02018;4-local seeds&#x02019;, which mainly because too many local seeds may force the framework to focus on a small portion of the region near the boundary and lead to incorrect segmentation results.<table-wrap id="Tab6"><label>Table 6</label><caption><p>The DSC results of different conditions for segmentation of liver,&#x000a0;with the conditions under which optimal DSC can be achieved in each setup&#x000a0;shown in bold.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Exp</th><th align="left">Different conditions</th><th align="left">DSC</th></tr></thead><tbody><tr><td align="left">1</td><td align="left">Without proxy</td><td align="left">0.9061</td></tr><tr><td align="left">2</td><td align="left">SI</td><td align="left">0.9311</td></tr><tr><td align="left">3</td><td align="left"><bold>SI&#x02009;+&#x02009;GPI</bold></td><td align="left"><bold>0.9343</bold></td></tr><tr><td align="left">4</td><td align="left">500 superpixels</td><td align="left">0.9126</td></tr><tr><td align="left">5</td><td align="left">900 superpixels</td><td align="left">0.9217</td></tr><tr><td align="left">6</td><td align="left"><bold>1300 superpixels</bold></td><td align="left"><bold>0.9343</bold></td></tr><tr><td align="left">7</td><td align="left">1700 superpixels</td><td align="left">0.9163</td></tr><tr><td align="left">8</td><td align="left">Without termination conditions</td><td align="left">0.5614</td></tr><tr><td align="left">9</td><td align="left">Condition 1</td><td align="left">0.6496</td></tr><tr><td align="left">10</td><td align="left">Condition 1, 2</td><td align="left">0.7817</td></tr><tr><td align="left">11</td><td align="left"><bold>Condition 1, 2, 3</bold></td><td align="left"><bold>0.9343</bold></td></tr><tr><td align="left">12</td><td align="left">Without closing operation</td><td align="left">0,8741</td></tr><tr><td align="left">13</td><td align="left">2D closing operation</td><td align="left">0.9117</td></tr><tr><td align="left">14</td><td align="left"><bold>2D, 3D closing operation</bold></td><td align="left"><bold>0.9343</bold></td></tr><tr><td align="left">15</td><td align="left">Without local seeds</td><td align="left">0.8577</td></tr><tr><td align="left">16</td><td align="left"><bold>2-local seeds</bold></td><td align="left"><bold>0.9343</bold></td></tr><tr><td align="left">17</td><td align="left">4-local seeds</td><td align="left">0.8814</td></tr></tbody></table></table-wrap></p></sec></sec><sec id="Sec22"><title>Discussion</title><p id="Par51">With the advancement of deep learning, there are increasingly more researches to conduct medical-industrial combination to solve tasks in medical image field. Combining emerging technologies to offset the limitations of traditional methods has become particularly meaningful, enhancing the analysis and processing capabilities for radiographic images. This study integrates deep learning, the proxy-bridging concept, and an improved region-growing algorithm to develop a novel hierarchical segmentation framework, aiming to match the performance of advanced deep learning models. To evaluate the effectiveness of the proposed method, we compared it with four other methods. The experimental results indicate that the proposed method could achieve great performance on liver and spleen segmentation.</p><p id="Par52">Furthermore, the ablation studies reveal that: (1) proxy-bridging technique that combines superpixel images as well as additional gradient and positional information can improve the fitness of the framework model to the segmentation task; (2) for vision tasks with hierarchical segmentation ideas, utilizing morphological closing operation for post-processing can considerably improve segmentation performance; (3) in terms of the region-growing algorithm, not only the location of the initial growing seeds need to be concerned, but the number of them also has a considerable impact on the segmentation performance. Therefore the determination of a specific number of seeds by a specific method for a specific task is also important.</p><p id="Par53">Figure&#x000a0;<xref rid="Fig9" ref-type="fig">9</xref> presents four cases with poor segmentation performance. In the initial pair of scenarios, the segmentation targets are strongly adhered to adjacent tissues with poor contrast, resulting in over-segmentation of hepatic tissue in Case 1 and splenic tissue in Case 2. The subsequent pair of cases reveal segmentation targets that are not contiguous within the imaging slice. Both the liver in Case 3 and the spleen in Case 4 suffer from the intrinsic limitations of the region-growing algorithm, which is ineffective at segmenting isolated outlier regions. These cases elucidate two significant limitations of the proposed framework: (1) it is difficult to segment voxels that are strongly adhered to irrelevant tissues and have extremely similar gray intensity, wherein the framework may treat these adhered voxels as entire and segment them; (2) due to the nature of region-growing, it has high requirements for the connectivity of segmented organ, and the non-connected organ cannot be completely segmented once time.<fig id="Fig9"><label>Figure 9</label><caption><p>Examples of cases with poor segmentation, with the magnified areas indicating the portions of segmentation errors. Cases 1 and 2 exhibit over-segmentation attributed to adhesion of tissues and low contrast, whereas Cases 3 and 4 present under-segmentation due to the discontinuity of the targeted segmentation regions.</p></caption><graphic xlink:href="41598_2024_60668_Fig9_HTML" id="MO10"/></fig></p></sec><sec id="Sec23"><title>Conclusions</title><p id="Par54">In this study, we propose a deep learning-aided 3D proxy-bridged region-growing framework designed for multi-organ segmentation. Specifically, the framework initially selects the key slice based on statistical information, setting the stage for proxy-bridging. It then identifies the semantic central patch based on deep learning methods and calculates growing seed. Subsequently segments it using region-growing algorithm, and finally iteratively segments the neighboring slices based on the segmentation result until completion. Experimental results demonstrate that the framework achieves satisfactory segmentation of the liver and spleen, with a DSC over 93%, a JSC around 88%, and significant improvements in HD95. In conclusion, this framework demonstrates performance comparable to various deep learning methods but requires fewer GPU resources and relies solely on a single-click for supervision (label) in the key slice, eliminating the need for pixel-level annotations. As a universal method, this framework can be generalized to other scenarios.</p></sec></body><back><fn-group><fn><p><bold>Publisher's note</bold></p><p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></fn></fn-group><notes notes-type="author-contribution"><title>Author contributions</title><p>Z.C. and G.F. presented the ideas, designed, and conducted relevant experiments in the manuscript. Z.C. and L.Y. wrote the manuscript. L.Y., Y.L., X.H. and G.F. are performed manuscript revision. Z.G., J.L. and J.Z. collected the samples used for the experiments. All authors read and approved the final manuscript.</p></notes><notes notes-type="funding-information"><title>Funding</title><p>This study is supported by National Natural Science Foundation of China (No. 61972107) and the open project of Guangdong Provincial Key Laboratory of Artificial Intelligence in Medical Image Analysis and Application (No. 2022B1212010011).</p></notes><notes notes-type="data-availability"><title>Data availability</title><p>The datasets analyzed during the current study are available in the AbdomenCT-1&#x000a0;K repository, <ext-link ext-link-type="uri" xlink:href="https://github.com/JunMa11/AbdomenCT-1K">https://github.com/JunMa11/AbdomenCT-1K</ext-link>.</p></notes><notes id="FPar1" notes-type="COI-statement"><title>Competing interests</title><p id="Par55">The authors declare no competing interests.</p></notes><ref-list id="Bib1"><title>References</title><ref id="CR1"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>S</given-names></name><name><surname>Liu</surname><given-names>M</given-names></name><name><surname>Lian</surname><given-names>J</given-names></name><name><surname>Shen</surname><given-names>D</given-names></name></person-group><article-title>Boundary coding representation for organ segmentation in prostate cancer radiotherapy</article-title><source>IEEE Trans. Med. Imaging</source><year>2020</year><volume>40</volume><fpage>310</fpage><lpage>320</lpage><pub-id pub-id-type="doi">10.1109/TMI.2020.3025517</pub-id><?supplied-pmid 32956051?><pub-id pub-id-type="pmid">32956051</pub-id>
</element-citation></ref><ref id="CR2"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Luo</surname><given-names>X</given-names></name><name><surname>Liao</surname><given-names>W</given-names></name><name><surname>Xiao</surname><given-names>J</given-names></name><name><surname>Chen</surname><given-names>J</given-names></name><name><surname>Song</surname><given-names>T</given-names></name><name><surname>Zhang</surname><given-names>X</given-names></name><etal/></person-group><article-title>WORD: A large scale dataset, benchmark and clinical applicable study for abdominal organ segmentation from CT image</article-title><source>Med. Image Anal.</source><year>2022</year><volume>82</volume><fpage>102642</fpage><pub-id pub-id-type="doi">10.1016/j.media.2022.102642</pub-id><?supplied-pmid 36223682?><pub-id pub-id-type="pmid">36223682</pub-id>
</element-citation></ref><ref id="CR3"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>C</given-names></name><name><surname>Mao</surname><given-names>Y</given-names></name><name><surname>Guo</surname><given-names>Y</given-names></name><name><surname>Li</surname><given-names>J</given-names></name><name><surname>Wang</surname><given-names>Y</given-names></name></person-group><article-title>Multi-dimensional cascaded net with uncertain probability reduction for abdominal multi-organ segmentation in CT sequences</article-title><source>Comput. Methods Programs Biomed.</source><year>2022</year><volume>221</volume><fpage>106887</fpage><pub-id pub-id-type="doi">10.1016/j.cmpb.2022.106887</pub-id><?supplied-pmid 35597204?><pub-id pub-id-type="pmid">35597204</pub-id>
</element-citation></ref><ref id="CR4"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Devi</surname><given-names>K</given-names></name><name><surname>Radhakrishnan</surname><given-names>R</given-names></name></person-group><article-title>Segmentation of multiple organ from abdominal CT images using 3D region growing and gradient vector flow</article-title><source>Int. J. Appl. Eng. Res.</source><year>2014</year><volume>9</volume><fpage>30023</fpage><lpage>30041</lpage></element-citation></ref><ref id="CR5"><label>5.</label><mixed-citation publication-type="other">J. Ma, F. Lin, S. Wesarg, and M. Erdt, "A novel bayesian model incorporating deep neural network and statistical shape model for pancreas segmentation," in <italic>International Conference on Medical Image Computing and Computer-Assisted Intervention</italic>, 2018, pp. 480&#x02013;487.</mixed-citation></ref><ref id="CR6"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Asaturyan</surname><given-names>H</given-names></name><name><surname>Gligorievski</surname><given-names>A</given-names></name><name><surname>Villarini</surname><given-names>B</given-names></name></person-group><article-title>Morphological and multi-level geometrical descriptor analysis in CT and MRI volumes for automatic pancreas segmentation</article-title><source>Comput. Med. Imaging Graph.</source><year>2019</year><volume>75</volume><fpage>1</fpage><lpage>13</lpage><pub-id pub-id-type="doi">10.1016/j.compmedimag.2019.04.004</pub-id><?supplied-pmid 31103856?><pub-id pub-id-type="pmid">31103856</pub-id>
</element-citation></ref><ref id="CR7"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lin</surname><given-names>D-T</given-names></name><name><surname>Lei</surname><given-names>C-C</given-names></name><name><surname>Hung</surname><given-names>S-W</given-names></name></person-group><article-title>Computer-aided kidney segmentation on abdominal CT images</article-title><source>IEEE Trans. Inf Technol. Biomed.</source><year>2006</year><volume>10</volume><fpage>59</fpage><lpage>65</lpage><pub-id pub-id-type="doi">10.1109/TITB.2005.855561</pub-id><?supplied-pmid 16445250?><pub-id pub-id-type="pmid">16445250</pub-id>
</element-citation></ref><ref id="CR8"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Okada</surname><given-names>T</given-names></name><name><surname>Linguraru</surname><given-names>MG</given-names></name><name><surname>Hori</surname><given-names>M</given-names></name><name><surname>Summers</surname><given-names>RM</given-names></name><name><surname>Tomiyama</surname><given-names>N</given-names></name><name><surname>Sato</surname><given-names>Y</given-names></name></person-group><article-title>Abdominal multi-organ segmentation from CT images using conditional shape&#x02013;location and unsupervised intensity priors</article-title><source>Med. Image Anal.</source><year>2015</year><volume>26</volume><fpage>1</fpage><lpage>18</lpage><pub-id pub-id-type="doi">10.1016/j.media.2015.06.009</pub-id><?supplied-pmid 26277022?><pub-id pub-id-type="pmid">26277022</pub-id>
</element-citation></ref><ref id="CR9"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tang</surname><given-names>P</given-names></name><name><surname>Zhao</surname><given-names>Y-Q</given-names></name><name><surname>Liao</surname><given-names>M</given-names></name></person-group><article-title>Automatic multi-organ segmentation from abdominal CT volumes with LLE-based graph partitioning and 3D Chan-Vese model</article-title><source>Comput. Biol. Med.</source><year>2021</year><volume>139</volume><fpage>105030</fpage><pub-id pub-id-type="doi">10.1016/j.compbiomed.2021.105030</pub-id><?supplied-pmid 34800809?><pub-id pub-id-type="pmid">34800809</pub-id>
</element-citation></ref><ref id="CR10"><label>10.</label><mixed-citation publication-type="other">Y. Zhou, Y. Wang, P. Tang, S. Bai, W. Shen, E. Fishman<italic>, et al.</italic>, "Semi-supervised 3D abdominal multi-organ segmentation via deep multi-planar co-training," in <italic>2019 IEEE Winter Conference on Applications of Computer Vision (WACV)</italic>, 2019, 121-140</mixed-citation></ref><ref id="CR11"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>J</given-names></name><name><surname>Bao</surname><given-names>Y</given-names></name><name><surname>Wen</surname><given-names>Y</given-names></name><name><surname>Lu</surname><given-names>H</given-names></name><name><surname>Luo</surname><given-names>H</given-names></name><name><surname>Xiang</surname><given-names>Y</given-names></name><etal/></person-group><article-title>Prior-attention residual learning for more discriminative COVID-19 screening in CT images</article-title><source>IEEE Trans. Med. Imaging</source><year>2020</year><volume>39</volume><fpage>2572</fpage><lpage>2583</lpage><pub-id pub-id-type="doi">10.1109/TMI.2020.2994908</pub-id><?supplied-pmid 32730210?><pub-id pub-id-type="pmid">32730210</pub-id>
</element-citation></ref><ref id="CR12"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Molanes</surname><given-names>RF</given-names></name><name><surname>Amarasinghe</surname><given-names>K</given-names></name><name><surname>Rodriguez-Andina</surname><given-names>J</given-names></name><name><surname>Manic</surname><given-names>M</given-names></name></person-group><article-title>Deep learning and reconfigurable platforms in the internet of things: Challenges and opportunities in algorithms and hardware</article-title><source>IEEE Ind. Electron. Mag.</source><year>2018</year><volume>12</volume><fpage>36</fpage><lpage>49</lpage><pub-id pub-id-type="doi">10.1109/MIE.2018.2824843</pub-id></element-citation></ref><ref id="CR13"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hu</surname><given-names>X</given-names></name><name><surname>Chu</surname><given-names>L</given-names></name><name><surname>Pei</surname><given-names>J</given-names></name><name><surname>Liu</surname><given-names>W</given-names></name><name><surname>Bian</surname><given-names>J</given-names></name></person-group><article-title>Model complexity of deep learning: A survey</article-title><source>Knowl. Inf. Syst.</source><year>2021</year><volume>63</volume><fpage>2585</fpage><lpage>2619</lpage><pub-id pub-id-type="doi">10.1007/s10115-021-01605-0</pub-id></element-citation></ref><ref id="CR14"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>Z</given-names></name><name><surname>Yan</surname><given-names>T</given-names></name><name><surname>Wang</surname><given-names>E</given-names></name><name><surname>Jiang</surname><given-names>H</given-names></name><name><surname>Tang</surname><given-names>Y</given-names></name><name><surname>Yu</surname><given-names>X</given-names></name><etal/></person-group><article-title>Detecting abnormal brain regions in schizophrenia using structural MRI via machine learning</article-title><source>Comput. Intell. Neurosci.</source><year>2020</year><volume>19</volume><issue>4</issue><fpage>1</fpage><lpage>26</lpage></element-citation></ref><ref id="CR15"><label>15.</label><mixed-citation publication-type="other">A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner<italic>, et al.</italic>, "An image is worth 16x16 words: Transformers for image recognition at scale," <italic>arXiv preprint </italic><ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2010.11929">arXiv:2010.11929</ext-link><italic>,</italic> 2020.</mixed-citation></ref><ref id="CR16"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Senthilkumaran</surname><given-names>N</given-names></name><name><surname>Vaithegi</surname><given-names>S</given-names></name></person-group><article-title>Image segmentation by using thresholding techniques for medical images</article-title><source>Comput. Sci. Eng.: Int. J.</source><year>2016</year><volume>6</volume><fpage>1</fpage><lpage>13</lpage></element-citation></ref><ref id="CR17"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>He</surname><given-names>L</given-names></name><name><surname>Huang</surname><given-names>S</given-names></name></person-group><article-title>Modified firefly algorithm based multilevel thresholding for color image segmentation</article-title><source>Neurocomputing</source><year>2017</year><volume>240</volume><fpage>152</fpage><lpage>174</lpage><pub-id pub-id-type="doi">10.1016/j.neucom.2017.02.040</pub-id></element-citation></ref><ref id="CR18"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mahdy</surname><given-names>LN</given-names></name><name><surname>Ezzat</surname><given-names>KA</given-names></name><name><surname>Torad</surname><given-names>M</given-names></name><name><surname>Hassanien</surname><given-names>AE</given-names></name></person-group><article-title>Automatic segmentation system for liver tumors based on the multilevel thresholding and electromagnetism optimization algorithm</article-title><source>Int. J. Imaging Syst. Technol.</source><year>2020</year><volume>30</volume><fpage>1256</fpage><lpage>1270</lpage><pub-id pub-id-type="doi">10.1002/ima.22432</pub-id></element-citation></ref><ref id="CR19"><label>19.</label><mixed-citation publication-type="other">B. Glocker, O. Pauly, E. Konukoglu, and A. Criminisi, "Joint classification-regression forests for spatially structured multi-object segmentation," in <italic>European Conference on Computer Vision</italic>, 2012, pp 870&#x02013;881.</mixed-citation></ref><ref id="CR20"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chakraborty</surname><given-names>T</given-names></name><name><surname>Banik</surname><given-names>SK</given-names></name><name><surname>Bhadra</surname><given-names>AK</given-names></name><name><surname>Nandi</surname><given-names>D</given-names></name></person-group><article-title>Dynamically learned PSO based neighborhood influenced fuzzy c-means for pre-treatment and post-treatment organ segmentation from CT images</article-title><source>Comput. Methods Programs Biomed.</source><year>2021</year><volume>202</volume><fpage>105971</fpage><pub-id pub-id-type="doi">10.1016/j.cmpb.2021.105971</pub-id><?supplied-pmid 33611030?><pub-id pub-id-type="pmid">33611030</pub-id>
</element-citation></ref><ref id="CR21"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ben&#x0010d;evi&#x00107;</surname><given-names>M</given-names></name><name><surname>Gali&#x00107;</surname><given-names>I</given-names></name><name><surname>Habijan</surname><given-names>M</given-names></name><name><surname>Pi&#x0017e;urica</surname><given-names>A</given-names></name></person-group><article-title>Recent progress in epicardial and pericardial adipose tissue segmentation and quantification based on deep learning: A systematic review</article-title><source>Appl. Sci.</source><year>2022</year><volume>12</volume><fpage>5217</fpage><pub-id pub-id-type="doi">10.3390/app12105217</pub-id></element-citation></ref><ref id="CR22"><label>22.</label><mixed-citation publication-type="other">B. Prencipe, N. Altini, G. D. Cascarano, A. Guerriero, and A. Brunetti, "A Novel approach based on region growing algorithm for liver and spleen segmentation from CT scans," in <italic>International Conference on Intelligent Computing</italic>, 2020, pp. 398&#x02013;410.</mixed-citation></ref><ref id="CR23"><label>23.</label><mixed-citation publication-type="other">P. M. Paithane, S. Kakarwal, and D. Kurmude, "Automatic Seeded Region Growing with Level Set Technique Used for Segmentation of Pancreas," in <italic>International Conference on Soft Computing and Pattern Recognition</italic>, 2020, pp. 374&#x02013;382.</mixed-citation></ref><ref id="CR24"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lei</surname><given-names>T</given-names></name><name><surname>Jia</surname><given-names>X</given-names></name><name><surname>Zhang</surname><given-names>Y</given-names></name><name><surname>He</surname><given-names>L</given-names></name><name><surname>Meng</surname><given-names>H</given-names></name><name><surname>Nandi</surname><given-names>AK</given-names></name></person-group><article-title>Significantly fast and robust fuzzy c-means clustering algorithm based on morphological reconstruction and membership filtering</article-title><source>IEEE Trans. Fuzzy Syst.</source><year>2018</year><volume>26</volume><fpage>3027</fpage><lpage>3041</lpage><pub-id pub-id-type="doi">10.1109/TFUZZ.2018.2796074</pub-id></element-citation></ref><ref id="CR25"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xu</surname><given-names>J</given-names></name><name><surname>Zhao</surname><given-names>T</given-names></name><name><surname>Feng</surname><given-names>G</given-names></name><name><surname>Ni</surname><given-names>M</given-names></name><name><surname>Ou</surname><given-names>S</given-names></name></person-group><article-title>A fuzzy C-means clustering algorithm based on spatial context model for image segmentation</article-title><source>Int. J. Fuzzy Syst.</source><year>2021</year><volume>23</volume><fpage>816</fpage><lpage>832</lpage><pub-id pub-id-type="doi">10.1007/s40815-020-01015-4</pub-id></element-citation></ref><ref id="CR26"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rasi</surname><given-names>D</given-names></name><name><surname>Deepa</surname><given-names>S</given-names></name></person-group><article-title>Hybrid optimization enabled deep learning model for colour image segmentation and classification</article-title><source>Neural Comput. Appl.</source><year>2022</year><volume>34</volume><fpage>21335</fpage><lpage>21352</lpage><pub-id pub-id-type="doi">10.1007/s00521-022-07614-6</pub-id></element-citation></ref><ref id="CR27"><label>27.</label><mixed-citation publication-type="other">Y. Li, J. Chen, X. Xie, K. Ma, and Y. Zheng, "Self-loop uncertainty: A novel pseudo-label for semi-supervised medical image segmentation," in <italic>Medical Image Computing and Computer Assisted Intervention&#x02013;MICCAI 2020: 23rd International Conference, Lima, Peru, October 4&#x02013;8, 2020, Proceedings, Part I 23</italic>, 2020, 614-623</mixed-citation></ref><ref id="CR28"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wu</surname><given-names>Y</given-names></name><name><surname>Ge</surname><given-names>Z</given-names></name><name><surname>Zhang</surname><given-names>D</given-names></name><name><surname>Xu</surname><given-names>M</given-names></name><name><surname>Zhang</surname><given-names>L</given-names></name><name><surname>Xia</surname><given-names>Y</given-names></name><etal/></person-group><article-title>Mutual consistency learning for semi-supervised medical image segmentation</article-title><source>Med. Image Anal.</source><year>2022</year><volume>81</volume><fpage>102530</fpage><pub-id pub-id-type="doi">10.1016/j.media.2022.102530</pub-id><?supplied-pmid 35839737?><pub-id pub-id-type="pmid">35839737</pub-id>
</element-citation></ref><ref id="CR29"><label>29.</label><mixed-citation publication-type="other">S. Lee, M. Lee, J. Lee, and H. Shim, "Railroad is not a train: Saliency as pseudo-pixel supervision for weakly supervised semantic segmentation," in <italic>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</italic>, 2021, 5495&#x02013;5505.</mixed-citation></ref><ref id="CR30"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>H</given-names></name><name><surname>Zeng</surname><given-names>Y</given-names></name><name><surname>Lu</surname><given-names>H</given-names></name><name><surname>Zhang</surname><given-names>L</given-names></name><name><surname>Li</surname><given-names>J</given-names></name><name><surname>Qi</surname><given-names>J</given-names></name></person-group><article-title>Learning to detect salient object with multi-source weak supervision</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2021</year><volume>44</volume><fpage>3577</fpage><lpage>3589</lpage></element-citation></ref><ref id="CR31"><label>31.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>F</given-names></name><name><surname>Shugurov</surname><given-names>I</given-names></name><name><surname>Busam</surname><given-names>B</given-names></name><name><surname>Yang</surname><given-names>S</given-names></name><name><surname>Ilic</surname><given-names>S</given-names></name></person-group><article-title>Ws-ope: Weakly supervised 6-d object pose regression using relative multi-camera pose constraints</article-title><source>IEEE Robotics Autom. Lett.</source><year>2022</year><volume>7</volume><fpage>3703</fpage><lpage>3710</lpage><pub-id pub-id-type="doi">10.1109/LRA.2022.3146924</pub-id></element-citation></ref><ref id="CR32"><label>32.</label><mixed-citation publication-type="other">G. Xu, Z. Song, Z. Sun, C. Ku, Z. Yang, C. Liu<italic>, et al.</italic>, "Camel: A weakly supervised learning framework for histopathology image segmentation," in <italic>Proceedings of the IEEE/CVF International Conference on computer vision</italic>, 2019, 10682&#x02013;10691.</mixed-citation></ref><ref id="CR33"><label>33.</label><mixed-citation publication-type="other">Z. Chen, Z. Tian, J. Zhu, C. Li, and S. Du, "C-cam: Causal cam for weakly supervised semantic segmentation on medical image," in <italic>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</italic>, 2022, 11676&#x02013;11685.</mixed-citation></ref><ref id="CR34"><label>34.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tian</surname><given-names>Z</given-names></name><name><surname>Liu</surname><given-names>L</given-names></name><name><surname>Zhang</surname><given-names>Z</given-names></name><name><surname>Fei</surname><given-names>B</given-names></name></person-group><article-title>Superpixel-based segmentation for 3D prostate MR images</article-title><source>IEEE Trans. Med. Imaging</source><year>2015</year><volume>35</volume><fpage>791</fpage><lpage>801</lpage><pub-id pub-id-type="doi">10.1109/TMI.2015.2496296</pub-id><?supplied-pmid 26540678?><pub-id pub-id-type="pmid">26540678</pub-id>
</element-citation></ref><ref id="CR35"><label>35.</label><mixed-citation publication-type="other">S. Li, Z. Gao, and X. He, "Superpixel-Guided Iterative Learning from Noisy Labels for Medical Image Segmentation," in <italic>International Conference on Medical Image Computing and Computer-Assisted Intervention</italic>, 2021, 525&#x02013;535.</mixed-citation></ref><ref id="CR36"><label>36.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhou</surname><given-names>K</given-names></name><name><surname>Li</surname><given-names>J</given-names></name><name><surname>Luo</surname><given-names>W</given-names></name><name><surname>Li</surname><given-names>Z</given-names></name><name><surname>Yang</surname><given-names>J</given-names></name><name><surname>Fu</surname><given-names>H</given-names></name><etal/></person-group><article-title>Proxy-bridged image reconstruction network for anomaly detection in medical images</article-title><source>IEEE Trans. Med. Imag.</source><year>2021</year><volume>41</volume><fpage>582</fpage><lpage>594</lpage><pub-id pub-id-type="doi">10.1109/TMI.2021.3118223</pub-id></element-citation></ref><ref id="CR37"><label>37.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ning</surname><given-names>Z</given-names></name><name><surname>Zhong</surname><given-names>S</given-names></name><name><surname>Feng</surname><given-names>Q</given-names></name><name><surname>Chen</surname><given-names>W</given-names></name><name><surname>Zhang</surname><given-names>Y</given-names></name></person-group><article-title>SMU-Net: saliency-guided morphology-aware U-net for breast lesion segmentation in ultrasound image</article-title><source>IEEE Trans. Med. Imaging</source><year>2021</year><volume>41</volume><fpage>476</fpage><lpage>490</lpage><pub-id pub-id-type="doi">10.1109/TMI.2021.3116087</pub-id></element-citation></ref><ref id="CR38"><label>38.</label><mixed-citation publication-type="other">H. Zhang, H. Xu, Y. Xiao, X. Guo, and J. Ma, "Rethinking the image fusion: A fast unified image fusion network based on proportional maintenance of gradient and intensity," in <italic>Proceedings of the AAAI Conference on Artificial Intelligence</italic>, 2020, pp. 12797&#x02013;12804.</mixed-citation></ref><ref id="CR39"><label>39.</label><mixed-citation publication-type="other">O. S. Kayhan and J. C. v. Gemert, "On translation invariance in cnns: Convolutional layers can exploit absolute spatial location," in <italic>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</italic>, 2020, pp. 14274&#x02013;14285.</mixed-citation></ref><ref id="CR40"><label>40.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Badakhshannoory</surname><given-names>H</given-names></name><name><surname>Saeedi</surname><given-names>P</given-names></name></person-group><article-title>A model-based validation scheme for organ segmentation in CT scan volumes</article-title><source>IEEE Trans. Biomed. Eng.</source><year>2011</year><volume>58</volume><fpage>2681</fpage><lpage>2693</lpage><pub-id pub-id-type="doi">10.1109/TBME.2011.2161987</pub-id><?supplied-pmid 21768040?><pub-id pub-id-type="pmid">21768040</pub-id>
</element-citation></ref><ref id="CR41"><label>41.</label><mixed-citation publication-type="other">. Wang, M. A. Zuluaga, R. Pratt, M. Aertsen, A. L. David, J. Deprest<italic>, et al.</italic>, "Slic-Seg: slice-by-slice segmentation propagation of the placenta in fetal MRI using one-plane scribbles and online learning," in <italic>Medical Image Computing and Computer-Assisted Intervention&#x02013;MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18</italic>, 2015, 29-37</mixed-citation></ref><ref id="CR42"><label>42.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Achanta</surname><given-names>R</given-names></name><name><surname>Shaji</surname><given-names>A</given-names></name><name><surname>Smith</surname><given-names>K</given-names></name><name><surname>Lucchi</surname><given-names>A</given-names></name><name><surname>Fua</surname><given-names>P</given-names></name><name><surname>S&#x000fc;sstrunk</surname><given-names>S</given-names></name></person-group><article-title>SLIC superpixels compared to state-of-the-art superpixel methods</article-title><source>IEEE transactions on pattern analysis and machine intelligence</source><year>2012</year><volume>34</volume><fpage>2274</fpage><lpage>2282</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2012.120</pub-id><?supplied-pmid 22641706?><pub-id pub-id-type="pmid">22641706</pub-id>
</element-citation></ref><ref id="CR43"><label>43.</label><mixed-citation publication-type="other">A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez<italic>, et al.</italic>, "Attention is all you need," <italic>Advances in neural information processing systems,</italic> vol. 30, 2017.</mixed-citation></ref><ref id="CR44"><label>44.</label><mixed-citation publication-type="other">A. Baevski and M. Auli, "Adaptive input representations for neural language modeling," <italic>arXiv preprint </italic><ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1809.10853">arXiv:1809.10853</ext-link><italic>,</italic> 2018.</mixed-citation></ref><ref id="CR45"><label>45.</label><mixed-citation publication-type="other">Q. Wang, B. Li, T. Xiao, J. Zhu, C. Li, D. F. Wong<italic>, et al.</italic>, "Learning deep transformer models for machine translation," <italic>arXiv preprint </italic><ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1906.01787">arXiv:1906.01787</ext-link><italic>,</italic> 2019.</mixed-citation></ref><ref id="CR46"><label>46.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Taha</surname><given-names>AA</given-names></name><name><surname>Hanbury</surname><given-names>A</given-names></name></person-group><article-title>Metrics for evaluating 3D medical image segmentation: Analysis, selection, and tool</article-title><source>BMC Med. Imaging</source><year>2015</year><volume>15</volume><fpage>1</fpage><lpage>28</lpage><pub-id pub-id-type="doi">10.1186/s12880-015-0068-x</pub-id><pub-id pub-id-type="pmid">25645550</pub-id>
</element-citation></ref><ref id="CR47"><label>47.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fick</surname><given-names>T</given-names></name><name><surname>van Doormaal</surname><given-names>JA</given-names></name><name><surname>Tosic</surname><given-names>L</given-names></name><name><surname>van Zoest</surname><given-names>RJ</given-names></name><name><surname>Meulstee</surname><given-names>JW</given-names></name><name><surname>Hoving</surname><given-names>EW</given-names></name><etal/></person-group><article-title>Fully automatic brain tumor segmentation for 3D evaluation in augmented reality</article-title><source>Neurosurgical Focus</source><year>2021</year><volume>51</volume><fpage>E14</fpage><pub-id pub-id-type="doi">10.3171/2021.5.FOCUS21200</pub-id><?supplied-pmid 34333477?><pub-id pub-id-type="pmid">34333477</pub-id>
</element-citation></ref><ref id="CR48"><label>48.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ma</surname><given-names>J</given-names></name><name><surname>Zhang</surname><given-names>Y</given-names></name><name><surname>Gu</surname><given-names>S</given-names></name><name><surname>Zhu</surname><given-names>C</given-names></name><name><surname>Ge</surname><given-names>C</given-names></name><name><surname>Zhang</surname><given-names>Y</given-names></name><etal/></person-group><article-title>Abdomenct-1k: Is abdominal organ segmentation a solved problem</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2021</year><volume>44</volume><fpage>6695</fpage><lpage>6714</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2021.3100536</pub-id></element-citation></ref><ref id="CR49"><label>49.</label><mixed-citation publication-type="other">A. L. Simpson, M. Antonelli, S. Bakas, M. Bilello, K. Farahani, B. Van Ginneken<italic>, et al.</italic>, "A large annotated medical image dataset for the development and evaluation of segmentation algorithms," <italic>arXiv preprint </italic><ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1902.09063">arXiv:1902.09063</ext-link><italic>,</italic> 2019.</mixed-citation></ref><ref id="CR50"><label>50.</label><mixed-citation publication-type="other">H. R. Roth, L. Lu, A. Farag, H.-C. Shin, J. Liu, E. B. Turkbey<italic>, et al.</italic>, "Deeporgan: Multi-level deep convolutional networks for automated pancreas segmentation," in <italic>International Conference on Medical Image Computing and Computer-Assisted Intervention</italic>, 2015, pp. 556&#x02013;564.</mixed-citation></ref><ref id="CR51"><label>51.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heller</surname><given-names>N</given-names></name><name><surname>Isensee</surname><given-names>F</given-names></name><name><surname>Maier-Hein</surname><given-names>KH</given-names></name><name><surname>Hou</surname><given-names>X</given-names></name><name><surname>Xie</surname><given-names>C</given-names></name><name><surname>Li</surname><given-names>F</given-names></name><etal/></person-group><article-title>The state of the art in kidney and kidney tumor segmentation in contrast-enhanced CT imaging: Results of the KiTS19 challenge</article-title><source>Med. Image Anal.</source><year>2021</year><volume>67</volume><fpage>101821</fpage><pub-id pub-id-type="doi">10.1016/j.media.2020.101821</pub-id><?supplied-pmid 33049579?><pub-id pub-id-type="pmid">33049579</pub-id>
</element-citation></ref><ref id="CR52"><label>52.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bilic</surname><given-names>P</given-names></name><name><surname>Christ</surname><given-names>P</given-names></name><name><surname>Li</surname><given-names>HB</given-names></name><name><surname>Vorontsov</surname><given-names>E</given-names></name><name><surname>Ben-Cohen</surname><given-names>A</given-names></name><name><surname>Kaissis</surname><given-names>G</given-names></name><etal/></person-group><article-title>The liver tumor segmentation benchmark (lits)</article-title><source>Med. Image Anal.</source><year>2023</year><volume>84</volume><fpage>102680</fpage><pub-id pub-id-type="doi">10.1016/j.media.2022.102680</pub-id><?supplied-pmid 36481607?><pub-id pub-id-type="pmid">36481607</pub-id>
</element-citation></ref><ref id="CR53"><label>53.</label><mixed-citation publication-type="other">D. P. Kingma and J. Ba, "Adam: A method for stochastic optimization," <italic>arXiv preprint </italic><ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1412.6980">arXiv:1412.6980</ext-link><italic>,</italic> 2014.</mixed-citation></ref><ref id="CR54"><label>54.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Deepa</surname><given-names>N</given-names></name><name><surname>Chokkalingam</surname><given-names>S</given-names></name></person-group><article-title>Optimization of VGG16 utilizing the arithmetic optimization algorithm for early detection of Alzheimer&#x02019;s disease</article-title><source>Biomed. Signal Process. Control</source><year>2022</year><volume>74</volume><fpage>103455</fpage><pub-id pub-id-type="doi">10.1016/j.bspc.2021.103455</pub-id></element-citation></ref><ref id="CR55"><label>55.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Salama</surname><given-names>WM</given-names></name><name><surname>Aly</surname><given-names>MH</given-names></name></person-group><article-title>Deep learning in mammography images segmentation and classification: Automated CNN approach</article-title><source>Alexandria Eng. J.</source><year>2021</year><volume>60</volume><fpage>4701</fpage><lpage>4709</lpage><pub-id pub-id-type="doi">10.1016/j.aej.2021.03.048</pub-id></element-citation></ref><ref id="CR56"><label>56.</label><mixed-citation publication-type="other">O. Oktay, J. Schlemper, L. L. Folgoc, M. Lee, M. Heinrich, K. Misawa<italic>, et al.</italic>, "Attention u-net: Learning where to look for the pancreas," <italic>arXiv preprint </italic><ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1804.03999">arXiv:1804.03999</ext-link><italic>,</italic> 2018.</mixed-citation></ref><ref id="CR57"><label>57.</label><mixed-citation publication-type="other">A. Hatamizadeh, Y. Tang, V. Nath, D. Yang, A. Myronenko, B. Landman<italic>, et al.</italic>, "Unetr: Transformers for 3d medical image segmentation," in <italic>Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</italic>, 2022, 574&#x02013;584.</mixed-citation></ref></ref-list></back></article>