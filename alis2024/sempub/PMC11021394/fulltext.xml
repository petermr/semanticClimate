<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd"> 
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="data-paper" dtd-version="1.3"><?properties open_access?><?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?><?DTDIdentifier.IdentifierType public?><?SourceDTD.DTDName A++V2.4.dtd?><?SourceDTD.Version 2.4?><?ConverterInfo.XSLTName springer2nlmx2.xsl?><?ConverterInfo.Version 1?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sci Data</journal-id><journal-id journal-id-type="iso-abbrev">Sci Data</journal-id><journal-title-group><journal-title>Scientific Data</journal-title></journal-title-group><issn pub-type="epub">2052-4463</issn><publisher><publisher-name>Nature Publishing Group UK</publisher-name><publisher-loc>London</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmcid">11021394</article-id><article-id pub-id-type="pmid">38627435</article-id>
<article-id pub-id-type="publisher-id">3240</article-id><article-id pub-id-type="doi">10.1038/s41597-024-03240-0</article-id><article-categories><subj-group subj-group-type="heading"><subject>Data Descriptor</subject></subj-group></article-categories><title-group><article-title>A construction waste landfill dataset of two districts in Beijing, China from high resolution satellite images</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-3653-6531</contrib-id><name><surname>Lin</surname><given-names>Shaofu</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid">http://orcid.org/0009-0002-7109-2056</contrib-id><name><surname>Huang</surname><given-names>Lei</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author" corresp="yes"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-5882-9389</contrib-id><name><surname>Liu</surname><given-names>Xiliang</given-names></name><address><email>liuxl@bjut.edu.cn</email></address><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author"><name><surname>Chen</surname><given-names>Guihong</given-names></name><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Fu</surname><given-names>Zhe</given-names></name><address><email>fuzhe@bda.gov.cn</email></address><xref ref-type="aff" rid="Aff3">3</xref></contrib><aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/037b1pp87</institution-id><institution-id institution-id-type="GRID">grid.28703.3e</institution-id><institution-id institution-id-type="ISNI">0000 0000 9040 3743</institution-id><institution>Faculty of Information Technology, </institution><institution>Beijing University of Technology, Chaoyang District, </institution></institution-wrap>Beijing, 100124 China </aff><aff id="Aff2"><label>2</label>Beijing Big Data Centre, Chaoyang District, Beijing, 100101 China </aff><aff id="Aff3"><label>3</label>Administrative Examination and Approval Bureau of the Beijing Economic-Technological Development Area, Beijing, 100176 China </aff></contrib-group><pub-date pub-type="epub"><day>16</day><month>4</month><year>2024</year></pub-date><pub-date pub-type="pmc-release"><day>16</day><month>4</month><year>2024</year></pub-date><pub-date pub-type="collection"><year>2024</year></pub-date><volume>11</volume><elocation-id>388</elocation-id><history><date date-type="received"><day>13</day><month>11</month><year>2023</year></date><date date-type="accepted"><day>5</day><month>4</month><year>2024</year></date></history><permissions><copyright-statement>&#x000a9; The Author(s) 2024</copyright-statement><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article&#x02019;s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article&#x02019;s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p></license></permissions><abstract id="Abs1"><p id="Par1">Construction waste is unavoidable in the process of urban development, causing serious environmental pollution. Accurate assessment of municipal construction waste generation requires building construction waste identification models using deep learning technology. However, this process requires high-quality public datasets for model training and validation. This study utilizes Google Earth and GF-2 images as the data source to construct a specific dataset of construction waste landfills in the Changping and Daxing districts of Beijing, China. This dataset contains 3,653 samples of the original image areas and provides mask-labeled images in the semantic segmentation domains. Each pixel within a construction waste landfill is classified into 4 categories of the image areas, including background area, vacant landfillable area, engineering facility area, and waste dumping area. The dataset contains 237,115,531 pixels of construction waste and 49,724,513 pixels of engineering facilities. The pixel-level semantic segmentation labels are provided to quantify the construction waste yield, which can serve as the basic data for construction waste extraction and yield estimation both for academic and industrial research.</p></abstract><kwd-group kwd-group-type="npg-subject"><title>Subject terms</title><kwd>Environmental impact</kwd><kwd>Research data</kwd></kwd-group><funding-group><award-group><funding-source><institution-wrap><institution-id institution-id-type="FundRef">https://doi.org/10.13039/501100004826</institution-id><institution>Natural Science Foundation of Beijing Municipality (Beijing Natural Science Foundation)</institution></institution-wrap></funding-source><award-id>9232008</award-id><principal-award-recipient><name><surname>Fu</surname><given-names>Zhe</given-names></name></principal-award-recipient></award-group></funding-group><custom-meta-group><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#x000a9; Springer Nature Limited 2024</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1"><title>Background &#x00026; Summary</title><p id="Par2">China is currently in a stage of rapid urbanization. The demolition of buildings during the urban renewal process generates construction waste, which is an unavoidable part and leads to significant environmental problems. Construction waste differs from household waste in that it contains hazardous materials. Heavy metals, asbestos, organic compounds, and other harmful organic materials pose a threat to the environment, so they cannot be directly dumped<sup><xref ref-type="bibr" rid="CR1">1</xref></sup>. Furthermore, construction activities may cause agricultural land loss, loss of soil, and air pollution<sup><xref ref-type="bibr" rid="CR2">2</xref></sup>. Waste is generated as a result of mismanagement, and there are devastating environmental concerns about this waste which are being faced by all of us<sup><xref ref-type="bibr" rid="CR3">3</xref></sup>. Due to the increasing amount of waste generated and its associated environmental impacts, there is an urgent need to accurately estimate the amount of construction waste so as to measure the cost of the urban renewal process<sup><xref ref-type="bibr" rid="CR4">4</xref></sup>. Currently, the identification methods of construction waste primarily include manual field investigation and remote sensing monitoring. Due to the widespread geographic distribution of construction waste landfills<sup><xref ref-type="bibr" rid="CR5">5</xref></sup>, artificial field investigations must be performed, which require a lot of human and material resources, yielding a low work efficiency. Based on remote sensing data sources, researchers have studied the spatial distribution of garbage dumps and solid waste using machine learning technology. For example, Ramnarayan <italic>et al</italic>. proposed a machine learning augmented approach to quantifying and recycling construction and demolition waste<sup><xref ref-type="bibr" rid="CR6">6</xref></sup>. Lu <italic>et al</italic>. estimated construction waste generation in China&#x02019;s Greater Bay Area using machine learning<sup><xref ref-type="bibr" rid="CR7">7</xref></sup> technology. However, traditional artificial field investigation and machine-learning-based approaches suffer from the manual selection of feature variables. These methods are only effective for specific types of wastes and it is challenging for professionals to design features that are highly robust and generalizable.</p><p id="Par3">Deep learning-based approaches promise to capture the expertise of professional image interpreters and apply it to train computer-aided tools to support solid waste management<sup><xref ref-type="bibr" rid="CR8">8</xref></sup>. For example, Gao <italic>et al</italic>. proposed a system for identifying and categorizing construction waste using remote sensing images by unmanned aerial vehicles and a multi-layer deep learning approach<sup><xref ref-type="bibr" rid="CR9">9</xref></sup>. Similarly, Zhao <italic>et al</italic>. proposed a method of construction waste recognition based on change detection and deep learning<sup><xref ref-type="bibr" rid="CR10">10</xref></sup>, while Lu <italic>et al</italic>. proposed an automatic identification method for the composition of construction waste mixtures using semantic segmentation in computer vision<sup><xref ref-type="bibr" rid="CR11">11</xref></sup>. Compared with traditional methods, the semi-automated extraction of construction waste from remote sensing images not only saves manpower and material resources but also has high efficiency and a short information extraction period<sup><xref ref-type="bibr" rid="CR12">12</xref></sup>. Open datasets such as ImageNet<sup><xref ref-type="bibr" rid="CR13">13</xref></sup> and MS COCO<sup><xref ref-type="bibr" rid="CR14">14</xref></sup> have greatly facilitated the development of deep learning methods for large-scale applications in the field of target extraction, enabling the extraction of rich feature characteristics from remote sensing images more quickly and accurately. The fusion of remote sensing images and deep learning can quickly and accurately obtain the changes of construction waste in construction waste landfills and can accurately estimate the production of construction waste at the macro level<sup><xref ref-type="bibr" rid="CR15">15</xref>,<xref ref-type="bibr" rid="CR16">16</xref></sup>. However, this method is extremely dependent on high-quality datasets for model training, validation, and testing. There is a dearth of publicly available datasets for construction waste landfill identification, and it is almost impossible to find a uniformly standardized dataset. This gap hinders research on construction waste landfill identification methods. Therefore, the researchers call for more shared solid waste image datasets to be opened for interested researchers to train and evaluate their algorithmic models<sup><xref ref-type="bibr" rid="CR17">17</xref></sup>.</p><p id="Par4">In the field of aerial remote sensing, solid waste recognition based on deep learning has been implemented in very few studies<sup><xref ref-type="bibr" rid="CR18">18</xref>,<xref ref-type="bibr" rid="CR19">19</xref></sup>. These studies demonstrate the potential of deep learning to achieve image decoding at different scales for various computer vision tasks, such as target detection and semantic segmentation. However, there is a limited number of studies exploring deep learning-based solid waste recognition in aerial remote sensing. The most similar dataset found publicly available is the landfill dataset from the BigEarthNet repository<sup><xref ref-type="bibr" rid="CR20">20</xref></sup>. However, this dataset mainly consists of generic scenes and contains small, coarse images, which may not be ideal for construction waste recognition.</p><p id="Par5">Currently, the main datasets commonly used for construction waste extraction are the AerialWaste dataset provided by Torres <italic>et al</italic>.<sup><xref ref-type="bibr" rid="CR21">21</xref></sup> and the SWAD dataset provided by Liming Zhou <italic>et al</italic>.<sup><xref ref-type="bibr" rid="CR22">22</xref></sup> AerialWaste is an illegal landfill detection dataset containing manually labeled airborne, WorldView-3, and Google Earth imagery. The AerialWaste dataset focuses on illegal landfill detection and includes manually labeled airborne, WorldView-3, and Google Earth imagery. The dataset provides information about the type of solid waste, storage methods, site types, and evidence and severity of violations, making it a valuable resource for detecting illegal landfills. The SWAD dataset, on the other hand, is based on remote sensing images collected from Google Earth in Henan Province, China. It contains 998 extended images from WorldView-2 and SPOT satellites in the format of JPG, covering various scenes, including urban, village, and mountainous areas, and includes different types of solid waste such as gravel, slag, industrial waste, and domestic waste. While AerialWaste and SWAD can be excellent datasets for extracting solid waste from remote sensing images, there are still several drawbacks. Firstly, AerialWaste does not provide the labeled information required for semantic segmentation, resulting in the classification being done only for the presence or absence of solid waste rather than on a pixel-level basis, which hinders the quantitative analysis of waste yield. Secondly, the spatial resolutions of the satellite images used in SWAD datasets are not at a sub-meter level, making it challenging to clearly distinguish construction waste landfills and identify typical features. The lack of detail in the images also makes it difficult to accurately differentiate the shape of the landfill and identify smaller surrounding facilities. Additionally, the low resolution may obscure slight colour differences between the construction waste landfill and its surroundings, and the height and shape of the waste pile may not be clear enough for accurate estimation of landfill activity. Table&#x000a0;<xref rid="Tab1" ref-type="table">1</xref> lists several existing CNN-based waste datasets. It can be seen that solid waste datasets in aerial images can be used for detection, classification, or segmentation.<table-wrap id="Tab1"><label>Table 1</label><caption><p>Comparison of Waste Datasets for CNN.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Dataset</th><th>Image Quantity</th><th>Instance Quantity</th><th>Category Quantity</th><th>Image Size</th><th>Annotation Type</th><th>Shooting Distance</th><th>Data Source</th></tr></thead><tbody><tr><td>SWAD<sup><xref ref-type="bibr" rid="CR22">22</xref></sup></td><td>1,996</td><td>5,562</td><td>1</td><td>1200&#x02009;&#x000d7;&#x02009;600&#x02013;2400&#x02009;&#x000d7;&#x02009;1200</td><td>Detection</td><td>Long</td><td>WV2, SPOT</td></tr><tr><td>AerialWaste<sup><xref ref-type="bibr" rid="CR42">42</xref></sup></td><td>10,434</td><td>10,434</td><td>2</td><td>700&#x02009;&#x000d7;&#x02009;700&#x02013;1000&#x02009;&#x000d7;&#x02009;1000</td><td>Classification</td><td>Medium and long</td><td>AGEA, WV3, GE</td></tr><tr><td>UAVVaste<sup><xref ref-type="bibr" rid="CR43">43</xref></sup></td><td>772</td><td>3,716</td><td>1</td><td>&#x02014;</td><td>Detection</td><td>Medium</td><td>UAV</td></tr><tr><td>MJU Waste<sup><xref ref-type="bibr" rid="CR44">44</xref></sup></td><td>2,475</td><td>2,532</td><td>1</td><td>640&#x02009;&#x000d7;&#x02009;480</td><td>Segmentation</td><td>Short</td><td>Camera</td></tr><tr><td>CWLD(Ours)<sup><xref ref-type="bibr" rid="CR30">30</xref></sup></td><td>3,653</td><td>10,959</td><td>5</td><td>200&#x02009;&#x000d7;&#x02009;300&#x02013;1800&#x02009;&#x000d7;&#x02009;1000</td><td>Segmentation</td><td>Long</td><td>GF-2, Google Earth</td></tr></tbody></table></table-wrap></p><p id="Par6">Based on these several key shortcomings identified, this paper proposes a new dataset, the Construction Waste Landfill Dataset (CWLD), to meet practical needs. The CWLD is designed to be used for training and evaluating semantic segmentation models, calculating construction waste production, and environmental monitoring and management. The dataset is constructed based on several criteria:<list list-type="bullet"><list-item><p id="Par7">The images come from different sources and have varying quality.</p></list-item><list-item><p id="Par8">The images are classified with the pixel-wise level.</p></list-item><list-item><p id="Par9">The images are associated with mask-labeled images that provide detailed information about the internal area of the entire construction waste landfill.</p></list-item><list-item><p id="Par10">The annotations are curated by professional photo interpreters who specialize in using remote sensing images for landfill detection.</p></list-item></list></p><p id="Par11">The CWLD dataset consists of construction waste landfills in Changping and Daxing districts of Beijing, China (as shown in Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>), and uses the Google Earth and GF-2 satellite remote sensing images as the data source. It includes 3,653 samples of original image regions with mask-labeled images for semantic segmentation. The dataset contains 237,115,531 pixels of construction waste regions and 49,724,513 pixels of engineering facilities regions. Providing semantic segmentation labels at the pixel level allows for the quantification of construction waste production, which can provide basic data for the study of high-resolution remote sensing images of construction waste yield and extraction.<fig id="Fig1"><label>Fig. 1</label><caption><p>Overview of the study area: (<bold>a</bold>) Beijing&#x02019;s location in China, (<bold>b</bold>) the location of Changping and Daxing districts in Beijing, where blue colour indicates the Changping district and green colour indicate the Daxing district, and (<bold>c</bold>,<bold>d</bold>) are the raw remote sensing images of the study area.</p></caption><graphic xlink:href="41597_2024_3240_Fig1_HTML" id="d33e472"/></fig></p></sec><sec id="Sec2"><title>Methods</title><p id="Par12">CWLD selects two representative study areas in Changping and Daxing districts of Beijing, as shown in Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>, for data collection. To enhance the authenticity and credibility of the dataset, we incorporated the spatial coordinates into high-resolution remote sensing images of the study area. By utilizing the officially published geographic coordinates of the construction waste landfill stations, we conducted manual screening and cutting to identify the locations of construction waste landfill sites on the remote sensing images where spatial coordinate information was available.</p><sec id="Sec3"><title>Data sources</title><p id="Par13">In this study, raw images are downloaded from the Google Earth and GF-2 remote sensing satellite.<list list-type="bullet"><list-item><p id="Par14">Images of Changping District are downloaded from the GF-2 satellite. The GF-2 satellite was launched in February 2014 and is the second high-resolution optical Earth observation satellite developed by the China National Space Administration (CNSA) as part of the China High-Resolution Earth Observation System (CHEOS). It incorporates all-digital technology and is equipped with a 0.8-m panchromatic camera and a 3.2-m multispectral camera. Spatial resolution is &#x02248;80&#x02009;cm GSD. The GF-2 satellite enables all-weather, all-day, high-resolution observation and detection, providing high-quality and clear image information. As a result, it has been widely utilized in remote sensing research<sup><xref ref-type="bibr" rid="CR23">23</xref>&#x02013;<xref ref-type="bibr" rid="CR27">27</xref></sup>.</p></list-item><list-item><p id="Par15">Images of Daxing District are downloaded from Google Earth using the Google API. Spatial resolution is &#x02248;50&#x02009;cm GSD. The size of the images is &#x02248;200&#x02009;&#x000d7;&#x02009;300&#x02013;1000&#x02009;&#x000d7;&#x02009;1000 pixels. Google images are free to the public and have been used in different remote sensing studies. Their use must respect the Google Earth terms and conditions<sup><xref ref-type="bibr" rid="CR28">28</xref></sup>.</p></list-item></list></p><p id="Par16">Two representative study areas in Changping and Daxing districts of Beijing were selected as the target areas for data collection, with specific information provided in Table&#x000a0;<xref rid="Tab2" ref-type="table">2</xref>. The process of creating the CWLD dataset is illustrated in Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref>.<table-wrap id="Tab2"><label>Table 2</label><caption><p>Information about the study areas.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Region name</th><th>Latitude and longitude ranges</th><th>Image time series</th><th>Image sizes</th></tr></thead><tbody><tr><td>Changping District, Beijing</td><td>115&#x000b0;50&#x02032;17&#x02033;&#x02013;116&#x000b0;29&#x02032;49&#x02033;E, 40&#x000b0;2&#x02032;18&#x02033;&#x02013;40&#x000b0;23&#x02032;13&#x02033;N</td><td>2019~2020</td><td>500&#x02009;&#x000d7;&#x02009;500 px</td></tr><tr><td>Daxing District, Beijing</td><td>116&#x000b0;13&#x02032;&#x02013;116&#x000b0;43&#x02032;E, 39&#x000b0;26&#x02032;&#x02013;39&#x000b0;51&#x02032;N</td><td>2016~2021</td><td>200&#x02009;&#x000d7;&#x02009;300~1800&#x02009;&#x000d7;&#x02009;1000 px</td></tr></tbody></table></table-wrap><fig id="Fig2"><label>Fig. 2</label><caption><p>The data annotation process.</p></caption><graphic xlink:href="41597_2024_3240_Fig2_HTML" id="d33e555"/></fig></p><p id="Par17">In the data preprocessing stage, the original satellite images were aligned, cut, and filtered to isolate the landfill images. During the manual annotation stage, data annotation software such as Photoshop was utilized to annotate 50% of the landfill images, forming an initial sample set. Subsequently, a neural network model was trained using this initial sample set to equip the model with initial processing capabilities. In the human-computer interaction annotation stage, the trained model was employed to label the remaining 50% of the data interactively. Finally, the dataset production was completed by obtaining segmented labeled images for all the data.</p></sec><sec id="Sec4"><title>Data set creation</title><p id="Par18">CWLD provides a comprehensive representation of the intricate construction waste landfill scenes captured in remote sensing images, offering detailed segmentation for each landfill&#x02019;s interior. Analysis of satellite imagery reveals that construction waste landfills exhibit a diverse array of shapes, sizes, and orientations, lacking uniformity in their internal areas. Typically, these landfills display a cluttered arrangement of buildings, engineering structures, solid waste, and construction debris, often exceeding the boundaries of the designated structures. Additionally, the area may encompass various other types of solid waste, such as plastics, metals, and wood, among others. Figure&#x000a0;<xref rid="Fig3" ref-type="fig">3</xref> illustrates an example of a construction waste landfill depicted in remote sensing images.<fig id="Fig3"><label>Fig. 3</label><caption><p>Examples of construction waste landfills. All images within the dataset exhibit significant characteristics of construction waste landfills. As can be seen from the red box, (<bold>a</bold>) The presence of diverse types of solid waste, including plastics, metals, and wood. (<bold>b</bold>) Overlapping of construction waste with the boundaries of associated engineering facilities and buildings. (<bold>c</bold>) Shadows cast by buildings partially cover certain areas. These examples (<bold>a</bold>&#x02013;<bold>c</bold>) clearly demonstrate the complexity of the internal areas within the construction waste landfills depicted in the images. They showcase variations in shapes, sizes, and orientations, all exhibiting accumulations of different materials and dispersed waste.</p></caption><graphic xlink:href="41597_2024_3240_Fig3_HTML" id="d33e587"/></fig></p><p id="Par19">Based on Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref>, the process of creating the dataset is further described in five main steps:<list list-type="bullet"><list-item><p id="Par20">Calibration of vector maps and remote sensing images: Utilize ArcMap<sup><xref ref-type="bibr" rid="CR29">29</xref></sup> software to query and acquire vector maps of Changping and Daxing districts in Beijing, along with their spatial coordinates. Then manually geo-reference the remote sensing images by aligning them with the vector maps using the geographic alignment function, and write the spatial coordinate information and export the aligned images in the format of TIF through ArcMap.</p></list-item><list-item><p id="Par21">Images Cutting: Utilize the obtained geographic coordinates of the construction waste landfills in Changping and Daxing districts. Locate the corresponding areas of construction waste landfills in the remote sensing images based on the spatial coordinate information. Cut out 228 images of size 500&#x02009;&#x000d7;&#x02009;500 pixels in Changping district and 457 images ranging from 200&#x02009;&#x000d7;&#x02009;300 to 1800&#x02009;&#x000d7;&#x02009;1000 pixels in Daxing district. The resulting images will all exhibit distinct features of construction waste landfills.</p></list-item><list-item><p id="Par22">Define annotation standards: Combine the manually labeled construction waste landfill image set with pixel-level classification features obtained from semantic segmentation. Assign different colours to delineate specific areas within the construction waste landfill, including: (1) Background areas surrounding the landfill are labeled with RGB (0,0,0). (2) Vacant landfillable areas within the landfill are labeled with RGB (255, 255, 255). (3) Buildings and engineering facilities areas within the landfill are labeled with RGB (0, 0, 255). (4) Areas where waste has been deposited within the landfill are labeled with RGB (255, 0, 0).</p></list-item><list-item><p id="Par23">Human-computer interaction annotation stage: First, In the manual annotation stage, 50% of the samples are labeled using Photoshop software to form an initial sample set. Subsequently, the neural network model is trained according to the initial sample set so that the model has the ability to segment the construction waste landfill with high accuracy. Finally, in the human-computer interaction annotation stage, the model is used to pre-annotate the remaining 50% of the data, and on the basis of the pre-annotation, professionals manually verify each labeled image to more finely annotate the category of each pixel.</p></list-item><list-item><p id="Par24">Data enhancement: Data enhancement techniques are applied to increase the diversity, generalizability, and robustness of the training dataset. Since the number of actual landfills may be limited, data expansion methods are employed, including brightness adjustment (50% brighter and 50% darker), introducing Gaussian noise, mirroring, and random scaling. These techniques are performed on the existing dataset to generate new samples with variations, as shown in Fig.&#x000a0;<xref rid="Fig4" ref-type="fig">4</xref>.<fig id="Fig4"><label>Fig. 4</label><caption><p>Data enhancement methods. (<bold>a</bold>) Original image: The original image of a construction waste landfill. (<bold>b</bold>) Brightening by 50%: The brightness of the original image is increased by 50%. (<bold>c</bold>) Darkening by 50%: The brightness of the original image is decreased by 50%. (<bold>d</bold>) Mirroring: The original image is flipped horizontally, resulting in a mirrored image. (<bold>e</bold>) Introducing Gaussian noise: A small amount of Gaussian noise is added to the original image, creating a slightly distorted version. (<bold>f</bold>) Random scaling: The original image is randomly scaled up or down, resulting in a larger or smaller version of the image.</p></caption><graphic xlink:href="41597_2024_3240_Fig4_HTML" id="d33e642"/></fig></p></list-item></list></p><p id="Par25">Figure&#x000a0;<xref rid="Fig5" ref-type="fig">5</xref> illustrates the proportion of the four types of labels within the construction waste landfill dataset. It is evident that the landfill occupies approximately 53.58% of the dataset. Notably, the proportions of each label type in the Daxing District dataset are consistent with those in the overall construction waste landfill dataset, indicating the high quality of the dataset. These features highlight the heterogeneity of the dataset, which in turn enhances the complexity of landfill identification and waste detection tasks. Moreover, these diverse features can be effectively utilized to train network models.<fig id="Fig5"><label>Fig. 5</label><caption><p>The percentage of each type of label in the dataset, (<bold>a</bold>) Changping District, (<bold>b</bold>) Daxing District, and (<bold>c</bold>) CWLD.</p></caption><graphic xlink:href="41597_2024_3240_Fig5_HTML" id="d33e664"/></fig></p><p id="Par26">Figure&#x000a0;<xref rid="Fig6" ref-type="fig">6</xref> displays the distribution of data from various sources, resulting in a total of 3,653 images depicting construction landfills within the study area.<fig id="Fig6"><label>Fig. 6</label><caption><p>Distribution of data sample resources.</p></caption><graphic xlink:href="41597_2024_3240_Fig6_HTML" id="d33e676"/></fig></p></sec><sec id="Sec5"><title>Data quality control and utilization processes</title><p id="Par27">To ensure data quality, a comprehensive quality control process has been implemented throughout the stages of image acquisition, pre-processing, manual labeling, and human-computer interactive labeling. Skilled technicians carry out image acquisition and labeling following standardized operating procedures. Multiple checks are conducted to ensure the reliability, completeness, and uniformity of the labeled data. The pre-processing phase involves cutting the original image batch based on the geographic coordinates of the construction waste landfill site. Secondary screening and manual examination are performed to identify images that exhibit characteristics of the construction waste landfill site. Distorted, blurred, or otherwise problematic images are manually removed to minimize interference factors. Figure&#x000a0;<xref rid="Fig7" ref-type="fig">7</xref> provides an illustration of the dataset utilization process.<fig id="Fig7"><label>Fig. 7</label><caption><p>Illustrates the dataset utilization process. The dataset utilization process involves loading the training and test images into a deep learning model (such as DeepLabV3&#x02009;+&#x02009;, UNet, SegNet) using Python&#x02019;s PIL library and relevant modules/packages from the OpenCV library. The model is capable of performing pixel-level semantic segmentation tasks. To evaluate the predictive performance of CNN models, standard metrics such as Accuracy, Precision, Recall, and F1-score are utilized for quantitative prediction assessment.</p></caption><graphic xlink:href="41597_2024_3240_Fig7_HTML" id="d33e690"/></fig></p></sec><sec id="Sec6"><title>Detailed description of the dataset</title><p id="Par28">The CWLD dataset contains 3,653 samples, each of which depicts clear construction waste landfill features. These images contain one to three types of targets. With the high spatial resolution of the dataset, it is easy to distinguish various ground features, thereby the dataset can fulfil the requirements of deep learning network model training tasks, as demonstrated in Fig.&#x000a0;<xref rid="Fig8" ref-type="fig">8</xref>.<fig id="Fig8"><label>Fig. 8</label><caption><p>Illustrates the presentation of the sample dataset. Groups (<bold>a</bold>), (<bold>c</bold>), and (<bold>e</bold>) depict remote sensing images, while groups (<bold>b</bold>), (<bold>d</bold>), and (<bold>f</bold>) represent the corresponding mask-labeled images. In the mask-labeled images, the white area represents vacant landfillable, the black area indicates the image background, the blue area represents the engineering facility area, and the red area represents the dumping area.</p></caption><graphic xlink:href="41597_2024_3240_Fig8_HTML" id="d33e724"/></fig></p></sec></sec><sec id="Sec7"><title>Data Records</title><p id="Par29">The CWLD dataset is available in the Zenodo repository<sup><xref ref-type="bibr" rid="CR30">30</xref></sup>. The structure of the repository consists of three folders: Original Dataset, Construction Waste Landfill Dataset, and Deep Learning Datasets. The remote sensing images are in *.tif format, and the labeled images are in *.png format. Table&#x000a0;<xref rid="Tab3" ref-type="table">3</xref> provides an overview of the file organization within the dataset, and this organization allows the reader to easily select the desired data.<list list-type="bullet"><list-item><p id="Par30"><bold>Original Dataset</bold>. This folder contains raw remote sensing images cut directly from Google Earth and GF-2 imagery that have not been processed by data enhancement techniques and the corresponding labeled images. It is stored according to different areas, including 228 images in Changping District and 457 images in Daxing District.</p></list-item><list-item><p id="Par31"><bold>The Construction Waste Landfill Dataset</bold>. Due to the limited number of actual construction waste landfills, the sample size is too small to support the training model. Therefore, the raw data are enriched using data enhancement techniques such as brightening, darkening, introducing noise, mirroring, and random scaling, resulting in a total of 7,306 pieces of data, which are stored separately according to the remote sensing images and the labeled images, which are 3,653 pieces each.</p></list-item><list-item><p id="Par32"><bold>Deep Learning Datasets</bold>. In order to accommodate the fixed input size requirement of neural network models, the data needs to be pre-processed. In this folder, the input data is adjusted to 512&#x02009;&#x000d7;&#x02009;512 pixels, which may result in image stretching and distortion. The folder is further divided into training and validation sets in the ratio of 8:2, where the training set is 2,922, and the validation set is 731. Each subfolder contains remote sensing image data files and segmentation label image files.</p></list-item></list><table-wrap id="Tab3"><label>Table 3</label><caption><p>Organization of the dataset files.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Parent directory</th><th>Subdirectory</th><th>Number of images</th><th>Content of the document</th><th>Description of the document</th></tr></thead><tbody><tr><td rowspan="4">Original data</td><td rowspan="2">Changping District</td><td rowspan="2">228</td><td>images/*.tif</td><td>Original images</td></tr><tr><td>label/*.png</td><td>Original image labels</td></tr><tr><td rowspan="2">Daxing District</td><td rowspan="2">457</td><td>images/*.tif</td><td>Original images</td></tr><tr><td>label/*.png</td><td>Original image labels</td></tr><tr><td rowspan="2">Construction Waste Landfill Dataset</td><td>images</td><td>3,653</td><td>images/*.tif</td><td>CWLD images</td></tr><tr><td>label</td><td>3,653</td><td>label/*.png</td><td>CWLD label</td></tr><tr><td rowspan="4">Datasets for Deep Learning (512&#x02009;&#x000d7;&#x02009;512px)</td><td rowspan="2">train</td><td rowspan="2">2,922</td><td>images/*.tif</td><td>Training set images</td></tr><tr><td>label/*.png</td><td>Training set image labels</td></tr><tr><td rowspan="2">val</td><td rowspan="2">731</td><td>images/*.tif</td><td>Validation set images</td></tr><tr><td>label/*.png</td><td>Validation set image labels</td></tr></tbody></table></table-wrap></p><p id="Par33">The dataset can be utilized for pixel-level-based semantic segmentation tasks using Python&#x02019;s PIL library and related modules or packages from the OpenCV library. These images serve as inputs to the deep learning model, following the same approach as standard datasets used in traditional semantic segmentation tasks.</p></sec><sec id="Sec8"><title>Technical Validation</title><sec id="Sec9"><title>Quantitative analysis of different models on the dataset</title><p id="Par34">The CWLD dataset has undergone technology validation to evaluate its application in semantic segmentation for building deep learning predictive models that automate the identification and analysis of municipal construction waste landfills. Semantic segmentation is one of the popular research areas in deep learning and image pixel-level classification<sup><xref ref-type="bibr" rid="CR31">31</xref>,<xref ref-type="bibr" rid="CR32">32</xref></sup>.</p><p id="Par35">To comprehensively analyze the dataset and network performance, six evaluation metrics are utilized, including Accuracy, Precision, Recall, Intersection over Union (IoU), F1 score (F1), and Bayesian error rate (BER). Table&#x000a0;<xref rid="Tab4" ref-type="table">4</xref> specifies that pixels within the waste dumping area are considered positive cases (Positive), while pixels outside of this area are negative cases (Negative). These cases are categorized as True Positive (TP), False Positive (FP), True Negative (TN), and False Negative (FN), forming the confusion matrix. These metrics help evaluate the recognition ability of the model. The definition of the confusion matrix is presented in Table&#x000a0;<xref rid="Tab4" ref-type="table">4</xref>.<list list-type="order"><list-item><p id="Par36">Accuracy: This is one of the most direct evaluation metrics to evaluate the accuracy of the algorithm and refers to the ratio of the number of correctly categorized pixels to the total number of pixels.<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$Acc=\frac{TP+TN}{TP+FP+FN+TN}$$\end{document}</tex-math><mml:math id="M2" display="block"><mml:mi>A</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:math><graphic xlink:href="41597_2024_3240_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula></p></list-item><list-item><p id="Par37">Precision: Precision is the proportion of correct predictions out of the total number of positive predictions.<disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$Precision=\frac{TP}{TP+FP}$$\end{document}</tex-math><mml:math id="M4" display="block"><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfrac></mml:math><graphic xlink:href="41597_2024_3240_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula></p></list-item><list-item><p id="Par38">Recall: Recall is the proportion of correctly predicted outcomes out of the total number of positive events.<disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$Recall=\frac{TP}{TP+FN}$$\end{document}</tex-math><mml:math id="M6" display="block"><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:math><graphic xlink:href="41597_2024_3240_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula></p></list-item><list-item><p id="Par39">IoU: IoU is the rate of overlap between predicted and real edges.<disp-formula id="Equ4"><label>4</label><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$IoU=\frac{TP}{TP+FP+FN}$$\end{document}</tex-math><mml:math id="M8" display="block"><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:mi>U</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:math><graphic xlink:href="41597_2024_3240_Article_Equ4.gif" position="anchor"/></alternatives></disp-formula></p></list-item><list-item><p id="Par40">F1: The F1 score is used to balance the relationship between precision and recall.<disp-formula id="Equ5"><label>5</label><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$F1=\frac{2\ast Precision\times Recall}{Precision+Recall}$$\end{document}</tex-math><mml:math id="M10" display="block"><mml:mi>F</mml:mi><mml:mn>1</mml:mn><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo>&#x02217;</mml:mo><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:mfrac></mml:math><graphic xlink:href="41597_2024_3240_Article_Equ5.gif" position="anchor"/></alternatives></disp-formula></p></list-item><list-item><p id="Par41">BER: We introduce Bayesian Error Rate (BER) as a measure of the lowest classification error rate achievable for target extraction, BER is a common evaluation metric in the medical field that indicates the lowest classification error rate achievable for a given model. We compare the Bayesian error with the training set error to determine if it still possesses avoidable bias, if there is room for optimization, and if it is overfitting.</p></list-item></list><disp-formula id="Equ6"><label>6</label><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$BER=\frac{1}{2}\ast \left(\frac{FN}{TP+FN}+\frac{FP}{FP+TN}\right)$$\end{document}</tex-math><mml:math id="M12" display="block"><mml:mi>B</mml:mi><mml:mi>E</mml:mi><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac><mml:mo>&#x02217;</mml:mo><mml:mfenced close=")" open="("><mml:mrow><mml:mfrac><mml:mrow><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced></mml:math><graphic xlink:href="41597_2024_3240_Article_Equ6.gif" position="anchor"/></alternatives></disp-formula><table-wrap id="Tab4"><label>Table 4</label><caption><p>The confusion matrix of construction waste landfill identification.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2">Real data</th><th colspan="2">Projected results</th></tr><tr><th>Waste dumping area</th><th>Other areas</th></tr></thead><tbody><tr><td>Waste dumping area</td><td>TP</td><td>FN</td></tr><tr><td>Other areas</td><td>FP</td><td>TN</td></tr></tbody></table></table-wrap></p><p id="Par42">In order to adapt to the complexity of construction waste landfill images, we trained a multi-scale convolutional neural network structure generalized to complex scenes and validated it on a large-scale dataset and the validation process can be summarized as follows:<list list-type="bullet"><list-item><p id="Par43">The dataset is divided into two parts, 80% of the images are used for training and 20% for validation.</p></list-item><list-item><p id="Par44">The construction of the improved DeepLabV3+<sup><xref ref-type="bibr" rid="CR33">33</xref></sup> network training model can be divided into four main components including the backbone network, the Atrous Spatial Pyramid Pooling (ASPP) module, the encoding structure and the decoding structure.</p></list-item><list-item><p id="Par45">The improved DeepLabV3&#x02009;+&#x02009;network was applied to the training set of CWLD to construct predictive models. The performance of these models was then tested on the validation set. The classifier achieved <bold>96.21% Accuracy, 88.28% Precision, 90.24% Recall, 88.89% F1 score, 82.08% IoU, and 5.07% BER</bold>. Figure&#x000a0;<xref rid="Fig9" ref-type="fig">9</xref> illustrates the changes in the metrics during the model training.<fig id="Fig9"><label>Fig. 9</label><caption><p>Changes in indicators during model training, where the horizontal coordinate is the number of training rounds and the vertical coordinate is the parameter size. From the curve, it can be clearly found that with the increase of training rounds, the loss value gradually decreases and tends to fit, and the Accuracy and F1 scores continue to rise and finally reach equilibrium.</p></caption><graphic xlink:href="41597_2024_3240_Fig9_HTML" id="d33e1173"/></fig></p></list-item><list-item><p id="Par46">Additionally, we evaluated classical networks in the field of semantic segmentation, including UNet<sup><xref ref-type="bibr" rid="CR34">34</xref></sup>, SegNet<sup><xref ref-type="bibr" rid="CR35">35</xref></sup>, and PSPNet<sup><xref ref-type="bibr" rid="CR36">36</xref></sup>. These networks were validated using the dataset, and the results demonstrated that the semantic segmentation model trained from the dataset effectively recognizes various regions within the construction waste landfill. The experimental results are presented in Table&#x000a0;<xref rid="Tab5" ref-type="table">5</xref>. Detailed comparative information on the ACC and F1-score metrics for the four networks on the dataset is provided in Fig.&#x000a0;<xref rid="Fig10" ref-type="fig">10</xref>.<table-wrap id="Tab5"><label>Table 5</label><caption><p>Quantitative comparison of different neural networks on validation sets (%).</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Model</th><th>Acc</th><th>Precision</th><th>Recall</th><th>F1</th><th>IoU</th><th>BER</th></tr></thead><tbody><tr><td>UNet</td><td>95.57</td><td>88.28</td><td>90.15</td><td>88.89</td><td>82.87</td><td>5.16</td></tr><tr><td>SegNet</td><td>95.72</td><td>88.18</td><td>90.24</td><td>88.89</td><td>82.08</td><td>5.07</td></tr><tr><td>PSPNet</td><td>92.69</td><td>82.85</td><td>79.80</td><td>79.55</td><td>69.41</td><td>10.98</td></tr><tr><td>Improved DeepLabV3+</td><td><bold>96.21</bold></td><td><bold>88.56</bold></td><td><bold>90.70</bold></td><td><bold>89.01</bold></td><td><bold>82.95</bold></td><td><bold>4.93</bold></td></tr></tbody></table></table-wrap><fig id="Fig10"><label>Fig. 10</label><caption><p>Accuracy and F1-Score curves of different models, (<bold>a</bold>) Accuracy curves; (<bold>b</bold>) F1-Score curves.</p></caption><graphic xlink:href="41597_2024_3240_Fig10_HTML" id="d33e1311"/></fig></p></list-item></list></p><p id="Par47">From Table&#x000a0;<xref rid="Tab5" ref-type="table">5</xref> and Fig.&#x000a0;<xref rid="Fig10" ref-type="fig">10</xref>, it is evident that the semantic segmentation model trained on the dataset is highly effective. In comparison with UNet, SegNet, and PSPNet, the Improved DeepLabV3&#x02009;+&#x02009;network model achieves better segmentation results in terms of accuracy (Acc) and F1-Score. Specifically, when compared to the poorer-performing PSPNet network, the Improved DeepLabV3&#x02009;+&#x02009;network model delivers 3.52% higher accuracy and 9.46% higher F1-Score in segmentation.</p></sec><sec id="Sec10"><title>Classification network architecture</title><p id="Par48">The DeepLabV3&#x02009;+&#x02009;network is a multi-scale, multipath parallel convolutional neural network proposed by Chen L.C <italic>et al</italic>. in 2018, known for its excellent image segmentation performance and robustness. Its core innovation lies in the encoding-decoding structure that combines low-level semantic information with high-level semantic information, thereby improving the segmentation accuracy of the network. In this paper, we present an improved version of the DeepLabV3&#x02009;+&#x02009;network. As shown in Fig.&#x000a0;<xref rid="Fig11" ref-type="fig">11</xref>, the network is divided into four main components, including the backbone network, the Atrous Spatial Pyramid Pooling (ASPP) module, the encoding structure, and the decoding structure.<fig id="Fig11"><label>Fig. 11</label><caption><p>Improved DeepLabV3&#x02009;+&#x02009;network using ResNet-101 and feature fusion techniques for technology validation in CWLD.</p></caption><graphic xlink:href="41597_2024_3240_Fig11_HTML" id="d33e1336"/></fig></p><p id="Par49">The backbone network of the improved DeepLabV3&#x02009;+&#x02009;model utilizes ResNet-101, a deep convolutional neural network in the ResNet series<sup><xref ref-type="bibr" rid="CR37">37</xref></sup>. ResNet-101 consists of four convolutional groups and a pooling layer, which helps extract features from the original image. The structure of the ResNet-101 network is shown in Fig.&#x000a0;<xref rid="Fig12" ref-type="fig">12</xref>.<fig id="Fig12"><label>Fig. 12</label><caption><p>ResNet-101 network structure.</p></caption><graphic xlink:href="41597_2024_3240_Fig12_HTML" id="d33e1352"/></fig></p><p id="Par50">Compared to shallower network structures like ResNet-34 and ResNet-50, ResNet-101 has a deeper network structure, allowing it to capture image features more effectively and improve model performance. With more layers and convolutional kernels, ResNet-101 can better learn image features, enhance model accuracy, and exhibit improved generalization ability.</p><p id="Par51">The Atrous Spatial Pyramid Pooling (ASPP) module, another component of the improved DeepLabV3&#x02009;+&#x02009;network, employs Atrous convolutions with different expansion rates to fuse features at various scales. This expansion increases the receptive field without sacrificing image information.</p><p id="Par52">The encoding structure is responsible for feature extraction while reducing the size of the feature map, thereby reducing computational complexity. On the other hand, the decoding structure performs up-sampling to recover spatial detail information and fuses deep and shallow features to achieve more precise recognition results.</p></sec><sec id="Sec11"><title>Qualitative assessment and validation with examples</title><p id="Par53">Quantitative analyses play a crucial role in assessing the performance of a model by providing numerical metrics. These metrics allow for a comprehensive and objective evaluation of the model&#x02019;s performance, facilitating the optimization of its hyperparameters and enabling comparisons between different models. However, it is equally important to complement these quantitative assessments with qualitative analyses through visual inspection. Such qualitative evaluations help to gain a deeper understanding of the quality of the model&#x02019;s predictions.</p><p id="Par54">In Fig.&#x000a0;<xref rid="Fig13" ref-type="fig">13</xref>, we present several examples of correctly predicted images to demonstrate the performance of the model. The first example showcases the model&#x02019;s ability to effectively distinguish between the construction waste pile area and the vacant landfillable area in an open area model. It accurately recognizes the contours of the waste pile and demonstrates high accuracy in identifying building areas, such as engineering facilities.<fig id="Fig13"><label>Fig. 13</label><caption><p>An example of a construction waste landfill projection. In (<bold>a</bold>), the original input sample is overlaid with manually drawn bounding boxes. To provide a closer look at the bounding box region, (<bold>b</bold>) offers a zoomed-in view. The manually mask-labeled image can be seen in (<bold>c</bold>), while (<bold>d</bold>) showcases the image predicted by the enhanced DeepLabV3+&#x02009;network.</p></caption><graphic xlink:href="41597_2024_3240_Fig13_HTML" id="d33e1387"/></fig></p><p id="Par55">The second example features an image with a diverse range of complex features, including small piles of waste, plastics, and other objects. By examining the prediction results, it becomes evident that the trained model&#x02019;s predictions in this complex scene largely align with the manually labeled image. This indicates the model&#x02019;s proficiency in handling intricate scenarios.</p><p id="Par56">The third and fourth examples consist of standardized images of a construction waste landfill. In both cases, the model successfully delineates different zones with clear contours between individual construction waste piles. These examples highlight the model&#x02019;s exceptional segmentation performance and its ability to maintain high accuracy across various complex scenarios.</p><p id="Par57">To summarize, the combination of quantitative analyses and qualitative evaluations through visual inspection provides a comprehensive assessment of the model&#x02019;s performance. The examples presented in Fig.&#x000a0;<xref rid="Fig13" ref-type="fig">13</xref> demonstrate the model&#x02019;s proficiency in accurately recognizing different areas, contours, and objects, reaffirming its effectiveness in segmentation tasks and its ability to excel in diverse and complex scenarios.</p></sec></sec><sec id="Sec12"><title>Usage Notes</title><sec id="Sec13"><title>Dataset usage</title><p id="Par58">The dataset described in the paper can be utilized to train models for various tasks, including binary and multi-label semantic segmentation as well as weakly supervised localization. To facilitate access to the dataset, the Code Availability module provides links to public repositories where it can be downloaded. In addition to the dataset itself, the study also presents statistical information about the dataset. This includes details such as the percentage of labels in each category. Distribution plots are employed to visualize this information, allowing readers to quickly grasp the overall characteristics of the dataset. To simplify data loading and batch processing, utilities are provided that leverage the PyTorch DataLoader <ext-link ext-link-type="uri" xlink:href="https://pytorch.org/docs/stable/data.html#torch.utils.data.DatasetPyTorch">https://pytorch.org/docs/stable/data.html#torch.utils.data.DatasetPyTorch</ext-link>. This enables efficient handling of the dataset during model training and evaluation.</p></sec><sec id="Sec14"><title>Dataset annotation tools</title><p id="Par59">This dataset primarily utilized Adobe Photoshop (PS)<sup><xref ref-type="bibr" rid="CR38">38</xref></sup> software for labeling.<list list-type="order"><list-item><p id="Par60">Adobe Photoshop (PS). Adobe Photoshop is a powerful image editing software that can be used for a variety of image processing tasks, including semantic segmentation image annotation.</p></list-item></list><list list-type="bullet"><list-item><p id="Par61"><bold>Drawing tool</bold>. Photoshop provides a variety of drawing tools that can be used for different labeling tasks. In semantic segmentation, usually use the Brush tool or the Polygonal Lasso Tool.</p></list-item><list-item><p id="Par62"><bold>Labeling method</bold>. The drawing tool is employed to create semantic segmentation labels on a new layer. Different tools can be used to draw lines, polygons, or fill areas to represent various semantic objects in the image. To differentiate between different objects, different colours or labels can be added.</p></list-item><list-item><p id="Par63"><bold>Save Label Result</bold>. The labeled image file is saved, typically by saving the label layer along with the original image for future editing or export.</p></list-item></list></p><p id="Par64">While Photoshop is a powerful tool, it may not be as efficient as specialized image annotation tools. For subsequent labeling tasks, we will use the professional labeling tool, LabelMe.<list list-type="simple"><list-item><label>(2)</label><p id="Par65">LabelMe<sup><xref ref-type="bibr" rid="CR39">39</xref></sup>. LabelMe is an online platform and a popular open-source annotation tool used for annotating images and creating labeled datasets for machine learning and computer vision tasks. LabelMe has been widely used in both research and industry for tasks such as object detection, image segmentation, and image classification. It has contributed to the development of labeled datasets used to train and evaluate machine learning models. Key features of LabelMe include:</p></list-item></list><list list-type="bullet"><list-item><p id="Par66"><bold>Annotation Variety</bold>. Users can create annotations for object detection, semantic segmentation, and instance segmentation tasks. This versatility makes it suitable for a wide range of computer vision projects.</p></list-item><list-item><p id="Par67"><bold>Data Management</bold>. LabelMe facilitates the organization and management of labeled data, making it easy to track annotations and associated metadata.</p></list-item><list-item><p id="Par68"><bold>Exporting Annotations</bold>. Annotations can be exported in various formats, including JSON and XML, which are commonly used in machine learning pipelines.</p></list-item></list></p></sec><sec id="Sec15"><title>Supported usage cases and extension</title><p id="Par69">The CWLD dataset is made available to the scientific community to facilitate advances in identifying and accounting for construction waste in remote sensing images and is a public dataset to finely delineate remote sensing images of the interior areas of construction waste landfills.</p><p id="Par70">CWLD can be used to train a multi-class segmentation model for construction waste landfills based on remote sensing images. This model can accurately predict the condition of each area inside a construction waste landfill. From an application point of view, this highly efficient and intelligent identification method is sufficient for relevant organizations seeking to speed up the urban monitoring process. When abnormal changes in the construction waste landfill at a certain location are detected, it can quickly identify the site according to the coordinates and improve the efficiency of urban construction waste management.</p><p id="Par71">Remote sensing images have high spatial resolution, strong timeliness, large amount of information, and macro-observation characteristics. Based on the combination of deep learning and remote sensing data, they can more accurately and quickly detect changes in construction waste in the process of urban regeneration. This is suitable for relevant departments and research institutes to use for preliminary theoretical research and assessment of the development of urban construction waste production measurement products.</p></sec><sec id="Sec16"><title>The future extensions of CWLD pursue different directions</title><p id="Par72">
<list list-type="bullet"><list-item><p id="Par73"><bold>Geographical expansion</bold>. Currently, the study area is limited to Beijing, which may introduce some selection bias despite considering the diversity of geographic environments and land types. To improve the dataset&#x02019;s diversity and enable the detection of construction waste landfills in Beijing and other regions of China, there are plans to expand the dataset to include data from other areas.</p></list-item><list-item><p id="Par74"><bold>More fine-grained categorization</bold>. The current dataset accurately divides the internal area of the construction waste landfill. However, considering the various types of construction waste and solid waste, it is necessary to identify different types of waste more accurately. To enhance the efficiency of municipal waste management, there are plans to add data on different types of solid waste to the dataset.</p></list-item><list-item><p id="Par75"><bold>Multi-modal imagery</bold>. Currently, CWLD only contains RGB images. To increase the dataset&#x02019;s diversity and support multi-modal data fusion, there are plans to incorporate images captured in different wavelength bands or spectra (e.g., visible, infrared, radar, laser) using various sensors such as satellites, airborne platforms, and ground-based systems.</p></list-item><list-item><p id="Par76"><bold>Multi-temporal imagery</bold>. Analysing changes over time in the same area can provide valuable information about landfill activity and facilitate accurate estimation of solid waste production, growth, or decrease<sup><xref ref-type="bibr" rid="CR40">40</xref></sup>. By adding different time series of images of the same site to CWLD, relevant environmental agencies can better plan interventions. There are already numerous cases in the CWLD dataset where different time points have been captured to track changes in solid waste areas.</p></list-item></list>
</p></sec></sec></body><back><fn-group><fn><p><bold>Publisher&#x02019;s note</bold> Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></fn></fn-group><ack><title>Acknowledgements</title><p>We would like to thank PIESAT Information Technology Co., Ltd. for its cooperation in acquiring the images and providing the location data. This work is supported by the project &#x0201c;Research on the application and practice of data sinking empowerment of grass-roots social governance based on urban planning coordination&#x0201d; of Beijing Big Data Centre, which is funded by the Natural Science Foundation of Beijing Municipality (No.9232008).</p></ack><notes notes-type="author-contribution"><title>Author contributions</title><p>All authors contributed to the study conception and design, as well as the writing of the manuscript. S.L. determined the topic, supervised the study, and revised the manuscript; L.H. performed the data processing and technical validation, conducted relevant experiments, and wrote the manuscript; X.L. provided methods, supervised the study, and revised the manuscript. G.C. provided GF-2 images and revised the manuscript; Z.F. guided the innovation of urban data governance application scenarios and revised the manuscript. All authors have read and agreed to the published version of the manuscript.</p></notes><notes notes-type="data-availability"><title>Code availability</title><p>The data and predictive models presented in this study are publicly available:</p><p><bold>&#x02022; Dataset</bold>. You can download the images from the Zenodo repository<sup><xref ref-type="bibr" rid="CR30">30</xref></sup>
<ext-link ext-link-type="uri" xlink:href="https://zenodo.org/record/8333888">https://zenodo.org/record/8333888</ext-link>. After downloading, place the train and val files from the Deep Learning Datasets folder into the data folder of the CWLD semantic segmentation model.</p><p><bold>&#x02022; CWLD semantic segmentation model</bold>. Code scripts and project instructions on how to use this dataset to train segmentation models are available for download in the Zenodo repository<sup><xref ref-type="bibr" rid="CR41">41</xref></sup>, and weight files for trained models are also provided for readers to try out the models without training. Visit <ext-link ext-link-type="uri" xlink:href="https://zenodo.org/records/10911443">https://zenodo.org/records/10911443</ext-link>. The requirements.txt file provides the libraries needed to run the project, and the README.md file describes in detail the deployment process and functionality of each module, as well as the role of the various toolkits in utils. In addition, the model and the corresponding code for executing the model are available on the GitHub platform at <ext-link ext-link-type="uri" xlink:href="https://github.com/huangleinxidimejd/CWLD_Model">https://github.com/huangleinxidimejd/CWLD_Model</ext-link>.</p></notes><notes id="FPar1" notes-type="COI-statement"><title>Competing interests</title><p id="Par77">The authors declare no competing interests.</p></notes><ref-list id="Bib1"><title>References</title><ref id="CR1"><label>1.</label><mixed-citation publication-type="other">Ali, T. H. <italic>et al</italic>. Application of Artifical Intelligence in Construction Waste Management. in <italic>2019 8th International Conference on Industrial Technology and Management (ICITM)</italic> 50&#x02013;55, 10.1109/ICITM.2019.8710680 (2019).</mixed-citation></ref><ref id="CR2"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Polat</surname><given-names>G</given-names></name><name><surname>Damci</surname><given-names>A</given-names></name><name><surname>Turkoglu</surname><given-names>H</given-names></name><name><surname>Gurgun</surname><given-names>AP</given-names></name></person-group><article-title>Identification of Root Causes of Construction and Demolition (C&#x00026;D) Waste: The Case of Turkey</article-title><source>Procedia Eng.</source><year>2017</year><volume>196</volume><fpage>948</fpage><lpage>955</lpage><pub-id pub-id-type="doi">10.1016/j.proeng.2017.08.035</pub-id></element-citation></ref><ref id="CR3"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Neffa Gobbi</surname><given-names>C</given-names></name><name><surname>Louren&#x000e7;o Sanches</surname><given-names>VM</given-names></name><name><surname>Acordi Vasques Pacheco</surname><given-names>EB</given-names></name><name><surname>de Oliveira Cavalcanti Guimar&#x000e3;es</surname><given-names>MJ</given-names></name><name><surname>Vasconcelos de Freitas</surname><given-names>MA</given-names></name></person-group><article-title>Management of plastic wastes at Brazilian ports and diagnosis of their generation</article-title><source>Mar. Pollut. Bull.</source><year>2017</year><volume>124</volume><fpage>67</fpage><lpage>73</lpage><pub-id pub-id-type="doi">10.1016/j.marpolbul.2017.07.004</pub-id><?supplied-pmid 28709521?><pub-id pub-id-type="pmid">28709521</pub-id>
</element-citation></ref><ref id="CR4"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>L&#x000f3;pez Ruiz</surname><given-names>LA</given-names></name><name><surname>Roca Ram&#x000f3;n</surname><given-names>X</given-names></name><name><surname>Gass&#x000f3; Domingo</surname><given-names>S</given-names></name></person-group><article-title>The circular economy in the construction and demolition waste sector &#x02013; A review and an integrative model approach</article-title><source>J. Clean. Prod.</source><year>2020</year><volume>248</volume><fpage>119238</fpage><pub-id pub-id-type="doi">10.1016/j.jclepro.2019.119238</pub-id></element-citation></ref><ref id="CR5"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>F</given-names></name><name><surname>Ju</surname><given-names>Y</given-names></name><name><surname>Dong</surname><given-names>P</given-names></name><name><surname>Santibanez Gonzalez</surname><given-names>ED</given-names></name></person-group><article-title>A fuzzy evaluation and selection of construction and demolition waste utilization modes in Xi&#x02019;an,</article-title><source>China. Waste Manag. Res.</source><year>2020</year><volume>38</volume><fpage>792</fpage><lpage>801</lpage><pub-id pub-id-type="doi">10.1177/0734242X20908925</pub-id><?supplied-pmid 32160832?><pub-id pub-id-type="pmid">32160832</pub-id>
</element-citation></ref><ref id="CR6"><label>6.</label><mixed-citation publication-type="other">Ramnarayan &#x00026; Malla, P. A Machine Learning-Enhanced Method for Quantifying and Recycling Construction and Demolition Waste in India. in <italic>2023 IEEE International Conference on Integrated Circuits and Communication Systems (ICICACS)</italic> 1&#x02013;7, 10.1109/ICICACS57338.2023.10099602 (2023).</mixed-citation></ref><ref id="CR7"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lu</surname><given-names>W</given-names></name><etal/></person-group><article-title>Estimating construction waste generation in the Greater Bay Area, China using machine learning</article-title><source>Waste Manag.</source><year>2021</year><volume>134</volume><fpage>78</fpage><lpage>88</lpage><pub-id pub-id-type="doi">10.1016/j.wasman.2021.08.012</pub-id><?supplied-pmid 34416673?><pub-id pub-id-type="pmid">34416673</pub-id>
</element-citation></ref><ref id="CR8"><label>8.</label><mixed-citation publication-type="other">Shahab, S., Anjum, M. &#x00026; Umar, M. S. Deep Learning Applications in Solid Waste Management: A Deep Literature Review. <italic>Int. J. Adv. Comput. Sci. Appl</italic>. <bold>13</bold>, (2022).</mixed-citation></ref><ref id="CR9"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gao</surname><given-names>S</given-names></name><etal/></person-group><article-title>IUNet-IF: identification of construction waste using unmanned aerial vehicle remote sensing and multi-layer deep learning methods</article-title><source>Int. J. Remote Sens.</source><year>2022</year><volume>43</volume><fpage>7181</fpage><lpage>7212</lpage><pub-id pub-id-type="doi">10.1080/01431161.2022.2155084</pub-id></element-citation></ref><ref id="CR10"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhao</surname><given-names>X</given-names></name><etal/></person-group><article-title>Identification of construction and demolition waste based on change detection and deep learning</article-title><source>Int. J. Remote Sens.</source><year>2022</year><volume>43</volume><fpage>2012</fpage><lpage>2028</lpage><pub-id pub-id-type="doi">10.1080/01431161.2022.2054296</pub-id></element-citation></ref><ref id="CR11"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lu</surname><given-names>W</given-names></name><name><surname>Chen</surname><given-names>J</given-names></name><name><surname>Xue</surname><given-names>F</given-names></name></person-group><article-title>Using computer vision to recognize composition of construction waste mixtures: A semantic segmentation approach</article-title><source>Resour. Conserv. Recycl.</source><year>2022</year><volume>178</volume><fpage>106022</fpage><pub-id pub-id-type="doi">10.1016/j.resconrec.2021.106022</pub-id></element-citation></ref><ref id="CR12"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Abdallah</surname><given-names>M</given-names></name><etal/></person-group><article-title>Artificial intelligence applications in solid waste management: A systematic research review</article-title><source>Waste Manag.</source><year>2020</year><volume>109</volume><fpage>231</fpage><lpage>246</lpage><pub-id pub-id-type="doi">10.1016/j.wasman.2020.04.057</pub-id><?supplied-pmid 32428727?><pub-id pub-id-type="pmid">32428727</pub-id>
</element-citation></ref><ref id="CR13"><label>13.</label><mixed-citation publication-type="other">Deng, J. <italic>et al</italic>. ImageNet: A large-scale hierarchical image database. in <italic>2009 IEEE Conference on Computer Vision and Pattern Recognition</italic> 248&#x02013;255, 10.1109/CVPR.2009.5206848 (2009).</mixed-citation></ref><ref id="CR14"><label>14.</label><mixed-citation publication-type="other">Lin, T.-Y. <italic>et al</italic>. Microsoft COCO: Common Objects in Context. in <italic>Computer Vision &#x02013; ECCV 2014</italic> (eds. Fleet, D., Pajdla, T., Schiele, B. &#x00026; Tuytelaars, T.) 740&#x02013;755, 10.1007/978-3-319-10602-1_48 (Springer International Publishing, Cham, 2014).</mixed-citation></ref><ref id="CR15"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xiaoyu</surname><given-names>LIU</given-names></name><etal/></person-group><article-title>Research on construction and demolition waste stacking point identification based on DeeplabV3+</article-title><source>Bull. Surv. Mapp.</source><year>2022</year><volume>0</volume><fpage>16</fpage></element-citation></ref><ref id="CR16"><label>16.</label><mixed-citation publication-type="other">Padubidri, C., Kamilaris, A. &#x00026; Karatsiolis, S. Accurate Detection of Illegal Dumping Sites Using High Resolution Aerial Photography and Deep Learning. in <italic>2022 IEEE International Conference on Pervasive Computing and Communications Workshops and other Affiliated Events (PerCom Workshops)</italic> 451&#x02013;456, 10.1109/PerComWorkshops53856.2022.9767451 (2022).</mixed-citation></ref><ref id="CR17"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lu</surname><given-names>W</given-names></name><name><surname>Chen</surname><given-names>J</given-names></name></person-group><article-title>Computer vision for solid waste sorting: A critical review of academic research</article-title><source>Waste Manag.</source><year>2022</year><volume>142</volume><fpage>29</fpage><lpage>43</lpage><pub-id pub-id-type="doi">10.1016/j.wasman.2022.02.009</pub-id><?supplied-pmid 35172271?><pub-id pub-id-type="pmid">35172271</pub-id>
</element-citation></ref><ref id="CR18"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Youme</surname><given-names>O</given-names></name><name><surname>Bayet</surname><given-names>T</given-names></name><name><surname>Dembele</surname><given-names>JM</given-names></name><name><surname>Cambier</surname><given-names>C</given-names></name></person-group><article-title>Deep Learning and Remote Sensing: Detection of Dumping Waste Using UAV</article-title><source>Procedia Comput. Sci.</source><year>2021</year><volume>185</volume><fpage>361</fpage><lpage>369</lpage><pub-id pub-id-type="doi">10.1016/j.procs.2021.05.037</pub-id></element-citation></ref><ref id="CR19"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Torres</surname><given-names>RN</given-names></name><name><surname>Fraternali</surname><given-names>P</given-names></name></person-group><article-title>Learning to Identify Illegal Landfills through Scene Classification in Aerial Images</article-title><source>Remote Sens.</source><year>2021</year><volume>13</volume><fpage>4520</fpage><pub-id pub-id-type="doi">10.3390/rs13224520</pub-id></element-citation></ref><ref id="CR20"><label>20.</label><mixed-citation publication-type="other">Sumbul, G., Charfuelan, M., Demir, B. &#x00026; Markl, V. Bigearthnet: A Large-Scale Benchmark Archive for Remote Sensing Image Understanding. in <italic>IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium</italic> 5901&#x02013;5904, 10.1109/IGARSS.2019.8900532 (2019).</mixed-citation></ref><ref id="CR21"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Torres</surname><given-names>RN</given-names></name><name><surname>Fraternali</surname><given-names>P</given-names></name></person-group><article-title>AerialWaste dataset for landfill discovery in aerial and satellite images</article-title><source>Sci. Data</source><year>2023</year><volume>10</volume><fpage>63</fpage><pub-id pub-id-type="doi">10.1038/s41597-023-01976-9</pub-id><?supplied-pmid 36720877?><pub-id pub-id-type="pmid">36720877</pub-id>
</element-citation></ref><ref id="CR22"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhou</surname><given-names>L</given-names></name><etal/></person-group><article-title>SWDet: Anchor-Based Object Detector for Solid Waste Detection in Aerial Images</article-title><source>IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens.</source><year>2023</year><volume>16</volume><fpage>306</fpage><lpage>320</lpage><pub-id pub-id-type="doi">10.1109/JSTARS.2022.3218958</pub-id></element-citation></ref><ref id="CR23"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ren</surname><given-names>K</given-names></name><name><surname>Sun</surname><given-names>W</given-names></name><name><surname>Meng</surname><given-names>X</given-names></name><name><surname>Yang</surname><given-names>G</given-names></name><name><surname>Du</surname><given-names>Q</given-names></name></person-group><article-title>Fusing China GF-5 Hyperspectral Data with GF-1, GF-2 and Sentinel-2A Multispectral Data: Which Methods Should Be Used?</article-title><source>Remote Sens.</source><year>2020</year><volume>12</volume><fpage>882</fpage><pub-id pub-id-type="doi">10.3390/rs12050882</pub-id></element-citation></ref><ref id="CR24"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lin</surname><given-names>S</given-names></name><etal/></person-group><article-title>MS-AGAN: Road Extraction via Multi-Scale Information Fusion and Asymmetric Generative Adversarial Networks from High-Resolution Remote Sensing Images under Complex Backgrounds</article-title><source>Remote Sens.</source><year>2023</year><volume>15</volume><fpage>3367</fpage><pub-id pub-id-type="doi">10.3390/rs15133367</pub-id></element-citation></ref><ref id="CR25"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lin</surname><given-names>S</given-names></name><etal/></person-group><article-title>Accurate Recognition of Building Rooftops and Assessment of Long-Term Carbon Emission Reduction from Rooftop Solar Photovoltaic Systems Fusing GF-2 and Multi-Source Data</article-title><source>Remote Sens.</source><year>2022</year><volume>14</volume><fpage>3144</fpage><pub-id pub-id-type="doi">10.3390/rs14133144</pub-id></element-citation></ref><ref id="CR26"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Z</given-names></name><etal/></person-group><article-title>Semantic segmentation of high-resolution remote sensing images based on a class feature attention mechanism fused with Deeplabv3+</article-title><source>Comput. Geosci.</source><year>2022</year><volume>158</volume><fpage>104969</fpage><pub-id pub-id-type="doi">10.1016/j.cageo.2021.104969</pub-id></element-citation></ref><ref id="CR27"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tang</surname><given-names>Z</given-names></name><etal/></person-group><article-title>Winter Wheat Lodging Area Extraction Using Deep Learning with GaoFen-2 Satellite Imagery</article-title><source>Remote Sens.</source><year>2022</year><volume>14</volume><fpage>4887</fpage><pub-id pub-id-type="doi">10.3390/rs14194887</pub-id></element-citation></ref><ref id="CR28"><label>28.</label><mixed-citation publication-type="other">Brand Resource Center | Products and Services - Geo Guidelines. <ext-link ext-link-type="uri" xlink:href="https://about.google/brand-resource-center/products-and-services/geo-guidelines/">https://about.google/brand-resource-center/products-and-services/geo-guidelines/</ext-link>.</mixed-citation></ref><ref id="CR29"><label>29.</label><mixed-citation publication-type="other">ArcMap Resources for ArcGIS Desktop | Documentation, Tutorials &#x00026; More. <ext-link ext-link-type="uri" xlink:href="https://www.esri.com/en-us/arcgis/products/arcgis-desktop/resources">https://www.esri.com/en-us/arcgis/products/arcgis-desktop/resources</ext-link>.</mixed-citation></ref><ref id="CR30"><label>30.</label><element-citation publication-type="data"><name><surname>Huang</surname><given-names>L</given-names></name><name><surname>Lin</surname><given-names>S</given-names></name><name><surname>Liu</surname><given-names>X</given-names></name><year>2023</year><data-title>A Construction Waste Landfill Dataset of Two Districts in Beijing, China from High Resolution Satellite Images</data-title><source>Zenodo</source><pub-id pub-id-type="doi">10.5281/zenodo.8333888</pub-id></element-citation></ref><ref id="CR31"><label>31.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guo</surname><given-names>Y</given-names></name><name><surname>Liu</surname><given-names>Y</given-names></name><name><surname>Georgiou</surname><given-names>T</given-names></name><name><surname>Lew</surname><given-names>MS</given-names></name></person-group><article-title>A review of semantic segmentation using deep neural networks</article-title><source>Int. J. Multimed. Inf. Retr.</source><year>2018</year><volume>7</volume><fpage>87</fpage><lpage>93</lpage><pub-id pub-id-type="doi">10.1007/s13735-017-0141-z</pub-id></element-citation></ref><ref id="CR32"><label>32.</label><mixed-citation publication-type="other">Wang, P. <italic>et al</italic>. Understanding Convolution for Semantic Segmentation. in <italic>2018 IEEE Winter Conference on Applications of Computer Vision (WACV)</italic> 1451&#x02013;1460, 10.1109/WACV.2018.00163 (2018).</mixed-citation></ref><ref id="CR33"><label>33.</label><mixed-citation publication-type="other">Chen, L.-C., Zhu, Y., Papandreou, G., Schroff, F. &#x00026; Adam, H. Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation. in <italic>Computer Vision &#x02013; ECCV 2018</italic> (eds. Ferrari, V., Hebert, M., Sminchisescu, C. &#x00026; Weiss, Y.) 833&#x02013;851, 10.1007/978-3-030-01234-2_49 (Springer International Publishing, Cham, 2018).</mixed-citation></ref><ref id="CR34"><label>34.</label><mixed-citation publication-type="other">Ronneberger, O., Fischer, P. &#x00026; Brox, T. U-Net: Convolutional Networks for Biomedical Image Segmentation. in <italic>Medical Image Computing and Computer-Assisted Intervention &#x02013; MICCAI 2015</italic> (eds. Navab, N., Hornegger, J., Wells, W. M. &#x00026; Frangi, A. F.) vol. 9351 234&#x02013;241 (Springer International Publishing, Cham, 2015).</mixed-citation></ref><ref id="CR35"><label>35.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Badrinarayanan</surname><given-names>V</given-names></name><name><surname>Kendall</surname><given-names>A</given-names></name><name><surname>Cipolla</surname><given-names>R</given-names></name></person-group><article-title>SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2017</year><volume>39</volume><fpage>2481</fpage><lpage>2495</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2016.2644615</pub-id><?supplied-pmid 28060704?><pub-id pub-id-type="pmid">28060704</pub-id>
</element-citation></ref><ref id="CR36"><label>36.</label><mixed-citation publication-type="other">Zhao, H., Shi, J., Qi, X., Wang, X. &#x00026; Jia, J. Pyramid Scene Parsing Network. in <italic>2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</italic> 6230&#x02013;6239, 10.1109/CVPR.2017.660 (2017).</mixed-citation></ref><ref id="CR37"><label>37.</label><mixed-citation publication-type="other">He, K., Zhang, X., Ren, S. &#x00026; Sun, J. Deep Residual Learning for Image Recognition. in <italic>2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</italic> 770&#x02013;778, 10.1109/CVPR.2016.90 (2016).</mixed-citation></ref><ref id="CR38"><label>38.</label><mixed-citation publication-type="other"><italic>Adobe Photoshop - Photo &#x00026; Design Software</italic>. <ext-link ext-link-type="uri" xlink:href="https://www.adobe.com/products/photoshop.html">https://www.adobe.com/products/photoshop.html</ext-link>.</mixed-citation></ref><ref id="CR39"><label>39.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Russell</surname><given-names>BC</given-names></name><name><surname>Torralba</surname><given-names>A</given-names></name><name><surname>Murphy</surname><given-names>KP</given-names></name><name><surname>Freeman</surname><given-names>WT</given-names></name></person-group><article-title>LabelMe: A Database and Web-Based Tool for Image Annotation</article-title><source>Int. J. Comput. Vis.</source><year>2008</year><volume>77</volume><fpage>157</fpage><lpage>173</lpage><pub-id pub-id-type="doi">10.1007/s11263-007-0090-8</pub-id></element-citation></ref><ref id="CR40"><label>40.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>H</given-names></name><name><surname>Shi</surname><given-names>Z</given-names></name></person-group><article-title>A Spatial-Temporal Attention-Based Method and a New Dataset for Remote Sensing Image Change Detection</article-title><source>Remote Sens.</source><year>2020</year><volume>12</volume><fpage>1662</fpage><pub-id pub-id-type="doi">10.3390/rs12101662</pub-id></element-citation></ref><ref id="CR41"><label>41.</label><element-citation publication-type="data"><name><surname>Huang</surname><given-names>L</given-names></name><year>2024</year><data-title>Semantic segmentation model of construction waste landfill based on high-resolution satellite images</data-title><source>Zenodo</source><pub-id pub-id-type="doi">10.5281/zenodo.10911443</pub-id></element-citation></ref><ref id="CR42"><label>42.</label><element-citation publication-type="data"><name><surname>Torres</surname><given-names>RN</given-names></name><name><surname>Fraternali</surname><given-names>P</given-names></name><year>2023</year><data-title>AerialWaste</data-title><source>Zenodo</source><pub-id pub-id-type="doi">10.5281/zenodo.7991872</pub-id></element-citation></ref><ref id="CR43"><label>43.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kraft</surname><given-names>M</given-names></name><name><surname>Piechocki</surname><given-names>M</given-names></name><name><surname>Ptak</surname><given-names>B</given-names></name><name><surname>Walas</surname><given-names>K</given-names></name></person-group><article-title>Autonomous, Onboard Vision-Based Trash and Litter Detection in Low Altitude Aerial Images Collected by an Unmanned Aerial Vehicle</article-title><source>Remote Sens.</source><year>2021</year><volume>13</volume><fpage>965</fpage><pub-id pub-id-type="doi">10.3390/rs13050965</pub-id></element-citation></ref><ref id="CR44"><label>44.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>T</given-names></name><name><surname>Cai</surname><given-names>Y</given-names></name><name><surname>Liang</surname><given-names>L</given-names></name><name><surname>Ye</surname><given-names>D</given-names></name></person-group><article-title>A Multi-Level Approach to Waste Object Segmentation</article-title><source>Sensors</source><year>2020</year><volume>20</volume><fpage>3816</fpage><pub-id pub-id-type="doi">10.3390/s20143816</pub-id><?supplied-pmid 32650515?><pub-id pub-id-type="pmid">32650515</pub-id>
</element-citation></ref></ref-list></back></article>