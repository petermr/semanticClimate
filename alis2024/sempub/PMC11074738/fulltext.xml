<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd"> 
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?DTDIdentifier.IdentifierValue -//NLM//DTD Journal Publishing DTD v3.0 20080202//EN?><?DTDIdentifier.IdentifierType public?><?SourceDTD.DTDName journalpublishing3.dtd?><?SourceDTD.Version 3.0?><?ConverterInfo.XSLTName jats2jats3.xsl?><?ConverterInfo.Version 1?><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Quant Imaging Med Surg</journal-id><journal-id journal-id-type="iso-abbrev">Quant Imaging Med Surg</journal-id><journal-id journal-id-type="publisher-id">QIMS</journal-id><journal-title-group><journal-title>Quantitative Imaging in Medicine and Surgery</journal-title></journal-title-group><issn pub-type="ppub">2223-4292</issn><issn pub-type="epub">2223-4306</issn><publisher><publisher-name>AME Publishing Company</publisher-name></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmcid">11074738</article-id><article-id pub-id-type="publisher-id">qims-14-05-3707</article-id><article-id pub-id-type="doi">10.21037/qims-23-1384</article-id><article-categories><subj-group subj-group-type="heading"><subject>Original Article</subject></subj-group></article-categories><title-group><article-title>Semi-supervised learning in diagnosis of infant hip dysplasia towards multisource ultrasound images</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Li</surname><given-names>Xuanpeng</given-names></name><contrib-id contrib-id-type="orcid">https://orcid.org/0000-0001-9320-0658</contrib-id><xref rid="aff1" ref-type="aff">
<sup>1</sup>
</xref></contrib><contrib contrib-type="author"><name><surname>Zhang</surname><given-names>Ruixiang</given-names></name><xref rid="aff1" ref-type="aff">
<sup>1</sup>
</xref></contrib><contrib contrib-type="author"><name><surname>Wang</surname><given-names>Zhibo</given-names></name><xref rid="aff1" ref-type="aff">
<sup>1</sup>
</xref></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Wang</surname><given-names>Jiakuan</given-names></name><xref rid="aff2" ref-type="aff">
<sup>2</sup>
</xref></contrib><aff id="aff1"><label>1</label><institution content-type="dept">School of Instrument Science and Engineering</institution>, <institution>Southeast University</institution>, <addr-line>Nanjing</addr-line>, <country country="cn">China</country>;</aff><aff id="aff2"><label>2</label><institution content-type="dept">Department of Orthopedics</institution>, <institution>Yangzhou Maternal and Child Health Care Service Centre</institution>, <addr-line>Yangzhou</addr-line>, <country country="cn">China</country></aff></contrib-group><author-notes><fn id="afn1"><p><italic>Contributions:</italic> (I) Conception and design: X Li, J Wang; (II) Administrative support: X Li; (III) Provision of study materials or patients: J Wang; (IV) Collection and assembly of data: Z Wang; (V) Data analysis and interpretation: R Zhang, Z Wang; (VI) Manuscript writing: All authors; (VII) Final approval of manuscript: All authors.</p></fn><corresp id="cor1"><italic>Correspondence to:</italic> Jiakuan Wang, MD. Associated Professor, Director of Orthopedics, Department of Orthopedics, Yangzhou Maternal and Child Health Care Service Centre, 395 Guoqing Road, Yangzhou 225002, China. Email: <email xlink:href="jkwangyz@126.com">jkwangyz@126.com</email>.</corresp></author-notes><pub-date pub-type="epub"><day>18</day><month>4</month><year>2024</year></pub-date><pub-date pub-type="ppub"><day>01</day><month>5</month><year>2024</year></pub-date><volume>14</volume><issue>5</issue><fpage>3707</fpage><lpage>3716</lpage><history><date date-type="received"><day>02</day><month>10</month><year>2023</year></date><date date-type="accepted"><day>19</day><month>3</month><year>2024</year></date></history><permissions><copyright-statement>2024 Quantitative Imaging in Medicine and Surgery. All rights reserved.</copyright-statement><copyright-year>2024</copyright-year><copyright-holder>Quantitative Imaging in Medicine and Surgery.</copyright-holder><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbyncndlicense">https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref><license-p><italic>Open Access Statement:</italic> This is an Open Access article distributed in accordance with the Creative Commons Attribution-NonCommercial-NoDerivs 4.0 International License (CC BY-NC-ND 4.0), which permits the non-commercial replication and distribution of the article with the strict proviso that no changes or edits are made and the original work is properly cited (including links to both the formal publication through the relevant DOI and the license). See: <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc-nd/4.0/">https://creativecommons.org/licenses/by-nc-nd/4.0</ext-link>.</license-p></license></permissions><abstract><sec><title>Background</title><p>Automated diagnosis of infant hip dysplasia is heavily affected by the individual differences among infants and ultrasound machines.</p></sec><sec><title>Methods</title><p>Hip sonographic images of 493 infants from various ultrasound machines were collected in the Department of Orthopedics in Yangzhou Maternal and Child Health Care Service Centre. Herein, we propose a semi-supervised learning method based on a feature pyramid network (FPN) and a contrastive learning scheme based on a Siamese architecture. A large amount of unlabeled data of ultrasound images was used via the Siamese network in the pre-training step, and then a small amount of annotated data for anatomical structures was adopted to train the model for landmark identification and standard plane recognition. The method was evaluated on our collected dataset.</p></sec><sec><title>Results</title><p>The method achieved a mean Dice similarity coefficient (DSC) of 0.7873 and a mean Hausdorff distance (HD) of 5.0102 in landmark identification, compared to the model without contrastive learning, which had a mean DSC of 0.7734 and a mean HD of 6.1586. The accuracy, precision, and recall of standard plane recognition were 95.4%, 91.64%, and 94.86%, respectively. The corresponding area under the curve (AUC) was 0.982.</p></sec><sec><title>Conclusions</title><p>This study proposes a semi-supervised deep learning method following Graf&#x02019;s principle, which can better utilize a large volume of ultrasound images from various devices and infants. This method can identify the landmarks of infant hips more accurately than manual operators, thereby improving the efficiency of diagnosis of infant hip dysplasia.</p></sec></abstract><kwd-group kwd-group-type="author"><title>Keywords: </title><kwd>Sonographic image</kwd><kwd>contrast learning</kwd><kwd>landmark identification</kwd><kwd>Semantic segmentation</kwd><kwd>infants hip dysplasia</kwd></kwd-group><funding-group><award-group><funding-source id="sp1">the Research Foundation of Jiangsu Provincial Commission of Health</funding-source><award-id rid="sp1">No. ZD2022049</award-id></award-group></funding-group></article-meta></front><body><sec sec-type="intro"><title>Introduction</title><p>Developmental dysplasia of the hip (DDH) has an incidence rate of 1.6&#x02013;28.5%, which is one of the most common musculoskeletal disorders that seriously affects infant health (<xref rid="r1" ref-type="bibr">1</xref>). Late diagnoses increase the need for operative intervention and have long term implications for patients and their families (<xref rid="r2" ref-type="bibr">2</xref>). Ultrasound, which can penetrate the infant hip and produce different echo strengths, is used to assess its structural abnormalities (<xref rid="r3" ref-type="bibr">3</xref>). Moreover, it is radiation-free and cost-effective, making it a primary tool for early DDH diagnosis. Graf introduced a standardized scanning technique for hip sonography examination, involving recognition of standard plane and anatomical structure (<xref rid="r4" ref-type="bibr">4</xref>). It categorizes DDH into 4 types and multiple subtypes, providing reliable results with repeatability.</p><p>Graf&#x02019;s ultrasound classification is crucial for determining whether infant hips are abnormal. It relies on the standard plane recognition and landmark identification of anatomical structures, which depend on the personal experience of specialized doctors and may lead to measurement variations among doctors.</p><p>In recent years, deep neural networks have been explored for their excellent image feature extraction capabilities in ultrasound diagnosis (<xref rid="r5" ref-type="bibr">5</xref>-<xref rid="r10" ref-type="bibr">10</xref>). Most auxiliary diagnostic methods based on deep learning rely heavily on the pixel-level annotated data (<xref rid="r11" ref-type="bibr">11</xref>). To mitigate the dependency of deep learning methods on annotation of various data sources, we introduced contrastive learning techniques based on a Siamese architecture to improve the performance of ultrasound diagnosis among various infants and ultrasound machines.</p></sec><sec sec-type="methods"><title>Methods</title><sec><title>Sample selection</title><p>A total of 493 infants who underwent hip ultrasound examinations were selected for this study at the Department of Orthopedics of Yangzhou Maternal and Child Health Care Service Centre between 2021 and 2022. The age of these infants ranged from 30 to 90 days. In the experiment, a total of 4,437 hip ultrasound images were collected. Among these, 1,479 images strictly adhered to the requirements of standard plane based on the Graf&#x02019;s method. These images were reviewed and categorized by 6 orthopedists with extensive clinical experience. The study was approved by the Research Ethics Committee of Southeast University and Yangzhou Maternal and Child Health Care Service Centre. The requirement for individual consent for this retrospective analysis was waived. The study was conducted in accordance with the Declaration of Helsinki (as revised in 2013).</p></sec><sec><title>Our approach</title><p>The components employed in this method are illustrated in <xref rid="f1" ref-type="fig"><italic>Figure 1</italic></xref>. Initially, ultrasound data were collected and partially annotated to construct the dataset. A subset of the dataset was annotated with semantic information of landmarks for anatomical structures. A feature pyramid network (FPN) (<xref rid="r12" ref-type="bibr">12</xref>) was pretrained using a contrastive learning approach via a Siamese network. Then, the model with 2 downstream task branches was trained by using the pretrained feature maps and labeled data. The downstream tasks consisted of standard plane recognition and landmarks identification based on semantic segmentation. Finally, &#x003b1; and &#x003b2; angles were calculated based on the landmarks. The final results were presented to assist doctors in the diagnosis process.</p><fig position="float" id="f1" fig-type="figure"><label>Figure 1</label><caption><p>Framework of our proposed method. (A) Contrastive learning; (B) feature pyramid network module; (C) downstream tasks including standard plane recognition and semantic segmentation.</p></caption><graphic xlink:href="qims-14-05-3707-f1" position="float"/></fig></sec><sec><title>Contrastive self-supervised learning</title><p>In order to fully leverage a large amount of unlabeled data, this approach utilizes contrastive learning for pre-training. Contrastive learning, as a novel machine learning technique, guides the model to learn common features among unlabeled data by teaching it the similarities and dissimilarities between data features. Its advantage lies in its ability to make full use of a large amount of unlabeled data by creating a proxy task where custom pseudo-labels are treated as training signals, and the learned representations can then be applied to downstream tasks (<xref rid="r11" ref-type="bibr">11</xref>).</p><p>Currently, there are several commonly used strategies of contrastive learning:</p><p>SimCLR (<xref rid="r13" ref-type="bibr">13</xref>): it generates positive samples using data augmentation and uses other data within the same batch as negative samples.</p><p>SwAV (<xref rid="r14" ref-type="bibr">14</xref>): it uses clustering to create positive and negative samples, avoiding the need for large batch sizes or MemoryBank.</p><p>BYOL (<xref rid="r15" ref-type="bibr">15</xref>): it uses BatchNormalization to eliminate the need for searching negative samples during training and prevents training collapse.</p><p>SimSiam (<xref rid="r16" ref-type="bibr">16</xref>): it employs the expectation-maximization (EM) strategy to control the gradient propagation during training, preventing training collapse more effectively and being easier to implement.</p><p>Contrastive learning proxy tasks typically encourage models to bring similar data pairs closer while pushing dissimilar data pairs apart. SimCLR directly uses co-occurring negative samples within the current batch, requiring a large batch size and resulting in significant memory consumption during training. SwAV combines clustering algorithms with neural networks to encode input information for contrast, integrating it into cluster centers, avoiding the use of large batch sizes but significantly increasing computational complexity during training. BYOL does not use negative sample pairs; it directly predicts the output of 1 view from another view of an image. BYOL is essentially a momentum encoder twin network and cannot entirely prevent model training collapse. Compared to the methods above, SimSiam is a kind of Siamese architecture, which directly maximizes the similarity of 1 image&#x02019;s 2 views, using neither negative pairs nor a momentum encoder. It works with typical batch sizes and does not rely on large-batch training (<xref rid="r16" ref-type="bibr">16</xref>).</p><p>In this study, the contrastive learning scheme based on SimSiam was used for the optimization of feature extractor parameters following these steps:</p><list list-type="roman-upper" id="L1"><list-item><p>Considering the following loss function:<disp-formula id="e1"><mml:math id="m1" display="block" overflow="scroll"><mml:mrow><mml:mi>L</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>&#x003b8;</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003b7;</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>l</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>&#x003b8;</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>&#x003b7;</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math><label>[1]</label></disp-formula>
</p><p>where <inline-formula><mml:math id="m2" overflow="scroll"><mml:mi>F</mml:mi></mml:math></inline-formula> is the feature extraction network with network parameters <inline-formula><mml:math id="m3" overflow="scroll"><mml:mi>&#x003b8;</mml:mi></mml:math></inline-formula>; <inline-formula><mml:math id="m4" overflow="scroll"><mml:mi>&#x00393;</mml:mi></mml:math></inline-formula> is the data enhancement function; <inline-formula><mml:math id="m5" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>&#x003b7;</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the optimal parameter network. Therefore, in order for the network to learn the optimal extraction capability, we need to solve <inline-formula><mml:math id="m6" overflow="scroll"><mml:mi>&#x003b8;</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="m7" overflow="scroll"><mml:mi>&#x003b7;</mml:mi></mml:math></inline-formula> simultaneously to minimize <inline-formula><mml:math id="m8" overflow="scroll"><mml:mi>L</mml:mi></mml:math></inline-formula>.</p></list-item><list-item><p>According to the <italic>EM</italic> algorithm, the process of minimizing L follows these 2 alternating steps:<disp-formula id="e2"><mml:math id="m9" display="block" overflow="scroll"><mml:mrow><mml:msup><mml:mi>&#x003b8;</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mo>&#x02190;</mml:mo><mml:munder><mml:mrow><mml:mi>arg</mml:mi><mml:mi>min</mml:mi></mml:mrow><mml:mi>&#x003b8;</mml:mi></mml:munder><mml:mi>L</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>&#x003b8;</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>&#x003b7;</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math><label>[2]</label></disp-formula>
<disp-formula id="e3"><mml:math id="m10" display="block" overflow="scroll"><mml:mrow><mml:msup><mml:mi>&#x003b7;</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mo>&#x02190;</mml:mo><mml:munder><mml:mrow><mml:mi>arg</mml:mi><mml:mi>min</mml:mi></mml:mrow><mml:mi>&#x003b7;</mml:mi></mml:munder><mml:mi>L</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>&#x003b8;</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:mi>&#x003b7;</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math><label>[3]</label></disp-formula>
</p><p>where <inline-formula><mml:math id="m11" overflow="scroll"><mml:mi>t</mml:mi></mml:math></inline-formula> is the index of the alternating solution. When solving <inline-formula><mml:math id="m12" overflow="scroll"><mml:mrow><mml:msup><mml:mi>&#x003b8;</mml:mi><mml:mi>t</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>, the parameter <inline-formula><mml:math id="m13" overflow="scroll"><mml:mrow><mml:msup><mml:mi>&#x003b7;</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> is controlled by stopping gradient descent; when solving <inline-formula><mml:math id="m14" overflow="scroll"><mml:mrow><mml:msup><mml:mi>&#x003b7;</mml:mi><mml:mi>t</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="m15" overflow="scroll"><mml:mrow><mml:msup><mml:mi>&#x003b8;</mml:mi><mml:mi>t</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> is the optimal parameters under the current data enhancement method <inline-formula><mml:math id="m16" overflow="scroll"><mml:mi>&#x00393;</mml:mi></mml:math></inline-formula>.</p></list-item></list><p>At the same time, the predictor was improved based on both the selected backbone network and the output feature map. The convolutional layer in FPN was used to replace the original fully connected layer, so that it could process both feature vectors and feature maps, as shown in <xref rid="f2" ref-type="fig"><italic>Figure 2</italic></xref>.</p><fig position="float" id="f2" fig-type="figure"><label>Figure 2</label><caption><p>The pretraining process. FPN, feature pyramid network.</p></caption><graphic xlink:href="qims-14-05-3707-f2" position="float"/></fig><p>The pre-training process was divided into 4 steps: (I) perform data augmentation in different ways to obtain multiple pairs of positive samples <inline-formula><mml:math id="m17" overflow="scroll"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>; (II) input <inline-formula><mml:math id="m18" overflow="scroll"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> into the feature extractor and obtain the feature maps <inline-formula><mml:math id="m19" overflow="scroll"><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>; (III) since the input <inline-formula><mml:math id="m20" overflow="scroll"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> are positive samples, we hope that the information extracted by the feature extractor should be consistent. Then, <inline-formula><mml:math id="m21" overflow="scroll"><mml:mrow><mml:mtext>&#x000a0;</mml:mtext><mml:msub><mml:mi>p</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> is obtained by the predictor with <inline-formula><mml:math id="m22" overflow="scroll"><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>; (IV) minimize the distance between <inline-formula><mml:math id="m23" overflow="scroll"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="m24" overflow="scroll"><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, and estimate the parameters of the feature extractor by controlling the gradient propagation.</p><p>A loss function based on cosine similarity was used as:</p><disp-formula id="e4">
<mml:math id="m25" display="block" overflow="scroll"><mml:mrow><mml:mi>D</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>&#x022c5;</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mo>&#x02016;</mml:mo><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>&#x02016;</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo>&#x000d7;</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mo>&#x02016;</mml:mo><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>&#x02016;</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:math>
<label>[4]</label>
</disp-formula><p>Combined with the EM algorithm, the loss function was defined as:</p><disp-formula id="e5">
<mml:math id="m26" display="block" overflow="scroll"><mml:mrow><mml:mtext>Loss</mml:mtext><mml:mo>=</mml:mo><mml:mi>D</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>p</mml:mi><mml:mi>g</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math>
<label>[5]</label>
</disp-formula><p>Among them, the encoder on <inline-formula><mml:math id="m27" overflow="scroll"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> receives the gradient from <inline-formula><mml:math id="m28" overflow="scroll"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> for update, and the encoder on <inline-formula><mml:math id="m29" overflow="scroll"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> does not update the parameters from <inline-formula><mml:math id="m30" overflow="scroll"><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> due to gradient stop. In order to realize the alternate update iteration of the encoder parameters, <inline-formula><mml:math id="m31" overflow="scroll"><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> is also processed through the predictor to obtain <inline-formula><mml:math id="m32" overflow="scroll"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>. The loss function is expanded to the following formula to update the encoder parameters on <inline-formula><mml:math id="m33" overflow="scroll"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> as follows:</p><disp-formula id="e6">
<mml:math id="m34" display="block" overflow="scroll"><mml:mrow><mml:mtext>Loss</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mi>D</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>p</mml:mi><mml:mi>g</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mi>D</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>p</mml:mi><mml:mi>g</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math>
<label>[6]</label>
</disp-formula></sec><sec><title>Data augmentation</title><p>In order to improve robustness of model, we further adopted random online data augmentation during the training process as follows: (I) affine transformation: Linear transformation and translation are performed on the original image. It does not change the collinearity of each pixel in the original image and the proportion of the contour. (II) Gaussian noise and random contrast are added to sonographic images. (III) Image compression and random cropping: It simulates the differences in the locations of landmarks in different images. (IV) Stochastic combination: randomly select and combine the above methods. Examples of the original sonographic images and the enhanced sonographic images are shown in <xref rid="f3" ref-type="fig"><italic>Figure 3</italic></xref>.</p><fig position="float" id="f3" fig-type="figure"><label>Figure 3</label><caption><p>Data augmentation: (A) affine transformation; (B) Gaussian noise and random contrast; (C) image compression and random cropping; (D) stochastic combination.</p></caption><graphic xlink:href="qims-14-05-3707-f3" position="float"/></fig></sec><sec><title>Dataset construction</title><p>The dataset contained 2,958 images of non-standard plane and 1,479 images of standard plane during the training and testing. Among the standard plane images, there were 400 samples with labels for semantic segmentation.</p><p>In order to verify the effect of each module in our method, we used 1,079 unlabeled data in the pre-training step, 320 labeled data as the training set, and 80 labeled data as the test set for the landmark identification. All standard plane images and non-standard plane images are used in the evaluation of standard plane recognition. Data allocation is shown in <xref rid="t1" ref-type="table"><italic>Table 1</italic></xref>.</p><table-wrap position="float" id="t1"><label>Table 1</label><caption><title>Data split in the experiment</title></caption><table frame="hsides" rules="groups"><col width="17.57%" span="1"/><col width="19.47%" span="1"/><col width="19.47%" span="1"/><col width="19.48%" span="1"/><col width="24.01%" span="1"/><thead><tr><th valign="middle" align="left" scope="col" rowspan="1" colspan="1">Classification</th><th valign="middle" align="center" scope="col" rowspan="1" colspan="1">Dataset</th><th valign="middle" align="center" scope="col" rowspan="1" colspan="1">Pre-training</th><th valign="middle" align="center" scope="col" rowspan="1" colspan="1">Downstream tasks</th><th valign="middle" align="center" scope="col" rowspan="1" colspan="1">Standard plane recognition</th></tr></thead><tbody><tr><td rowspan="2" valign="top" align="left" scope="row" colspan="1">Standard plane</td><td valign="top" align="center" rowspan="1" colspan="1">Training set</td><td valign="top" align="center" rowspan="1" colspan="1">1,079 (unlabeled)</td><td valign="top" align="center" rowspan="1" colspan="1">320 (labeled)</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;</td></tr><tr><td valign="top" colspan="1" align="center" scope="row" rowspan="1">Test set</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;</td><td valign="top" align="center" rowspan="1" colspan="1">80 (labeled)</td><td valign="top" align="center" rowspan="1" colspan="1">1,479</td></tr><tr><td rowspan="2" valign="top" align="left" scope="row" colspan="1">Non-standard plane</td><td valign="top" align="center" rowspan="1" colspan="1">Training set</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;</td></tr><tr><td valign="top" colspan="1" align="center" scope="row" rowspan="1">Test set</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;</td><td valign="top" align="center" rowspan="1" colspan="1">2,958</td></tr><tr><td valign="top" align="left" scope="row" rowspan="1" colspan="1">Total</td><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1">1,079</td><td valign="top" align="center" rowspan="1" colspan="1">400</td><td valign="top" align="center" rowspan="1" colspan="1">4,437</td></tr></tbody></table></table-wrap><p>The annotated data contained pixel-level semantic landmark information of anatomical structures. All images and landmarks were subject to strict quality control to ensure the authenticity and reliability of model training. The diagram of anatomical structure annotation is shown in <xref rid="f4" ref-type="fig"><italic>Figure 4</italic></xref>. In terms of Graf&#x02019;s method, a sonographic image should be decided as a standard plane by the principle that 3 landmarks of anatomical structure should appear simultaneously, including the lower limb of the bony ilium in the depth of the acetabular fossa, the mid portion of the acetabular roof, and the acetabular labrum.</p><fig position="float" id="f4" fig-type="figure"><label>Figure 4</label><caption><p>Schematic diagram of anatomical structure of infant acetabulum. boneConn, the junction of cartilage and bone; cartilage, hyaline cartilage preformed acetabular roof; femoralHead, femoral head; jointCap, joint capsule; labrum, acetabular labrum; ilium&#x00026;lowerIlium, bony part of acetabular roof; synovium, synovial fold.</p></caption><graphic xlink:href="qims-14-05-3707-f4" position="float"/></fig></sec><sec><title>Training</title><p>We used ResNeXt-FPN-50 as the backbone network in the experiment (<xref rid="r12" ref-type="bibr">12</xref>). The model was implemented via the PyTorch framework. First, unlabeled data were used to perform comparative-learning-based pre-training on the backbone network on a server with 4 NVIDIA 3080Ti graphics cards, and 300 epochs were trained with a learning rate of 0.001. Then, semantic segmentation model and landmark identification were trained for 100 epochs with a learning rate of 0.001. The entire training process took 10 hours.</p></sec><sec><title>Evaluation metrics</title><p>In this study, we evaluated the effect of contrastive learning and compare our method (&#x0201c;Ori&#x0201d;) with other methods, including fully convolutional network (FCN) (<xref rid="r17" ref-type="bibr">17</xref>), Unet (<xref rid="r18" ref-type="bibr">18</xref>), and deeplabv3 (<xref rid="r19" ref-type="bibr">19</xref>). In this experiment, the following metrics were selected:</p><list list-type="roman-upper" id="L2"><list-item><p>Dice similarity coefficient (DSC): it is used to measure the similarity of 2 sets, with a range of 0 to 1. The larger the value is, the more similar the 2 sets are. It is often used to calculate the similarity of closed regions.<disp-formula id="e7"><mml:math id="m35" display="block" overflow="scroll"><mml:mrow><mml:mtext>DSC</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi>X</mml:mi><mml:mo>&#x02229;</mml:mo><mml:mi>Y</mml:mi></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mi>X</mml:mi><mml:mo>|</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mo>|</mml:mo><mml:mi>Y</mml:mi><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math><label>[7]</label></disp-formula>
</p><p>where <inline-formula><mml:math id="m36" overflow="scroll"><mml:mi>X</mml:mi></mml:math></inline-formula> represents the predicted point set, and <inline-formula><mml:math id="m37" overflow="scroll"><mml:mrow><mml:mi>Y</mml:mi><mml:mtext>&#x000a0;</mml:mtext></mml:mrow></mml:math></inline-formula> represents the labeled point set.</p></list-item><list-item><p>Hausdorff distance (HD): it is sensitive to the segmented boundaries, used to measure the distance between 2 edge point sets and reflect the similarity between 2 contours.<disp-formula id="e8"><mml:math id="m38" display="block" overflow="scroll"><mml:mrow><mml:mi>h</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>B</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>max</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>min</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math><label>[8]</label></disp-formula>
<disp-formula id="e9"><mml:math id="m39" display="block" overflow="scroll"><mml:mrow><mml:mtext>HD</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>B</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>max</mml:mi><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mi>h</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>B</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>h</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>B</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math><label>[9]</label></disp-formula>
</p><p>where <inline-formula><mml:math id="m40" overflow="scroll"><mml:mrow><mml:mi>d</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mo>.</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the distance between 2 points; <inline-formula><mml:math id="m41" overflow="scroll"><mml:mi>A</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="m42" overflow="scroll"><mml:mi>B</mml:mi></mml:math></inline-formula> are the contour point sets; <inline-formula><mml:math id="m43" overflow="scroll"><mml:mi>a</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="m44" overflow="scroll"><mml:mi>b</mml:mi></mml:math></inline-formula> are the points in the contour point sets in pixel.</p></list-item><list-item><p>In addition, the metrics, accuracy, precision, and recall are used to evaluate the performance of standard plane recognition.</p></list-item></list></sec></sec><sec sec-type="results"><title>Results</title><p>The comparison of landmark identification between different models on DSC and HD is shown in <xref rid="t2" ref-type="table"><italic>Table 2</italic></xref> and <xref rid="t3" ref-type="table"><italic>Table 3</italic></xref><italic>,</italic> respectively. The method with contrastive learning, denoted as &#x0201c;+CL&#x0201d;, had a mean DSC of 0.7873 and a mean HD of 5.0102 in landmarks identification, whereas the method without contrastive learning had a mean DSC of 0.7734 and a mean HD of 6.1586. These values were obtained from all tested images (80 samples). The average performance of the model pretrained based on the contrastive learning was better, as shown in <xref rid="f5" ref-type="fig"><italic>Figure 5</italic></xref>. In <xref rid="f6" ref-type="fig"><italic>Figure 6</italic></xref><italic>,</italic> taking the bony part of acetabular roof as an example, the feature extraction networks based on contrastive learning could better extract features and ultimately perform better on semantic segmentation of landmarks.</p><table-wrap position="float" id="t2"><label>Table 2</label><caption><title>Comparison of landmark identification between different network models on DSC</title></caption><table frame="hsides" rules="groups"><col width="9.24%" span="1"/><col width="10.34%" span="1"/><col width="9.57%" span="1"/><col width="12.13%" span="1"/><col width="8.3%" span="1"/><col width="10.06%" span="1"/><col width="8.18%" span="1"/><col width="11.56%" span="1"/><col width="11.13%" span="1"/><col width="9.49%" span="1"/><thead><tr><th valign="middle" align="left" scope="col" rowspan="1" colspan="1">Method</th><th valign="middle" align="center" scope="col" rowspan="1" colspan="1">BoneConn</th><th valign="middle" align="center" scope="col" rowspan="1" colspan="1">Cartilage</th><th valign="middle" align="center" scope="col" rowspan="1" colspan="1">FemoralHead</th><th valign="middle" align="center" scope="col" rowspan="1" colspan="1">Ilium</th><th valign="middle" align="center" scope="col" rowspan="1" colspan="1">JointCap</th><th valign="middle" align="center" scope="col" rowspan="1" colspan="1">Labrum</th><th valign="middle" align="center" scope="col" rowspan="1" colspan="1">LowerIlium</th><th valign="middle" align="center" scope="col" rowspan="1" colspan="1">Synovium</th><th valign="middle" align="center" scope="col" rowspan="1" colspan="1">Avg</th></tr></thead><tbody><tr><td valign="top" align="left" scope="row" rowspan="1" colspan="1">Unet</td><td valign="top" align="center" rowspan="1" colspan="1">0.6851</td><td valign="top" align="center" rowspan="1" colspan="1">0.0000</td><td valign="top" align="center" rowspan="1" colspan="1">0.8761</td><td valign="top" align="center" rowspan="1" colspan="1">0.7519</td><td valign="top" align="center" rowspan="1" colspan="1">0.1654</td><td valign="top" align="center" rowspan="1" colspan="1">0.5696</td><td valign="top" align="center" rowspan="1" colspan="1">0.3792</td><td valign="top" align="center" rowspan="1" colspan="1">0.5860</td><td valign="top" align="center" rowspan="1" colspan="1">0.5016</td></tr><tr><td valign="top" align="left" scope="row" rowspan="1" colspan="1">FCN</td><td valign="top" align="center" rowspan="1" colspan="1">0.7175</td><td valign="top" align="center" rowspan="1" colspan="1">0.0647</td><td valign="top" align="center" rowspan="1" colspan="1">0.8976</td><td valign="top" align="center" rowspan="1" colspan="1">0.6802</td><td valign="top" align="center" rowspan="1" colspan="1">0.5711</td><td valign="top" align="center" rowspan="1" colspan="1">0.7356</td><td valign="top" align="center" rowspan="1" colspan="1">0.5382</td><td valign="top" align="center" rowspan="1" colspan="1">0.7059</td><td valign="top" align="center" rowspan="1" colspan="1">0.6138</td></tr><tr><td valign="top" align="left" scope="row" rowspan="1" colspan="1">Deeplabv3</td><td valign="top" align="center" rowspan="1" colspan="1">0.7431</td><td valign="top" align="center" rowspan="1" colspan="1">0.3077</td><td valign="top" align="center" rowspan="1" colspan="1">0.8995</td><td valign="top" align="center" rowspan="1" colspan="1">0.7586</td><td valign="top" align="center" rowspan="1" colspan="1">0.7045</td><td valign="top" align="center" rowspan="1" colspan="1">0.7604</td><td valign="top" align="center" rowspan="1" colspan="1">0.6951</td><td valign="top" align="center" rowspan="1" colspan="1">0.8143</td><td valign="top" align="center" rowspan="1" colspan="1">0.7104</td></tr><tr><td valign="top" align="left" scope="row" rowspan="1" colspan="1">Ori</td><td valign="top" align="center" rowspan="1" colspan="1">0.8086</td><td valign="top" align="center" rowspan="1" colspan="1">0.5593</td><td valign="top" align="center" rowspan="1" colspan="1">0.8602</td><td valign="top" align="center" rowspan="1" colspan="1">0.8701</td><td valign="top" align="center" rowspan="1" colspan="1">0.7382</td><td valign="top" align="center" rowspan="1" colspan="1">0.7652</td><td valign="top" align="center" rowspan="1" colspan="1">0.8192</td><td valign="top" align="center" rowspan="1" colspan="1">0.7663</td><td valign="top" align="center" rowspan="1" colspan="1">0.7734</td></tr><tr><td valign="top" align="left" scope="row" rowspan="1" colspan="1">+CL</td><td valign="top" align="center" rowspan="1" colspan="1">0.8636</td><td valign="top" align="center" rowspan="1" colspan="1">0.5653</td><td valign="top" align="center" rowspan="1" colspan="1">0.8738</td><td valign="top" align="center" rowspan="1" colspan="1">0.8854</td><td valign="top" align="center" rowspan="1" colspan="1">0.7269</td><td valign="top" align="center" rowspan="1" colspan="1">0.8141</td><td valign="top" align="center" rowspan="1" colspan="1">0.7928</td><td valign="top" align="center" rowspan="1" colspan="1">0.7765</td><td valign="top" align="center" rowspan="1" colspan="1">0.7873</td></tr></tbody></table><table-wrap-foot><p>DSC, Dice similarity coefficient; BoneConn, the junction of cartilage and bone; cartilage, hyaline cartilage preformed acetabular roof; FemoralHead, femoral head; JointCap, joint capsule; Labrum, acetabular labrum; Ilium&#x00026;lowerIlium, bony part of acetabular roof; Synovium, synovial fold; Avg, average; Unet, the U-net model; FCN, fully convolutional network; Deeplabv3, the Deeplab v3 model; Ori, the original model; +CL,&#x000a0;the original model with contrastive learning.</p></table-wrap-foot></table-wrap><table-wrap position="float" id="t3"><label>Table 3</label><caption><title>Comparison of landmark identification between different network models on HD</title></caption><table frame="hsides" rules="groups"><col width="11.97%" span="1"/><col width="9.74%" span="1"/><col width="10.1%" span="1"/><col width="12.4%" span="1"/><col width="8.52%" span="1"/><col width="9.44%" span="1"/><col width="8.77%" span="1"/><col width="10.91%" span="1"/><col width="9.38%" span="1"/><col width="8.77%" span="1"/><thead><tr><th valign="middle" align="left" scope="col" rowspan="1" colspan="1">Method</th><th valign="middle" align="center" scope="col" rowspan="1" colspan="1">BoneConn</th><th valign="middle" align="center" scope="col" rowspan="1" colspan="1">Cartilage</th><th valign="middle" align="center" scope="col" rowspan="1" colspan="1">FemoralHead</th><th valign="middle" align="center" scope="col" rowspan="1" colspan="1">Ilium</th><th valign="middle" align="center" scope="col" rowspan="1" colspan="1">JointCap</th><th valign="middle" align="center" scope="col" rowspan="1" colspan="1">Labrum</th><th valign="middle" align="center" scope="col" rowspan="1" colspan="1">LowerIlium</th><th valign="middle" align="center" scope="col" rowspan="1" colspan="1">Synovium</th><th valign="middle" align="center" scope="col" rowspan="1" colspan="1">Avg</th></tr></thead><tbody><tr><td valign="top" align="left" scope="row" rowspan="1" colspan="1">Unet</td><td valign="top" align="center" rowspan="1" colspan="1">15.7367</td><td valign="top" align="center" rowspan="1" colspan="1">142.4057</td><td valign="top" align="center" rowspan="1" colspan="1">11.8173</td><td valign="top" align="center" rowspan="1" colspan="1">17.9590</td><td valign="top" align="center" rowspan="1" colspan="1">14.1463</td><td valign="top" align="center" rowspan="1" colspan="1">9.7828</td><td valign="top" align="center" rowspan="1" colspan="1">20.0939</td><td valign="top" align="center" rowspan="1" colspan="1">16.3002</td><td valign="top" align="center" rowspan="1" colspan="1">31.0302</td></tr><tr><td valign="top" align="left" scope="row" rowspan="1" colspan="1">FCN</td><td valign="top" align="center" rowspan="1" colspan="1">6.6909</td><td valign="top" align="center" rowspan="1" colspan="1">84.3727</td><td valign="top" align="center" rowspan="1" colspan="1">3.4329</td><td valign="top" align="center" rowspan="1" colspan="1">7.3286</td><td valign="top" align="center" rowspan="1" colspan="1">3.9515</td><td valign="top" align="center" rowspan="1" colspan="1">3.3489</td><td valign="top" align="center" rowspan="1" colspan="1">8.9629</td><td valign="top" align="center" rowspan="1" colspan="1">2.8928</td><td valign="top" align="center" rowspan="1" colspan="1">15.1227</td></tr><tr><td valign="top" align="left" scope="row" rowspan="1" colspan="1">Deeplabv3</td><td valign="top" align="center" rowspan="1" colspan="1">4.0278</td><td valign="top" align="center" rowspan="1" colspan="1">16.7851</td><td valign="top" align="center" rowspan="1" colspan="1">3.4537</td><td valign="top" align="center" rowspan="1" colspan="1">8.2089</td><td valign="top" align="center" rowspan="1" colspan="1">3.0556</td><td valign="top" align="center" rowspan="1" colspan="1">2.8471</td><td valign="top" align="center" rowspan="1" colspan="1">7.4389</td><td valign="top" align="center" rowspan="1" colspan="1">2.6112</td><td valign="top" align="center" rowspan="1" colspan="1">6.0535</td></tr><tr><td valign="top" align="left" scope="row" rowspan="1" colspan="1">Ori</td><td valign="top" align="center" rowspan="1" colspan="1">7.1437</td><td valign="top" align="center" rowspan="1" colspan="1">9.6309</td><td valign="top" align="center" rowspan="1" colspan="1">8.1311</td><td valign="top" align="center" rowspan="1" colspan="1">4.6308</td><td valign="top" align="center" rowspan="1" colspan="1">3.1921</td><td valign="top" align="center" rowspan="1" colspan="1">3.4713</td><td valign="top" align="center" rowspan="1" colspan="1">5.0061</td><td valign="top" align="center" rowspan="1" colspan="1">8.0626</td><td valign="top" align="center" rowspan="1" colspan="1">6.1586</td></tr><tr><td valign="top" align="left" scope="row" rowspan="1" colspan="1">+CL</td><td valign="top" align="center" rowspan="1" colspan="1">5.0624</td><td valign="top" align="center" rowspan="1" colspan="1">5.6635</td><td valign="top" align="center" rowspan="1" colspan="1">8.3374</td><td valign="top" align="center" rowspan="1" colspan="1">6.5300</td><td valign="top" align="center" rowspan="1" colspan="1">5.9187</td><td valign="top" align="center" rowspan="1" colspan="1">3.0374</td><td valign="top" align="center" rowspan="1" colspan="1">2.3865</td><td valign="top" align="center" rowspan="1" colspan="1">3.1459</td><td valign="top" align="center" rowspan="1" colspan="1">5.0102</td></tr></tbody></table><table-wrap-foot><p>HD, Hausdorff distance; BoneConn, the junction of cartilage and bone; cartilage, hyaline cartilage preformed acetabular roof; FemoralHead, femoral head; JointCap, joint capsule; Labrum, acetabular labrum; Ilium&#x00026;lowerIlium, bony part of acetabular roof; Synovium, synovial fold; Avg, average; Unet, the U-net model; FCN, fully convolutional network; Deeplabv3, the Deeplab v3 model; Ori, the original model; +CL,&#x000a0;the original model with contrastive learning.</p></table-wrap-foot></table-wrap><fig position="float" id="f5" fig-type="figure"><label>Figure 5</label><caption><p>Comparison between various models and our proposed method. +CL, the original model with contrastive learning; Ori, the original model; Unet, the U-net model; FCN, fully convolutional network; Deeplabv3, the Deeplab v3 model.</p></caption><graphic xlink:href="qims-14-05-3707-f5" position="float"/></fig><fig position="float" id="f6" fig-type="figure"><label>Figure 6</label><caption><p>Comparison of semantic segmentation about effects of contrastive learning. +CL, the original model with contrastive learning; Ori, the original model.</p></caption><graphic xlink:href="qims-14-05-3707-f6" position="float"/></fig><p>Furthermore, the results of standard plane classification on the model &#x0201c;Ori+CL&#x0201d; were as follows: accuracy of 95.4%, precision of 91.64%, and recall of 94.86%, compared to the model without contrastive learning which had an accuracy of 93.66%, precision of 84.32%, and recall of 91.68%. A big difference on the precision value was apparent because our method via contrastive learning could improve the capacity of identification of image details. The corresponding confusion matrix is as shown in <xref rid="t4" ref-type="table"><italic>Table 4</italic></xref> and <xref rid="t5" ref-type="table"><italic>Table 5</italic></xref>. The receiver operating characteristic (ROC) curve of the model &#x0201c;Ori+CL&#x0201d; is illustrated in <xref rid="f7" ref-type="fig"><italic>Figure 7</italic></xref>. The corresponding area under the curve (AUC) was 0.982.</p><table-wrap position="float" id="t4"><label>Table 4</label><caption><title>Confusion matrix of standard plane recognition with contrastive learning</title></caption><table frame="hsides" rules="groups"><col width="35.92%" span="1"/><col width="30.37%" span="1"/><col width="33.71%" span="1"/><thead><tr><th rowspan="2" valign="middle" align="left" scope="col" style="border-bottom: solid 0.50pt" colspan="1">Actual value</th><th valign="middle" colspan="2" align="center" scope="colgroup" style="border-bottom: solid 0.50pt" rowspan="1">Predicted value</th></tr><tr><th valign="middle" colspan="1" align="center" scope="colgroup" style="border-top: solid 0.50pt" rowspan="1">Positive</th><th valign="middle" align="center" scope="col" style="border-top: solid 0.50pt" rowspan="1" colspan="1">Negative</th></tr></thead><tbody><tr><td valign="top" align="left" scope="row" rowspan="1" colspan="1">Positive</td><td valign="top" align="center" rowspan="1" colspan="1">1,403</td><td valign="top" align="center" rowspan="1" colspan="1">76</td></tr><tr><td valign="top" align="left" scope="row" rowspan="1" colspan="1">Negative</td><td valign="top" align="center" rowspan="1" colspan="1">128</td><td valign="top" align="center" rowspan="1" colspan="1">2,803</td></tr></tbody></table></table-wrap><table-wrap position="float" id="t5"><label>Table 5</label><caption><title>Confusion matrix of standard plane recognition without contrastive learning</title></caption><table frame="hsides" rules="groups"><col width="36.12%" span="1"/><col width="30.55%" span="1"/><col width="33.33%" span="1"/><thead><tr><th rowspan="2" valign="middle" align="left" scope="col" style="border-bottom: solid 0.50pt" colspan="1">Actual value</th><th valign="middle" colspan="2" align="center" scope="colgroup" style="border-bottom: solid 0.50pt" rowspan="1">Predicted value</th></tr><tr><th valign="middle" colspan="1" align="center" scope="colgroup" style="border-top: solid 0.50pt" rowspan="1">Positive</th><th valign="middle" align="center" scope="col" style="border-top: solid 0.50pt" rowspan="1" colspan="1">Negative</th></tr></thead><tbody><tr><td valign="top" align="left" scope="row" rowspan="1" colspan="1">Positive</td><td valign="top" align="center" rowspan="1" colspan="1">1,356</td><td valign="top" align="center" rowspan="1" colspan="1">123</td></tr><tr><td valign="top" align="left" scope="row" rowspan="1" colspan="1">Negative</td><td valign="top" align="center" rowspan="1" colspan="1">252</td><td valign="top" align="center" rowspan="1" colspan="1">2,706</td></tr></tbody></table></table-wrap><fig position="float" id="f7" fig-type="figure"><label>Figure 7</label><caption><p>The ROC curve of our proposed model. ROC, receiver operating characteristic; AUC, area under the curve.</p></caption><graphic xlink:href="qims-14-05-3707-f7" position="float"/></fig><p>By means of 1,500 ultrasound images (500 positive samples and 1,000 negative samples), we implemented comparison experiments between the proposed algorithm and the professional manual operators in terms of false positive ratio (FPR) and false negative ratio (FNR), which were expressed as follows.</p><disp-formula id="e10">
<mml:math id="m45" display="block" overflow="scroll"><mml:mrow><mml:mtext>FPR</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math>
<label>[10]</label>
</disp-formula><disp-formula id="e11">
<mml:math id="m46" display="block" overflow="scroll"><mml:mrow><mml:mtext>FNR</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math>
<label>[11]</label>
</disp-formula><p>where FP denotes the number of false positive samples, TN means the number of true negative samples, FN represents the number of false negative samples, and TP indicates the number of true positive samples.</p><p>The concrete comparison results are provided in <xref rid="t6" ref-type="table"><italic>Table 6</italic></xref>. From this table, we can see that the performance of our algorithm was better than that of manual operators. Specifically, the FPR of the proposed method decreased by 39.4%, and the FNR of the manual operators was 9.4%, which is far higher than that of our algorithm (5%). Obviously, the established model can assist doctors in diagnosis to improve medical efficiency.</p><table-wrap position="float" id="t6"><label>Table 6</label><caption><title>Comparison between our algorithm and manual operators</title></caption><table frame="hsides" rules="groups"><col width="45.24%" span="1"/><col width="27.38%" span="1"/><col width="27.38%" span="1"/><thead><tr><th valign="middle" align="left" scope="col" rowspan="1" colspan="1">Methods</th><th valign="middle" align="center" scope="col" rowspan="1" colspan="1">FPR (%)</th><th valign="middle" align="center" scope="col" rowspan="1" colspan="1">FNR (%)</th></tr></thead><tbody><tr><td valign="top" align="left" scope="row" rowspan="1" colspan="1">Ours</td><td valign="top" align="center" rowspan="1" colspan="1">4.3</td><td valign="top" align="center" rowspan="1" colspan="1">5</td></tr><tr><td valign="top" align="left" scope="row" rowspan="1" colspan="1">Manual operators</td><td valign="top" align="center" rowspan="1" colspan="1">7.1</td><td valign="top" align="center" rowspan="1" colspan="1">9.4</td></tr></tbody></table><table-wrap-foot><p>FPR, false positive ratio; FNR, false negative ratio.</p></table-wrap-foot></table-wrap></sec><sec sec-type="discussion"><title>Discussion</title><p>In recent years, deep neural networks have been gradually explored in hip ultrasound screening of infants, due to their excellent image feature extraction capabilities. In 2016, Golan <italic>et al.</italic> introduced a deep convolutional neural network (CNN) for the first time (<xref rid="r5" ref-type="bibr">5</xref>). They used CNN to divide the flat iliac bone and lower limbs to automatically calculate the Graf&#x02019;s alpha angle in infant sonographic images. In 2017, Hareendranathan <italic>et al.</italic> proposed a method to automatically segment the acetabulum bone and derive geometric indices of hip dysplasia (<xref rid="r6" ref-type="bibr">6</xref>). In 2018, Zhang <italic>et al.</italic> introduced region of interest (ROI) into a fully CNN to improve the recognition accuracy of acetabulum (<xref rid="r7" ref-type="bibr">7</xref>). In 2019, Sezer <italic>et al.</italic> input the segmented image segments into CNN to directly diagnose the patient&#x02019;s hip development (<xref rid="r8" ref-type="bibr">8</xref>). In the same year, El-Hariri <italic>et al.</italic> proposed a feature-based deep learning method that utilizes the multi-channel input of U-Net to achieve iliac segmentation (<xref rid="r9" ref-type="bibr">9</xref>). The methods used in these studies require a large amount of preliminary data annotation work in practical applications. This study applied contrastive learning to DDH ultrasound examination. It was found that our pre-training method can effectively moderate the demands of data annotation, and improve the accuracy of ultrasound detection, especially when there are multiple data sources. These findings are of great significance to guide clinical treatment.</p><p>In this paper, we propose a semi-supervised learning framework for assisting DDH diagnosis, by using data enhancement and contrastive learning to deal with the problem of distribution shift due to ultrasound data from various objects and machines. A backbone network based on the FPN structure is used to effectively identify landmarks at different scales. The performance of detection is improved in terms of accuracy and robustness. The contrastive learning method in this article has a mean DSC of 0.7873 and a mean HD of 5.0102 in landmark identification. The mean DSC without the contrast learning method is 0.7734 and the mean HD is 6.1586. As a reference, the metrics of FCN, Unet, and deeplabv3 are worse than our proposed method. By using this method, the accuracy of standard plane recognition is 95.4%, the precision is 91.64%, and the recall is 94.86%.</p><p>This study shows that the contrastive self-supervised learning method and the FPN structure can effectively extract the features of sonographic images and improve the performance of analysis and measurement. The pre-training method can effectively reduce the need for data annotation and lower the threshold for using deep learning in DDH inspection. This solution enables automated measurement and evaluation of sonographic images. It can also promote large-scale DDH screening, help patients detect and treat early-stage diseases in time, and improve prognosis. This model can assist doctors in diagnosis via a visible interpretation and automated measurement, improve medical efficiency, and has important clinical significance. At present, our method is in the process of clinical verifications.</p><p>This study had some limitations: (I) in terms of standard plane acquisition, this model can only provide simple prompts based on the identified landmarks. Further work is needed on how to automatically obtain standard images from the examination process to better serve the clinic. (II) In terms of downstream task, the training data used by this model only includes type I and type II hip data. More data on dislocated hips need to be collected and annotated for further research. (III) Artificial intelligence (AI)-based methods can only provide assistance to doctors. In actual clinical work, doctors can obtain diagnosis data from various patients to improve diagnostic accuracy.</p></sec><sec sec-type="conclusions"><title>Conclusions</title><p>We have proposed a semi-supervised deep learning method following Graf&#x02019;s principle, which can better utilize past ultrasound examination data to more accurately identify the landmarks of infant hips, thereby improving the efficiency of diagnosis of infant hip dysplasia. Early screening of DDH by auxiliary doctors is of great significance and has broad clinical application prospects.</p></sec><sec sec-type="supplementary-material"><title>Supplementary</title><supplementary-material position="float" content-type="local-data"><p>The article&#x02019;s supplementary files as</p></supplementary-material><supplementary-material position="anchor" id="su1" content-type="local-data"><media xlink:href="qims-14-05-3707-coif.pdf" id="d66e1840" position="anchor"><object-id pub-id-type="doi">10.21037/qims-23-1384</object-id></media></supplementary-material></sec></body><back><ack><title>Acknowledgments</title><p><italic>Funding:</italic> This study was supported by <funding-source rid="sp1">the Research Foundation of Jiangsu Provincial Commission of Health</funding-source> (<award-id rid="sp1">No. ZD2022049</award-id>).</p></ack><fn-group><fn fn-type="COI-statement"><p><italic>Conflicts of Interest:</italic> All authors have completed the ICMJE uniform disclosure form (available at <ext-link xlink:href="https://qims.amegroups.com/article/view/10.21037/qims-23-1384/coif" ext-link-type="uri">https://qims.amegroups.com/article/view/10.21037/qims-23-1384/coif</ext-link>). All authors report that this study was supported by the Research Foundation of Jiangsu Provincial Commission of Health (No. ZD2022049) from Jiangsu Provincial Commission of Health, China. The authors have no other conflicts of interest to declare.</p></fn></fn-group><notes><p content-type="note added in proof"><italic>Ethical Statement:</italic> The authors are accountable for all aspects of the work in ensuring that questions related to the accuracy or integrity of any part of the work are appropriately investigated and resolved. The study was approved by the Research Ethics Committee of Southeast University and Yangzhou Maternal and Child Health Care Service Centre. The requirement for individual consent for this retrospective analysis was waived. The study was conducted in accordance with the Declaration of Helsinki (as revised in 2013).</p></notes><ref-list><title>References</title><ref id="r1"><label>1</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gulati</surname><given-names>V</given-names></name><name><surname>Eseonu</surname><given-names>K</given-names></name><name><surname>Sayani</surname><given-names>J</given-names></name><name><surname>Ismail</surname><given-names>N</given-names></name><name><surname>Uzoigwe</surname><given-names>C</given-names></name><name><surname>Choudhury</surname><given-names>MZ</given-names></name><name><surname>Gulati</surname><given-names>P</given-names></name><name><surname>Aqil</surname><given-names>A</given-names></name><name><surname>Tibrewal</surname><given-names>S</given-names></name></person-group>. <article-title>Developmental dysplasia of the hip in the newborn: A systematic review.</article-title>
<source>World J Orthop</source>
<year>2013</year>;<volume>4</volume>:<fpage>32</fpage>-<lpage>41</lpage>. <pub-id pub-id-type="doi">10.5312/wjo.v4.i2.32</pub-id><?supplied-pmid 23610749?><pub-id pub-id-type="pmid">23610749</pub-id>
</mixed-citation></ref><ref id="r2"><label>2</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nicholson</surname><given-names>A</given-names></name><name><surname>Dunne</surname><given-names>K</given-names></name><name><surname>Taaffe</surname><given-names>S</given-names></name><name><surname>Sheikh</surname><given-names>Y</given-names></name><name><surname>Murphy</surname><given-names>J</given-names></name></person-group>. <article-title>Developmental dysplasia of the hip in infants and children.</article-title>
<source>BMJ</source>
<year>2023</year>;<volume>383</volume>:<elocation-id>e074507</elocation-id>. <pub-id pub-id-type="doi">10.1136/bmj-2022-074507</pub-id></mixed-citation></ref><ref id="r3"><label>3</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yan</surname><given-names>H</given-names></name><name><surname>Du</surname><given-names>L</given-names></name><name><surname>Liu</surname><given-names>J</given-names></name><name><surname>Yang</surname><given-names>X</given-names></name><name><surname>Luo</surname><given-names>Y</given-names></name></person-group>. <article-title>Developmental retardation of femoral head size and femoral head ossification in mild and severe developmental dysplasia of the hip in infants: a preliminary cross-sectional study based on ultrasound images.</article-title>
<source>Quant Imaging Med Surg</source>
<year>2023</year>;<volume>13</volume>:<fpage>185</fpage>-<lpage>95</lpage>. <pub-id pub-id-type="doi">10.21037/qims-22-513</pub-id><?supplied-pmid 36620134?><pub-id pub-id-type="pmid">36620134</pub-id>
</mixed-citation></ref><ref id="r4"><label>4</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O'Beirne</surname><given-names>JG</given-names></name><name><surname>Chlapoutakis</surname><given-names>K</given-names></name><name><surname>Alshryda</surname><given-names>S</given-names></name><name><surname>Aydingoz</surname><given-names>U</given-names></name><name><surname>Baumann</surname><given-names>T</given-names></name><name><surname>Casini</surname><given-names>C</given-names></name><etal/></person-group>
<article-title>International Interdisciplinary Consensus Meeting on the Evaluation of Developmental Dysplasia of the Hip.</article-title>
<source>Ultraschall Med</source>
<year>2019</year>;<volume>40</volume>:<fpage>454</fpage>-<lpage>64</lpage>. <pub-id pub-id-type="doi">10.1055/a-0924-5491</pub-id><?supplied-pmid 31195424?><pub-id pub-id-type="pmid">31195424</pub-id>
</mixed-citation></ref><ref id="r5"><label>5</label><mixed-citation publication-type="confproc">Golan D, Donner Y, Mansi C, Jaremko J, Ramachandran M. Fully automating Graf&#x02019;s method for DDH diagnosis using deep convolutional neural networks. International Workshop on Large-Scale Annotation of Biomedical Data and Expert Label Synthesis International Workshop on Deep Learning in Medical Image Analysis 2016:130-41.</mixed-citation></ref><ref id="r6"><label>6</label><mixed-citation publication-type="confproc">Hareendranathan AR, Zonoobi D, Mabee M, Cobzas D, Punithakumar K, Noga M, Jaremko JL. 2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017), Melbourne, VIC, Australia, 2017:982-5.</mixed-citation></ref><ref id="r7"><label>7</label><mixed-citation publication-type="confproc">Zhang Z, Tang M, Cobzas D, Zonoobi D, Jagersand M, Jaremko JL. End-to-end detection-segmentation network with ROI convolution. 2018 IEEE 15th International Symposium on Biomedical Imaging (ISBI 2018), Washington, DC, USA, 2018:1509-12.</mixed-citation></ref><ref id="r8"><label>8</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sezer</surname><given-names>A</given-names></name><name><surname>Sezer</surname><given-names>HB</given-names></name></person-group>. <article-title>Deep Convolutional Neural Network-Based Automatic Classification of Neonatal Hip Ultrasound Images: A Novel Data Augmentation Approach with Speckle Noise Reduction.</article-title>
<source>Ultrasound Med Biol</source>
<year>2020</year>;<volume>46</volume>:<fpage>735</fpage>-<lpage>49</lpage>. <pub-id pub-id-type="doi">10.1016/j.ultrasmedbio.2019.09.018</pub-id><?supplied-pmid 31882168?><pub-id pub-id-type="pmid">31882168</pub-id>
</mixed-citation></ref><ref id="r9"><label>9</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>El-Hariri</surname><given-names>H</given-names></name><name><surname>Mulpuri</surname><given-names>K</given-names></name><name><surname>Hodgson</surname><given-names>A</given-names></name><name><surname>Garbi</surname><given-names>R</given-names></name></person-group>. <article-title>Comparative evaluation of hand-engineered and deep-learned features for neonatal hip bone segmentation in ultrasound. Medical Image Computing and Computer Assisted Intervention &#x02013; MICCAI 2019. MICCAI 2019.</article-title>
<source>Lecture Notes in Computer Science</source>, <year>2019</year>:<fpage>12</fpage>-<lpage>20</lpage>. <pub-id pub-id-type="doi">10.1007/978-3-030-32245-8_2</pub-id></mixed-citation></ref><ref id="r10"><label>10</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hu</surname><given-names>X</given-names></name><name><surname>Wang</surname><given-names>L</given-names></name><name><surname>Yang</surname><given-names>X</given-names></name><name><surname>Zhou</surname><given-names>X</given-names></name><name><surname>Xue</surname><given-names>W</given-names></name><name><surname>Cao</surname><given-names>Y</given-names></name><name><surname>Liu</surname><given-names>S</given-names></name><name><surname>Huang</surname><given-names>Y</given-names></name><name><surname>Guo</surname><given-names>S</given-names></name><name><surname>Shang</surname><given-names>N</given-names></name><name><surname>Ni</surname><given-names>D</given-names></name><name><surname>Gu</surname><given-names>N.</given-names></name></person-group>
<article-title>Joint Landmark and Structure Learning for Automatic Evaluation of Developmental Dysplasia of the Hip.</article-title>
<source>IEEE J Biomed Health Inform</source>
<year>2022</year>;<volume>26</volume>:<fpage>345</fpage>-<lpage>58</lpage>. <pub-id pub-id-type="doi">10.1109/JBHI.2021.3087494</pub-id><?supplied-pmid 34101608?><pub-id pub-id-type="pmid">34101608</pub-id>
</mixed-citation></ref><ref id="r11"><label>11</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>X</given-names></name><name><surname>Zhang</surname><given-names>F</given-names></name><name><surname>Hou</surname><given-names>Z</given-names></name><name><surname>Mian</surname><given-names>L</given-names></name><name><surname>Wang</surname><given-names>Z</given-names></name><name><surname>Zhang</surname><given-names>J</given-names></name><name><surname>Tang</surname><given-names>J.</given-names></name></person-group>
<article-title>Self-supervised learning: generative or contrastive.</article-title>
<source>IEEE Transactions on Knowledge and Data Engineering</source>
<year>2023</year>;<volume>35</volume>:<fpage>857</fpage>-<lpage>76</lpage>.</mixed-citation></ref><ref id="r12"><label>12</label><mixed-citation publication-type="confproc">Lin TY, Doll&#x000e1;r P, Girshick R, He K, Hariharan B, Belongie S. Feature pyramid networks for object detection. 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Honolulu, HI, USA, 2017:2117-25.</mixed-citation></ref><ref id="r13"><label>13</label><mixed-citation publication-type="confproc">Chen T, Kornblith S, Norouzi M, Hinton G. A simple framework for contrastive learning of visual representations. ICML'20: Proceedings of the 37th International Conference on Machine Learning, 2020:1597-607.</mixed-citation></ref><ref id="r14"><label>14</label><mixed-citation publication-type="confproc">Caron M, Misra I, Mairal J, Goyal P, Bojanowski P, Joulin A. Unsupervised learning of visual features by contrasting cluster assignments. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada, 2020:9912-24.</mixed-citation></ref><ref id="r15"><label>15</label><mixed-citation publication-type="confproc">Grill JB, Strub F, Altch&#x000e9; F, Tallec C, Richemond P, Buchatskaya E, Doersch C, Pires BA, Guo ZD, Azar MG, Piot B, Kavukcuoglu K, Munos R, Valko M. Bootstrap your own latent: A new approach to self-supervised learning. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada, 2020:21271-84.</mixed-citation></ref><ref id="r16"><label>16</label><mixed-citation publication-type="confproc">Chen X, He K. Exploring simple siamese representation learning. 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Nashville, TN, USA, 2021:15750-8.</mixed-citation></ref><ref id="r17"><label>17</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shelhamer</surname><given-names>E</given-names></name><name><surname>Long</surname><given-names>J</given-names></name><name><surname>Darrell</surname><given-names>T.</given-names></name></person-group>
<article-title>Fully Convolutional Networks for Semantic Segmentation.</article-title>
<source>IEEE Trans Pattern Anal Mach Intell</source>
<year>2017</year>;<volume>39</volume>:<fpage>640</fpage>-<lpage>51</lpage>. <pub-id pub-id-type="doi">10.1109/TPAMI.2016.2572683</pub-id><?supplied-pmid 27244717?><pub-id pub-id-type="pmid">27244717</pub-id>
</mixed-citation></ref><ref id="r18"><label>18</label><mixed-citation publication-type="book">Ronneberger O, Fischer P, Brox T. U-net: Convolutional networks for biomedical image segmentation. In: Navab N, Hornegger J, Wells W, Frangi A. editors. Medical Image Computing and Computer-Assisted Intervention &#x02013; MICCAI 2015. MICCAI 2015. Lecture Notes in Computer Science, vol 9351. Springer, Cham, 2015:234-41.</mixed-citation></ref><ref id="r19"><label>19</label><mixed-citation publication-type="book">Chen LC, ZhuangbilityY, Papandreou G, Schroff F, Adam H. Encoder-decoder with atrous separable convolution for semantic image segmentation. In: Ferrari V, Hebert M, Sminchisescu C, Weiss Y. editors. Computer Vision &#x02013; ECCV 2018. ECCV 2018. Lecture Notes in Computer Science, vol 11211. Springer, Cham, 2018:801-18.</mixed-citation></ref></ref-list></back></article>