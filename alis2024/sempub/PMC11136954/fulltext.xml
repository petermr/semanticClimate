<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd"> 
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><?properties open_access?><?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?><?DTDIdentifier.IdentifierType public?><?SourceDTD.DTDName A++V2.4.dtd?><?SourceDTD.Version 2.4?><?ConverterInfo.XSLTName springer2nlmx2.xsl?><?ConverterInfo.Version 1?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sci Rep</journal-id><journal-id journal-id-type="iso-abbrev">Sci Rep</journal-id><journal-title-group><journal-title>Scientific Reports</journal-title></journal-title-group><issn pub-type="epub">2045-2322</issn><publisher><publisher-name>Nature Publishing Group UK</publisher-name><publisher-loc>London</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmcid">11136954</article-id><article-id pub-id-type="publisher-id">61981</article-id><article-id pub-id-type="doi">10.1038/s41598-024-61981-9</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>An accurate semantic segmentation model for bean seedlings and weeds identification based on improved ERFnet</article-title></title-group><contrib-group><contrib contrib-type="author" equal-contrib="yes"><name><surname>Gao</surname><given-names>Haozhang</given-names></name><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author" equal-contrib="yes"><name><surname>Qi</surname><given-names>Mingyang</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author"><name><surname>Du</surname><given-names>Baoxia</given-names></name><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author"><name><surname>Yang</surname><given-names>Shuang</given-names></name><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author"><name><surname>Li</surname><given-names>Han</given-names></name><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author"><name><surname>Wang</surname><given-names>Tete</given-names></name><xref ref-type="aff" rid="Aff3">3</xref></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Zhong</surname><given-names>Wenyu</given-names></name><address><email>zhongwenyu@jlnku.edu.cn</email></address><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Tang</surname><given-names>You</given-names></name><address><email>tangyou9000@163.com</email></address><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff2">2</xref></contrib><aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/04w5zb891</institution-id><institution-id institution-id-type="GRID">grid.507914.e</institution-id><institution>Electrical and Information Engineering College, </institution><institution>Jilin Agricultural Science and Technology University, </institution></institution-wrap>Jilin, 132101 China </aff><aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="GRID">grid.443416.0</institution-id><institution-id institution-id-type="ISNI">0000 0000 9865 0124</institution-id><institution>School of Information and Control Engineering, </institution><institution>Jilin Institute of Chemical Technology, </institution></institution-wrap>Jilin, 132022 China </aff><aff id="Aff3"><label>3</label>R &#x00026;D Department, Jilin Province Electric Innovation Information Technology Limited Company, Changchun, 130117 China </aff></contrib-group><pub-date pub-type="epub"><day>29</day><month>5</month><year>2024</year></pub-date><pub-date pub-type="pmc-release"><day>29</day><month>5</month><year>2024</year></pub-date><pub-date pub-type="collection"><year>2024</year></pub-date><volume>14</volume><elocation-id>12288</elocation-id><history><date date-type="received"><day>6</day><month>12</month><year>2023</year></date><date date-type="accepted"><day>13</day><month>5</month><year>2024</year></date></history><permissions><copyright-statement>&#x000a9; The Author(s) 2024</copyright-statement><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article&#x02019;s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article&#x02019;s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p></license></permissions><abstract id="Abs1"><p id="Par1">In agricultural production activities, the growth of crops always accompanies the competition of weeds for nutrients and sunlight. In order to mitigate the adverse effects of weeds on yield, we apply semantic segmentation techniques to differentiate between seedlings and weeds, leading to precision weeding. The proposed EPAnet employs a loss function coupled with Cross-entropy loss and Dice loss to enhance attention to feature information. A multi-Decoder cooperative module based on ERFnet is designed to enhance information transfer during feature mapping. The SimAM is introduced to enhance position recognition. DO-CONV is used to replace the traditional convolution Feature Pyramid Networks (FPN) connection layer to integrate feature information, improving the model&#x02019;s performance on leaf edge processing, and is named FDPN. Moreover, the Overall Accuracy has been improved by 0.65%, the mean Intersection over Union (mIoU) by 1.91%, and the Frequency-Weighted Intersection over Union (FWIoU) by 1.19%. Compared to other advanced methods, EPAnet demonstrates superior image segmentation results in complex natural environments with uneven lighting, leaf interference, and shadows.</p></abstract><kwd-group kwd-group-type="npg-subject"><title>Subject terms</title><kwd>Computer science</kwd><kwd>Information technology</kwd></kwd-group><funding-group><award-group><funding-source><institution>The Digital Agriculture: An Emerging Interdisciplinary Field in Jilin Province</institution></funding-source><award-id>2018730519</award-id><principal-award-recipient><name><surname>Zhong</surname><given-names>Wenyu</given-names></name></principal-award-recipient></award-group></funding-group><funding-group><award-group><funding-source><institution>The Jilin Province Science and Technology Development Program Project</institution></funding-source><award-id>YDZJ202201ZYTS692</award-id><principal-award-recipient><name><surname>Tang</surname><given-names>You</given-names></name></principal-award-recipient></award-group></funding-group><custom-meta-group><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#x000a9; Springer Nature Limited 2024</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1"><title>Introduction</title><p id="Par2">Green bean, scientifically known as Phaseolus vulgaris L., holds significant importance as a vegetable crop in numerous developing nations, serving as a valuable protein and nutrient source. According to the Food and Agriculture Organization (FAO) report<sup><xref ref-type="bibr" rid="CR1">1</xref></sup>, the global cultivation area dedicated to green beans amounts to 1,527,613 hectares, resulting in an impressive production of 21,720,588 tons. China occupies the leading position as the world&#x02019;s primary producer of green beans, with a substantial cultivated area spanning 635,385 hectares. Notably, China&#x02019;s green bean production stands at an astounding 17,031,702 tons<sup><xref ref-type="bibr" rid="CR1">1</xref></sup>. This underscores China&#x02019;s pivotal position as the foremost producer of green beans globally, emphasizing the widespread impact of this vegetable crop in meeting nutritional needs and supporting food security, particularly in developing countries. However, the final yield of beans is influenced by various growth factors and climate conditions<sup><xref ref-type="bibr" rid="CR2">2</xref></sup>, with the negative effects of weeds being particularly significant. And weeds not only compete for the soil fertility required during the growth of bean seedlings but also promote diseases that jeopardize the seedlings<sup><xref ref-type="bibr" rid="CR3">3</xref></sup>. Therefore, the agricultural sector urgently needs to accurately distinguish between crops and weeds to effectively address the weed problem. The conventional weeding techniques predominantly depend on pesticide application, which, besides its restricted efficacy, can result in the wastage of manpower and environmental contamination<sup><xref ref-type="bibr" rid="CR4">4</xref></sup>. As computer technology perpetually evolves, deep learning and computer vision methodologies have garnered extensive utilization. In pursuit of pollution-free weeding, researchers are required to pinpoint the best image segmentation techniques for distinguishing crops from weeds, enhance identification precision, assist robots in accurate weeding. Semantic segmentation of images remains a critical aspect of computer vision endeavors. This direction also stands as a key research topic for numerous computer scholars globally. The application scenarios for semantic segmentation are vast, including autonomous driving of vehicles<sup><xref ref-type="bibr" rid="CR5">5</xref>&#x02013;<xref ref-type="bibr" rid="CR7">7</xref></sup>, identification and recognition of medical images<sup><xref ref-type="bibr" rid="CR8">8</xref></sup>, urban planning<sup><xref ref-type="bibr" rid="CR3">3</xref></sup>, and smart agriculture<sup><xref ref-type="bibr" rid="CR9">9</xref></sup>, among others. By removing the fully connected modules, semantic segmentation successfully realizes the pixel-by-pixel of the fully convolutional networks. However, the performance of fully convolutional networks during the inference phase is not ideal. In subsequent work, by addressing the intrinsic association issues of semantic positions, the inference results were noticeably improved. Semantic segmentation models commonly adopt an encoder-decoder structure. The encoder primarily focuses on learning and refining image features, while the decoder performs pixel-level categorization of these features. To capture the semantics in images more deeply, the encoder typically consists of several convolutional layers. For instance, UNet is a classic encoder-decoder structured algorithm that employs a skip-connection strategy, effectively achieving feature fusion and thereby addressing potential information loss during the downsampling process<sup><xref ref-type="bibr" rid="CR10">10</xref></sup>. The downsampling of features plays a pivotal role in the model<sup><xref ref-type="bibr" rid="CR11">11</xref></sup>. However, considering computational costs, the resolution of feature maps is often reduced, leading to more abstract features extracted by the encoder. And the receptive field of the model also significantly expands<sup><xref ref-type="bibr" rid="CR12">12</xref></sup>. The attention mechanism module is frequently integrated with the Fully Convolutional Networks (FCN) framework in the architecture of semantic segmentation models<sup><xref ref-type="bibr" rid="CR13">13</xref>,<xref ref-type="bibr" rid="CR14">14</xref></sup>, and this has led to the derivation of various multi-level feature fusion model variants<sup><xref ref-type="bibr" rid="CR15">15</xref>,<xref ref-type="bibr" rid="CR16">16</xref></sup>. For example, Deeplabv3, as one of the Deeplab series models, has a more streamlined and efficient decoder module, which allows for more detailed feature extraction and simultaneously enhances semantic segmentation performance<sup><xref ref-type="bibr" rid="CR11">11</xref>,<xref ref-type="bibr" rid="CR17">17</xref></sup>. It introduces dilated convolution to expand the receptive field, while PSPNet employs parallel pooling at different scales to extract various types of features, thereby enhancing the model&#x02019;s segmentation results<sup><xref ref-type="bibr" rid="CR18">18</xref></sup>.</p><p id="Par3">Deep learning extensive application in image segmentation has become evident<sup><xref ref-type="bibr" rid="CR19">19</xref></sup>. The pace of deep learning has accelerated, starting from fully convolutional networks, but there&#x02019;s still substantial room for improvement in segmentation algorithms<sup><xref ref-type="bibr" rid="CR9">9</xref></sup>. Deep learning algorithms based on convolutional neural networks made significant progress in the field of plant phenomics, especially in the application of plant leaf segmentation<sup><xref ref-type="bibr" rid="CR20">20</xref></sup>. Mehdipour et al. proposed a simplified CNN model in the 2015 plant species identification competition<sup><xref ref-type="bibr" rid="CR21">21</xref></sup>. The core idea of this model is to learn weights through principal component analysis with mean removal, achieving over 90% plant leaf recognition rate in images. However, this method slightly falls short in recognizing leaf images with complex backgrounds. Daniel D. Morris proposed a multi-scale pyramid convolutional neural network for segmenting densely packed plant leaves, allowing for more precise delineation of leaf areas in images<sup><xref ref-type="bibr" rid="CR22">22</xref></sup>. Dmitry Kuznichov et al. introduced a method focused on preserving the geometric structural information of plant leaves to approximate real scenarios<sup><xref ref-type="bibr" rid="CR23">23</xref></sup>. And they employed non-maximum suppression to address the occlusion problem during the training of FCN, achieving precise leaf segmentation in dense scenarios. Although the studies mentioned have shown advancements in the realm of plant leaf segmentation, optimal results remain elusive.</p><p id="Par4">Under natural conditions, influenced by factors like natural lighting, weather conditions, the angle of the image sensor, and interference from surrounding leaves<sup><xref ref-type="bibr" rid="CR24">24</xref></sup>, how to effectively perform image segmentation under such conditions<sup><xref ref-type="bibr" rid="CR25">25</xref></sup>has become an urgent problem to address. To confront this challenge in the context of natural conditions, Zheng L and his team used algorithms like Mean-shift and BP neural network to segment leaves from different vegetables, yielding impressive outcomes<sup><xref ref-type="bibr" rid="CR26">26</xref></sup>. To improve the accuracy of plant leaf segmentation, many people have conducted studies based on features like the shape and color of the leaves. Omrani et al. utilized the K-means clustering algorithm to identify and segment various plant leaves<sup><xref ref-type="bibr" rid="CR27">27</xref></sup>, and employed the features of cluster centers for precise segmentation and perfectly mapped the results to the original images. Additionally, they also employed support vector machines to classify the leaves. In 2020, Praveen Kumar et al. proposed an efficient leaf segmentation algorithm based on Deep Convolutional Neural Networks (DCNN), and extracted leaf features by DCNN and applied orthogonal transformation techniques for precise segmentation of specific plant leaves<sup><xref ref-type="bibr" rid="CR28">28</xref></sup>. Notably, during the segmentation process, they also utilized the CMYK color space for denoising, further enhancing the efficiency of edge detection for leaves<sup><xref ref-type="bibr" rid="CR29">29</xref></sup>.</p><p id="Par5">In summary, beans, as significant agricultural crops, play a crucial role in human society. However, weeds seriously impact their growth and yield. The role of image segmentation in environmentally-friendly weed control becomes increasingly significant. People continuously explore various algorithms and methods to achieve precise image segmentation between crops and weeds, providing valuable support for the future development of agriculture. In this context, we propose the EPAnet algorithm for semantic segmentation of crops and weeds. Moreover, the algorithm presented in this paper has improved the Overall Accuracy by 0.65% compared to the benchmark model ERFnet, with a 1.91% increase in mIoU and a 1.19% increase in FWIoU. The contributions of this paper are:<list list-type="order"><list-item><p id="Par6"> Expanding public datasets can enhance generalization capabilities and accuracy of the model.</p></list-item><list-item><p id="Par7">Adopting a dual-loss coupled function as the loss function stabilizes the gradient descent process and accelerates convergence while balancing class imbalance.</p></list-item><list-item><p id="Par8">The SimAM is incorporated during the downsampling phase, helping the model focus on the parts of the image most relevant to the task, thus avoiding interference from irrelevant information.</p></list-item><list-item><p id="Par9"> Given the design of ERFnet as a real-time segmentation algorithm, it intentionally omits some information during the downsampling to increase its operational speed, which is a primary reason for its less-than-ideal accuracy. In consideration of precision, we propose a new FDPN connection module to enhance the information exchange during both the downsampling and upsampling stages, thus elevating the accuracy of the algorithm.</p></list-item><list-item><p id="Par10">In the decoder section, fusion with PSANet<sup><xref ref-type="bibr" rid="CR30">30</xref></sup> is implemented. Through the supplementation of multi-decoder experiments, the algorithm&#x02019;s accuracy is significantly improved.</p></list-item></list></p></sec><sec id="Sec2"><title>Materials and methods</title><sec id="Sec3"><title>Dataset</title><p id="Par11">The dataset used in this paper is sourced from a public repository (10.15454/JMKP9S). This dataset originates from a genuine bean sprout cultivation base in Avignon, France (latitude 46<sup>&#x02218;</sup>20&#x02019;30.3&#x0201d; N, longitude 3<sup>&#x02218;</sup>26&#x02019;33.6&#x0201d; E). The collection equipment utilized is a multispectral camera with six bands at 450/570/675/710/730/850 nm. The images showcase green bean alongside various native weeds such as yarrows, amaranth, geranium, plantago, etc. The collection conditions include rainy days, cloudy days, various lighting conditions, and different time periods. The dataset contains a total of 300 folders, each of which contains one original image and one spectral image collected at 570nm, named as &#x0201c;false.png&#x0201d; and &#x0201c;image.tiff&#x0201d; respectively. There are two label files corresponding to the original images, named as &#x0201c;gt.png&#x0201d; and &#x0201c;gt.xml&#x0201d; respectively. In addition, there is one black-and-white image of green beans and weeds (not available in some folders) named as &#x0201c;index.png&#x0201d;. A total of 1376 files are contained in all the folders. After manually sorting the required &#x0201c;false.png&#x0201d; and &#x0201c;gt.png&#x0201d; for this study, we obtained a total of 300 pairs of bean crop and weed images and their corresponding label images. The images and annotations of the data set are shown in Figs.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref> and&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref>. The dataset is comprehensive and possesses all the elements required for our experiments. The image quality is high, and the labels are fine-grained, which provides a high degree of reliability for the experiments. In the experiment, the dataset is divided into training, testing, and validation sets at a ratio of 7:2:1, nine offline data augmentation techniques were totally employed (These are horizontal flip, vertical flip, rotate, translate, crop and pad, rotate and crop, gaussian blur, sharpen, and brightness) to address the issue of the dataset being too small. At the same time, data enhancement generates new images that are similar to the original images but have different characteristics. Taken together, these image data enhancements can help the model better understand and adapt to various changes, thereby improving its performance and generalization capabilities.<fig id="Fig1"><label>Figure 1</label><caption><p>Original image.</p></caption><graphic xlink:href="41598_2024_61981_Fig1_HTML" id="MO1"/></fig><fig id="Fig2"><label>Figure 2</label><caption><p>Ground truth.</p></caption><graphic xlink:href="41598_2024_61981_Fig2_HTML" id="MO2"/></fig></p></sec><sec id="Sec4"><title>The structure of the EPAnet</title><p id="Par12">In this section, we describe the network architecture of EPAnet, which is an improvement based on the ERFnet<sup><xref ref-type="bibr" rid="CR31">31</xref></sup>. To optimize convergence during model training and enhance the overall performance of the model, we adopted a novel loss function that combines the cross-entropy loss function with the Dice loss function. And aiming to achieve the goal of elevating precision, we executed several optimizations on the benchmark model. The overall structure of EPAnet is illustrated in Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3</xref>.</p><p id="Par13">EPAnet has 24 layers, with layers 1 to 16 being the Encoder part and layers 17 to 24 as the Decoder. The Encoder is divided into three stages: First stage: The input passes through the Downsampler and SimAM. Second stage: The output from the previous part is further processed by downsample, SimAM, and five Non-bottleneck-1D (Non-bt-1D) layers<sup><xref ref-type="bibr" rid="CR31">31</xref></sup>. Third stage: The output from the second part is further processed by downsample, SimAM, and eight Non-bt-1D layers. The Non-bt-1D balances accuracy and parameter quantity, achieving better context information than Bottleneck. And the experimental Decoder is also divided into three stages: First stage: The output from the Encoder is first processed by upsampling and two Non-bt-1D layers. Second stage: The output from the first stage continues to be processed by upsampling and two Non-bt-1D layers, then passed to the PSA decoder head<sup><xref ref-type="bibr" rid="CR30">30</xref></sup>. Third stage: The data is finally processed by upsampling to produce the output results. In summary, this structure is more balanced, and experimental results indicate a clear improvement in model accuracy.<fig id="Fig3"><label>Figure 3</label><caption><p>The structure of the EPAnet. Input, output (segmentation category), the feature size obtained at each layer and the input size are not fixed, the corresponding feature mapping is related to the input size.</p></caption><graphic xlink:href="41598_2024_61981_Fig3_HTML" id="MO3"/></fig></p><sec id="Sec5"><title>Encoder</title><p id="Par14">After the image is fed into the model, similar to ERFnet. The data goes through a 3 <inline-formula id="IEq3"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\times$$\end{document}</tex-math><mml:math id="M2"><mml:mo>&#x000d7;</mml:mo></mml:math><inline-graphic xlink:href="41598_2024_61981_Article_IEq3.gif"/></alternatives></inline-formula> 3 convolution with a stride of 2, followed by a max-pooling layer for downsampling, then processed by SimAM, and also utilizing the proposed FDPN connection layer to process the image. These advancements have made a significant contribution to our aim of enhancing segmentation precision.</p></sec><sec id="Sec6"><title>Non-bottleneck-1D</title><p id="Par15">Non-bottleneck-1D is a newly proposed design for a residual layer, aiming to enhance learning capacity and efficiency. It utilizes the advantages of residual connections, adopts sparsity to accelerate and diminishes parameters of Non-bottleneck structures.</p></sec><sec id="Sec7"><title>SimAM<sup><xref ref-type="bibr" rid="CR32">32</xref></sup></title><p id="Par16">The attention mechanism emulates the concentration exhibited by the human brain on particular information throughout the cognitive process. In complex computational scenarios, selecting valuable information can effectively save computational resources<sup><xref ref-type="bibr" rid="CR33">33</xref></sup>. At present, there are primarily two types of attention mechanisms: spatial attention and channel attention. Spatial attention zeroes in on specific spatial positions and resultant features but may not be particularly attuned to inter-channel communication. Conversely, channel attention might neglect the interplay of spatial information. For optimal outcomes, it&#x02019;s typically essential to combine both these attention forms<sup><xref ref-type="bibr" rid="CR34">34</xref>,<xref ref-type="bibr" rid="CR35">35</xref></sup>. The CBAM attention mechanism encompasses both spatial and channel attention and, by adopting max-pooling structures, it has made a significant contribution to model performance improvement. However, integrating these two attentions might increase computational load and could even lead to model convergence issues. Bahdanau, D. et al. proposed the Shuffle Attention module, a method to make the model more lightweight and efficient<sup><xref ref-type="bibr" rid="CR36">36</xref></sup>. Given the enhancement brought about by attention to algorithmic performance, most algorithms now incorporate attention research. After several attempts in this experiment, a suitable method of adding attention was identified.</p><p id="Par17">There is a method to combine BAM (spatial attention module) and CBAM (channel attention module) in parallel or serial manners. In the human brain, these two attention mechanisms often work collaboratively. To more accurately simulate this brain-like attention mechanism, we need to evaluate the importance of each neuron. In neuroscience, information-rich neurons often exhibit different firing patterns compared to their neighboring neurons. When a neuron is activated, it suppresses the nearby neurons, a phenomenon known as spatial inhibitory effect. Neurons with the spatial inhibitory effect appear particularly significant. There are various methods to identify key neurons, one of the most intuitive being measuring the linear separability between neurons. Based on this idea, Yang L. et al. proposed the following energy function:<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} e_{t} \left( w_{t},b_{t},y,x_{i} \right) = \left( y_{t} -{\hat{t}}\right) ^{2} +\frac{1}{M-1} \sum _{i=1}^{M-1}\left( y_{0} -\hat{x_{i}}\right) ^{2} \end{aligned}$$\end{document}</tex-math><mml:math id="M4" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mfenced close=")" open="("><mml:msub><mml:mi>w</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mfenced><mml:mo>=</mml:mo><mml:msup><mml:mfenced close=")" open="("><mml:msub><mml:mi>y</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:mover accent="true"><mml:mi>t</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mfenced><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>M</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:msup><mml:mfenced close=")" open="("><mml:msub><mml:mi>y</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>-</mml:mo><mml:mover accent="true"><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mfenced><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_61981_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula>The process of minimizing (1) corresponds to enhancing the linear separability between neuron and its peers within the same channel. Utilizing binary labels for streamlining and incorporating a regularization component, the resultant energy function can be articulated as:<disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} e_{t} \left( w_{t},b_{t},y,x_{i} \right) = \frac{1}{M-1} \sum _{i=1}^{M-1} \left( -1-\left( w_{t}x_{i} +b_{t} \right) \right) ^{2}+\left( 1-\left( w_{t} t+b_{t} \right) \right) ^{2} + \lambda w_{t} ^{2} \end{aligned}$$\end{document}</tex-math><mml:math id="M6" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mfenced close=")" open="("><mml:msub><mml:mi>w</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mfenced><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>M</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:msup><mml:mfenced close=")" open="("><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mfenced close=")" open="("><mml:msub><mml:mi>w</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mfenced></mml:mfenced><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mfenced close=")" open="("><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mfenced close=")" open="("><mml:msub><mml:mi>w</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mfenced></mml:mfenced><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:mi>&#x003bb;</mml:mi><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_61981_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula>The ultimately derived analytical solution is as follows:<disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} w_{t}= &#x00026; {} -\frac{2\left( t-\mu t \right) }{\left( t-\mu t \right) ^{2} -2\sigma _{t} ^{2} +2\lambda } \end{aligned}$$\end{document}</tex-math><mml:math id="M8" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mrow/><mml:mo>-</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mfenced close=")" open="("><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mi>&#x003bc;</mml:mi><mml:mi>t</mml:mi></mml:mfenced></mml:mrow><mml:mrow><mml:msup><mml:mfenced close=")" open="("><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mi>&#x003bc;</mml:mi><mml:mi>t</mml:mi></mml:mfenced><mml:mn>2</mml:mn></mml:msup><mml:mo>-</mml:mo><mml:mn>2</mml:mn><mml:msubsup><mml:mi>&#x003c3;</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:mi>&#x003bb;</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_61981_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ4"><label>4</label><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} b_{t}= &#x00026; {} -\frac{1}{2} \left( t+\mu _{t} \right) w_{t} \end{aligned}$$\end{document}</tex-math><mml:math id="M10" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mrow/><mml:mo>-</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mfenced close=")" open="("><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>&#x003bc;</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mfenced><mml:msub><mml:mi>w</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_61981_Article_Equ4.gif" position="anchor"/></alternatives></disp-formula>Given that all neurons in each channel adhere to the same distribution, one can initially compute the mean and variance on the H and W dimensions of the input features to avoid redundant computations:<disp-formula id="Equ5"><label>5</label><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} e_{t} ^{*} =\frac{4\left( {\hat{\sigma }}^{2} +\lambda \right) }{\left( t-{\hat{\mu }}^{2} \right) +2{\hat{\sigma }}^{2} +2\lambda } \end{aligned}$$\end{document}</tex-math><mml:math id="M12" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msubsup><mml:mi>e</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mrow/><mml:mo>&#x02217;</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>4</mml:mn><mml:mfenced close=")" open="("><mml:msup><mml:mrow><mml:mover accent="true"><mml:mi>&#x003c3;</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:mi>&#x003bb;</mml:mi></mml:mfenced></mml:mrow><mml:mrow><mml:mfenced close=")" open="("><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:msup><mml:mrow><mml:mover accent="true"><mml:mi>&#x003bc;</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mfenced><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:msup><mml:mrow><mml:mover accent="true"><mml:mi>&#x003c3;</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:mi>&#x003bb;</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_61981_Article_Equ5.gif" position="anchor"/></alternatives></disp-formula>The entire process described above can be represented as:<disp-formula id="Equ6"><label>6</label><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} {\widetilde{X}} =sigmoid\left( \frac{1}{E} \right) \odot X \end{aligned}$$\end{document}</tex-math><mml:math id="M14" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo stretchy="true">~</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi><mml:mfenced close=")" open="("><mml:mfrac><mml:mn>1</mml:mn><mml:mi>E</mml:mi></mml:mfrac></mml:mfenced><mml:mo>&#x02299;</mml:mo><mml:mi>X</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_61981_Article_Equ6.gif" position="anchor"/></alternatives></disp-formula>SimAM is an efficient attention mechanism, which can enhance network features, improve model performance, and offer enhanced interpretability for the model&#x02019;s decisions. The integration of SimAM has significantly improved the accuracy of this model. The comparison of the three different attention generation methods is shown in Fig.&#x000a0;<xref rid="Fig4" ref-type="fig">4</xref>. In Fig.&#x000a0;<xref rid="Fig4" ref-type="fig">4</xref>a, b, the two approaches can only progressively form attention, but c SimAM can immediately determine three-dimensional weights. In Fig.&#x000a0;<xref rid="Fig4" ref-type="fig">4</xref>a&#x02013;c, identical coloring signifies that the same color indicates a unit scalar is used for each channel.<fig id="Fig4"><label>Figure 4</label><caption><p>Comparison chart of three different attention generation methods.</p></caption><graphic xlink:href="41598_2024_61981_Fig4_HTML" id="MO4"/></fig></p></sec><sec id="Sec8"><title>FDPN module</title><p id="Par18">The FDPN module is an improvement on the FPN, aiming to address the issue of information loss during the downsampling phase of the original algorithm. In the experiment, the FPN module is incorporated to fill in the lost information, enabling the algorithm to better capture target features at different scales and to achieve information fusion between different levels. In this paper, the proposed FPN module adopts DO-CONV convolution<sup><xref ref-type="bibr" rid="CR37">37</xref></sup>, and this choice significantly enhances the performance of the algorithm. Experimental results show that the model has improved accuracy (detailed experimental results are presented in the ablation study).</p></sec><sec id="Sec9"><title>Decoder</title><p id="Par19">In the decoding layer, we employed a coupled function of cross-entropy loss and Dice loss. Additionally, for structural balance, we appended the PSA Decoder Head after the original Decoder Head. The features captured by the Encoder are processed through deconvolution for upsampling and output.</p></sec><sec id="Sec10"><title>Cross entropy loss</title><p id="Par20">Cross-Entropy loss is a common loss function in multi-class classification tasks. It primarily describes the discrepancy between the actual output probability and the expected output probability. Moreover, the smaller the value of the cross-entropy, the closer the two probability distributions are to each other. Assuming probability distribution as the expected output, as the actual output, and as the cross-entropy, the calculation is as follows:<disp-formula id="Equ7"><label>7</label><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} H\left( p,q \right) =-\sum _{x=1}^{x} \left( p\left( x \right) logq\left( x \right) \right) \end{aligned}$$\end{document}</tex-math><mml:math id="M16" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>H</mml:mi><mml:mfenced close=")" open="("><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>q</mml:mi></mml:mfenced><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>x</mml:mi></mml:munderover><mml:mfenced close=")" open="("><mml:mi>p</mml:mi><mml:mfenced close=")" open="("><mml:mi>x</mml:mi></mml:mfenced><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mi>q</mml:mi><mml:mfenced close=")" open="("><mml:mi>x</mml:mi></mml:mfenced></mml:mfenced></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_61981_Article_Equ7.gif" position="anchor"/></alternatives></disp-formula></p></sec><sec id="Sec11"><title>Dice loss</title><p id="Par21">Dice loss is commonly used in pixel-level semantic segmentation tasks to measure the similarity between predicted results and the actual target. Assuming there are two sets A and B, the calculation formula for the Dice coefficient is as follows:<disp-formula id="Equ8"><label>8</label><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} Dice=\frac{2\cdot \left| A\cap B \right| }{\left| A \right| +\left| B \right| } \end{aligned}$$\end{document}</tex-math><mml:math id="M18" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo>&#x000b7;</mml:mo><mml:mfenced close="|" open="|"><mml:mi>A</mml:mi><mml:mo>&#x02229;</mml:mo><mml:mi>B</mml:mi></mml:mfenced></mml:mrow><mml:mrow><mml:mfenced close="|" open="|"><mml:mi>A</mml:mi></mml:mfenced><mml:mo>+</mml:mo><mml:mfenced close="|" open="|"><mml:mi>B</mml:mi></mml:mfenced></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_61981_Article_Equ8.gif" position="anchor"/></alternatives></disp-formula>Where A represents the model&#x02019;s predicted results, and B represents the actual target mask (label). Dice loss is derived by converting the Dice coefficient into a loss value through the complement operation.<disp-formula id="Equ9"><label>9</label><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} DiceLoss=1-\frac{2\cdot \left| A\cap B \right| }{\left| A \right| +\left| B \right| } \end{aligned}$$\end{document}</tex-math><mml:math id="M20" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>L</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo>&#x000b7;</mml:mo><mml:mfenced close="|" open="|"><mml:mi>A</mml:mi><mml:mo>&#x02229;</mml:mo><mml:mi>B</mml:mi></mml:mfenced></mml:mrow><mml:mrow><mml:mfenced close="|" open="|"><mml:mi>A</mml:mi></mml:mfenced><mml:mo>+</mml:mo><mml:mfenced close="|" open="|"><mml:mi>B</mml:mi></mml:mfenced></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_61981_Article_Equ9.gif" position="anchor"/></alternatives></disp-formula></p></sec><sec id="Sec12"><title>Total loss</title><p id="Par22">The proposed loss function is a combination of cross-entropy loss and dice loss. The cross-entropy loss and dice loss are respectively weighted according to the weight coefficients, and then added together to obtain the final comprehensive loss, as shown in Eq. (<xref rid="Equ10" ref-type="disp-formula">10</xref>).<disp-formula id="Equ10"><label>10</label><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} TotalLoss=0.6\times \left[ -\sum _{x=1}^{x} \left( p\left( x \right) logq\left( x \right) \right) \right] +0.4\times \left( \frac{2\cdot \left| A\cap B \right| }{\left| A \right| +\left| B \right| } \right) \end{aligned}$$\end{document}</tex-math><mml:math id="M22" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>T</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>L</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>0.6</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mfenced close="]" open="["><mml:mo>-</mml:mo><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>x</mml:mi></mml:munderover><mml:mfenced close=")" open="("><mml:mi>p</mml:mi><mml:mfenced close=")" open="("><mml:mi>x</mml:mi></mml:mfenced><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mi>q</mml:mi><mml:mfenced close=")" open="("><mml:mi>x</mml:mi></mml:mfenced></mml:mfenced></mml:mfenced><mml:mo>+</mml:mo><mml:mn>0.4</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mfenced close=")" open="("><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo>&#x000b7;</mml:mo><mml:mfenced close="|" open="|"><mml:mi>A</mml:mi><mml:mo>&#x02229;</mml:mo><mml:mi>B</mml:mi></mml:mfenced></mml:mrow><mml:mrow><mml:mfenced close="|" open="|"><mml:mi>A</mml:mi></mml:mfenced><mml:mo>+</mml:mo><mml:mfenced close="|" open="|"><mml:mi>B</mml:mi></mml:mfenced></mml:mrow></mml:mfrac></mml:mfenced></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_61981_Article_Equ10.gif" position="anchor"/></alternatives></disp-formula>Coupling the loss functions can integrate the advantages of both, reflecting better generalization capabilities, preventing model overfitting, and also better focusing on the area to be detected. Experimental results demonstrates that our designed loss function performs well (specific results will be detailed in the ablation study).</p></sec><sec id="Sec13"><title>Decoder head</title><p id="Par23">The decoder used by ERFnet is based on the symmetrical structure of Segnet, employing the innovative non-bottleneck-1D derived from convolutional methods and utilizing transposed convolution for upsampling. In this paper, the proposed method incorporates the PSA Head operation following the Decoder Head step.</p></sec><sec id="Sec14"><title>PSA decoder head</title><p id="Par24">Convolutional neural networks increase receptive fields by stacking multiple layers, but the effect is not ideal. Some scholars have utilized dilated convolutions to expand receptive fields, thereby enlarging the model&#x02019;s receptive field. Although dilated convolutions can make the receptive field larger, this operation tends to neglect some information in the image. Additionally, traditional convolution operations confine information flow to local regions, leading to a lack of connection between local and global information. Based on the reasons above, we added PSANet to the second-to-last layer of EPAnet. PSANet is a point-wise spatial attention network that can integrate long-range contextual information. The design of this network allows a point in the feature map to be connected to other points through learnable convolutions, thereby integrating information from nearby and distant points. Additionally, PSANet is designed with bidirectional information flow, enhancing its ability to understand complex scenes. However, PSANet consumes a large amount of memory. Considering the issue of model parameter count, we introduced PSANet in the second-to-last layer of EPAnet to enhance the model&#x02019;s performance. The Decoder Head of PSANet as shown in Fig.&#x000a0;<xref rid="Fig5" ref-type="fig">5</xref>, provides a detailed description of the data processing flow of the PSA Decoder Head. In the model, the two branches (upper and lower) are entirely symmetrical, with the upper being the Collect branch and the lower the Distribute branch.</p><p id="Par25">The algorithm input is defined <italic>t</italic> as a three-dimensional data X: H (height), W (width), and <inline-formula id="IEq4"><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$C_{\text {1}}$$\end{document}</tex-math><mml:math id="M24"><mml:msub><mml:mi>C</mml:mi><mml:mtext>1</mml:mtext></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_61981_Article_IEq4.gif"/></alternatives></inline-formula> (number of channels). (Given that both branches are the same, we will only detail the Collect branch here.) X first goes through a 1<inline-formula id="IEq5"><alternatives><tex-math id="M25">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\times$$\end{document}</tex-math><mml:math id="M26"><mml:mo>&#x000d7;</mml:mo></mml:math><inline-graphic xlink:href="41598_2024_61981_Article_IEq5.gif"/></alternatives></inline-formula>1 convolution to adjust the number of channels, reducing the computational load. The reduced data is then fed into the &#x0201c;Adaption &#x00026; Conv&#x0201d; module. Using a 1x1 convolution layer, a new feature map <inline-formula id="IEq6"><alternatives><tex-math id="M27">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$H^{c}$$\end{document}</tex-math><mml:math id="M28"><mml:msup><mml:mi>H</mml:mi><mml:mi>c</mml:mi></mml:msup></mml:math><inline-graphic xlink:href="41598_2024_61981_Article_IEq6.gif"/></alternatives></inline-formula>, of size <italic>H</italic><inline-formula id="IEq7"><alternatives><tex-math id="M29">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\times$$\end{document}</tex-math><mml:math id="M30"><mml:mo>&#x000d7;</mml:mo></mml:math><inline-graphic xlink:href="41598_2024_61981_Article_IEq7.gif"/></alternatives></inline-formula><italic>W</italic><inline-formula id="IEq8"><alternatives><tex-math id="M31">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\times$$\end{document}</tex-math><mml:math id="M32"><mml:mo>&#x000d7;</mml:mo></mml:math><inline-graphic xlink:href="41598_2024_61981_Article_IEq8.gif"/></alternatives></inline-formula><italic>(2H-1)(2W-1)</italic> is obtained.Through the above methods, we can obtain <inline-formula id="IEq9"><alternatives><tex-math id="M33">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$H^{c}$$\end{document}</tex-math><mml:math id="M34"><mml:msup><mml:mi>H</mml:mi><mml:mi>c</mml:mi></mml:msup></mml:math><inline-graphic xlink:href="41598_2024_61981_Article_IEq9.gif"/></alternatives></inline-formula>. The output data from the &#x0201c;Adaption &#x00026; Conv&#x0201d; module is channeled to the &#x0201c;Collect Attention Generation&#x0201d; module (This module is responsible for generating a new Attention Map, denoted as <inline-formula id="IEq10"><alternatives><tex-math id="M35">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$A^{c}$$\end{document}</tex-math><mml:math id="M36"><mml:msup><mml:mi>A</mml:mi><mml:mi>c</mml:mi></mml:msup></mml:math><inline-graphic xlink:href="41598_2024_61981_Article_IEq10.gif"/></alternatives></inline-formula>, for each position). The conversion process from <inline-formula id="IEq11"><alternatives><tex-math id="M37">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$H^{c}$$\end{document}</tex-math><mml:math id="M38"><mml:msup><mml:mi>H</mml:mi><mml:mi>c</mml:mi></mml:msup></mml:math><inline-graphic xlink:href="41598_2024_61981_Article_IEq11.gif"/></alternatives></inline-formula> to <inline-formula id="IEq12"><alternatives><tex-math id="M39">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$A^{c}$$\end{document}</tex-math><mml:math id="M40"><mml:msup><mml:mi>A</mml:mi><mml:mi>c</mml:mi></mml:msup></mml:math><inline-graphic xlink:href="41598_2024_61981_Article_IEq12.gif"/></alternatives></inline-formula> will be explained in detail in the next paragraph. Integrating the above steps, the produced attention weights are combined with corresponding data through a weighted fusion to obtain <inline-formula id="IEq13"><alternatives><tex-math id="M41">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$Z^{c}$$\end{document}</tex-math><mml:math id="M42"><mml:msup><mml:mi>Z</mml:mi><mml:mi>c</mml:mi></mml:msup></mml:math><inline-graphic xlink:href="41598_2024_61981_Article_IEq13.gif"/></alternatives></inline-formula> and <inline-formula id="IEq14"><alternatives><tex-math id="M43">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$Z^{d}$$\end{document}</tex-math><mml:math id="M44"><mml:msup><mml:mi>Z</mml:mi><mml:mi>d</mml:mi></mml:msup></mml:math><inline-graphic xlink:href="41598_2024_61981_Article_IEq14.gif"/></alternatives></inline-formula>. Both <inline-formula id="IEq15"><alternatives><tex-math id="M45">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$Z^{c}$$\end{document}</tex-math><mml:math id="M46"><mml:msup><mml:mi>Z</mml:mi><mml:mi>c</mml:mi></mml:msup></mml:math><inline-graphic xlink:href="41598_2024_61981_Article_IEq15.gif"/></alternatives></inline-formula> and <inline-formula id="IEq16"><alternatives><tex-math id="M47">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$Z^{d}$$\end{document}</tex-math><mml:math id="M48"><mml:msup><mml:mi>Z</mml:mi><mml:mi>d</mml:mi></mml:msup></mml:math><inline-graphic xlink:href="41598_2024_61981_Article_IEq16.gif"/></alternatives></inline-formula> enter the &#x0201c;Concat &#x00026; Projection&#x0201d; module separately. They are then linked with another segment having dimension <inline-formula id="IEq17"><alternatives><tex-math id="M49">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$C_{\text {1}}$$\end{document}</tex-math><mml:math id="M50"><mml:msub><mml:mi>C</mml:mi><mml:mtext>1</mml:mtext></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_61981_Article_IEq17.gif"/></alternatives></inline-formula>. This process involves integrating data from different origins and performing a linear transformation and projection. And all data streams converge in a module labeled &#x0201c;Concat&#x0201d;. The output is characterized as a block with dimensions H, W,and 2<inline-formula id="IEq18"><alternatives><tex-math id="M51">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$C_{\text {1}}$$\end{document}</tex-math><mml:math id="M52"><mml:msub><mml:mi>C</mml:mi><mml:mtext>1</mml:mtext></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_61981_Article_IEq18.gif"/></alternatives></inline-formula>.<fig id="Fig5"><label>Figure 5</label><caption><p>The structural details of the PSA module, the processing flow and output after the data in the previous step enters the PSA.</p></caption><graphic xlink:href="41598_2024_61981_Fig5_HTML" id="MO5"/></fig><fig id="Fig6"><label>Figure 6</label><caption><p>The diagram of point-wise spatial attention, collect attention and distribute attention generation process.</p></caption><graphic xlink:href="41598_2024_61981_Fig6_HTML" id="MO6"/></fig></p><p id="Par26">Figure&#x000a0;<xref rid="Fig6" ref-type="fig">6</xref> mainly illustrates how the original information in Fig.&#x000a0;<xref rid="Fig5" ref-type="fig">5</xref> is converted to <inline-formula id="IEq19"><alternatives><tex-math id="M53">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$H^{c}$$\end{document}</tex-math><mml:math id="M54"><mml:msup><mml:mi>H</mml:mi><mml:mi>c</mml:mi></mml:msup></mml:math><inline-graphic xlink:href="41598_2024_61981_Article_IEq19.gif"/></alternatives></inline-formula>, and the method of converting <inline-formula id="IEq20"><alternatives><tex-math id="M55">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$H^{c}$$\end{document}</tex-math><mml:math id="M56"><mml:msup><mml:mi>H</mml:mi><mml:mi>c</mml:mi></mml:msup></mml:math><inline-graphic xlink:href="41598_2024_61981_Article_IEq20.gif"/></alternatives></inline-formula> to <inline-formula id="IEq21"><alternatives><tex-math id="M57">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$A^{c}$$\end{document}</tex-math><mml:math id="M58"><mml:msup><mml:mi>A</mml:mi><mml:mi>c</mml:mi></mml:msup></mml:math><inline-graphic xlink:href="41598_2024_61981_Article_IEq21.gif"/></alternatives></inline-formula>. The size of the module <inline-formula id="IEq22"><alternatives><tex-math id="M59">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$H^{c}$$\end{document}</tex-math><mml:math id="M60"><mml:msup><mml:mi>H</mml:mi><mml:mi>c</mml:mi></mml:msup></mml:math><inline-graphic xlink:href="41598_2024_61981_Article_IEq22.gif"/></alternatives></inline-formula> is <italic>H</italic><inline-formula id="IEq23"><alternatives><tex-math id="M61">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\times$$\end{document}</tex-math><mml:math id="M62"><mml:mo>&#x000d7;</mml:mo></mml:math><inline-graphic xlink:href="41598_2024_61981_Article_IEq23.gif"/></alternatives></inline-formula><italic>W</italic><inline-formula id="IEq24"><alternatives><tex-math id="M63">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\times$$\end{document}</tex-math><mml:math id="M64"><mml:mo>&#x000d7;</mml:mo></mml:math><inline-graphic xlink:href="41598_2024_61981_Article_IEq24.gif"/></alternatives></inline-formula><italic>(2H &#x02212; 1)(2W &#x02212; 1)</italic>, Therefore, each position i in H corresponds to a feature map of size <italic>1</italic><inline-formula id="IEq25"><alternatives><tex-math id="M65">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\times$$\end{document}</tex-math><mml:math id="M66"><mml:mo>&#x000d7;</mml:mo></mml:math><inline-graphic xlink:href="41598_2024_61981_Article_IEq25.gif"/></alternatives></inline-formula><italic>1</italic><inline-formula id="IEq26"><alternatives><tex-math id="M67">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\times$$\end{document}</tex-math><mml:math id="M68"><mml:mo>&#x000d7;</mml:mo></mml:math><inline-graphic xlink:href="41598_2024_61981_Article_IEq26.gif"/></alternatives></inline-formula><italic>(2H &#x02212; 1)(2W &#x02212; 1)</italic>, it can be reshaped into a <italic>(2H &#x02212; 1)</italic><inline-formula id="IEq27"><alternatives><tex-math id="M69">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\times$$\end{document}</tex-math><mml:math id="M70"><mml:mo>&#x000d7;</mml:mo></mml:math><inline-graphic xlink:href="41598_2024_61981_Article_IEq27.gif"/></alternatives></inline-formula><italic>(2W &#x02212; 1)</italic> feature map, After that, calculate the positional relationship between i and j, but only a part of it is functional in the feature map (as shown by the dashed part in Fig. <xref rid="Fig6" ref-type="fig">6</xref>a). The size is <italic>H</italic><inline-formula id="IEq28"><alternatives><tex-math id="M71">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\times$$\end{document}</tex-math><mml:math id="M72"><mml:mo>&#x000d7;</mml:mo></mml:math><inline-graphic xlink:href="41598_2024_61981_Article_IEq28.gif"/></alternatives></inline-formula><italic>W</italic>. At the same time, <inline-formula id="IEq29"><alternatives><tex-math id="M73">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$H^{c}$$\end{document}</tex-math><mml:math id="M74"><mml:msup><mml:mi>H</mml:mi><mml:mi>c</mml:mi></mml:msup></mml:math><inline-graphic xlink:href="41598_2024_61981_Article_IEq29.gif"/></alternatives></inline-formula> can have <italic>H</italic><inline-formula id="IEq30"><alternatives><tex-math id="M75">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\times$$\end{document}</tex-math><mml:math id="M76"><mml:mo>&#x000d7;</mml:mo></mml:math><inline-graphic xlink:href="41598_2024_61981_Article_IEq30.gif"/></alternatives></inline-formula><italic>W</italic> positions i. By analogy, <italic>H</italic><inline-formula id="IEq31"><alternatives><tex-math id="M77">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\times$$\end{document}</tex-math><mml:math id="M78"><mml:mo>&#x000d7;</mml:mo></mml:math><inline-graphic xlink:href="41598_2024_61981_Article_IEq31.gif"/></alternatives></inline-formula><italic>W</italic> similar feature maps will be generated. The reorganization of the feature maps is <inline-formula id="IEq32"><alternatives><tex-math id="M79">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$A^{c}$$\end{document}</tex-math><mml:math id="M80"><mml:msup><mml:mi>A</mml:mi><mml:mi>c</mml:mi></mml:msup></mml:math><inline-graphic xlink:href="41598_2024_61981_Article_IEq32.gif"/></alternatives></inline-formula>.(Fig.&#x000a0;<xref rid="Fig6" ref-type="fig">6</xref> illustrates the spatial attention generation method in the PSA Decoder head).</p><p id="Par27">This module distinctly marks the most evident improvement of the entire experiment. Compared to the baseline model, this module&#x02019;s Overall Accuracy improved by 0.36%, mIoU by 1.15%, and FWIoU by 0.65%.</p></sec></sec><sec id="Sec15"><title>Ethical statement</title><p id="Par28">Plant sampling complies with the IUCN Policy Statement on Research Involving Species at Risk of Extinction and the Convention on the Trade in Endangered Species of Wild Fauna and Flora.</p></sec></sec><sec id="Sec16"><title>Experiments</title><sec id="Sec17"><title>Experimental setup</title><p id="Par29">In this study, all experiments were conducted in a uniform setting to ensure both objectivity and validity of the results. Throughout the experiments, the operational system employed is Ubuntu 22.04.2 LTS, powered by Intel(R) Xeon(R) Silver 4110 CPU @ 2.10GHz, and equipped with an NVIDIA GTX3090 graphics card and 24GB VRAM. The programming language used is Python 3.8.17, and the deep learning framework is PyTorch 1.10.0 and CUDA 11.1. The experimental learning rate is set to 0.0025, and the number of epochs is set to 400.</p></sec><sec id="Sec18"><title>Evaluation metrics</title><p id="Par30">To comprehensively quantify the segmentation performance of different networks, commonly used evaluation metrics in image segmentation were employed, including precision, recall, F1-Score, Overall Accuracy, IoU, mIoU, and FWIoU. It should be noted that:</p><p id="Par31">TP (true positives) denotes the count of pixels which the model accurately identified as the positive category.</p><p id="Par32">FP (false positives) represents the number of pixels that the model misclassified as the positive category, even though they don&#x02019;t belong to it.</p><p id="Par33">FN (false negative) refers to the number of pixels that should have been predicted as a certain class but were not predicted as that class.</p><p id="Par34">TN (true negative) indicates the number of pixels that the model correctly predicted as belonging to the negative class.</p><p id="Par35">Precision: a higher precision means that the model is more accurate when predicting as positive, reducing the risk of false alarms. However, a higher precision may result in more missed detections. Therefore, in some cases, it&#x02019;s necessary to consider other evaluation metrics, such as recall, to balance the trade-off between precision and recall. The calculation formula is as follows:<disp-formula id="Equ11"><label>11</label><alternatives><tex-math id="M81">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} \textrm{Precision} =\frac{TP}{TP+FP} \end{aligned}$$\end{document}</tex-math><mml:math id="M82" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mtext>Precision</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="italic">TP</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_61981_Article_Equ11.gif" position="anchor"/></alternatives></disp-formula>Recall is another important metric used to measure the performance of a classification model, particularly suitable for binary classification problems. Recall measures the proportion of actual positive samples that the model successfully predicts as positive. The calculation formula for recall is as follows:<disp-formula id="Equ12"><label>12</label><alternatives><tex-math id="M83">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} \textrm{Recall} =\frac{TP}{TP+FN} \end{aligned}$$\end{document}</tex-math><mml:math id="M84" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mtext>Recall</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="italic">TP</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_61981_Article_Equ12.gif" position="anchor"/></alternatives></disp-formula>Recall quantifies the extent to which the model covers positive samples, meaning how many actual positives are recognized by the model. A higher recall indicates that the model can capture positives well but might also lead to more false positives. In some applications, such as disease diagnosis, the importance of recall becomes paramount because missing a true positive might have serious consequences. And a balancing act exists between recall and precision. Increasing the model&#x02019;s recall often decreases precision because capturing more positives might lead to some negatives being incorrectly predicted as positives. Therefore, in practical applications, based on the characteristics of the task, one should consider both recall and precision, choosing an appropriate threshold or other methods to balance these two metrics.</p><p id="Par36">F1-score represents the harmonic average of Precision and Recall. The formula for calculating F1-score is:<disp-formula id="Equ13"><label>13</label><alternatives><tex-math id="M85">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} \textrm{F1} =2\times \frac{\mathrm {Precision\times Recall} }{\mathrm {Precision\times Recall} } \end{aligned}$$\end{document}</tex-math><mml:math id="M86" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mtext>F1</mml:mtext><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">Precision</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi mathvariant="normal">Recall</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">Precision</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi mathvariant="normal">Recall</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_61981_Article_Equ13.gif" position="anchor"/></alternatives></disp-formula>Where the range of F1-score is [0, 1]. A F1-score close to 1 indicates excellent classifier performance, whereas a score close to 0 indicates poor performance. F1-score offers an integrated measure to assess the balanced performance of a model in both aspects.</p><p id="Par37">Overall accuracy stands as one of the most straightforward and intuitive measures for evaluation. For semantic segmentation, the calculation formula for overall accuracy is:<disp-formula id="Equ14"><label>14</label><alternatives><tex-math id="M87">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} Overall Accuracy=\frac{TP+TN}{TP+TN+FP+FN} \end{aligned}$$\end{document}</tex-math><mml:math id="M88" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>O</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mi>A</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_61981_Article_Equ14.gif" position="anchor"/></alternatives></disp-formula>While overall accuracy is a simple and intuitive metric, it might mask the model&#x02019;s weaknesses in certain specific categories. If a category&#x02019;s pixel count is much lower than others, then even with completely wrong predictions for that category, the overall accuracy might remain high. In practice, other evaluation metrics, such as the F1 score or IoU (Intersection over Union), are often considered to achieve a more comprehensive assessment of model performance.</p><p id="Par38">The Intersection over Union (IoU) represents the ratio of the intersection to the union of the model&#x02019;s predicted results for a specific category and its true values. For object detection, IoU pertains to the ratio between the detected bounding box and the actual box. For image segmentation, it calculates the ratio between the predicted mask and the true mask. The calculation formula is as follows:<disp-formula id="Equ15"><label>15</label><alternatives><tex-math id="M89">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} IoU=\frac{TP}{TP+FP+FN} \end{aligned}$$\end{document}</tex-math><mml:math id="M90" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:mi>U</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="italic">TP</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_61981_Article_Equ15.gif" position="anchor"/></alternatives></disp-formula>mIoU is the intersection of the predicted area and the actual area divided by the union of the predicted area and the actual area. This calculation gives the IoU for a single category. Repeat this algorithm to calculate the IoU for other categories and then compute their average. It signifies the model&#x02019;s ratio of intersection to union for each category&#x02019;s predicted results and true values. The calculation formula is as follows:<disp-formula id="Equ16"><label>16</label><alternatives><tex-math id="M91">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} mIoU=\frac{1}{N} \sum _{i=1}^{N} IoU_{i} \end{aligned}$$\end{document}</tex-math><mml:math id="M92" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>m</mml:mi><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:mi>U</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mi>U</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_61981_Article_Equ16.gif" position="anchor"/></alternatives></disp-formula>Where N is the total number of categories. mIoU is a commonly used evaluation metric in image segmentation tasks, reflecting the model&#x02019;s ability to segment different category targets. A higher mIoU indicates that the model can more accurately segment different object categories. However, one must acknowledge that mIoU might not adeptly discern the category imbalances, meaning certain categories may be rarer than others. Under such circumstances, FWIoU can offer a more accurate representation of each category&#x02019;s significance.</p><p id="Par39">FWIoU is an enhancement of mIoU, taking into account the influence of category frequency on mIoU. It is especially suitable for addressing imbalanced category distributions. Assuming there are a total of N categories, with the Intersection over Union for each category being IoU1, IoU2, ..., IoUn and the pixel count (frequency) for each category being N1, N2, ..., NN, the calculation formula for FWIoU is:<disp-formula id="Equ17"><label>17</label><alternatives><tex-math id="M93">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} \textrm{FWIoU} =\frac{\sum _{i=1}^{N} \textrm{IoU} _{i} \times N_{i} }{\sum _{i=1}^{N} N_{i} } \end{aligned}$$\end{document}</tex-math><mml:math id="M94" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mtext>FWIoU</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup><mml:msub><mml:mtext>IoU</mml:mtext><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x000d7;</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup><mml:msub><mml:mi>N</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_61981_Article_Equ17.gif" position="anchor"/></alternatives></disp-formula>where FWIoU factors are in the product of the Intersection over Union for each category with its frequency in the image. The combined product across categories, divided by the sum of all pixels, results in an mIoU that holistically accounts for frequency. The advantage of FWIoU is that it offers a more equitable assessment for imbalanced category distributions. Categories appearing more frequently will dominate the evaluation outcomes, whereas those less frequent will have a diminished influence. This ensures that the evaluation results better reflect the actual performance of each category. In conclusion, FWIoU is a performance metric for image segmentation models that considers category frequency, effectively addressing imbalanced category distributions.</p></sec></sec><sec id="Sec19"><title>Results</title><sec id="Sec20"><title>Ablation study</title><p id="Par40">
<table-wrap id="Tab1"><label>Table 1</label><caption><p>Result of ablation experiment.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Methods</th><th align="left">Classes</th><th align="left">Precision (%)</th><th align="left">Recall (%)</th><th align="left">F1 score (%)</th><th align="left">Overall accuracy (%)</th><th align="left">iou</th><th align="left">mIoU</th><th align="left">FWIoU</th></tr></thead><tbody><tr><td align="left">ERFnet (cross entropy loss)</td><td align="left"><p>Background</p><p>Bean seedling</p><p>Weed</p></td><td align="left"><p>98.41</p><p>87.48</p><p>91.38</p></td><td align="left"><p>98.29</p><p>87.21</p><p>92.09</p></td><td align="left"><p>98.35</p><p>87.34</p><p>91.73</p></td><td align="left">96.58</td><td align="left"><p>96.75</p><p>77.54</p><p>84.73</p></td><td align="left">86.34</td><td align="left">93.59</td></tr><tr><td align="left">ERF + PSA (cross entropy loss)</td><td align="left"><p>Background</p><p>Bean seedling</p><p>Weed</p></td><td align="left"><p>98.55</p><p>89.17</p><p>92.19</p></td><td align="left"><p>98.59</p><p>87.83</p><p>92.68</p></td><td align="left"><p>98.57</p><p>88.49</p><p>92.43</p></td><td align="left">96.94</td><td align="left"><p>97.19</p><p>79.36</p><p>85.93</p></td><td align="left">87.49</td><td align="left">94.24</td></tr><tr><td align="left"><p>ERF + PSA</p><p>+ double loss</p></td><td align="left"><p>Background</p><p>Bean seedling</p><p>Weed</p></td><td align="left"><p>98.75</p><p>88.65</p><p>92.15</p></td><td align="left"><p>98.49</p><p>88.68</p><p>93.40</p></td><td align="left"><p>98.62</p><p>88.66</p><p>92.77</p></td><td align="left">97.04</td><td align="left"><p>97.27</p><p>79.64</p><p>86.51</p></td><td align="left">87.81</td><td align="left">94.41</td></tr><tr><td align="left"><p>ERF + PSA</p><p>+ double loss</p><p>+ SimAM</p></td><td align="left"><p>Background</p><p>Bean seedling</p><p>Weed</p></td><td align="left"><p>98.76</p><p>89.36</p><p>92.38</p></td><td align="left"><p>98.70</p><p>87.83</p><p>93.43</p></td><td align="left"><p>98.73</p><p>88.59</p><p>92.90</p></td><td align="left">97.14</td><td align="left"><p>97.49</p><p>79.51</p><p>86.74</p></td><td align="left">87.92</td><td align="left">94.61</td></tr><tr><td align="left"><p>ERF + PSA</p><p>+ double loss</p><p>+SimAM</p><p>+FDPN</p></td><td align="left"><p>Background</p><p>Bean seedling</p><p>Weed</p></td><td align="left"><p>98.89</p><p>90.90</p><p>91.61</p></td><td align="left"><p>98.70</p><p>87.26</p><p>94.39</p></td><td align="left"><p>98.79</p><p>89.04</p><p>92.98</p></td><td align="left">97.23</td><td align="left"><p>97.62</p><p>80.25</p><p>86.88</p></td><td align="left">88.25</td><td align="left">94.78</td></tr></tbody></table><table-wrap-foot><p>Add different modules one by one to evaluate the impact and role of each module on the model performance. The data in the table are the optimal results of the experiment.</p></table-wrap-foot></table-wrap>
</p><p id="Par41">To evaluate the impact of each module on the segmentation performance, we employed an ablation method to test each network module, using ERFnet as the benchmark model and progressively introducing different modules for the ablation study. The outcomes of the ablation experiment are demonstrated in Table&#x000a0;<xref rid="Tab1" ref-type="table">1</xref>. First, both our baseline experiments and the experiments with the PSANet module used the Cross Entropy Loss, as shown in our ablation experimental Table <xref rid="Tab1" ref-type="table">1</xref>. ERFnet (cross entropy loss) and ERF+PSA (cross entropy loss) indicate that we used the cross entropy loss as the loss function when conducting ERFnet and ERF+PSA experiments, upon the addition of the PSA module, the metrics of Overall Accuracy, mIoU, and FWIoU improved to 96.94%, 87.49%, and 94.24% respectively. The enhancement is particularly substantial with mIoU witnessing a growth of 1.15%. Second, By incorporating a dual loss function, we can observe that the Overall Accuracy has increased from 96.94 to 97.04%, and the mIoU has increased from 87.49 to 87.81%, compared to the previous experimental results. We then incorporated the SimAM, which improved the mIoU to 87.92% and the Overall Accuracy to 97.14%. Last, after the integration of the FDPN module, compared with the benchmark algorithm, the Overall Accuracy increased by 0.65%, mIoU by 1.91%, and FWIoU by 1.19%. Moreover, each time an innovative aspect was introduced, there was an enhancement in the F1-score. Based on the results in Table&#x000a0;<xref rid="Tab1" ref-type="table">1</xref>, after incorporating these network modules, our proposed algorithm evidently outperforms the original benchmark in terms of segmentation on this dataset. This conclusively attests to the successful enhancements we made to the reference model.</p></sec><sec id="Sec21"><title>Comparison with different network architectures</title><p id="Par42">To better demonstrate the superiority of our experiment, we further reproduced the current mainstream algorithms, conducted comparative experiments, and presented the comparison results combined with the introduced evaluation metrics. The table below clearly indicates that our proposed method boasts an accuracy rate of 97.23% and an mIoU of 88.25%, excelling in other evaluation criteria as well. Considering that we are conducting a three-class task, we observed that among all categories, the accuracy and recall rate of the bean sprouts are generally lower, indicating that segmenting bean sprouts is a significant challenge. In terms of bean sprout accuracy, our model holds a pronounced edge over others, surpassing them by varying margins between 7% and 11%. In terms of recall rate for bean sprouts, our algorithm performs the best, reaching 87.26%. In contrast, the lowest is the FCN model at just 83.25%, meaning our model has a relative improvement of 4.01%.<table-wrap id="Tab2"><label>Table 2</label><caption><p>Evaluation results of the proposed EPAnet algorithm compared with current algorithms. Data represents the best experimental results.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Methods</th><th align="left">Classes</th><th align="left">Precision (%)</th><th align="left">Recall (%)</th><th align="left">F1 score (%)</th><th align="left">Overall accuracy (%)</th><th align="left">iou</th><th align="left">mIoU</th><th align="left">FWIoU</th></tr></thead><tbody><tr><td align="left">Icnet</td><td align="left"><p>Background</p><p>Bean seedling</p><p>Weed</p></td><td align="left"><p>98.02</p><p>79.45</p><p>87.66</p></td><td align="left"><p>97.18</p><p>83.73</p><p>89.51</p></td><td align="left"><p>97.60</p><p>81.53</p><p>88.58</p></td><td align="left">95.15</td><td align="left"><p>95.30</p><p>68.82</p><p>79.49</p></td><td align="left">81.20</td><td align="left">91.05</td></tr><tr><td align="left">UNet</td><td align="left"><p>Background</p><p>Bean seedling</p><p>Weed</p></td><td align="left"><p>98.52</p><p>83.16</p><p>88.19</p></td><td align="left"><p>97.83</p><p>83.37</p><p>91.43</p></td><td align="left"><p>98.17</p><p>83.26</p><p>89.78</p></td><td align="left">95.89</td><td align="left"><p>96.42</p><p>71.33</p><p>81.46</p></td><td align="left">83.07</td><td align="left">92.40</td></tr><tr><td align="left">FCN</td><td align="left"><p>Background</p><p>Bean seedling</p><p>Weed</p></td><td align="left"><p>98</p><p>82.14</p><p>97.17</p></td><td align="left"><p>97.15</p><p>83.25</p><p>90.70</p></td><td align="left"><p>97.57</p><p>82.69</p><p>93.82</p></td><td align="left">95.25</td><td align="left"><p>95.26</p><p>70.49</p><p>80.02</p></td><td align="left">81.92</td><td align="left">91.22</td></tr><tr><td align="left">Deeplabv3</td><td align="left"><p>Background</p><p>Bean seedling</p><p>Weed</p></td><td align="left"><p>98.02</p><p>83.39</p><p>88.65</p></td><td align="left"><p>97.41</p><p>85.09</p><p>90.71</p></td><td align="left"><p>97.71</p><p>84.23</p><p>89.67</p></td><td align="left">95.57</td><td align="left"><p>95.53</p><p>72.76</p><p>81.27</p></td><td align="left">83.18</td><td align="left">91.78</td></tr><tr><td align="left">Fast_scnn<sup><xref ref-type="bibr" rid="CR38">38</xref></sup></td><td align="left"><p>Background</p><p>Bean seedling</p><p>Weed</p></td><td align="left"><p>98.41</p><p>82.25</p><p>89.10</p></td><td align="left"><p>97.67</p><p>85.65</p><p>90.98</p></td><td align="left"><p>98.04</p><p>83.92</p><p>90.03</p></td><td align="left">95.87</td><td align="left"><p>96.16</p><p>72.28</p><p>81.87</p></td><td align="left">83.44</td><td align="left">92.33</td></tr><tr><td align="left">Deeplabv3+</td><td align="left"><p>Background</p><p>Bean seedling</p><p>Weed</p></td><td align="left"><p>98.47</p><p>83.80</p><p>90.11</p></td><td align="left"><p>97.87</p><p>87.12</p><p>91.45</p></td><td align="left"><p>98.17</p><p>85.43</p><p>90.78</p></td><td align="left">96.19</td><td align="left"><p>96.42</p><p>74.56</p><p>83.10</p></td><td align="left">84.70</td><td align="left">92.88</td></tr><tr><td align="left">ERFnet</td><td align="left"><p>Background</p><p>Bean seedling</p><p>Weed</p></td><td align="left"><p>98.41</p><p>87.48</p><p>91.38</p></td><td align="left"><p>98.29</p><p>87.21</p><p>92.09</p></td><td align="left"><p>98.35</p><p>87.34</p><p>91.73</p></td><td align="left">96.58</td><td align="left"><p>96.75</p><p>77.54</p><p>84.73</p></td><td align="left">86.34</td><td align="left">93.59</td></tr><tr><td align="left">EPAnet (ours)</td><td align="left"><p>Background</p><p>Bean seedling</p><p>Weed</p></td><td align="left"><p>98.89</p><p>90.90</p><p>91.61</p></td><td align="left"><p>98.70</p><p>87.26</p><p>94.39</p></td><td align="left"><p>98.80</p><p>89.04</p><p>92.98</p></td><td align="left">97.23</td><td align="left"><p>97.62</p><p>80.25</p><p>86.88</p></td><td align="left">88.25</td><td align="left">94.78</td></tr></tbody></table></table-wrap></p><p id="Par43">In terms of weeds, our algorithm achieved a recall rate of 94.39%, which is 4.88% higher than the Icnet model<sup><xref ref-type="bibr" rid="CR39">39</xref></sup>, which has the lowest recall rate. And in terms of Overall Accuracy, the Icnet model performs at 95.15%, while other mainstream algorithms are at a similar level. However, our algorithm achieved 97.23%, a 2.08% increase. As shown in Table&#x000a0;<xref rid="Tab2" ref-type="table">2</xref>, in terms of the FWIoU metric, among all mainstream algorithms, Icnet performs the lowest at 91.05%, while our proposed EPAnet model achieves 94.78%, surpassing the Icnet model by 3.73%.<fig id="Fig7"><label>Figure 7</label><caption><p>Diagram of mIoU comparison between EPAnet and other models. The enlarged view of the data at the tail of the experiment is shown in the center of the image.The model name and corresponding color lines are displayed on the right.</p></caption><graphic xlink:href="41598_2024_61981_Fig7_HTML" id="MO7"/></fig><fig id="Fig8"><label>Figure 8</label><caption><p>Comparison of segmentation visualization effects of original labels with EPAnet and other contrastive models.</p></caption><graphic xlink:href="41598_2024_61981_Fig8_HTML" id="MO8"/></fig></p><p id="Par44">As shown in Fig.&#x000a0;<xref rid="Fig7" ref-type="fig">7</xref>, to demonstrate the performance comparison between the proposed method and other algorithms, we employ the mIoU line graph during the training process for an intuitive comparison. Due to the involvement of multiple comparative algorithms in this experiment, to better showcase the details, we detailed subfigures in the chart. It is evident that the proposed algorithm significantly outperforms other algorithms in terms of mIoU performance, indicating that our algorithm exhibits superior segmentation results. As shown in Fig.&#x000a0;<xref rid="Fig8" ref-type="fig">8</xref>, compared to the subpar segmentation results of other models, the proposed EPAnet model excels in tackling segmentation challenges such as leaf edge detection and small object recognition. Its segmentation is very close to the actual Labels, demonstrating the superiority of the EPAnet algorithm.</p></sec></sec><sec id="Sec22"><title>Conclusions and discussions</title><p id="Par45">This paper proposes a multi-decoder architecture algorithm based on the ERFnet algorithm, suitable for weed and crop segmentation under natural conditions. Compared to existing segmentation algorithms, it delivers the best results. The method also showcases superior performance under diverse circumstances like different weather patterns, light conditions, and overlapping of leaves. The proposed coupled dual-loss function improves the model&#x02019;s focus on vital categories, achieving a higher mIoU performance. In comparison to other models, our multi-Decoder Head design captures a broader range of data and recognizes dynamic correlations among them, ensuring more precise segmentation outcomes. And compared to the baseline model, the Overall Accuracy increased by 0.65%, mIoU rose by 1.91%, and FWIoU improved by 1.19%.</p><p id="Par46">Our study demonstrates the effectiveness of our model on mung bean and weed datasets. However, its generalizability to other legume crops and weeds might be limited. Additionally, the required distance between camera and objects for optimal image quality hinders its applicability in drone-based agriculture. Future research will focus on enriching data resources by creating dedicated mung bean and weed datasets to enhance model performance. Furthermore, we aim to optimize the model for edge devices by simultaneously reducing its parameter count and improving evaluation metrics, ultimately leading to faster real-time image segmentation. We will also investigate the optimal weighting scheme for combining cross-entropy and Dice coefficient loss functions, along with exploring the potential integration of reinforcement learning to reduce reliance on labeled data.</p></sec></body><back><fn-group><fn><p><bold>Publisher's note</bold></p><p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></fn><fn><p>These authors contributed equally: Haozhang Gao and Mingyang Qi.</p></fn></fn-group><ack><title>Acknowledgements</title><p>This work was supported in part by the Jilin Province Science and Technology Development Program Project under Grant YDZJ202201ZYTS692, in part by the Digital Agriculture: An Emerging Interdisciplinary Field in Jilin Province under Grant 2018730519.</p></ack><notes notes-type="author-contribution"><title>Author contributions</title><p>Conceptualization, H.G., M.Q., and B.D.; data curation and formal analysis, H.G., and H.L.; funding acquisition, W.Z., and Y.T.; investigation, S.Y.; methodology, H.G., B.D., and H.L.; resources and software, H.G.; supervision, Y.T., and M.Q.; validation, H.G., B.D., and S.Y.; visualization, H.G., and T.W; writing original draft, H.G., M.Q., and S.Y.; writing-review and editing, H.G., B.D., S.Y.; All authors have read and agreed to the published version of the manuscript.</p></notes><notes notes-type="data-availability"><title>Data availability</title><p>The datasets used and/or analysed during the current study available from the corresponding author on reasonable request.</p></notes><notes id="FPar1" notes-type="COI-statement"><title>Competing interests</title><p id="Par47">The authors declare no competing interests.</p></notes><ref-list id="Bib1"><title>References</title><ref id="CR1"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Picon</surname><given-names>A</given-names></name><etal/></person-group><article-title>Deep learning-based segmentation of multiple species of weeds and corn crop using synthetic and real image datasets</article-title><source>Comput. Electron. Agric.</source><year>2022</year><volume>194</volume><fpage>106719</fpage><pub-id pub-id-type="doi">10.1016/j.compag.2022.106719</pub-id></element-citation></ref><ref id="CR2"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Singh</surname><given-names>N</given-names></name><name><surname>Sharma</surname><given-names>AK</given-names></name><name><surname>Sarkar</surname><given-names>I</given-names></name><name><surname>Prabhu</surname><given-names>S</given-names></name><name><surname>Chadaga</surname><given-names>K</given-names></name></person-group><article-title>Iot-based greenhouse technologies for enhanced crop production: A comprehensive study of monitoring, control, and communication techniques</article-title><source>Syst. Sci. Control Eng.</source><year>2024</year><volume>12</volume><fpage>2306825</fpage><pub-id pub-id-type="doi">10.1080/21642583.2024.2306825</pub-id></element-citation></ref><ref id="CR3"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>You</surname><given-names>J</given-names></name><name><surname>Liu</surname><given-names>W</given-names></name><name><surname>Lee</surname><given-names>J</given-names></name></person-group><article-title>A DNN-based semantic segmentation for detecting weed and crop</article-title><source>Comput. Electron. Agric.</source><year>2020</year><volume>178</volume><fpage>105750</fpage><pub-id pub-id-type="doi">10.1016/j.compag.2020.105750</pub-id></element-citation></ref><ref id="CR4"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Slaughter</surname><given-names>DC</given-names></name><name><surname>Giles</surname><given-names>D</given-names></name><name><surname>Downey</surname><given-names>D</given-names></name></person-group><article-title>Autonomous robotic weed control systems: A review</article-title><source>Comput. Electron. Agric.</source><year>2008</year><volume>61</volume><fpage>63</fpage><lpage>78</lpage><pub-id pub-id-type="doi">10.1016/j.compag.2007.05.008</pub-id></element-citation></ref><ref id="CR5"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>B</given-names></name><etal/></person-group><article-title>Segvit: Semantic segmentation with plain vision transformers</article-title><source>Adv. Neural Inf. Process. Syst.</source><year>2022</year><volume>35</volume><fpage>4971</fpage><lpage>4982</lpage></element-citation></ref><ref id="CR6"><label>6.</label><mixed-citation publication-type="other">Strudel, R., Garcia, R., Laptev, I. &#x00026; Schmid, C. Segmenter: Transformer for semantic segmentation. In <italic>Proceedings of the IEEE/CVF International Conference on Computer Vision</italic>. 7262&#x02013;7272 (2021).</mixed-citation></ref><ref id="CR7"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sun</surname><given-names>J</given-names></name><etal/></person-group><article-title>Masa-segnet: A semantic segmentation network for polsar images</article-title><source>Remote Sens.</source><year>2023</year><volume>15</volume><fpage>3662</fpage><pub-id pub-id-type="doi">10.3390/rs15143662</pub-id></element-citation></ref><ref id="CR8"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ibragimov</surname><given-names>A</given-names></name><etal/></person-group><article-title>Deep semantic segmentation of angiogenesis images</article-title><source>Int. J. Mol. Sci.</source><year>2023</year><volume>24</volume><fpage>1102</fpage><pub-id pub-id-type="doi">10.3390/ijms24021102</pub-id><?supplied-pmid 36674617?><pub-id pub-id-type="pmid">36674617</pub-id>
</element-citation></ref><ref id="CR9"><label>9.</label><mixed-citation publication-type="other">Long, J., Shelhamer, E. &#x00026; Darrell, T. Fully convolutional networks for semantic segmentation. In <italic>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</italic>. 3431&#x02013;3440 (2015).</mixed-citation></ref><ref id="CR10"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vayssade</surname><given-names>J-A</given-names></name><name><surname>Jones</surname><given-names>G</given-names></name><name><surname>G&#x000e9;e</surname><given-names>C</given-names></name><name><surname>Paoli</surname><given-names>J-N</given-names></name></person-group><article-title>Pixelwise instance segmentation of leaves in dense foliage</article-title><source>Comput. Electron. Agricult.</source><year>2022</year><volume>195</volume><fpage>106797</fpage><pub-id pub-id-type="doi">10.1016/j.compag.2022.106797</pub-id></element-citation></ref><ref id="CR11"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>L-C</given-names></name><name><surname>Papandreou</surname><given-names>G</given-names></name><name><surname>Kokkinos</surname><given-names>I</given-names></name><name><surname>Murphy</surname><given-names>K</given-names></name><name><surname>Yuille</surname><given-names>AL</given-names></name></person-group><article-title>Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFS</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2017</year><volume>40</volume><fpage>834</fpage><lpage>848</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2017.2699184</pub-id><?supplied-pmid 28463186?><pub-id pub-id-type="pmid">28463186</pub-id>
</element-citation></ref><ref id="CR12"><label>12.</label><mixed-citation publication-type="other">Li, X. <italic>et&#x000a0;al.</italic> Improving semantic segmentation via decoupled body and edge supervision. In <italic>Computer Vision&#x02014;ECCV 2020: 16th European Conference, Glasgow, UK, August 23&#x02013;28, 2020, Proceedings, Part XVII 16</italic>. 435&#x02013;452 (Springer, 2020).</mixed-citation></ref><ref id="CR13"><label>13.</label><mixed-citation publication-type="other">Huang, Z. <italic>et&#x000a0;al.</italic> Ccnet: Criss-cross attention for semantic segmentation. In <italic>Proceedings of the IEEE/CVF International Conference on Computer Vision</italic>. 603&#x02013;612 (2019).</mixed-citation></ref><ref id="CR14"><label>14.</label><mixed-citation publication-type="other">Li, X. <italic>et&#x000a0;al.</italic> Global aggregation then local distribution in fully convolutional networks. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1909.07229">arXiv:1909.07229</ext-link> (2019).</mixed-citation></ref><ref id="CR15"><label>15.</label><mixed-citation publication-type="other">Noh, H., Hong, S. &#x00026; Han, B. Learning deconvolution network for semantic segmentation. In <italic>Proceedings of the IEEE International Conference on Computer Vision</italic>. 1520&#x02013;1528 (2015).</mixed-citation></ref><ref id="CR16"><label>16.</label><mixed-citation publication-type="other">Ronneberger, O., Fischer, P. &#x00026; Brox, T. U-net: Convolutional networks for biomedical image segmentation. In <italic>Medical Image Computing and Computer-Assisted Intervention&#x02014;MICCAI 2015: 18th International Conference, Munich, Germany, October 5&#x02013;9, 2015, Proceedings, Part III 18</italic>. 234&#x02013;241 (Springer, 2015).</mixed-citation></ref><ref id="CR17"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>J</given-names></name><name><surname>Zhang</surname><given-names>X</given-names></name><name><surname>Yan</surname><given-names>T</given-names></name><name><surname>Tan</surname><given-names>A</given-names></name></person-group><article-title>Dpnet: Dual-pyramid semantic segmentation network based on improved deeplabv3 plus</article-title><source>Electronics</source><year>2023</year><volume>12</volume><fpage>3161</fpage><pub-id pub-id-type="doi">10.3390/electronics12143161</pub-id></element-citation></ref><ref id="CR18"><label>18.</label><mixed-citation publication-type="other">Zhao, H., Shi, J., Qi, X., Wang, X. &#x00026; Jia, J. Pyramid scene parsing network. In <italic>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</italic>. 2881&#x02013;2890 (2017).</mixed-citation></ref><ref id="CR19"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cheng</surname><given-names>S</given-names></name><name><surname>Li</surname><given-names>B</given-names></name><name><surname>Sun</surname><given-names>L</given-names></name><name><surname>Chen</surname><given-names>Y</given-names></name></person-group><article-title>Hrrnet: Hierarchical refinement residual network for semantic segmentation of remote sensing images</article-title><source>Remote Sens.</source><year>2023</year><volume>15</volume><fpage>1244</fpage><pub-id pub-id-type="doi">10.3390/rs15051244</pub-id></element-citation></ref><ref id="CR20"><label>20.</label><mixed-citation publication-type="other">Jiang, Y. &#x00026; Li, C. Convolutional neural networks for image-based high-throughput plant phenotyping: A review. Plant Phenom. <pub-id pub-id-type="doi">10.34133/2020/4152816</pub-id> (2020)</mixed-citation></ref><ref id="CR21"><label>21.</label><mixed-citation publication-type="other">Ghazi, M.&#x000a0;M., Yanikoglu, B., Aptoula, E., Muslu, O. &#x00026; Ozdemir, M.&#x000a0;C. Sabanci-okan system in lifeclef 2015 plant identification competition. In <italic>Working Notes of CLEF 2015 Conference</italic> (2015).</mixed-citation></ref><ref id="CR22"><label>22.</label><mixed-citation publication-type="other">Morris, D. A pyramid CNN for dense-leaves segmentation. In <italic>2018 15th Conference on Computer and Robot Vision (CRV)</italic>. 238&#x02013;245 (IEEE, 2018).</mixed-citation></ref><ref id="CR23"><label>23.</label><mixed-citation publication-type="other">Kuznichov, D., Zvirin, A., Honen, Y. &#x00026; Kimmel, R. Data augmentation for leaf segmentation and counting tasks in rosette plants. In <italic>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</italic> (2019).</mixed-citation></ref><ref id="CR24"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Quideau</surname><given-names>S</given-names></name><name><surname>Deffieux</surname><given-names>D</given-names></name><name><surname>Douat-Casassus</surname><given-names>C</given-names></name><name><surname>Pouys&#x000e9;gu</surname><given-names>L</given-names></name></person-group><article-title>Plant polyphenols: Chemical properties, biological activities, and synthesis</article-title><source>Angew. Chem. Int. Ed.</source><year>2011</year><volume>50</volume><fpage>586</fpage><lpage>621</lpage><pub-id pub-id-type="doi">10.1002/anie.201000044</pub-id></element-citation></ref><ref id="CR25"><label>25.</label><mixed-citation publication-type="other">Jiang, Y. &#x00026; Li, C. Convolutional neural networks for image-based high-throughput plant phenotyping: A review. Plant phenomics 2020. <italic>Sci. Partner J.</italic>10.34133/2020/4152816 (2020).</mixed-citation></ref><ref id="CR26"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zheng</surname><given-names>L</given-names></name><name><surname>Zhang</surname><given-names>J</given-names></name><name><surname>Wang</surname><given-names>Q</given-names></name></person-group><article-title>Mean-shift-based color segmentation of images containing green vegetation</article-title><source>Comput. Electron. Agric.</source><year>2009</year><volume>65</volume><fpage>93</fpage><lpage>98</lpage><pub-id pub-id-type="doi">10.1016/j.compag.2008.08.002</pub-id></element-citation></ref><ref id="CR27"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Omrani</surname><given-names>E</given-names></name><etal/></person-group><article-title>Potential of radial basis function-based support vector regression for apple disease detection</article-title><source>Measurement</source><year>2014</year><volume>55</volume><fpage>512</fpage><lpage>519</lpage><pub-id pub-id-type="doi">10.1016/j.measurement.2014.05.033</pub-id></element-citation></ref><ref id="CR28"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Praveen Kumar</surname><given-names>J</given-names></name><name><surname>Domnic</surname><given-names>S</given-names></name></person-group><article-title>Rosette plant segmentation with leaf count using orthogonal transform and deep convolutional neural network</article-title><source>Mach. Vis. Appl.</source><year>2020</year><volume>31</volume><fpage>6</fpage><pub-id pub-id-type="doi">10.1007/s00138-019-01056-2</pub-id></element-citation></ref><ref id="CR29"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lian</surname><given-names>Z</given-names></name><etal/></person-group><article-title>Deep-fel: Decentralized, efficient and privacy-enhanced federated edge learning for healthcare cyber physical systems</article-title><source>IEEE Trans. Netw. Sci. Eng.</source><year>2022</year><volume>9</volume><fpage>3558</fpage><lpage>3569</lpage><pub-id pub-id-type="doi">10.1109/TNSE.2022.3175945</pub-id></element-citation></ref><ref id="CR30"><label>30.</label><mixed-citation publication-type="other">Zhao, H. <italic>et&#x000a0;al.</italic> Psanet: Point-wise spatial attention network for scene parsing. In <italic>Proceedings of the European Conference on Computer Vision (ECCV)</italic>. 267&#x02013;283 (2018).</mixed-citation></ref><ref id="CR31"><label>31.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Romera</surname><given-names>E</given-names></name><name><surname>Alvarez</surname><given-names>JM</given-names></name><name><surname>Bergasa</surname><given-names>LM</given-names></name><name><surname>Arroyo</surname><given-names>R</given-names></name></person-group><article-title>Erfnet: Efficient residual factorized convnet for real-time semantic segmentation</article-title><source>IEEE Trans. Intell. Transport. Syst.</source><year>2017</year><volume>19</volume><fpage>263</fpage><lpage>272</lpage><pub-id pub-id-type="doi">10.1109/TITS.2017.2750080</pub-id></element-citation></ref><ref id="CR32"><label>32.</label><mixed-citation publication-type="other">Yang, L., Zhang, R.-Y., Li, L. &#x00026; Xie, X. Simam: A simple, parameter-free attention module for convolutional neural networks. In <italic>International Conference on Machine Learning</italic>. 11863&#x02013;11874 (PMLR, 2021).</mixed-citation></ref><ref id="CR33"><label>33.</label><mixed-citation publication-type="other">Cheng, H.&#x000a0;K., Chung, J., Tai, Y.-W. &#x00026; Tang, C.-K. Cascadepsp: Toward class-agnostic and very high-resolution segmentation via global and local refinement. In <italic>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</italic>. 8890&#x02013;8899 (2020).</mixed-citation></ref><ref id="CR34"><label>34.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weng</surname><given-names>W</given-names></name><name><surname>Zhu</surname><given-names>X</given-names></name><name><surname>Jing</surname><given-names>L</given-names></name><name><surname>Dong</surname><given-names>M</given-names></name></person-group><article-title>Attention mechanism trained with small datasets for biomedical image segmentation</article-title><source>Electronics</source><year>2023</year><volume>12</volume><fpage>682</fpage><pub-id pub-id-type="doi">10.3390/electronics12030682</pub-id></element-citation></ref><ref id="CR35"><label>35.</label><mixed-citation publication-type="other">Fukui, H., Hirakawa, T., Yamashita, T. &#x00026; Fujiyoshi, H. Attention branch network: Learning of attention mechanism for visual explanation. In <italic>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</italic>. 10705&#x02013;10714 (2019).</mixed-citation></ref><ref id="CR36"><label>36.</label><mixed-citation publication-type="other">Bahdanau, D., Cho, K. &#x00026; Bengio, Y. Neural machine translation by jointly learning to align and translate. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1409.0473">arXiv:1409.0473</ext-link> (2014).</mixed-citation></ref><ref id="CR37"><label>37.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>Y</given-names></name><name><surname>Shen</surname><given-names>J</given-names></name><name><surname>Yang</surname><given-names>L</given-names></name><name><surname>Bian</surname><given-names>G</given-names></name><name><surname>Yu</surname><given-names>H</given-names></name></person-group><article-title>Resdo-unet: A deep residual network for accurate retinal vessel segmentation from fundus images</article-title><source>Biomed. Signal Process. Control</source><year>2023</year><volume>79</volume><fpage>104087</fpage><pub-id pub-id-type="doi">10.1016/j.bspc.2022.104087</pub-id></element-citation></ref><ref id="CR38"><label>38.</label><mixed-citation publication-type="other">Poudel, R.&#x000a0;P., Liwicki, S. &#x00026; Cipolla, R. Fast-SCNN: Fast semantic segmentation network. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1902.04502">arXiv:1902.04502</ext-link> (2019).</mixed-citation></ref><ref id="CR39"><label>39.</label><mixed-citation publication-type="other">Zhao, H., Qi, X., Shen, X., Shi, J. &#x00026; Jia, J. Icnet for real-time semantic segmentation on high-resolution images. In <italic>Proceedings of the European Conference on Computer Vision (ECCV)</italic>. 405&#x02013;420 (2018).</mixed-citation></ref></ref-list></back></article>