<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd"> 
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><?properties open_access?><?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?><?DTDIdentifier.IdentifierType public?><?SourceDTD.DTDName A++V2.4.dtd?><?SourceDTD.Version 2.4?><?ConverterInfo.XSLTName springer2nlmx2.xsl?><?ConverterInfo.Version 1?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sci Rep</journal-id><journal-id journal-id-type="iso-abbrev">Sci Rep</journal-id><journal-title-group><journal-title>Scientific Reports</journal-title></journal-title-group><issn pub-type="epub">2045-2322</issn><publisher><publisher-name>Nature Publishing Group UK</publisher-name><publisher-loc>London</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmcid">11111678</article-id><article-id pub-id-type="pmid">38778219</article-id>
<article-id pub-id-type="publisher-id">62331</article-id><article-id pub-id-type="doi">10.1038/s41598-024-62331-5</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Polyp segmentation based on implicit edge-guided cross-layer fusion networks</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Liu</surname><given-names>Junqing</given-names></name><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Zhang</surname><given-names>Weiwei</given-names></name><address><email>youn_xxt@163.com</email></address><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author"><name><surname>Liu</surname><given-names>Yong</given-names></name><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author"><name><surname>Zhang</surname><given-names>Qinghe</given-names></name><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff2">2</xref></contrib><aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/0419nfc77</institution-id><institution-id institution-id-type="GRID">grid.254148.e</institution-id><institution-id institution-id-type="ISNI">0000 0001 0033 6389</institution-id><institution>Hubei Engineering and Technology Research Center for Construction Quality Inspection Equipment, </institution><institution>China Three Gorges University, </institution></institution-wrap>Yichang, 443002 Hubei People&#x02019;s Republic of China </aff><aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/0419nfc77</institution-id><institution-id institution-id-type="GRID">grid.254148.e</institution-id><institution-id institution-id-type="ISNI">0000 0001 0033 6389</institution-id><institution>College of Computer and Information Technology, </institution><institution>China Three Gorges University, </institution></institution-wrap>Yichang, 443002 Hubei People&#x02019;s Republic of China </aff></contrib-group><pub-date pub-type="epub"><day>22</day><month>5</month><year>2024</year></pub-date><pub-date pub-type="pmc-release"><day>22</day><month>5</month><year>2024</year></pub-date><pub-date pub-type="collection"><year>2024</year></pub-date><volume>14</volume><elocation-id>11678</elocation-id><history><date date-type="received"><day>9</day><month>10</month><year>2023</year></date><date date-type="accepted"><day>15</day><month>5</month><year>2024</year></date></history><permissions><copyright-statement>&#x000a9; The Author(s) 2024</copyright-statement><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p></license></permissions><abstract id="Abs1"><p id="Par1">Polyps are abnormal tissue clumps growing primarily on the inner linings of the gastrointestinal tract. While such clumps are generally harmless, they can potentially evolve into pathological tumors, and thus require long-term observation and monitoring. Polyp segmentation in gastrointestinal endoscopy images is an important stage for polyp monitoring and subsequent treatment. However, this segmentation task faces multiple challenges: the low contrast of the polyp boundaries, the varied polyp appearance, and the co-occurrence of multiple polyps. So, in this paper, an implicit edge-guided cross-layer fusion network (IECFNet) is proposed for polyp segmentation. The codec pair is used to generate an initial saliency map, the implicit edge-enhanced context attention module aggregates the feature graph output from the encoding and decoding to generate the rough prediction, and the multi-scale feature reasoning module is used to generate final predictions. Polyp segmentation experiments have been conducted on five popular polyp image datasets (Kvasir, CVC-ClinicDB, ETIS, CVC-ColonDB, and CVC-300), and the experimental results show that the proposed method significantly outperforms a conventional method, especially with an accuracy margin of 7.9% on the ETIS dataset.</p></abstract><kwd-group xml:lang="en"><title>Keywords</title><kwd>Polyp segmentation</kwd><kwd>Implicit edge</kwd><kwd>Feature fusion</kwd><kwd>Multi-scale feature reasoning</kwd></kwd-group><kwd-group kwd-group-type="npg-subject"><title>Subject terms</title><kwd>Colonoscopy</kwd><kwd>Image processing</kwd></kwd-group><funding-group><award-group><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001809</institution-id><institution>National Natural Science Foundation of China</institution></institution-wrap></funding-source><award-id>62371271</award-id><principal-award-recipient><name><surname>Zhang</surname><given-names>Qinghe</given-names></name></principal-award-recipient></award-group></funding-group><custom-meta-group><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#x000a9; Springer Nature Limited 2024</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1"><title>Introduction</title><p id="Par2">Medical image segmentation is one of the key stages in medical image analysis, where regions of interest (such as tumors, organs, blood vessels, and other structures) are identified in medical image data<sup><xref ref-type="bibr" rid="CR1">1</xref></sup>. Segmentation is particularly important in locating polyps, which are abnormal tissue clumps that grow on mucosal surfaces of the human body. While polyps are mostly benign, some of them might be cancerous, and so long-term regular polyp monitoring is necessary. This monitoring process crucially depends on the accuracy of polyp segmentation for early diagnosis of polyp diseases<sup><xref ref-type="bibr" rid="CR2">2</xref></sup>.</p><p id="Par3">The main components of colon polyps are the intestinal mucosa, submucosa and muscularis propria, which are usually bounded by the surrounding normal mucosa. Depending on the tissue structure and morphology, colon polyps can be divided into different types, of which the most common is adenomatous polyp. Adenomatous polyps are formed by the proliferation of glandular epithelial cells and may sometimes develop into malignant lesions, which are one of the main precursor lesions of colorectal cancer. Colonoscopy allows the physician to directly visualize mucosa, blood vessels, lesions and foreign bodies in the colon, and to perform biopsies or resections to obtain tissue samples for pathology. Accurate polyp segmentation is a challenging task for two main reasons: (I) polyps show wide variability in size and color; (II) the boundaries between polyps and their surrounding mucosa are quite blurry and of low contrast<sup><xref ref-type="bibr" rid="CR2">2</xref></sup>.</p><p id="Par4">Most existing attention-based segmentation methods are designed to enhance the model's attention capability and flexibility<sup><xref ref-type="bibr" rid="CR3">3</xref></sup>. However, the adoption of attention mechanism methods can lead to problems such as high computational complexity, poor generalization ability, overfitting risks, and sensitivity to data skewness. Zhou et al.<sup><xref ref-type="bibr" rid="CR4">4</xref></sup> utilized a context auto-regressive attention mechanism to address these issues. By combining the context auto-regression method, the model could better consider previously generated structural information during neural architecture search. They integrated full attention and context auto-regression to construct a full attention-based neural architecture search framework, significantly increasing the computational complexity of the model. However, the introduction of full attention mechanism also leads to the problem of the model's over-reliance on local information, thereby increasing the risk of overfitting, which becomes more prominent when training data is insufficient or noisy. Tan et al.<sup><xref ref-type="bibr" rid="CR5">5</xref></sup> proposed the EfficientDet V2 model, which adopted a self-attention mechanism to enhance the model's attention capability on input features and reduce redundant computations. EfficientDet V2 introduced self-attention mechanism to dynamically adjust the feature correlations between different positions. The introduction of self-attention modules increases the training difficulty of the model, as additional parameter tuning and hyperparameter adjustments are required to ensure model stability and convergence. This paper addresses these issues by improving the context attention mechanism through edge guidance, solving the problem of the model's over-reliance on local information. It also utilizes a multi-scale inference module to detect fused multi-scale image features, thereby improving the model's generalization and stability.</p><p id="Par5">In recent years, deep-learning-based methods have led to significant progress in polyp segmentation<sup><xref ref-type="bibr" rid="CR4">4</xref></sup>. These methods use deep neural networks to learn more discriminative feature representations from endoscopic polyp images. However, since bounding-box detectors are usually used for polyp detection, polyp boundaries can&#x02019;t be accurately located. To address this issue, Brandao et al.<sup><xref ref-type="bibr" rid="CR6">6</xref></sup> used fully convolutional networks (FCN) with pre-trained models to identify and segment polyps. Qadir et al.<sup><xref ref-type="bibr" rid="CR7">7</xref></sup> proposed a method utilizing Fully Convolutional Neural Networks (FCNNs) to predict 2D Gaussian shapes, aiming to achieve faster detection speeds by employing the FCNN model for polyp detection. Subsequently, good polyp segmentation performance was achieved by a U-Net<sup><xref ref-type="bibr" rid="CR8">8</xref></sup> architecture, which mainly consisted of a contracting path to capture context and a symmetric expanding path for precise localization. However, these methods focus on segmenting the entire polyp regions, but ignore the region boundary constraints. So, region and boundary constraints were jointly utilized in Psi-Net<sup><xref ref-type="bibr" rid="CR9">9</xref></sup> for polyp segmentation, but still the region-boundary relations were not fully exploited. The PolypSegNet<sup><xref ref-type="bibr" rid="CR10">10</xref></sup> proposed by Mahmud et al. focuses on introducing an improved encoder-decoder architecture for automating the segmentation of polyps from colonoscopy images. Guo et al.<sup><xref ref-type="bibr" rid="CR11">11</xref></sup> proposed a confidence-aware resampling method aimed at addressing non-equivalent images and pixels issues in polyp segmentation tasks. Through meta-learning mixup techniques, the method aims to enhance the model's generalization across different samples.In addition, Fan et al<italic>.</italic><sup><xref ref-type="bibr" rid="CR2">2</xref></sup> proposed the parallel reverse attention network (PraNet) model based on the deep salient object detection network proposed by Chen et al<italic>.</italic><sup><xref ref-type="bibr" rid="CR12">12</xref></sup>. While the PraNet model has generally demonstrated remarkable segmentation performance, its effectiveness in solving multi-scale problems is still limited.</p><p id="Par6">To address the limitations of the aforementioned polyp segmentation methods, a new implicit-edge-guided cross-layer fusion network is introduced in this paper. This network focuses on uncertain regions of saliency that are highly correlated with polyp boundaries, and these saliency regions are used as attention maps in the proposed network to extract refined low-level features. Finally, multi-scale feature reasoning is employed to detect and fuse different multi-scale image features, and thereby obtain final polyp segmentation outcomes. The key contributions of this paper are as follows:<list list-type="simple"><list-item><label>i.</label><p id="Par7">A new deep network model is proposed for polyp segmentation. This model enhances the segmentation outcomes by effectively exploiting global contextual information, cross-level feature fusion, low-level feature refinement, and multi-scale feature inference.</p></list-item><list-item><label>ii.</label><p id="Par8">In order to expand the spatial receptive field of the backbone network, an attention encoding&#x02013;decoding pair is proposed for the receptive-field coordinates.</p></list-item><list-item><label>iii.</label><p id="Par9">To compensate for the absence of explicit shape boundary information, an implicit-edge-enhanced contextual attention module is designed based on multi-headed self-attention and edge information.</p></list-item><list-item><label>iv.</label><p id="Par10">A multi-scale feature reasoning module is proposed to refine the low-level features with the rough prediction maps obtained from the high-level fused features, and thereby obtain final segmentation outcomes.</p></list-item></list></p><p id="Par11">The remainder of this paper is organized as follows. Firstly, related work on automated polyp segmentation methods is briefly reviewed in &#x0201c;<xref rid="Sec2" ref-type="sec">Related work</xref>&#x0201d; section. Then, the proposed polyp segmentation model and each of its modules are explained in detail in &#x0201c;<xref rid="Sec6" ref-type="sec">Method</xref>&#x0201d; section. Thus, &#x0201c;<xref rid="Sec11" ref-type="sec">Experimental setup and results</xref>&#x0201d; section highlights the experimental setup and the results of the experiments, an ablation study, and comparative analysis. Finally, conclusions are made in &#x0201c;<xref rid="Sec19" ref-type="sec">Conclusion</xref>&#x0201d; section.</p></sec><sec id="Sec2"><title>Related work</title><p id="Par12">In this section, we briefly review the literature on existing related methods of semantic segmentation, salient object detection, and context-aware deep learning.</p><sec id="Sec3"><title>Semantic segmentation</title><p id="Par13">In a semantic segmentation task, each image pixel should be labelled with the most likely semantic class. With the recent emergence of deep learning methods, these methods have gradually become the mainstream ones for semantic segmentation. For example, U-Net is a semantic segmentation model based on convolutional neural networks. This model essentially employs a symmetric encoder-decoder structure and introduces jump connections to boost segmentation performance. In addition, a mask R-CNN<sup><xref ref-type="bibr" rid="CR13">13</xref>,<xref ref-type="bibr" rid="CR14">14</xref></sup> jointly detects objects and performs semantic segmentation. A dual attention network<sup><xref ref-type="bibr" rid="CR15">15</xref></sup> employs a self-attention mechanism and a spatial-channel dual-branch network for local and global feature fusion. EfficientNet is an efficient neural network architecture that achieves good performance in semantic segmentation by scaling the network width, depth, and resolution, even when computational resources are limited. HRNet is a multi-scale, high-resolution neural network structure. It maintains information flow at various resolutions by parallelly connecting multiple feature maps and constructs dense feature representations at each resolution. This design enables HRNet to effectively capture semantic information at different scales, leading to significant performance improvements in tasks like image segmentation tasks.</p></sec><sec id="Sec4"><title>Salient object detection</title><p id="Par14">Instead of locating and classifying entire image regions, salient object detection (SOD)<sup><xref ref-type="bibr" rid="CR16">16</xref></sup> focuses on identifying the most important target objects or regions. Unlike semantic segmentation, SOD does not employ simple powerful baseline models. Instead, the state-of-the-art SOD approaches use object boundary regions as supplementary information to improve the saliency estimation quality through multi-task learning strategies. One of the most prominent SOD approaches employs an edge-guided network (EGNet)<sup><xref ref-type="bibr" rid="CR17">17</xref></sup>, where a bottom-up edge detection branch and a side-out fusion strategy are used towards top-down aggregation of salient object branches. Alternatively, a boundary-aware network (BANet)<sup><xref ref-type="bibr" rid="CR18">18</xref></sup> performs side-out fusion on boundary branches, while only a single stream is used for object branches. However, BANet does not treat edge detection as a separate task, but rather combines edge and target detection results for saliency map generation. All these methods led to competitive experimental results, and thus demonstrated the usefulness of edge guidance for obtaining reliable object representations. However, the complexity of edge detection is generally high, and edge detectors (such as the Canny edge detector<sup><xref ref-type="bibr" rid="CR6">6</xref></sup>) usually produce redundant edges that are unrelated to the object of interest. For more accurate segmentation, self-attention<sup><xref ref-type="bibr" rid="CR12">12</xref></sup> considers predicted inverse regions and captures saliency details.</p><p id="Par15">The above approaches inspired the following intuitive idea: without explicit edge guidance, edge-related contextual information can be alternately obtained from saliency maps. To realize this idea, we create uncertainty regions without explicit edge information and design a reverse significance plot with additional implicit edge regions. Our approach does not favor neither foreground nor background implicit regions, and thus leads to effective acquisition of edge-related contextual information. In the absence of explicit edge information, we thus define uncertain regions and design reverse saliency maps with implicit edge regions.</p></sec><sec id="Sec5"><title>Contextual awareness</title><p id="Par16">Contextual information can lead to significantly enhanced feature representations, and hence this type of information can play a crucial role in boosting object segmentation performance. For instance, Zhao et al.<sup><xref ref-type="bibr" rid="CR19">19</xref></sup> proposed the PSPNet architecture, which establishes a multi-scale representation around each image pixel to get rich contextual information. Chen et al.<sup><xref ref-type="bibr" rid="CR20">20</xref></sup> constructed ASPP with different dilated convolutions to capture essential contextual information. In addition, rich contextual information has been obtained through self-attention mechanisms, including those used in DANet <sup><xref ref-type="bibr" rid="CR15">15</xref></sup> and CCNet<sup><xref ref-type="bibr" rid="CR21">21</xref></sup>. The former uses non-local modules to extract contextual information<sup><xref ref-type="bibr" rid="CR22">22</xref></sup>, while the latter uses multiple cascaded cross-attention modules to obtain dense contextual information. In addition, contextual information has been also heavily exploited for target segmentation. For example, Zhang et al.<sup><xref ref-type="bibr" rid="CR23">23</xref></sup>used multi-scale context-aware modules to extract rich contextual features. As well, Liu et al.<sup><xref ref-type="bibr" rid="CR24">24</xref></sup> proposed PoolNet, a deep architecture for salient object detection based on highly relevant contextual features extracted using a pyramid structure. Furthermore, Chen et al.<sup><xref ref-type="bibr" rid="CR25">25</xref></sup> proposed an enhanced global context-aware segmentation method in which features containing global semantic information are transformed into multi-layer features at different stages.</p></sec></sec><sec id="Sec6"><title>Method</title><p id="Par17">In this section, the proposed IECFNet framework is holistically introduced, and then the details of its three major modules are given.</p><sec id="Sec7"><title>Overall architecture</title><p id="Par18">As shown in Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>, IECFNet consists of a backbone network as well as upper and lower hierarchical networks. In particular, a Res2Net<sup><xref ref-type="bibr" rid="CR26">26</xref></sup> backbone network is used to extract multi-scale features f<sub><italic>i</italic></sub> (<italic>i</italic>&#x02009;=&#x02009;1, 2, &#x02026;, 5) from the input images. The lower cascade network gradually obtains more refined saliency maps (P<sub><italic>1</italic></sub>&#x02009;&#x02192;&#x02009;P<sub><italic>2</italic></sub>&#x02009;&#x02192;&#x02009;P<sub><italic>3</italic></sub>) from the bottom up. The obtained maps are thus used in the upper fusion network to get refined lower-level features. The upper fusion network first performs multi-scale feature fusion, and then carries out multi-scale feature inference to produce the final segmentation outcomes. Constructing the proposed IECFNet architecture involves the design of several modules: the receptive-field coordinate attention encoder (RFCA-e), the receptive-field coordinate attention decoder (RFCA-d), the implicit edge-enhanced context attention (IECA) module, and the multi-scale feature reasoning (MSFR) module.<fig id="Fig1"><label>Figure 1</label><caption><p>IECFNet Overall Architecture, RFCA-d and RFCA-e are used to reduce the number of channels in the input feature map, IECA implements cross-layer fusion and MSFR implements multi-scale feature inference.</p></caption><graphic xlink:href="41598_2024_62331_Fig1_HTML" id="MO1"/></fig></p><p id="Par19">As shown in Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>, for the lower cascade network, the feature maps generated by the high-level backbone network are introduced into the RFCA-e modules. These modules not only expand the network receptive field, but also help reduce the computing cost by reducing the number of feature map channels. Specifically, the feature maps of the three RFCA-e modules are connected and fed to the RFCA-d module. From the bottom to the top, the RFCA-d module in the first layer of the lower cascade network predicts the initial polyp significance map (<italic>P</italic><sub>1</sub>). In the second layer, the output feature maps from the RFCA-e and RFCA-d modules are connected and input to IECA, and <italic>P</italic><sub>1</sub> is used as contextual information to generate the significance map P2, which represents further contextual information. The feature map f4 generated by the fourth layer of the backbone network is input to the RFCA-e module, whose output is connected with the IECA feature map in the second layer. The convolution results are input to the IECA map in the third layer to generate the final coarse segmentation map P3. The resulting <italic>P</italic><sub>1</sub>, <italic>P</italic><sub>2</sub>, and <italic>P</italic><sub>3</sub> are compared with the Ground Truth. Binary Cross Entropy loss and Intersection over Union loss are employed, and the three calculated losses are aggregated to obtain the average value. Utilizing the average loss enables more accurate model training and accelerates convergence speed through regression calculations.</p><p id="Par20">Furthermore, bilinear up-sampling is performed on <italic>P</italic><sub>3</sub>, and the result (<italic>f</italic><sub><italic>d</italic></sub>) is sent to the upper fusion network for low-level feature refinement. Specifically, <italic>f</italic><sub><italic>d</italic></sub> is multiplied with the feature maps f1, f2 and f3 respectively, and the results are sent to three receptive field blocks (RFB). Since the size of <italic>f</italic><sub>3</sub> is half of those of <italic>f</italic><sub>1</sub> and <italic>f</italic><sub>2</sub>, it is necessary to upsample the feature map obtained after passing <italic>f</italic><sub>3</sub>&#x02297;<italic>f</italic><sub><italic>d</italic></sub> through the RFB module. Then, the output is connected with the feature map obtained after passing <italic>f</italic><sub>2</sub>&#x02297;<italic>f</italic><sub><italic>d</italic></sub> through the RFB module. Similarly, the result is connected with the feature map obtained after passing <italic>f</italic><sub>1</sub>&#x02297;<italic>f</italic><sub><italic>d</italic></sub> through the RFB module. Finally, the result is sent to the MSFR module to get the final segmentation map. The details of each of the above modules are described separately below.</p></sec><sec id="Sec8"><title>Receptive-field coordinate attention encode and decoder pair</title><p id="Par21">In deep learning network models, context modules are beneficial for extracting fine-grained feature maps with high-level semantic information and low-level details. In particular, context can be essentially accounted for through self-attention mechanisms, but such mechanisms are computationally intensive. However, receptive-field coordinate attention (RFCA)<sup><xref ref-type="bibr" rid="CR27">27</xref></sup> can reduce the computational cost to a certain extent through performing and composing non-local operations on coordinate pairs.</p><p id="Par22">Inspired by the coordinate attention mechanism, a new coordinate attention encoder is proposed (as shown in Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref>) based on the RFB design proposed by Song et al.<sup><xref ref-type="bibr" rid="CR28">28</xref></sup>.<fig id="Fig2"><label>Figure 2</label><caption><p>Receptive-field coordinate attention encode network structure.</p></caption><graphic xlink:href="41598_2024_62331_Fig2_HTML" id="MO2"/></fig></p><p id="Par23">Previous studies have shown that RFCA can enhance the expressiveness of learned features in mobile networks. As shown in Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref>, RFCA-e aggregates low-level feature maps for bottom-up streaming, but this will inevitably increase the number of model parameters and computational complexity.</p><p id="Par24">To reduce this complexity, the number of channels should be reduced without losing information details. Therefore, the RFCA-e module achieves this by employing the RFB module, expanding the receptive field via convolutions of different scales, and exploiting feature reuse and parameter sharing.</p><p id="Par25">Figure&#x000a0;<xref rid="Fig3" ref-type="fig">3</xref> is referred to as the receptive-field coordinate attention decode network structure, it is labeled as RFCA-d.After up-sampling the RFCA-e outputs, these outputs are concatenated along the channel dimension, and then features are extracted via convolution. The obtained features are then globally refined and relatively enriched in the RFCA module. At this point, four convolutional layers are used to obtain more enhanced features. Finally, a saliency map fused with multi-scale features is obtained.<fig id="Fig3"><label>Figure 3</label><caption><p>Receptive-field coordinate attention encode network structure.</p></caption><graphic xlink:href="41598_2024_62331_Fig3_HTML" id="MO3"/></fig></p></sec><sec id="Sec9"><title>Implicit edge-enhanced context attention module</title><p id="Par26">The performance of reverse attention<sup><xref ref-type="bibr" rid="CR12">12</xref></sup> in salient target detection and polyp segmentation tasks can be improved by boundary-guided SOD networks<sup><xref ref-type="bibr" rid="CR18">18</xref>,<xref ref-type="bibr" rid="CR29">29</xref></sup>. In such networks, using target boundaries as complementary supervision information generally improves the polyp detection accuracy. Therefore, reverse attention can be used as an effective method for implicit module.</p><p id="Par27">Focusing on saliency and reverse saliency maps through reverse attention, boundaries generally appeared in areas with average significance scores in adjacent parts. Specifically, the average scores of boundary areas were around 0.5. Based on this observation, the saliency and reverse saliency maps could be assumed to have almost the same amount of edge information, and thus a simple subtraction operation can produce the reverse saliency map. Based on this idea, the implicit-edge-enhanced context attention (IECAM) module is proposed (as shown in Fig.&#x000a0;<xref rid="Fig4" ref-type="fig">4</xref>) for extracting rich semantic features without additional boundary guidance in combination with uncertain regions.<fig id="Fig4"><label>Figure 4</label><caption><p>Implicit Edge-enhanced Context Attention.</p></caption><graphic xlink:href="41598_2024_62331_Fig4_HTML" id="MO4"/></fig></p><p id="Par28">Specifically, denote the previously obtained input saliency map by m. Also, denote the corresponding foreground map, background map, and uncertain boundary region map as mf, mb, and mu, respectively. The relationships between these maps are expressed in Eqs.&#x000a0;(<xref rid="Equ1" ref-type="disp-formula">1</xref>) and (<xref rid="Equ2" ref-type="disp-formula">2</xref>) as follows:<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${m}_{f}=\text{max}\left(m-\text{0.5,0}\right),$$\end{document}</tex-math><mml:math id="M2" display="block"><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mi>f</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mtext>max</mml:mtext><mml:mfenced close=")" open="("><mml:mi>m</mml:mi><mml:mo>-</mml:mo><mml:mtext>0.5,0</mml:mtext></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:math><graphic xlink:href="41598_2024_62331_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${m}_{b}=\text{max}\left(0.5-m,0\right),$$\end{document}</tex-math><mml:math id="M4" display="block"><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mi>b</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mtext>max</mml:mtext><mml:mfenced close=")" open="("><mml:mn>0.5</mml:mn><mml:mo>-</mml:mo><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:math><graphic xlink:href="41598_2024_62331_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${m}_{u}=0.5-abs\left(m-0.5\right).$$\end{document}</tex-math><mml:math id="M6" display="block"><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mi>u</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn><mml:mo>-</mml:mo><mml:mi>a</mml:mi><mml:mi>b</mml:mi><mml:mi>s</mml:mi><mml:mfenced close=")" open="("><mml:mi>m</mml:mi><mml:mo>-</mml:mo><mml:mn>0.5</mml:mn></mml:mfenced><mml:mo>.</mml:mo></mml:mrow></mml:math><graphic xlink:href="41598_2024_62331_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula></p><p id="Par29">In Eqs.&#x000a0;(<xref rid="Equ1" ref-type="disp-formula">1</xref>) and (<xref rid="Equ2" ref-type="disp-formula">2</xref>), the foreground and background maps are calculated using maximum values, so the corresponding regions are not only separated from each other, but also from uncertain regions. However, if Eq.&#x000a0;(<xref rid="Equ3" ref-type="disp-formula">3</xref>) is used to find the uncertain boundary region map mu, redundant information can&#x02019;t be easily obtained, and the computed map would be of reduced reliability.</p><p id="Par30">Therefore, each pixel value is multiplied and summed with each corresponding pixel value in the input feature map X, and vector representations of the foreground, background, and uncertain region maps are calculated as follows,<disp-formula id="Equ4"><label>4</label><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${w}_{f}=\sum_{i\in I}{m}_{{f}_{i}}{x}_{i},$$\end{document}</tex-math><mml:math id="M8" display="block"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>f</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:munder><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mi>m</mml:mi><mml:msub><mml:mi>f</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:math><graphic xlink:href="41598_2024_62331_Article_Equ4.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ5"><label>5</label><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${w}_{b}=\sum_{i\in I}{m}_{{b}_{i}}{x}_{i},$$\end{document}</tex-math><mml:math id="M10" display="block"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>b</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:munder><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mi>m</mml:mi><mml:msub><mml:mi>b</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:math><graphic xlink:href="41598_2024_62331_Article_Equ5.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ6"><label>6</label><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${w}_{u}=\sum_{i\in I}{m}_{{u}_{i}}{x}_{i},$$\end{document}</tex-math><mml:math id="M12" display="block"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>u</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:munder><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mi>m</mml:mi><mml:msub><mml:mi>u</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:math><graphic xlink:href="41598_2024_62331_Article_Equ6.gif" position="anchor"/></alternatives></disp-formula>where <italic>i</italic>&#x02009;&#x02208;&#x02009;<italic>I</italic> denotes the image pixel. As shown in Fig.&#x000a0;<xref rid="Fig4" ref-type="fig">4</xref>, each vector represents the most typical feature vector in the feature space, so that <italic>w</italic><sub><italic>f</italic></sub> and <italic>w</italic><sub><italic>u</italic></sub> can effectively express the foreground and uncertain boundary regions. The pairwise inter-pixel similarity scores of the <italic>w</italic><sub><italic>f</italic></sub>, <italic>w</italic><sub><italic>b</italic></sub> and <italic>w</italic><sub><italic>u</italic></sub> vectors (after applying the feature map xi) are calculated as follows,<disp-formula id="Equ7"><label>7</label><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\left\{\begin{array}{c}{t}_{{f}_{i}}^{{{\prime}}}={\Psi \left({x}_{i}\right)}^{T}\varnothing \left({w}_{f}\right),\\ {t}_{{b}_{i}}^{{{\prime}}}={\Psi \left({x}_{i}\right)}^{T}\varnothing \left({w}_{b}\right),\\ {t}_{{u}_{i}}^{{{\prime}}}={\Psi \left({x}_{i}\right)}^{T}\varnothing \left({w}_{u}\right).\end{array}\right.$$\end{document}</tex-math><mml:math id="M14" display="block"><mml:mfenced open="{"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msubsup><mml:mi>t</mml:mi><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>&#x02032;</mml:mo></mml:msubsup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="normal">&#x003a8;</mml:mi><mml:mfenced close=")" open="("><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mfenced></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:mi>&#x02205;</mml:mi><mml:mfenced close=")" open="("><mml:msub><mml:mi>w</mml:mi><mml:mi>f</mml:mi></mml:msub></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mrow/><mml:msubsup><mml:mi>t</mml:mi><mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>&#x02032;</mml:mo></mml:msubsup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="normal">&#x003a8;</mml:mi><mml:mfenced close=")" open="("><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mfenced></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:mi>&#x02205;</mml:mi><mml:mfenced close=")" open="("><mml:msub><mml:mi>w</mml:mi><mml:mi>b</mml:mi></mml:msub></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mrow/><mml:msubsup><mml:mi>t</mml:mi><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>&#x02032;</mml:mo></mml:msubsup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="normal">&#x003a8;</mml:mi><mml:mfenced close=")" open="("><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mfenced></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:mi>&#x02205;</mml:mi><mml:mfenced close=")" open="("><mml:msub><mml:mi>w</mml:mi><mml:mi>u</mml:mi></mml:msub></mml:mfenced><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:math><graphic xlink:href="41598_2024_62331_Article_Equ7.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ8"><label>8</label><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${s}_{{f}_{i}}=\frac{{e}^{{t}_{{f}_{i}}^{{{\prime}}}}}{N}, {s}_{{b}_{i}}=\frac{{e}^{{t}_{{b}_{i}}^{{{\prime}}}}}{N},{s}_{{u}_{i}}=\frac{{e}^{{t}_{{u}_{i}}^{{{\prime}}}}}{N},$$\end{document}</tex-math><mml:math id="M16" display="block"><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:msub><mml:mi>f</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:msubsup><mml:mi>t</mml:mi><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>&#x02032;</mml:mo></mml:msubsup></mml:msup><mml:mi>N</mml:mi></mml:mfrac><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:msub><mml:mi>b</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:msubsup><mml:mi>t</mml:mi><mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>&#x02032;</mml:mo></mml:msubsup></mml:msup><mml:mi>N</mml:mi></mml:mfrac><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:msub><mml:mi>u</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:msubsup><mml:mi>t</mml:mi><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>&#x02032;</mml:mo></mml:msubsup></mml:msup><mml:mi>N</mml:mi></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:math><graphic xlink:href="41598_2024_62331_Article_Equ8.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ9"><label>9</label><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\text{N}={e}^{{t}_{{f}_{i}}^{{{\prime}}}}+{e}^{{t}_{{b}_{i}}^{{{\prime}}}}+{e}^{{t}_{{u}_{i}}^{{{\prime}}}}.$$\end{document}</tex-math><mml:math id="M18" display="block"><mml:mrow><mml:mtext>N</mml:mtext><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:msubsup><mml:mi>t</mml:mi><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>&#x02032;</mml:mo></mml:msubsup></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:msubsup><mml:mi>t</mml:mi><mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>&#x02032;</mml:mo></mml:msubsup></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:msubsup><mml:mi>t</mml:mi><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>&#x02032;</mml:mo></mml:msubsup></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:math><graphic xlink:href="41598_2024_62331_Article_Equ9.gif" position="anchor"/></alternatives></disp-formula></p><p id="Par31">The similarity scores <inline-formula id="IEq1"><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${s}_{{f}_{i}}$$\end{document}</tex-math><mml:math id="M20"><mml:msub><mml:mi>s</mml:mi><mml:msub><mml:mi>f</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_62331_Article_IEq1.gif"/></alternatives></inline-formula>, <inline-formula id="IEq2"><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${s}_{{b}_{i}}$$\end{document}</tex-math><mml:math id="M22"><mml:msub><mml:mi>s</mml:mi><mml:msub><mml:mi>b</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_62331_Article_IEq2.gif"/></alternatives></inline-formula> and <inline-formula id="IEq3"><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${s}_{{u}_{i}}$$\end{document}</tex-math><mml:math id="M24"><mml:msub><mml:mi>s</mml:mi><mml:msub><mml:mi>u</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_62331_Article_IEq3.gif"/></alternatives></inline-formula> are then used to compute contextual feature maps for <italic>w</italic><sub><italic>f</italic></sub>, <italic>w</italic><sub><italic>b</italic></sub> and <italic>w</italic><sub><italic>u</italic></sub> as follows,<disp-formula id="Equ10"><label>10</label><alternatives><tex-math id="M25">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${t}_{i}=\delta \left({s}_{{f}_{i}}\tau \left({w}_{f}\right)+{s}_{{b}_{i}}\tau \left({w}_{b}\right)+{s}_{{u}_{i}}\tau \left({w}_{u}\right)\right).$$\end{document}</tex-math><mml:math id="M26" display="block"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>&#x003b4;</mml:mi><mml:mfenced close=")" open="("><mml:msub><mml:mi>s</mml:mi><mml:msub><mml:mi>f</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub><mml:mi>&#x003c4;</mml:mi><mml:mfenced close=")" open="("><mml:msub><mml:mi>w</mml:mi><mml:mi>f</mml:mi></mml:msub></mml:mfenced><mml:mo>+</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:msub><mml:mi>b</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub><mml:mi>&#x003c4;</mml:mi><mml:mfenced close=")" open="("><mml:msub><mml:mi>w</mml:mi><mml:mi>b</mml:mi></mml:msub></mml:mfenced><mml:mo>+</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:msub><mml:mi>u</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub><mml:mi>&#x003c4;</mml:mi><mml:mfenced close=")" open="("><mml:msub><mml:mi>w</mml:mi><mml:mi>u</mml:mi></mml:msub></mml:mfenced></mml:mfenced><mml:mo>.</mml:mo></mml:mrow></mml:math><graphic xlink:href="41598_2024_62331_Article_Equ10.gif" position="anchor"/></alternatives></disp-formula>where <italic>&#x003a8;(&#x000b7;)</italic>, <italic>&#x003a6;(&#x000b7;)</italic>, <italic>&#x003c4;(&#x000b7;)</italic> and <italic>&#x003b4;(&#x000b7;)</italic> are pointwise convolution functions, and the value of each pixel <italic>t</italic><sub><italic>i</italic></sub> in the contextual map is the weighted average of the three vectors <italic>w</italic><sub><italic>f</italic></sub>, <italic>w</italic><sub><italic>b</italic></sub> and <italic>w</italic><sub><italic>u</italic></sub>.</p><p id="Par32">Given an input feature <italic>x</italic> and a contextual feature map t, the query (<italic>Q</italic>), the key (<italic>K</italic>) and the value (<italic>V</italic>) are first computed using three convolutional layers:<disp-formula id="Equ11"><label>11</label><alternatives><tex-math id="M27">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$Q={L}_{q}\cdot x, \;\; K={L}_{k}\cdot t, \;\; V={L}_{v}\cdot t,$$\end{document}</tex-math><mml:math id="M28" display="block"><mml:mrow><mml:mi>Q</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mi>q</mml:mi></mml:msub><mml:mo>&#x000b7;</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mspace width="0.277778em"/><mml:mspace width="0.277778em"/><mml:mi>K</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>&#x000b7;</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mspace width="0.277778em"/><mml:mspace width="0.277778em"/><mml:mi>V</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mi>v</mml:mi></mml:msub><mml:mo>&#x000b7;</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:math><graphic xlink:href="41598_2024_62331_Article_Equ11.gif" position="anchor"/></alternatives></disp-formula>where <italic>L</italic><sub><italic>q</italic></sub>, <italic>L</italic><sub><italic>k</italic></sub> and <italic>L</italic><sub><italic>v</italic></sub> are the corresponding convolutional layer weights.</p><p id="Par33">In order to achieve better generalizability, lower computational complexity, and more effective modeling of complex spatial relationships, a multi-head self-attention mechanism is used where attention is computed based on input features. With this mechanism, the feature space is divided into multiple subspaces, such that the proposed model can focus on different information aspects. Attention computation is as follows,<disp-formula id="Equ12"><label>12</label><alternatives><tex-math id="M29">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\left\{\begin{array}{c}{Q}_{i}=Q{W}_{i}^{Q},\\ {K}_{i}=K{W}_{i}^{K},\\ {V}_{i}=V{W}_{i}^{V},\end{array}\right.$$\end{document}</tex-math><mml:math id="M30" display="block"><mml:mfenced open="{"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>Q</mml:mi><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mi>Q</mml:mi></mml:msubsup><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mrow/><mml:msub><mml:mi>K</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>K</mml:mi><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mi>K</mml:mi></mml:msubsup><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mrow/><mml:msub><mml:mi>V</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>V</mml:mi><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mi>V</mml:mi></mml:msubsup><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:math><graphic xlink:href="41598_2024_62331_Article_Equ12.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ13"><label>13</label><alternatives><tex-math id="M31">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${head}_{i}=Attention\left({Q}_{i},{K}_{i},{V}_{i}\right), \quad i=1,\dots ,8$$\end{document}</tex-math><mml:math id="M32" display="block"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="italic">head</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mi>t</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mfenced close=")" open="("><mml:msub><mml:mi>Q</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>K</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mfenced><mml:mo>,</mml:mo><mml:mspace width="1em"/><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>&#x022ef;</mml:mo><mml:mo>,</mml:mo><mml:mn>8</mml:mn></mml:mrow></mml:math><graphic xlink:href="41598_2024_62331_Article_Equ13.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ14"><label>14</label><alternatives><tex-math id="M33">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$MultiHead\left(Q,K,V\right)=MH$$\end{document}</tex-math><mml:math id="M34" display="block"><mml:mrow><mml:mi>M</mml:mi><mml:mi>u</mml:mi><mml:mi>l</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>H</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mfenced close=")" open="("><mml:mi>Q</mml:mi><mml:mo>,</mml:mo><mml:mi>K</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi></mml:mfenced><mml:mo>=</mml:mo><mml:mi>M</mml:mi><mml:mi>H</mml:mi></mml:mrow></mml:math><graphic xlink:href="41598_2024_62331_Article_Equ14.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ15"><label>15</label><alternatives><tex-math id="M35">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$MH=Concact({head}_{1},\dots ,{head}_{8}){W}^{O}$$\end{document}</tex-math><mml:math id="M36" display="block"><mml:mrow><mml:mi>M</mml:mi><mml:mi>H</mml:mi><mml:mo>=</mml:mo><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="italic">head</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x022ef;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="italic">head</mml:mi></mml:mrow><mml:mn>8</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mi>O</mml:mi></mml:msup></mml:mrow></mml:math><graphic xlink:href="41598_2024_62331_Article_Equ15.gif" position="anchor"/></alternatives></disp-formula></p><p id="Par34">From Eq.&#x000a0;(<xref rid="Equ12" ref-type="disp-formula">12</xref>), 8 heads are used in association with the weights W<sub><italic>i</italic></sub> to form the triples <italic>Q</italic><sub><italic>i</italic></sub>, <italic>K</italic><sub><italic>i</italic></sub>, <italic>V</italic><sub><italic>i</italic></sub> (<italic>i</italic>&#x02009;=&#x02009;1, &#x02026;, 8). Then, the Attention weight matrix is calculated as<disp-formula id="Equ16"><label>16</label><alternatives><tex-math id="M37">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${z}_{i}=softmax\left(\frac{{Q}_{i}{K}_{i}^{T}}{\sqrt{{d}_{k}}}\right){V}_{i}$$\end{document}</tex-math><mml:math id="M38" display="block"><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>s</mml:mi><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>t</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mfenced close=")" open="("><mml:mfrac><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msubsup><mml:mi>K</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mi>T</mml:mi></mml:msubsup></mml:mrow><mml:msqrt><mml:msub><mml:mi>d</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:msqrt></mml:mfrac></mml:mfenced><mml:msub><mml:mi>V</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math><graphic xlink:href="41598_2024_62331_Article_Equ16.gif" position="anchor"/></alternatives></disp-formula></p><p id="Par35">Thus, each <italic>z</italic><sub><italic>i</italic></sub> is merged to form <italic>Z</italic><sub><italic>i</italic></sub>, and the outcomes of the 8 headers are subsequently merged as <italic>Z</italic><sup><italic>C</italic></sup>,<disp-formula id="Equ17"><label>17</label><alternatives><tex-math id="M39">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${Z}^{C}=concact({Z}_{1},\dots {Z}_{8})$$\end{document}</tex-math><mml:math id="M40" display="block"><mml:mrow><mml:msup><mml:mrow><mml:mi>Z</mml:mi></mml:mrow><mml:mi>C</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>Z</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x022ef;</mml:mo><mml:msub><mml:mi>Z</mml:mi><mml:mn>8</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math><graphic xlink:href="41598_2024_62331_Article_Equ17.gif" position="anchor"/></alternatives></disp-formula></p><p id="Par36">Finally, pointwise multiplication is performed via 3&#x02009;&#x000d7;&#x02009;3 convolution with <italic>x</italic>, and the output obtained after a series of convolution operations is summed with the corresponding pixel values of the original output <italic>X</italic> to obtain the output features (as shown in Fig.&#x000a0;<xref rid="Fig4" ref-type="fig">4</xref>).</p></sec><sec id="Sec10"><title>Multi-scale feature reasoning</title><p id="Par37">To effectively utilize multi-scale features, the output <italic>f</italic><sub><italic>d</italic></sub> of the IECA module is used for low-level feature refinement as shown in Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>. The RFB module can expand the perceptual field, extract rich features, and reduce the computational cost. As shown in Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>, <italic>f</italic><sub><italic>d</italic></sub> is convolved with <italic>f</italic><sub>1</sub>, <italic>f</italic><sub>2</sub>, and <italic>f</italic><sub>3</sub> to refine the three low-level feature maps, respectively. The refined feature maps are independently fed to the RFB module to get features with larger receptive fields: R(<italic>f</italic><sub>1</sub>&#x02297;<italic>f</italic><sub><italic>d</italic></sub>), R(<italic>f</italic><sub>2</sub>&#x02297;<italic>f</italic><sub><italic>d</italic></sub>), and R(<italic>f</italic><sub>3</sub>&#x02297;<italic>f</italic><sub><italic>d</italic></sub>). Then, R(<italic>f</italic><sub>1</sub>&#x02297;<italic>f</italic><sub><italic>d</italic></sub>) and R(<italic>f</italic><sub>2</sub>&#x02297;<italic>f</italic><sub><italic>d</italic></sub>) are cascaded and fed to the convolution block, and the block outputs are further cascaded with R(<italic>f</italic><sub>3</sub>&#x02297;<italic>f</italic><sub><italic>d</italic></sub>) and fed to a 16-channel convolution block. Finally, a multi-scale feature reasoning module utilizes low-level features and multi-scale strategies to generate the final segmentation outcomes.</p><p id="Par38">As shown in Fig.&#x000a0;<xref rid="Fig5" ref-type="fig">5</xref>, the MSFR module employs four convolution units and two multi-scale residual blocks (MRB) for detecting multi-scale features and generating the final segmentation outcomes. Specifically, as shown in Fig.&#x000a0;<xref rid="Fig6" ref-type="fig">6</xref>, a dual-branch network is constructed, where each branch uses a different convolutional kernel. To retain the original information of the input <italic>X</italic>, residual learning is used for obtaining the MRB output by adding <italic>X</italic> and fusing the multi-scale features.<fig id="Fig5"><label>Figure 5</label><caption><p>Multi-scale Feature Reasoning module.</p></caption><graphic xlink:href="41598_2024_62331_Fig5_HTML" id="MO5"/></fig><fig id="Fig6"><label>Figure 6</label><caption><p>Multi-scale Residuals Block.</p></caption><graphic xlink:href="41598_2024_62331_Fig6_HTML" id="MO6"/></fig></p></sec></sec><sec id="Sec11"><title>Experimental setup and results</title><p id="Par39">This section gives details of the experimental environment and conditions, the experimental dataset, comparison against other methods, an ablation study, and experimental data analysis.</p><sec id="Sec12"><title>Experimental environment and condition</title><p id="Par40">The PyTorch framework was used to implement the proposed polyp segmentation model, using Res2Net<sup><xref ref-type="bibr" rid="CR26">26</xref></sup> as a backbone network. The number of channels in the convolutional layers outside the backbone network was uniformly set to 32 for the small model. To facilitate training and testing, each image was uniformly resized to 352&#x02009;&#x000d7;&#x02009;352. Resizing also allows a close approximation to realistic situations of colonoscopy, because the lens may rotate and zoom during colonoscopy examinations.</p><p id="Par41">In addition, random expansion and erosion were also applied to the ground-truth labels to enhance generalizability. Model training was carried out using an initial learning rate of 10<sup>&#x02013;4</sup> and a polynomial learning rate decay<sup><xref ref-type="bibr" rid="CR26">26</xref></sup> with a factor of (<inline-formula id="IEq4"><alternatives><tex-math id="M41">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$1-{(\frac{iter}{{iter}_{max}})}^{0.9}$$\end{document}</tex-math><mml:math id="M42"><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="italic">iter</mml:mi></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="italic">iter</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">max</mml:mi></mml:mrow></mml:msub></mml:mfrac><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mn>0.9</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_62331_Article_IEq4.gif"/></alternatives></inline-formula>). Two Tesla T4 16G GPUs were employed for model training.</p></sec><sec id="Sec13"><title>Datasets</title><p id="Par42">In our experiments, a total of 2248 colonoscopy images were used where the images came from several datasets: Kvasir, CVC-ClinicDB, CVC-300, CVC-ColonDB, and ETIS. Moreover, 1450 images were randomly selected from the Kvasir and CVC-ClinicDB datasets to form the training dataset. The training data count represented about 55% of Kvasir and CVC-ClinicDB and 43% of all five datasets. The test dataset consisted of two parts: the non-training images of Kvasir and CVC-ClinicDB (denoted as T<sub>1</sub>) and all images of the three other datasets (CVC-300, CVC-ColonDB, and ETIS). And we will make a detailed comparison with other SOTA models on these datasets.<list list-type="bullet"><list-item><p id="Par43">The Kvasir<sup><xref ref-type="bibr" rid="CR30">30</xref></sup> dataset consists of 1000 polyp images, with image sizes varying from 332&#x02009;&#x000d7;&#x02009;487 to 1920&#x02009;&#x000d7;&#x02009;1072. The polyps in the images show blurred borders and low contrast with size and shape variations. This dataset was split into 900 images for training and 100 images for testing.</p></list-item><list-item><p id="Par44">The CVC-ClinicDB<sup><xref ref-type="bibr" rid="CR31">31</xref></sup> dataset has 612 images of sub-25 colonoscopy videos, from which 29 sequences were selected. The size of each image is 384&#x02009;&#x000d7;&#x02009;288. Training and testing were performed with 550 and 62 images, respectively.</p></list-item><list-item><p id="Par45">The CVC-300 dataset was selected from the EndoScene test dataset<sup><xref ref-type="bibr" rid="CR32">32</xref></sup>, which contains 912 images from 44 colonoscopy sequences. Following Fan et al.<sup><xref ref-type="bibr" rid="CR2">2</xref></sup>, CVC-300 was used as a test dataset with 60 test samples.</p></list-item><list-item><p id="Par46">The CVC-ColonDB<sup><xref ref-type="bibr" rid="CR33">33</xref></sup> dataset was mainly collected from 15 different colonoscopy sequences, with a total of 380 samples.</p></list-item><list-item><p id="Par47">The ETIS<sup><xref ref-type="bibr" rid="CR34">34</xref></sup> dataset contains 196 image samples collected from 34 colonoscopy videos. The size of each image is 1225&#x02009;&#x000d7;&#x02009;966, and this is the largest size among the explored datasets. This dataset is challenging since the polyp samples vary in size and shape, and some polyps are small and difficult to find.</p></list-item></list></p></sec><sec id="Sec14"><title>Comparative analysis with the state of the art</title><p id="Par48">After the training dataset was completed for IECFNet, it was first evaluated on the T<sub>1</sub> test dataset, and Table <xref rid="Tab1" ref-type="table">1</xref> shows the evaluation results. The results show that the IECFNet model significantly outperforms the other models.<table-wrap id="Tab1"><label>Table 1</label><caption><p>Comparison of experimental results with previous SOTA models on T1 dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Dataset</th><th align="left">Model</th><th align="left">Mean dice</th><th align="left">Mean IoU</th><th align="left">MAE</th></tr></thead><tbody><tr><td align="left" rowspan="6">KVASIR</td><td align="left">U-Net</td><td char="." align="char">0.818</td><td char="." align="char">0.746</td><td align="left">0.055</td></tr><tr><td align="left">U-Net++&#x02009;</td><td char="." align="char">0.821</td><td char="." align="char">0.743</td><td align="left">0.048</td></tr><tr><td align="left">ResUNet++</td><td char="." align="char">0.813</td><td char="." align="char">0.793</td><td align="left">&#x02013;</td></tr><tr><td align="left">SFA</td><td char="." align="char">0.723</td><td char="." align="char">0.611</td><td align="left">0.075</td></tr><tr><td align="left">PraNet</td><td char="." align="char">0.898</td><td char="." align="char">0.840</td><td align="left">0.030</td></tr><tr><td align="left">Ours</td><td char="." align="char">0.907</td><td char="." align="char">0.856</td><td align="left">0.028</td></tr><tr><td align="left" rowspan="6">CVC-CLINICDB</td><td align="left">U-Net</td><td char="." align="char">0.823</td><td char="." align="char">0.755</td><td align="left">0.019</td></tr><tr><td align="left">U-Net++</td><td char="." align="char">0.794</td><td char="." align="char">0.729</td><td align="left">0.022</td></tr><tr><td align="left">ResUNet++</td><td char="." align="char">0.796</td><td char="." align="char">0.796</td><td align="left">&#x02013;</td></tr><tr><td align="left">SFA</td><td char="." align="char">0.700</td><td char="." align="char">0.607</td><td align="left">0.042</td></tr><tr><td align="left">PraNet</td><td char="." align="char">0.899</td><td char="." align="char">0.849</td><td align="left">0.009</td></tr><tr><td align="left">Ours</td><td char="." align="char">0.924</td><td char="." align="char">0.873</td><td align="left">0.007</td></tr></tbody></table></table-wrap></p><p id="Par49">The predictive performance and the generalization ability between IECFNet and mainstream methods are compared. These methods are U-Net<sup><xref ref-type="bibr" rid="CR22">22</xref></sup>, U-Net++<sup><xref ref-type="bibr" rid="CR21">21</xref>,<xref ref-type="bibr" rid="CR35">35</xref></sup>, ResUNet++, SFA, PraNet. U-Net and U-Net++&#x02009;are the classical methods. SFA and PraNet are the state-of-the-art methods.</p><p id="Par50">As Table <xref rid="Tab1" ref-type="table">1</xref> Comparison of experimental results with previous SOTA models on T1 dataset demonstrates, we provide a comprehensive comparison of our ensembles with the SOTA results reported in the literature IECFNet outperforms other models on CVC-ClinicDB for all metrics. Specifically, the IECFNet model has a mean Dice coefficient exceeding those of U-Net++&#x02009;and ResUNet++&#x02009;by 3% and 2.5%, respectively. This performance improvement is because that the proposed RFCE-e and RFCE-d modules can effectively extract rich fine-grained feature maps with high-level semantic information and low-level details. Moreover, the IECA module exploits fuzzy.</p><p id="Par51">Moreover, as mentioned earlier, the image sizes for the Kvasir dataset vary from 332&#x02009;&#x000d7;&#x02009;487 to 1920&#x02009;&#x000d7;&#x02009;1072, and these images show wide variations in polyp size and shape (see Fig.&#x000a0;<xref rid="Fig7" ref-type="fig">7</xref>). The IECFNet model can deal with such large variations, and clearly outperforms the PraNet and SFA models on this dataset, thanks to the proposed multi-scale feature reasoning module with a two-branch structure for capturing multiscale features.<fig id="Fig7"><label>Figure 7</label><caption><p>Comparison of qualitative results with the state-of-the-art methods on five different data sets.</p></caption><graphic xlink:href="41598_2024_62331_Fig7_HTML" id="MO7"/></fig></p><p id="Par52">To evaluate the generalization capabilities of our model, comparative experiments were conducted on three datasets: CVC-300, CVC-ColonDB, and ETIS. The results are shown in Table <xref rid="Tab2" ref-type="table">2</xref>. The ETIS dataset turned to be the most challenging dataset, and the IECFNet model achieved a mean Dice coefficient of 70.7% on this dataset, with a margin of improvement of 7.9% compared to the PraNet model. Figure&#x000a0;<xref rid="Fig7" ref-type="fig">7</xref> Comparison of qualitative results with the state-of-the-art methods on five different data sets shows sample qualitative results for the proposed model and the other state-of-the-art models on the five datasets.<table-wrap id="Tab2"><label>Table 2</label><caption><p>Comparison of experimental results with other SOTA models on ETIS, CVC-ColonDB and CVC-300 datasets.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Dataset</th><th align="left">Model</th><th align="left">Mean dice</th><th align="left">Mean IoU</th><th align="left">MAE</th></tr></thead><tbody><tr><td align="left" rowspan="5">ETIS</td><td align="left">U-Net</td><td char="." align="char">0.398</td><td char="." align="char">0.335</td><td char="." align="char">0.036</td></tr><tr><td align="left">U-Net++</td><td char="." align="char">0.401</td><td char="." align="char">0.344</td><td char="." align="char">0.034</td></tr><tr><td align="left">SFA</td><td char="." align="char">0.297</td><td char="." align="char">0.217</td><td char="." align="char">0.109</td></tr><tr><td align="left">PraNet</td><td char="." align="char">0.628</td><td char="." align="char">0.567</td><td char="." align="char">0.031</td></tr><tr><td align="left">Ours</td><td char="." align="char">0.707</td><td char="." align="char">0.632</td><td char="." align="char">0.016</td></tr><tr><td align="left" rowspan="5">CVC-COLONDB</td><td align="left">U-Net</td><td char="." align="char">0.512</td><td char="." align="char">0.444</td><td char="." align="char">0.061</td></tr><tr><td align="left">U-Net++</td><td char="." align="char">0.483</td><td char="." align="char">0.410</td><td char="." align="char">0.064</td></tr><tr><td align="left">SFA</td><td char="." align="char">0.469</td><td char="." align="char">0.347</td><td char="." align="char">0.094</td></tr><tr><td align="left">PraNet</td><td char="." align="char">0.709</td><td char="." align="char">0.640</td><td char="." align="char">0.045</td></tr><tr><td align="left">Ours</td><td char="." align="char">0.775</td><td char="." align="char">0.632</td><td char="." align="char">0.019</td></tr><tr><td align="left" rowspan="5">CVC-300</td><td align="left">U-Net</td><td char="." align="char">0.710</td><td char="." align="char">0.627</td><td char="." align="char">0.022</td></tr><tr><td align="left">U-Net++</td><td char="." align="char">0.707</td><td char="." align="char">0.624</td><td char="." align="char">0.018</td></tr><tr><td align="left">SFA</td><td char="." align="char">0.467</td><td char="." align="char">0.329</td><td char="." align="char">0.065</td></tr><tr><td align="left">PraNet</td><td char="." align="char">0.871</td><td char="." align="char">0.797</td><td char="." align="char">0.010</td></tr><tr><td align="left">Ours</td><td char="." align="char">0.912</td><td char="." align="char">0.850</td><td char="." align="char">0.005</td></tr></tbody></table></table-wrap></p><p id="Par53">As shown in Table <xref rid="Tab2" ref-type="table">2</xref>, the IECFNet model achieved good performance on all metrics. For example, the mean Dice coefficient reaches 91.2% on the CVC-300 dataset, and IECFNet leads PraNet by 7.9% on the ETIS dataset. In contrast, the SFA model performance decreases sharply. Note that the images in the ETIS dataset have dimensions of 1225&#x02009;&#x000d7;&#x02009;966, the largest among the five datasets. Thus, one image in this dataset can have multiple challenging-to-segment polyps of different shapes and sizes (as shown in Fig.&#x000a0;<xref rid="Fig7" ref-type="fig">7</xref>). Still, the evaluation results show that the IECFNet model has significant comparative advantages in dealing with multiple targets of different scales.</p><p id="Par54">As shown in Fig.&#x000a0;<xref rid="Fig7" ref-type="fig">7</xref>, the polyps in the third row are small, with blurred borders and low contrast, and thus these polyps are difficult to detect even with the naked eye. The IECFNet model still shows good segmentation results, while the segmentation results of the other methods are obviously not satisfactory. Also, the segmentation results of the 2nd, 4th, and 5th rows show that the IECFNet model slightly outperformed other models in dealing with some polyps with large differences in shape and appearance. In conclusion, for polyps with different shapes and sizes as well as for the multi-polyp cases, our IECFNet model demonstrated remarkably better results.</p></sec><sec id="Sec15"><title>Ablation experiments</title><p id="Par55">To evaluate the effectiveness of the modules in the IECFNet model, three ablation experiments were conducted. The ablation studies were performed on the CVC-ClinicDB and ETIS datasets because CVC-ClinicDB was sampled for training purposes, but ETIS was not. The ablation experimental network model is still trained using the T1 training set in section IV.B of the chapter.</p><sec id="Sec16"><title>Ablation experiments on RFCA-e and RFCA-d</title><p id="Par56">To verify the effectiveness of the RFCA-e and RFCA-d modules, a baseline IECFNet variant was constructed without the RFCA-e and RFCA-d modules (this model variant is denoted by "A").</p><p id="Par57">Segmentation performance was evaluated using three metrics: mean Dice coefficient (mDice), mean intersection over union (mIoU), and mean absolute error (MAE). The experimental results are shown in Table <xref rid="Tab3" ref-type="table">3</xref>. Moreover, the segmentation results can be qualitatively analyzed and visualized with a heatmap of the predicted segmentation probabilities for different image regions. Such a heatmap clearly shows the degree of model attention and confidence for different regions.<table-wrap id="Tab3"><label>Table 3</label><caption><p>Experimental results of RFCA module.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Dataset</th><th align="left">Model</th><th align="left">Mean dice</th><th align="left">Mean IoU</th><th align="left">MAE</th></tr></thead><tbody><tr><td align="left" rowspan="2">CVC-ClinicDB</td><td align="left">A</td><td char="." align="char">0.913</td><td char="." align="char">0.862</td><td char="." align="char">0.009</td></tr><tr><td align="left">IECFNet</td><td char="." align="char">0.924</td><td char="." align="char">0.873</td><td char="." align="char">0.007</td></tr><tr><td align="left" rowspan="2">ETIS</td><td align="left">A</td><td char="." align="char">0.664</td><td char="." align="char">0.585</td><td char="." align="char">0.021</td></tr><tr><td align="left">IECFNet</td><td char="." align="char">0.707</td><td char="." align="char">0.632</td><td char="." align="char">0.016</td></tr></tbody></table></table-wrap></p></sec><sec id="Sec17"><title>Ablation experiments on IECA</title><p id="Par58">Further ablation experiments were designed to demonstrate the effectiveness of the IECA module. Specifically, an IECFNet variant was built where the IECA module was replaced with a contextual attention (CA) module (this model variant is denoted by "B"). The output feature maps of the attention module were visualized for both settings to qualitatively verify the validity of the uncertainty regions. The segmentation results show that the IECFNet network with the IECA module easily identifies the uncertainty region that are closely related to the polyp boundaries. The experimental results are shown in Table <xref rid="Tab4" ref-type="table">4</xref>.<table-wrap id="Tab4"><label>Table 4</label><caption><p>Results of ablation experiments of IECA modules.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Dataset</th><th align="left">Model</th><th align="left">Mean dice</th><th align="left">Mean IoU</th><th align="left">MAE</th></tr></thead><tbody><tr><td align="left" rowspan="2">CVC-ClinicDB</td><td align="left">B</td><td char="." align="char">0.920</td><td char="." align="char">0.871</td><td char="." align="char">0.008</td></tr><tr><td align="left">IECFNet</td><td char="." align="char">0.924</td><td char="." align="char">0.873</td><td char="." align="char">0.007</td></tr><tr><td align="left" rowspan="2">ETIS</td><td align="left">B</td><td char="." align="char">0.714</td><td char="." align="char">0.625</td><td char="." align="char">0.015</td></tr><tr><td align="left">IECFNet</td><td char="." align="char">0.707</td><td char="." align="char">0.632</td><td char="." align="char">0.016</td></tr></tbody></table></table-wrap></p><p id="Par59">Also, the model with the IECA module performed better than the one with the CA module in terms of dealing with inaccurate localization and boundary blurring (as shown in the second row of Fig.&#x000a0;<xref rid="Fig8" ref-type="fig">8</xref>). By comparing the heatmaps and feature maps of the segmentation results of the different models, the IECFNet model demonstrated better performance on small polyps with blurred boundaries, while the model with the CA module misidentified the normal tissues in the neighborhood of small polyps.<fig id="Fig8"><label>Figure 8</label><caption><p>Ablation experimental model segmentation results. (<bold>a</bold>) Represents an image, (<bold>b</bold>) represents the ground-truth segmentation, (<bold>c</bold>) represents the segmentation result of the ablation experiment, (<bold>d</bold>) represents the segmentation heatmap of (<bold>c</bold>), (<bold>e</bold>) represents the IECFNet segmentation result, and (<bold>f</bold>) represents the segmentation heatmap of (<bold>e</bold>).</p></caption><graphic xlink:href="41598_2024_62331_Fig8_HTML" id="MO8"/></fig></p></sec><sec id="Sec18"><title>Ablation experiments on MSFR</title><p id="Par60">In these experiments, the MSFR module was removed from the proposed IECFNet model, while the other model components remained the same (this model variant is denoted by "C"). The experimental results are shown in Table <xref rid="Tab5" ref-type="table">5</xref>. The MSFR module actually made good use of the multi-scale information of the low-level features, and hence boosted the polyp segmentation performance.<table-wrap id="Tab5"><label>Table 5</label><caption><p>Results of MSFR module ablation experiments.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Dataset</th><th align="left">Model</th><th align="left">Mean dice</th><th align="left">Mean IoU</th><th align="left">MAE</th></tr></thead><tbody><tr><td align="left" rowspan="2">CVC-ClinicDB</td><td align="left">C</td><td char="." align="char">0.902</td><td char="." align="char">0.854</td><td char="." align="char">0.010</td></tr><tr><td align="left">IECFNet</td><td char="." align="char">0.924</td><td char="." align="char">0.873</td><td char="." align="char">0.007</td></tr><tr><td align="left" rowspan="2">ETIS</td><td align="left">C</td><td char="." align="char">0.659</td><td char="." align="char">0.584</td><td char="." align="char">0.025</td></tr><tr><td align="left">IECFNet</td><td char="." align="char">0.707</td><td char="." align="char">0.632</td><td char="." align="char">0.016</td></tr></tbody></table></table-wrap></p><p id="Par61">As shown in the fourth row of Fig.&#x000a0;<xref rid="Fig8" ref-type="fig">8</xref>, the IECFNet model is better than the model without the MSFR module in terms of dealing with the polyp size and shape variations and in handling multiple polyps. Actually, the IECFNet model effectively segments the small polyps in the upper left corner and improves the segmentation performance on polyps of larger sizes. This shows that the MSFR module of the IECFNet model is essential for dealing with polyps of different sizes and shapes.</p></sec></sec></sec><sec id="Sec19"><title>Conclusion</title><p id="Par62">A new polyp segmentation network, called IECFNet, is proposed. This network first enhances regions of uncertainty by targeting saliency maps that are highly correlated with polyp boundaries. Then, the network refines low-level features using saliency-based attention maps. Finally, the network detects fused image features of different scales and performs multi-scale feature reasoning for accurate polyp detection. In the absence of edge labels, we use implicit edge regions for boundary representation. We also propose the receptive-field coordinate attention encoder (RFCA-e) module and the receptive-field coordinate attention decoder (RFCA-d) module to focus on the spatial features of the perceptual field. As well, a multi-scale feature reasoning (MSFR) module is proposed to get enhanced features after cross-layer feature fusion. Through a series of quantitative and qualitative experiments, the IECFNet model performs well compared to previous state-of-the-art methods.</p></sec></body><back><fn-group><fn><p><bold>Publisher's note</bold></p><p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></fn></fn-group><ack><title>Acknowledgements</title><p>This research was partly supported by the National Science Foundation of China (No.: 62371271).</p></ack><notes notes-type="author-contribution"><title>Author contributions</title><p>Junqing Liu: conceptualization, formal analysis, writing&#x02014;review and editing. Weiwei Zhang: methodology, validation, writing&#x02014;original draft. Yong Liu: reviewing, supervision. Qinghe Zhang: supervision, writing&#x02014;review and editing.</p></notes><notes notes-type="data-availability"><title>Data availability</title><p>The dataset used in this study can be found at <ext-link ext-link-type="uri" xlink:href="https://github.com/Zhangweiwei-ctgu">https://github.com/Zhangweiwei-ctgu</ext-link>.</p></notes><notes id="FPar1" notes-type="COI-statement"><title>Competing interests</title><p id="Par63">The authors declare no competing interests.</p></notes><ref-list id="Bib1"><title>References</title><ref id="CR1"><label>1.</label><mixed-citation publication-type="other">Oktay, O. <italic>et al.</italic> Attention u-net: Learning where to look for the pancreas. arXiv preprint arXiv: <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1804.03999">http://arxiv.org/abs/1804.03999</ext-link> (2018).</mixed-citation></ref><ref id="CR2"><label>2.</label><mixed-citation publication-type="other">Fan, D-P. <italic>et al.</italic> Pranet: Parallel reverse attention network for polyp segmentation. In <italic>International Conference on Medical Image Computing and Computer-Assisted Intervention</italic>, 263&#x02013;273 (Springer, 2020).</mixed-citation></ref><ref id="CR3"><label>3.</label><mixed-citation publication-type="other">Yuan, Y., Chen, X. &#x00026; Wang, J. Object-contextual representations for semantic segmentation. arXiv preprint arXiv: <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1909.11065">http://arxiv.org/abs/1909.11065</ext-link> (2019).</mixed-citation></ref><ref id="CR4"><label>4.</label><mixed-citation publication-type="other">Zhou, Y. <italic>et al.</italic> Full-attention based neural architecture search using context auto-regression. arXiv preprint arXiv: <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2111.07139">http://arxiv.org/abs/2111.07139</ext-link>, 2021.</mixed-citation></ref><ref id="CR5"><label>5.</label><mixed-citation publication-type="other">Tan, M. &#x00026; Le, Q. Efficientnetv2: Smaller models and faster training. In <italic>International Conference on Machine Learning</italic>, 10096&#x02013;10106 (PMLR, 2021).</mixed-citation></ref><ref id="CR6"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Canny</surname><given-names>J</given-names></name></person-group><article-title>A computational approach to edge detection</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell. PAMI-8</source><year>1986</year><volume>6</volume><fpage>679</fpage><lpage>698</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.1986.4767851</pub-id></element-citation></ref><ref id="CR7"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Qadir</surname><given-names>HA</given-names></name><name><surname>Shin</surname><given-names>Y</given-names></name><name><surname>Solhusvik</surname><given-names>J</given-names></name><name><surname>Bergsland</surname><given-names>J</given-names></name><name><surname>Aabakken</surname><given-names>L</given-names></name><name><surname>Balasingham</surname><given-names>I</given-names></name></person-group><article-title>Toward real-time polyp detection using fully CNNs for 2D Gaussian shapes prediction</article-title><source>Med. Image Anal.</source><year>2021</year><volume>68</volume><fpage>101897</fpage><pub-id pub-id-type="doi">10.1016/j.media.2020.101897</pub-id><?supplied-pmid 33260111?><pub-id pub-id-type="pmid">33260111</pub-id>
</element-citation></ref><ref id="CR8"><label>8.</label><mixed-citation publication-type="other">Kingma, D. P. &#x00026; Ba, J. Adam: A method for stochastic optimization. arXiv preprint arXiv: <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1412.6980">http://arxiv.org/abs/1412.6980</ext-link> (2014)</mixed-citation></ref><ref id="CR9"><label>9.</label><mixed-citation publication-type="other">Murugesan, B. <italic>et al.</italic> Psi-Net: Shape and boundary aware joint multi-task deep net-work for medical image segmentation. In <italic>IEEE EMBC</italic>, 7223&#x02013;7226 (2019)</mixed-citation></ref><ref id="CR10"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mahmud</surname><given-names>T</given-names></name><name><surname>Paul</surname><given-names>B</given-names></name><name><surname>Fattah</surname><given-names>SA</given-names></name></person-group><article-title>PolypSegNet: A modified encoder-decoder architecture for automated polyp segmentation from colonoscopy images</article-title><source>Comput. Biol. Med.</source><year>2021</year><volume>128</volume><fpage>104119</fpage><pub-id pub-id-type="doi">10.1016/j.compbiomed.2020.104119</pub-id><?supplied-pmid 33254083?><pub-id pub-id-type="pmid">33254083</pub-id>
</element-citation></ref><ref id="CR11"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guo</surname><given-names>X</given-names></name><name><surname>Chen</surname><given-names>Z</given-names></name><name><surname>Liu</surname><given-names>J</given-names></name><name><surname>Yuan</surname><given-names>Y</given-names></name></person-group><article-title>Non-equivalent images and pixels: Confidence-aware resampling with meta-learning mixup for polyp segmentation</article-title><source>Med. Image Anal.</source><year>2022</year><volume>78</volume><fpage>102394</fpage><pub-id pub-id-type="doi">10.1016/j.media.2022.102394</pub-id><?supplied-pmid 35219939?><pub-id pub-id-type="pmid">35219939</pub-id>
</element-citation></ref><ref id="CR12"><label>12.</label><mixed-citation publication-type="other">Chen, S., Tan, X., Wang, B. &#x00026; Hu, X. Reverse attention for salient object detection. In <italic>Proceedings of the European Conference on Computer Vision (ECCV)</italic>, 234&#x02013;250 (2018).</mixed-citation></ref><ref id="CR13"><label>13.</label><mixed-citation publication-type="other">He, K. <italic>et al.</italic> Mask r-cnn. In <italic>Proceedings of the IEEE International Conference on Computer Vision</italic>, 2961&#x02013;2969 (2017).</mixed-citation></ref><ref id="CR14"><label>14.</label><mixed-citation publication-type="other">Qadir, H. A. <italic>et al.</italic> Polyp detection and segmentation using mask R-CNN: Does a deeper feature extractor CNN always perform better? In <italic>2019 13th International Symposium on Medical Information and Communication Technology (ISMICT)</italic>, 1&#x02013;6 (IEEE, 2019).</mixed-citation></ref><ref id="CR15"><label>15.</label><mixed-citation publication-type="other">Fu, J. <italic>et al.</italic> Dual attention network for scene segmentation. In <italic>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</italic>, 3146&#x02013;3154 (2019).</mixed-citation></ref><ref id="CR16"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>W</given-names></name><name><surname>Lai</surname><given-names>Q</given-names></name><name><surname>Fu</surname><given-names>H</given-names></name><name><surname>Shen</surname><given-names>J</given-names></name><name><surname>Ling</surname><given-names>H</given-names></name><name><surname>Yang</surname><given-names>R</given-names></name></person-group><article-title>Salient object detection in the deep learning era: An in-depth survey</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2022</year><volume>44</volume><issue>6</issue><fpage>3239</fpage><lpage>3259</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2021.3051099</pub-id><?supplied-pmid 33434124?><pub-id pub-id-type="pmid">33434124</pub-id>
</element-citation></ref><ref id="CR17"><label>17.</label><mixed-citation publication-type="other">Zhao, J. X. <italic>et al. EGNet: Edge Guidance Network for Salient Object Detection</italic>. 10.48550/arXiv.1908.08297 (2019).</mixed-citation></ref><ref id="CR18"><label>18.</label><mixed-citation publication-type="other">Su, J., Li, J., Zhang, Y., Xia, C. &#x00026; Tian, Y. Selectivity or invariance: Boundary-aware salient object detection. In <italic>Proceedings of the IEEE/CVF International Conference on Computer Vision</italic>, 3799&#x02013;3808 (2019).</mixed-citation></ref><ref id="CR19"><label>19.</label><mixed-citation publication-type="other">Zhao, H., Shi, J., Qi, X., Wang, X. &#x00026; Jia, J. Pyramid scene parsing network. In <italic>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</italic>, 2881&#x02013;2890 (2017).</mixed-citation></ref><ref id="CR20"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>LC</given-names></name><name><surname>Papandreou</surname><given-names>G</given-names></name><name><surname>Kokkinos</surname><given-names>I</given-names></name><name><surname>Murphy</surname><given-names>K</given-names></name><name><surname>Yuille</surname><given-names>AL</given-names></name></person-group><article-title>Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2017</year><volume>40</volume><issue>4</issue><fpage>834</fpage><lpage>848</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2017.2699184</pub-id><?supplied-pmid 28463186?><pub-id pub-id-type="pmid">28463186</pub-id>
</element-citation></ref><ref id="CR21"><label>21.</label><mixed-citation publication-type="other">Huang, Z., Wang, X., Huang, L., Huang, C., Wei, Y. &#x00026; Liu, W. Ccnet: Criss-cross attention for semantic segmentation. In <italic>Proceedings of the IEEE/CVF International Conference on Computer Vision</italic>, 603&#x02013;612 (2019).</mixed-citation></ref><ref id="CR22"><label>22.</label><mixed-citation publication-type="other">Wang, X., Girshick, R., Gupta, A. &#x00026; He, K. Non-local neural networks. In <italic>Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</italic>, 7794&#x02013;7803 (2018).</mixed-citation></ref><ref id="CR23"><label>23.</label><mixed-citation publication-type="other">Zhang, L., Dai, J., Lu, H., He, Y. &#x00026; Wang, G. A bi-directional message passing model for salient object detection. In <italic>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</italic>, 1741&#x02013;1750 (2018).</mixed-citation></ref><ref id="CR24"><label>24.</label><mixed-citation publication-type="other">Liu, J.-J., Hou, Q., Cheng, M.-M., Feng, J. &#x00026; Jiang, J. A simple pooling-based design for real-time salient object detection. In <italic>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</italic>, 3917&#x02013;3926 (2019).</mixed-citation></ref><ref id="CR25"><label>25.</label><mixed-citation publication-type="other">Chen, Z., Xu, Q., Cong, R. &#x00026; Huang, Q. Global context-aware progressive aggregation network for salient object detection. In <italic>Proceedings of the AAAI Conference on Artificial Intelligence</italic>, Vol. 34, No. 07, 10599&#x02013;10606 (2020).</mixed-citation></ref><ref id="CR26"><label>26.</label><mixed-citation publication-type="other">Chen, L. C., Papandreou, G., Schroff, F. &#x00026; Adam, H. Rethinking atrous convolution for semantic image segmentation. arXiv preprint arXiv: <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1706.05587">http://arxiv.org/abs/1706.05587</ext-link> (2017).</mixed-citation></ref><ref id="CR27"><label>27.</label><mixed-citation publication-type="other">Zhang, X. <italic>et al.</italic><italic>RFAConv:</italic><italic>Innovating Spatital Attention and Standard Convolutional Operation</italic>. arXiv preprint arXiv: <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2304.03198">http://arxiv.org/abs/2304.03198</ext-link>, 2023.</mixed-citation></ref><ref id="CR28"><label>28.</label><mixed-citation publication-type="other">Liu, S. &#x00026; Huang, D. Receptive&#x000a0;field&#x000a0;block&#x000a0;net&#x000a0;for&#x000a0;accurate&#x000a0;and&#x000a0;fast&#x000a0;object&#x000a0;detection. In <italic>Proceedings&#x000a0;of&#x000a0;the&#x000a0;European</italic>&#x000a0;<italic>Conference</italic>&#x000a0;<italic>on</italic>&#x000a0;<italic>Computer&#x000a0;Vision</italic>&#x000a0;<italic>(ECCV)</italic> (2018).</mixed-citation></ref><ref id="CR29"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>B</given-names></name><name><surname>Zhang</surname><given-names>X</given-names></name><name><surname>Chen</surname><given-names>L</given-names></name><name><surname>Yang</surname><given-names>H</given-names></name><name><surname>Gao</surname><given-names>Z</given-names></name></person-group><article-title>Edge guided salient object detection</article-title><source>Neurocomputing</source><year>2017</year><volume>221</volume><fpage>60</fpage><lpage>71</lpage><pub-id pub-id-type="doi">10.1016/j.neucom.2016.09.062</pub-id></element-citation></ref><ref id="CR30"><label>30.</label><mixed-citation publication-type="other">Jha, D. <italic>et al.</italic> Kvasir-seg: A segmented polyp dataset. In <italic>International Conference on Multimedia Modeling</italic>, 451&#x02013;462 (Springer, 2020).</mixed-citation></ref><ref id="CR31"><label>31.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bernal</surname><given-names>J</given-names></name><name><surname>S&#x000e1;nchez</surname><given-names>FJ</given-names></name><name><surname>Fern&#x000e1;ndez-Esparrach</surname><given-names>G</given-names></name><name><surname>Gil</surname><given-names>D</given-names></name><name><surname>Rodr&#x000ed;guez</surname><given-names>C</given-names></name><name><surname>Vilari&#x000f1;o</surname><given-names>F</given-names></name></person-group><article-title>WM-DOV A maps for accurate polyp highlighting in colonoscopy: Validation vs. saliency maps from physicians</article-title><source>Comput. Med. Imaging Graph.</source><year>2015</year><volume>43</volume><fpage>99</fpage><lpage>111</lpage><pub-id pub-id-type="doi">10.1016/j.compmedimag.2015.02.007</pub-id><?supplied-pmid 25863519?><pub-id pub-id-type="pmid">25863519</pub-id>
</element-citation></ref><ref id="CR32"><label>32.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>V&#x000e1;zquez</surname><given-names>D</given-names></name><name><surname>Bernal</surname><given-names>J</given-names></name><name><surname>S&#x000e1;nchez</surname><given-names>FJ</given-names></name><name><surname>Fern&#x000e1;ndez-Esparrach</surname><given-names>G</given-names></name><name><surname>L&#x000f3;pez</surname><given-names>AM</given-names></name><name><surname>Romero</surname><given-names>A</given-names></name><name><surname>Drozdzal</surname><given-names>M</given-names></name><name><surname>Courville</surname><given-names>A</given-names></name></person-group><article-title>A benchmark for endoluminal scene segmentation of colonoscopy images</article-title><source>J. Healthc. Eng.</source><year>2017</year><volume>2017</volume><fpage>4037190</fpage><pub-id pub-id-type="doi">10.1155/2017/4037190</pub-id><?supplied-pmid 29065595?><pub-id pub-id-type="pmid">29065595</pub-id>
</element-citation></ref><ref id="CR33"><label>33.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bernal</surname><given-names>J</given-names></name><name><surname>S&#x000e1;nchez</surname><given-names>J</given-names></name><name><surname>Vilari&#x000f1;o</surname><given-names>F</given-names></name></person-group><article-title>Towards automatic polyp detection with a polyp appearance model</article-title><source>Pattern Recogn.</source><year>2012</year><volume>45</volume><issue>9</issue><fpage>3166</fpage><lpage>3182</lpage><pub-id pub-id-type="doi">10.1016/j.patcog.2012.03.002</pub-id></element-citation></ref><ref id="CR34"><label>34.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Silva</surname><given-names>J</given-names></name><name><surname>Histace</surname><given-names>A</given-names></name><name><surname>Romain</surname><given-names>O</given-names></name><name><surname>Dray</surname><given-names>X</given-names></name><name><surname>Granado</surname><given-names>B</given-names></name></person-group><article-title>Toward embedded detection of polyps in wce images for early diagnosis of colorectal cancer</article-title><source>Int. J. Comput. Assist. Radiol. Surg.</source><year>2014</year><volume>9</volume><fpage>283</fpage><lpage>293</lpage><pub-id pub-id-type="doi">10.1007/s11548-013-0926-3</pub-id><?supplied-pmid 24037504?><pub-id pub-id-type="pmid">24037504</pub-id>
</element-citation></ref><ref id="CR35"><label>35.</label><mixed-citation publication-type="other">Zhou, Z., Rahman Siddiquee, M. M., Tajbakhsh, N. &#x00026; Liang, J. Unet++: A nested u-net architecture for medical image segmentation. In <italic>Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support</italic>, 3&#x02013;11 (Springer, 2018).</mixed-citation></ref></ref-list></back></article>