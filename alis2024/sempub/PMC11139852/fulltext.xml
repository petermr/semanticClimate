<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd"> 
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><?properties open_access?><?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?><?DTDIdentifier.IdentifierType public?><?SourceDTD.DTDName A++V2.4.dtd?><?SourceDTD.Version 2.4?><?ConverterInfo.XSLTName springer2nlmx2.xsl?><?ConverterInfo.Version 1?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sci Rep</journal-id><journal-id journal-id-type="iso-abbrev">Sci Rep</journal-id><journal-title-group><journal-title>Scientific Reports</journal-title></journal-title-group><issn pub-type="epub">2045-2322</issn><publisher><publisher-name>Nature Publishing Group UK</publisher-name><publisher-loc>London</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmcid">11139852</article-id><article-id pub-id-type="pmid">38816456</article-id>
<article-id pub-id-type="publisher-id">63257</article-id><article-id pub-id-type="doi">10.1038/s41598-024-63257-8</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>DASUNet: a deeply supervised change detection network integrating full-scale features</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Miao</surname><given-names>Ru</given-names></name><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author"><name><surname>Meng</surname><given-names>Geng</given-names></name><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Zhou</surname><given-names>Ke</given-names></name><address><email>zhouke@henu.edu.cn</email></address><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff2">2</xref><xref ref-type="aff" rid="Aff3">3</xref></contrib><contrib contrib-type="author"><name><surname>Li</surname><given-names>Yi</given-names></name><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author"><name><surname>Chang</surname><given-names>Ranran</given-names></name><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author"><name><surname>Zhang</surname><given-names>Guangyu</given-names></name><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff2">2</xref></contrib><aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/003xyzq10</institution-id><institution-id institution-id-type="GRID">grid.256922.8</institution-id><institution-id institution-id-type="ISNI">0000 0000 9139 560X</institution-id><institution>School of Computer and Information Engineering, </institution><institution>Henan University, </institution></institution-wrap>Kaifeng, 475004 People&#x02019;s Republic of China </aff><aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/003xyzq10</institution-id><institution-id institution-id-type="GRID">grid.256922.8</institution-id><institution-id institution-id-type="ISNI">0000 0000 9139 560X</institution-id><institution>Henan Province Engineering Research Center of Spatial Information Processing, </institution><institution>Henan University, </institution></institution-wrap>Kaifeng, 475004 People&#x02019;s Republic of China </aff><aff id="Aff3"><label>3</label>Henan Provincial Spatio-Temporal Big Data Technology Innovation Center, Zhengzhou, 450046 People&#x02019;s Republic of China </aff></contrib-group><pub-date pub-type="epub"><day>30</day><month>5</month><year>2024</year></pub-date><pub-date pub-type="pmc-release"><day>30</day><month>5</month><year>2024</year></pub-date><pub-date pub-type="collection"><year>2024</year></pub-date><volume>14</volume><elocation-id>12464</elocation-id><history><date date-type="received"><day>7</day><month>3</month><year>2024</year></date><date date-type="accepted"><day>27</day><month>5</month><year>2024</year></date></history><permissions><copyright-statement>&#x000a9; The Author(s) 2024</copyright-statement><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p></license></permissions><abstract id="Abs1"><p id="Par1">The change detection (CD) technology has greatly improved the ability to interpret land surface changes. Deep learning (DL) methods have been widely used in the field of CD due to its high detection accuracy and application range. DL-based CD methods usually cannot fuse the extracted feature information at full scale, leaving out effective information, and commonly use transfer learning methods, which rely on the original dataset and training weights. To address the above issues, we propose a deeply supervised (DS) change detection network (DASUNet) that fuses full-scale features, which adopts a Siamese architecture, fuses full-scale feature information, and realizes end-to-end training. In order to obtain higher feature information, the network uses atrous spatial pyramid pooling (ASPP) module in the coding stage. In addition, the DS module is used in the decoding stage to exploit feature information at each scale in the final prediction. The experimental comparison shows that the proposed network has the current state-of-the-art performance on the CDD and the WHU-CD, reaching 94.32% and 90.37% on F1, respectively.</p></abstract><kwd-group xml:lang="en"><title>Keywords</title><kwd>Change detection</kwd><kwd>Atrous spatial pyramid pooling</kwd><kwd>Full-scale feature fusion</kwd><kwd>Deeply supervised layers</kwd></kwd-group><kwd-group kwd-group-type="npg-subject"><title>Subject terms</title><kwd>Computer science</kwd><kwd>Environmental impact</kwd><kwd>Computational science</kwd></kwd-group><custom-meta-group><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#x000a9; Springer Nature Limited 2024</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1"><title>Introduction</title><p id="Par2">In practical applications, change detection (CD) is to identify differences in different time-phase remote sensing images in the same area. At present, with the advancement of high-resolution remote sensing satellite processing and application technology, a large amount of remote sensing image data has emerged, with larger coverage and finer display accuracy. By analyzing remote sensing images of different phases, CD can judge the change characteristics of the same area with less labor cost and higher accuracy, and identify them, so as to provide decision support for land protection and utilization, disaster monitoring, urban planning, etc.</p><p id="Par3">Traditional CD methods can generally be divided into: (1) pixel-based methods and (2) object-based methods. In pixel-based methods, arithmetic operations are usually used to compare the pixel values of a two-phase image, such as image differences<sup><xref ref-type="bibr" rid="CR1">1</xref></sup>, Image regression<sup><xref ref-type="bibr" rid="CR2">2</xref></sup> and image ratios<sup><xref ref-type="bibr" rid="CR3">3</xref></sup>. Then, according to the threshold, the image pixels are divided into variation or non-variation classes, which mainly focus on spectral values and mostly ignore spatial context information<sup><xref ref-type="bibr" rid="CR4">4</xref></sup>. Based on Bayesian theory, Bruzzone et al. proposed two image difference recognition techniques<sup><xref ref-type="bibr" rid="CR5">5</xref></sup>. Zerrouki et al. combines a multivariate exponential weighted moving average (MEWMA) plot with a support vector machine (SVM) to detect changes in the land surface<sup><xref ref-type="bibr" rid="CR6">6</xref></sup>. In the object-based approach, object features are usually established based on the spectra, texture, geometry and other information in the image, such as change vector analysis (CVA)<sup><xref ref-type="bibr" rid="CR4">4</xref></sup>, multivariate alteration detection (MAD)<sup><xref ref-type="bibr" rid="CR7">7</xref></sup> and principal component analysis (PCA)<sup><xref ref-type="bibr" rid="CR8">8</xref></sup>, and so on. Although this kind of method takes into account spatial context information, artificial feature extraction is complex<sup><xref ref-type="bibr" rid="CR9">9</xref></sup>. Based on multi-scale uncertainty analysis, Zhang et al. proposed a new object-based change detection technology<sup><xref ref-type="bibr" rid="CR10">10</xref></sup>. Wu et al. designed a post-classification method based on Bayesian soft fusion and Iterative Slow Feature Analysis (ISFA)<sup><xref ref-type="bibr" rid="CR11">11</xref></sup>.</p><p id="Par4">Since 2012, deep learning technologies have demonstrated significant potential in the image detection and classification. Deep neural networks are particularly suitable for processing detailed feature in high-resolution images, so CD networks are generally closer to deep learning. In the CD networks using deep learning, the pixel-based method is difficult to fully utilize the image spatial information, and the object-based method is limited by the uncertainty of segmenting the object<sup><xref ref-type="bibr" rid="CR12">12</xref></sup>, while the method based on depth features directly learns end-to-end from the labeled change map, which effectively overcomes the influence of light intensity, seasonal change and other factors, and shows good performance<sup><xref ref-type="bibr" rid="CR13">13</xref></sup>. At present, CD methods are mainly based on the extraction of deep features, which utilizes a fully convolutional deep neural network (FCN) to convert the bitemporal images into a high-dimensional space, then uses the depth features as an analysis unit to generate the final change map<sup><xref ref-type="bibr" rid="CR13">13</xref>&#x02013;<xref ref-type="bibr" rid="CR17">17</xref></sup>. The deep feature methods can be further divided into early fusion (EF) and Siamese architectures according to the single-flow structure and dual-flow structure. Daudt et al. first proposed these two architectures and applied them to urban multispectral image CD, and later fused fully convolutional neural network to propose an early fusion and Siamese architecture based on UNet, which used the end-to-end approach to realize the semantic-level segmentation of bitemporal images<sup><xref ref-type="bibr" rid="CR13">13</xref>,<xref ref-type="bibr" rid="CR17">17</xref></sup>. In the early stage of fusion, bitemporal images were fed into the neural network after combining along the channel dimension, because the network of semantic segmentation of a single image is often used, which is prone to missed detection or false detection in large areas. Peng et al. used EF for UNet++, and concatenated different hierarchical change diagrams of the multi-sided output<sup><xref ref-type="bibr" rid="CR18">18</xref></sup>. The Siamese architecture generally uses a network with shared weights to extract the depth features of bitemporal images. Daudt et al. compared the Siamese architecture with the early fusion, and the results showed that the Siamese architecture retains more features of the position information of the bitemporal images, and the detection accuracy is greatly improved<sup><xref ref-type="bibr" rid="CR13">13</xref></sup>. Based on Siamese architecture, Chen et al. designed a spatiotemporal attention module using the self-attention mechanism, and divided the image into multiple scale subregions, which can obtain spatiotemporal correlation at different scales<sup><xref ref-type="bibr" rid="CR19">19</xref></sup>. Lei et al. proposed a pseudo-Siamese structure, which extracts features by a dual-stream structure, but the weights are not shared<sup><xref ref-type="bibr" rid="CR20">20</xref></sup>. Shi et al. adopt Siamese architecture, and proposed a new network based on deeply supervised attention measurement<sup><xref ref-type="bibr" rid="CR21">21</xref></sup>. Zhang et al. used a Siamese architecture to design a deeply supervised network that fuses channels and spatial attention<sup><xref ref-type="bibr" rid="CR12">12</xref></sup>. The success of transformer in natural language processing (NLP) has led researchers to apply it to a variety of computer vision tasks, and Siamese change detection methods using transformer to process features have emerged. Bandara et al. proposed a transformer-based end-to-end Siamese network architecture for change detection<sup><xref ref-type="bibr" rid="CR22">22</xref></sup>. Based on the Siamese architecture, Chen et al. proposed a bitemporal image transformer to efficiently and effectively model contexts within the spatial&#x02013;temporal domain<sup><xref ref-type="bibr" rid="CR23">23</xref></sup>.</p><p id="Par5">The existing deep learning CD networks often draws on the semantic segmentation networks of a single image. The skip connections structure in semantic segmentation can combine low-level detail information with high-level semantic information, so that the prediction region boundary and shape information obtained are more accurate<sup><xref ref-type="bibr" rid="CR13">13</xref>,<xref ref-type="bibr" rid="CR24">24</xref>,<xref ref-type="bibr" rid="CR25">25</xref></sup>. Among them, the UNet series has achieved good detection results with its unique skip connections structure, so it is widely used in the field of CD<sup><xref ref-type="bibr" rid="CR24">24</xref>,<xref ref-type="bibr" rid="CR26">26</xref>,<xref ref-type="bibr" rid="CR27">27</xref></sup>. Daudt et al. proposed the early fusion and the Siamese architecture based on UNet<sup><xref ref-type="bibr" rid="CR13">13</xref></sup>. Codegoni et al. designed a Siamese UNet backbone network for feature extraction by drawing on the UNet structure<sup><xref ref-type="bibr" rid="CR28">28</xref></sup>. Fang et al. used the Siamese structure for UNet++ and designed a densely connected Siamese network for CD<sup><xref ref-type="bibr" rid="CR29">29</xref></sup>. The application of transformer in semantic segmentation also draws on the UNet series. Based on the Swin transformer, Cao et al. proposed a UNet-like pure Transformer network, which is used for medical image segmentation<sup><xref ref-type="bibr" rid="CR30">30</xref></sup>. Chen et al. combined UNet++ with Swin Transformer to propose an automatic medical image segmentation method<sup><xref ref-type="bibr" rid="CR31">31</xref></sup>. The above semantic segmentation model can be modified for change detection, and there are also change detection methods based on transformer. Tang et al. combined Swin Transformer, UNet and Siamese architecture to design a network for remote sensing image change detection<sup><xref ref-type="bibr" rid="CR32">32</xref></sup>. To solve the problem of the quality of feature differences, Guo et al. proposed iterative difference-enhanced transformers (IDET) to optimize feature differences<sup><xref ref-type="bibr" rid="CR33">33</xref></sup>.</p><p id="Par6">However, existing CD networks still have some problems. First of all, previous studies did not fully utilize the multi-scale features extracted in the feature fusion stage, and often only used the features of two adjacent scales. Therefore, in the subsequent prediction, areas of change may be missed in terms of location and shape. Secondly, the information extracted by the hidden layers are not fully utilized, which can significantly affect the subsequent prediction, resulting in insufficient boundary or shape detection of the change area. In addition, transformer has the problems of low computational efficiency and lack of space limitation in the field of computer vision<sup><xref ref-type="bibr" rid="CR34">34</xref>&#x02013;<xref ref-type="bibr" rid="CR36">36</xref></sup>, and at the same time, compared with convolutional neural networks (CNNs), this architecture lacks advantage in parameter sharing and dealing with the problem of bitemporal images change detection<sup><xref ref-type="bibr" rid="CR20">20</xref>,<xref ref-type="bibr" rid="CR28">28</xref></sup>. Finally, in order to speed up the training, many methods use transfer learning, but ignore the differences between the trained dataset and the change detection dataset, which affects the final detection effect.</p><p id="Par7">To address the above issues, a deeply supervised change detection network integrating full-scale features is proposed. Firstly, based on CNNs, this network uses the Siamese structure to extract bitemporal features, receives full-scale feature information in the decoding stage, fuses global-scale features, and realizes end-to-end training. Secondly, the network uses the ASPP module in the coding stage<sup><xref ref-type="bibr" rid="CR37">37</xref></sup>, fused with multi-scale convolutional kernels, and obtained higher-level feature representations. To accelerate model convergence, a deep supervision mechanism is used in the decoding stage to fully leverage the role of feature at each scale in the final prediction.</p><p id="Par8">The main contributions of this article are as follows:<list list-type="order"><list-item><p id="Par9">A full-scale skip connections structure is proposed for CD networks, which allows each decoder layer to combine the larger scale feature maps from the decoder and the smaller and same-scale feature maps from the encoder to obtain richer feature information.</p></list-item><list-item><p id="Par10">We propose a new CD network, DASUNet, which integrates ASPP module into the encoder layer and uses the DS layer to obtain more discriminative features.</p></list-item><list-item><p id="Par11">The proposed DASUNet achieves state-of-the-art (SOTA) performance on the CDD benchmark dataset and the WHU-CD building dataset, with F1 scores of up to 94.32% and 90.37%, respectively.</p></list-item></list></p><p id="Par12">The structure of this paper is shown below: &#x0201c;<xref rid="Sec2" ref-type="sec">Materials and methods</xref>&#x0201d; provides the proposed networks, while &#x0201c;<xref rid="Sec8" ref-type="sec">Results</xref>&#x0201d; presents the setup and results of all experiments. The Discussion is presented in &#x0201c;<xref rid="Sec16" ref-type="sec">Discussion</xref>&#x0201d;. In &#x0201c;<xref rid="Sec17" ref-type="sec">Conclusions</xref>&#x0201d;, we summarized the article.</p></sec><sec id="Sec2"><title>Materials and methods</title><p id="Par13">This section, we describe the network model DASUNet in detail. First, we briefly describe how the various parts of the DASUNet network work. Then, the main structures designed in the network will be detailed, including the full-scale skip connection structure in CD, ASPP, and deep supervision. Finally, we will introduce the loss function, which is closely related to deep supervision.</p><sec id="Sec3"><title>The proposed DASUNet network</title><p id="Par14">In the section, we provide a brief overview of the proposed DASUNet. Figure&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref> shows the architecture of this network. It comprises an encoding stage, a decoding stage, and a DS module. In the encoding stage, the encoders that share weights extract the features of the bitemporal images separately, and then in this stage, the ASPP module is used to extract higher-level feature representations. After that, the bitemporal features extracted from each encoder layer are concatenated. During the decoding phase, the concatenative information is passed to the decoder layer via a full-size skip connection. Finally, deep supervision is used to learn for each encoder layer.<fig id="Fig1"><label>Figure 1</label><caption><p>Overview of the DASUNet<sup><xref ref-type="bibr" rid="CR38">38</xref></sup>. (<bold>a</bold>) The first phase image t1, (<bold>b</bold>) the second phase image t2.</p></caption><graphic xlink:href="41598_2024_63257_Fig1_HTML" id="MO1"/></fig></p></sec><sec id="Sec4"><title>Full-scale skip connection structure in CD</title><p id="Par15">In the field of CD, the objects to be detected are often complex and diverse, ranging from buildings to automobiles, and vary in size. In feature information fusion, the decoder layer of the previous network usually only uses the feature information of adjacent scales, and does not fully utilize the feature information of the whole scale, resulting in the loss of small targets or abnormal target positions.</p><p id="Par16">In the decoding stage of this article, full-scale skip connections are adopted, which can combine low-level details and high-level semantics from feature maps at different scales. In order to accurately identify changing objects, both accurate high-level semantic information and position information are required, and full-scale skip connections can send the information to each decoder layer to fuse global features at each scale.</p><p id="Par17">In Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref>, the subscript of X is divided into A and B, where A represents the first phase encoder and B represents the second phase encoder. An X with a superscript (x, 0) indicates the encoder, where x is 0, 1, 2, 3, representing encoders of different scales. The number of channels for the encoder to extract features is 64, 128, 256, and 512, and the width and height are 256&#x02009;&#x000d7;&#x02009;256, 128&#x02009;&#x000d7;&#x02009;128, 64&#x02009;&#x000d7;&#x02009;64, 32&#x02009;&#x000d7;&#x02009;32, respectively. An X with a superscript (x, 1) represents the decoder, where x is 2, 1, 0, which represents the full-scale feature convolution block that receives the extraction. The number of channels for the decoder to extract features is 64, 64, and 64, respectively, and the width and height are 64&#x02009;&#x000d7;&#x02009;64, 128&#x02009;&#x000d7;&#x02009;128, 256&#x02009;&#x000d7;&#x02009;256, respectively.<fig id="Fig2"><label>Figure 2</label><caption><p>Full-scale skip connections in CD.</p></caption><graphic xlink:href="41598_2024_63257_Fig2_HTML" id="MO2"/></fig></p><p id="Par18">Compared with the semantic segmentation of a single image, change detection places greater emphasis on the matching of bitemporal feature maps. In view of the particularity of CD, the full-scale skip connections in this article no longer performs the channel alignment operation of each scale feature map.</p><p id="Par19">Taking the decoder X<sup>1,1</sup> as an example, you need to accept the bitemporal features extracted by the X<sup>3,0</sup>, X<sup>1,0</sup>, X<sup>0,0</sup> encoder and high semantic features after decoder X<sup>2,1</sup> processing. Let x<sup>1,1</sup> represents the output of X<sup>1,1</sup>, and X<sup>(x,0)</sup> is the bitemporal features extracted by the X<sup>(x,0)</sup> encoder. The stack of feature maps represented by x<sup>1,1</sup> is computed as:<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${x}^{\text{1,1}}=h\left(\left[u\left({x}^{\text{3,0}}\right),{u\left({x}^{\text{2,1}}\right),x}^{\text{1,0}},p\left({x}^{\text{0,0}}\right)\right]\right)$$\end{document}</tex-math><mml:math id="M2" display="block"><mml:mrow><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mtext>1,1</mml:mtext></mml:msup><mml:mo>=</mml:mo><mml:mi>h</mml:mi><mml:mfenced close=")" open="("><mml:mfenced close="]" open="["><mml:mi>u</mml:mi><mml:mfenced close=")" open="("><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mtext>3,0</mml:mtext></mml:msup></mml:mfenced><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>u</mml:mi><mml:mfenced close=")" open="("><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mtext>2,1</mml:mtext></mml:msup></mml:mfenced><mml:mo>,</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mtext>1,0</mml:mtext></mml:msup><mml:mo>,</mml:mo><mml:mi>p</mml:mi><mml:mfenced close=")" open="("><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mtext>0,0</mml:mtext></mml:msup></mml:mfenced></mml:mfenced></mml:mfenced></mml:mrow></mml:math><graphic xlink:href="41598_2024_63257_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula>where h(&#x02219;) represents the convolutional block operation, [&#x02219;] represents the concatenation, u(&#x02219;) indicates an up-sampling operation, and p(&#x02219;) indicates a down-sampling operation.</p><p id="Par20">The encoder layer is indexed with i, and x<sup>(i,0)</sup> represent the two-phase features extracted by the encoder layer X<sup>(i,0)</sup>. The decoder layer is indexed with j to represent the high semantic features generated by the decoder layer. The decoder output can be expressed as:<disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${x}^{j,1}=\left\{\begin{array}{c}h\left(\left[p\left({x}_{i&#x0003c;j}^{i,0}\right),{x}_{i=j}^{j,0},u\left({x}_{i&#x0003e;j,i&#x0003c;3}^{i,1}\right),u\left({x}_{i=3}^{3,0}\right)\right]\right), \quad j&#x0003c;2;i=\text{0,1},\text{2,3}\\ h\left(\left[p\left({x}_{i&#x0003c;j}^{i,0}\right),{x}_{i=j}^{i,0},u\left({x}_{i&#x0003e;j}^{i,0}\right)\right]\right), \quad j=2;i=\text{0,1},\text{2,3}\end{array}\right.$$\end{document}</tex-math><mml:math id="M4" display="block"><mml:mrow><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mfenced open="{"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mi>h</mml:mi><mml:mfenced close=")" open="("><mml:mfenced close="]" open="["><mml:mi>p</mml:mi><mml:mfenced close=")" open="("><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#x0003c;</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msubsup></mml:mfenced><mml:mo>,</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mi>u</mml:mi><mml:mfenced close=")" open="("><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#x0003e;</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>&#x0003c;</mml:mo><mml:mn>3</mml:mn></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mfenced><mml:mo>,</mml:mo><mml:mi>u</mml:mi><mml:mfenced close=")" open="("><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msubsup></mml:mfenced></mml:mfenced></mml:mfenced><mml:mo>,</mml:mo><mml:mspace width="1em"/><mml:mi>j</mml:mi><mml:mo>&#x0003c;</mml:mo><mml:mn>2</mml:mn><mml:mo>&#x0037e;</mml:mo><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mtext>0,1</mml:mtext><mml:mo>,</mml:mo><mml:mtext>2,3</mml:mtext></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mrow/><mml:mi>h</mml:mi><mml:mfenced close=")" open="("><mml:mfenced close="]" open="["><mml:mi>p</mml:mi><mml:mfenced close=")" open="("><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#x0003c;</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msubsup></mml:mfenced><mml:mo>,</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mi>u</mml:mi><mml:mfenced close=")" open="("><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#x0003e;</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msubsup></mml:mfenced></mml:mfenced></mml:mfenced><mml:mo>,</mml:mo><mml:mspace width="1em"/><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mo>&#x0037e;</mml:mo><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mtext>0,1</mml:mtext><mml:mo>,</mml:mo><mml:mtext>2,3</mml:mtext></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mrow></mml:math><graphic xlink:href="41598_2024_63257_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula>where h(&#x02219;) represents the convolutional block operation, [&#x02219;] represents the concatenation, u(&#x02219;) indicates an up-sampling operation, and p(&#x02219;) indicates a down-sampling operation.</p><p id="Par21">In this article, the convolution block adopts a residual structure (Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3</xref>), and the residual connection line is placed after the first convolutional layer, and an additional 1&#x02009;&#x000d7;&#x02009;1 convolutional layer is no longer required for the channel number transformation. On the one hand, this design reduces the number of parameters compared with the traditional residual convolutional block design. On the other hand, the 3&#x02009;&#x000d7;&#x02009;3 convolutional layer has a larger receptive field than the 1&#x02009;&#x000d7;&#x02009;1 convolutional layer, extracts more abundant feature information, and has more advantages in the identity mapping of the residual structure.<fig id="Fig3"><label>Figure 3</label><caption><p>The convolutional blocks.</p></caption><graphic xlink:href="41598_2024_63257_Fig3_HTML" id="MO3"/></fig></p></sec><sec id="Sec5"><title>ASPP module</title><p id="Par22">High-resolution images contain rich information, and the detection targets in the images are often more complex and diverse. Therefore, in this article, the original image is down-sampled by a factor of eight instead of sixteen to preserve more of the original information. The traditional convolutional block uses 3&#x02009;&#x000d7;&#x02009;3 convolutional kernels, and the field of view is very small and is difficult to distinguish between pairs of features that represent non-obvious. In this article, the ASPP module (Fig.&#x000a0;<xref rid="Fig4" ref-type="fig">4</xref>) is used to expand the convolution field by using dilated convolution, and the spatial pyramid structure is utilized to obtain rich feature information.<fig id="Fig4"><label>Figure 4</label><caption><p>The ASPP module.</p></caption><graphic xlink:href="41598_2024_63257_Fig4_HTML" id="MO4"/></fig></p><p id="Par23">Specifically, the ASPP module divides the input into five pathways: three atrous convolutions, with kernel sizes of 3&#x02009;&#x000d7;&#x02009;3 and atrous rates of 1, 2, and 3, respectively, which are mainly used to expand the receptive field and extract richer feature information; a 1&#x02009;&#x000d7;&#x02009;1 convolution for dimensionality reduction; Image Pooling is used to complement global features. Finally, the output of these five layers is concatenated, and the dimensionality is reduced to a given number of channels with a 1&#x02009;&#x000d7;&#x02009;1 convolutional layer. Let x<sup>in</sup> and x<sup>out</sup> represent the input and output features, respectively, and the ASPP module can be represented as follows:<disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${x}^{out}=C\left(\left[{AC}^{1}\left({x}^{in}\right),{AC}^{2}\left({x}^{in}\right),{AC}^{3}\left({x}^{in}\right),C\left({x}^{in}\right),U\left(C\left(P\left({x}^{in}\right)\right)\right)\right]\right)$$\end{document}</tex-math><mml:math id="M6" display="block"><mml:mrow><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">out</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>C</mml:mi><mml:mfenced close=")" open="("><mml:mfenced close="]" open="["><mml:msup><mml:mrow><mml:mi mathvariant="italic">AC</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msup><mml:mfenced close=")" open="("><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">in</mml:mi></mml:mrow></mml:msup></mml:mfenced><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="italic">AC</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mfenced close=")" open="("><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">in</mml:mi></mml:mrow></mml:msup></mml:mfenced><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="italic">AC</mml:mi></mml:mrow><mml:mn>3</mml:mn></mml:msup><mml:mfenced close=")" open="("><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">in</mml:mi></mml:mrow></mml:msup></mml:mfenced><mml:mo>,</mml:mo><mml:mi>C</mml:mi><mml:mfenced close=")" open="("><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">in</mml:mi></mml:mrow></mml:msup></mml:mfenced><mml:mo>,</mml:mo><mml:mi>U</mml:mi><mml:mfenced close=")" open="("><mml:mi>C</mml:mi><mml:mfenced close=")" open="("><mml:mi>P</mml:mi><mml:mfenced close=")" open="("><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">in</mml:mi></mml:mrow></mml:msup></mml:mfenced></mml:mfenced></mml:mfenced></mml:mfenced></mml:mfenced></mml:mrow></mml:math><graphic xlink:href="41598_2024_63257_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula>where C(&#x02219;) stands for the convolutional block. AC(&#x02219;) stands for the atrous convolutional block, and both padding and dilation rates are determined by superscript. [&#x02219;] represents the concatenation, U(&#x02219;) indicates an up-sampling operation, and P(&#x02219;) indicates a down-sampling operation.</p><p id="Par24">Considering that the ASPP module in this article is located in the last layer of the encoding stage, and the original image has been pooled for multiple rounds, the void rate of the void convolution is set to 1, 2, 3.</p></sec><sec id="Sec6"><title>DS module</title><p id="Par25">In general, most traditional end-to-end deep convolutional neural networks only provide supervision of the output layer. However, the training of the hidden layer of deep convolutional networks is unsupervised, which will inevitably affect the subsequent prediction.</p><p id="Par26">Therefore, this article uses a DS module to supervise all three decoder layers, which helps the hidden layers learn more discriminative features to improve the prediction accuracy.</p><p id="Par27">As an example, we can express the weights of each layer from input to output as W <sup>(1)</sup>, &#x02026;, W<sup>(n)</sup> for a common end-to-end convolutional network of N layers. And the weight of the output layer is W<sup>(n)</sup>. The weights of the output layer and all previous layers are recorded as W<sup>n</sup>&#x02009;=&#x02009;{W<sup>(1)</sup>, &#x02026;, W<sup>(n)</sup>}, and the objective function can be computed as:<disp-formula id="Equ4"><label>4</label><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$P\left(W\right)=L\left({W}^{n},T\right)$$\end{document}</tex-math><mml:math id="M8" display="block"><mml:mrow><mml:mi>P</mml:mi><mml:mfenced close=")" open="("><mml:mi>W</mml:mi></mml:mfenced><mml:mo>=</mml:mo><mml:mi>L</mml:mi><mml:mfenced close=")" open="("><mml:msup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:mi>T</mml:mi></mml:mfenced></mml:mrow></mml:math><graphic xlink:href="41598_2024_63257_Article_Equ4.gif" position="anchor"/></alternatives></disp-formula>where T represents the true label, and L (W<sup>n</sup>, T) is the loss directly determined by W<sup>n</sup>.</p><p id="Par28">The outputs of the two additional hidden layers and the final layer this article are represented as out-1, out-2 and out-3. We can express the weight of each layer from input to output as W <sup>(1)</sup>, &#x02026;, W<sup>(out-1)</sup>, &#x02026;, W<sup>(out-2)</sup>, &#x02026;, W <sup>(out-3)</sup>, where the weight of the output layers are W<sup>(out-1)</sup>, W<sup>(out-2)</sup> and W<sup>(out-3)</sup>, respectively. Denote the weights of the three output layers and all previous layers as W<sup>out-1</sup>&#x02009;=&#x02009;{W<sup>(1)</sup>,&#x02026;,W<sup>(out-1)</sup>}, W<sup>out-2</sup>&#x02009;=&#x02009;{W<sup>(1)</sup>,&#x02026;,W<sup>(out-1)</sup>,&#x02026;,W<sup>(out-2)</sup>} and W<sup>out-3</sup>&#x02009;=&#x02009;{W<sup>(1)</sup>,&#x02026;,W<sup>(out-1)</sup>,&#x02026;,W<sup>(out-2)</sup>,&#x02026;,W<sup>(out-3)</sup>}. The objective function in this article can be computed as:<disp-formula id="Equ5"><label>5</label><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$P\left(W\right)=\sum_{m=1}^{3}{a}_{m}L\left({W}^{out-m},T\right)$$\end{document}</tex-math><mml:math id="M10" display="block"><mml:mrow><mml:mi>P</mml:mi><mml:mfenced close=")" open="("><mml:mi>W</mml:mi></mml:mfenced><mml:mo>=</mml:mo><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>3</mml:mn></mml:munderover><mml:msub><mml:mi>a</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mi>L</mml:mi><mml:mfenced close=")" open="("><mml:msup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi>T</mml:mi></mml:mfenced></mml:mrow></mml:math><graphic xlink:href="41598_2024_63257_Article_Equ5.gif" position="anchor"/></alternatives></disp-formula>where m represents the output layer index, and a is the weight factor of the corresponding output layer in the total loss function.</p><p id="Par29">It is worth mentioning that, the outputs of the two additional hidden layers are fed into a 1&#x02009;&#x000d7;&#x02009;1 convolutional layer, and then restored to the original image size through bilinear up-sampling.</p></sec><sec id="Sec7"><title>Loss function</title><p id="Par30">In this article, to weaken the influence of positive and negative sample imbalance, the loss function combines cross-entropy (ce) loss and dice loss (dice). The formula for the composite loss function is computed as:<disp-formula id="Equ6"><label>6</label><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$L={L}_{ce}+{L}_{dice}.$$\end{document}</tex-math><mml:math id="M12" display="block"><mml:mrow><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi mathvariant="italic">ce</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi mathvariant="italic">dice</mml:mi></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mrow></mml:math><graphic xlink:href="41598_2024_63257_Article_Equ6.gif" position="anchor"/></alternatives></disp-formula></p><p id="Par31">The cross-entropy loss formula is computed as:<disp-formula id="Equ7"><label>7</label><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${L}_{ce}=\frac{1}{N}\sum_{n=1}^{N}-{Y}_{n}\text{log}\left({P}_{n}\right)-\left(1-{Y}_{n}\right)\text{log}(1-{P}_{n}).$$\end{document}</tex-math><mml:math id="M14" display="block"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi mathvariant="italic">ce</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mo>-</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mtext>log</mml:mtext><mml:mfenced close=")" open="("><mml:msub><mml:mi>P</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mfenced><mml:mo>-</mml:mo><mml:mfenced close=")" open="("><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mfenced><mml:mtext>log</mml:mtext><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math><graphic xlink:href="41598_2024_63257_Article_Equ7.gif" position="anchor"/></alternatives></disp-formula></p><p id="Par32">The formula for the loss of the dice coefficient is as follows:<disp-formula id="Equ8"><label>8</label><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${L}_{dice}=1-\frac{2\sum_{n=1}^{N}{Y}_{n}{P}_{n}}{\sum_{n=1}^{N}{Y}_{n}+\sum_{n=1}^{N}{P}_{n}}.$$\end{document}</tex-math><mml:math id="M16" display="block"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi mathvariant="italic">dice</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup><mml:msub><mml:mi>Y</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:msub><mml:mi>P</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup><mml:msub><mml:mi>Y</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup><mml:msub><mml:mi>P</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:math><graphic xlink:href="41598_2024_63257_Article_Equ8.gif" position="anchor"/></alternatives></disp-formula>where N is the number of pixels, Y<sub>n</sub> is the true value of the category, and P<sub>n</sub> is the predicted value of the model.</p><p id="Par33">In this article, the deep supervision mechanism is adopted, the weight value of each side output is set to 1, and the loss function of the model can be calculated as:<disp-formula id="Equ9"><label>9</label><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$L=\sum_{n=1}^{3}{L}^{n}.$$\end{document}</tex-math><mml:math id="M18" display="block"><mml:mrow><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>3</mml:mn></mml:munderover><mml:msup><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:math><graphic xlink:href="41598_2024_63257_Article_Equ9.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq1"><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${L}^{n}={L}_{ce}^{n}+{L}_{dice}^{n}$$\end{document}</tex-math><mml:math id="M20"><mml:mrow><mml:msup><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>L</mml:mi><mml:mrow><mml:mi mathvariant="italic">ce</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>L</mml:mi><mml:mrow><mml:mi mathvariant="italic">dice</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msubsup></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_63257_Article_IEq1.gif"/></alternatives></inline-formula>.</p></sec></sec><sec id="Sec8"><title>Results</title><sec id="Sec9"><title>Experimental setup</title><p id="Par34">In this section, experimental environment, experimental datasets and corresponding evaluation indicators are described in detail. Then we conducted experiments on CDD and WHU-CD datasets to verify the model effectiveness. The advantages of this model are pointed out by comparing the model with similar models, and then the contribution of each submodule is verified by ablation experiments.</p><sec id="Sec10"><title>Experimental environment</title><p id="Par35">In this experiment, the model iteration is set to 100 times, the initial learning rate is 0.001. The learning rate is updated by using a fixed-length decay strategy, and the learning rate is halved every 6 epochs, and the batch size is set to 8. AdamW was used to optimize the model parameters.</p><p id="Par36">To increase the diversity of data, the training dataset is enhanced during training, including vertical and horizontal flipping, and random 90-degree, 180-degree, and 270-degree rotation of the image. All methods are implemented based on the Pytorch framework, and the hardware environment is NVIDIA Tesla-T4 16&#x000a0;GB GPU.</p></sec><sec id="Sec11"><title>Datasets</title><p id="Par37">The CDD dataset is a public seasonal CD dataset. The dataset contains 11 pairs of seasonal change images, four pairs of sizes 1900&#x02009;&#x000d7;&#x02009;1000 and seven are 4725&#x02009;&#x000d7;&#x02009;2200. The spatial resolution of the image is 3&#x02013;100&#x000a0;cm/px<sup><xref ref-type="bibr" rid="CR39">39</xref></sup>. The image is cropped into sub-images of a size of 256&#x02009;&#x000d7;&#x02009;256. The final dataset contains 16,000 image pairs, which are divided into a training set, a test set, and a validation set according to 10:3:3.</p><p id="Par38">WHU-CD is a public building CD dataset<sup><xref ref-type="bibr" rid="CR40">40</xref></sup>. The original dataset contains two datasets, in which the training set contains a pair of aerial images of 21,243&#x02009;&#x000d7;&#x02009;15,354 in 2012 and 2018, and the test set contains a pair of aerial images of 11,265&#x02009;&#x000d7;&#x02009;15,354 of the same age, all with a spatial resolution of 0.075&#x000a0;m. According to the dataset division standard, the fused aerial images of 32,507&#x02009;&#x000d7;&#x02009;15,354 are cropped into blocks of 256&#x02009;&#x000d7;&#x02009;256 size, and there was no overlap. Then the whole images were randomly divided into 5204 pairs of training set, 744 pairs of validation set and 1486 pairs of testing set according to the ratio of 7:1:2.</p></sec><sec id="Sec12"><title>Evaluation metrics</title><p id="Par39">In this article, we used four indicators to evaluate the model performance on the CDD dataset and WHU-CD dataset, namely: accuracy (OA), precision (P), recall (R), F1score (F1). These metrics are defined as:<disp-formula id="Equ10"><label>10</label><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$OA=\frac{TP+TN}{TP+TN+FP+FN}$$\end{document}</tex-math><mml:math id="M22" display="block"><mml:mrow><mml:mi>O</mml:mi><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math><graphic xlink:href="41598_2024_63257_Article_Equ10.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ11"><label>11</label><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$P=\frac{TP}{TP+FP}$$\end{document}</tex-math><mml:math id="M24" display="block"><mml:mrow><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="italic">TP</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math><graphic xlink:href="41598_2024_63257_Article_Equ11.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ12"><label>12</label><alternatives><tex-math id="M25">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$R=\frac{TP}{TP+FN}$$\end{document}</tex-math><mml:math id="M26" display="block"><mml:mrow><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="italic">TP</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math><graphic xlink:href="41598_2024_63257_Article_Equ12.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ13"><label>13</label><alternatives><tex-math id="M27">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${F}_{1}=\frac{2PR}{P+R}$$\end{document}</tex-math><mml:math id="M28" display="block"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mi>P</mml:mi><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math><graphic xlink:href="41598_2024_63257_Article_Equ13.gif" position="anchor"/></alternatives></disp-formula>where TP, TN, FP and FN refer to true positives, true negatives, false positives and false negatives, respectively.</p></sec></sec><sec id="Sec13"><title>Comparison with SOTA networks</title><p id="Par40">We compare the SOTA model with DASUNet to verify the effectiveness of the model in this article. The comparison model is as follows***:</p><p id="Par41">FC-EF<sup><xref ref-type="bibr" rid="CR13">13</xref></sup> uses early fusion for CD.</p><p id="Par42">FC-Siam-Diff<sup><xref ref-type="bibr" rid="CR13">13</xref></sup> achieves CD by fusing the differential features of the Siamese network.</p><p id="Par43">FC-Siam-Conc<sup><xref ref-type="bibr" rid="CR13">13</xref></sup> achieves CD by fusing bitemporal features of the Siamese network.</p><p id="Par44">L-UNet<sup><xref ref-type="bibr" rid="CR41">41</xref></sup> uses a UNet-like structure to model encoder extraction features through an integrated fully convolutional LSTM block to achieve CD.</p><p id="Par45">IFNet<sup><xref ref-type="bibr" rid="CR12">12</xref></sup> designs a depth-supervised differential discriminant network.</p><p id="Par46">SNUNet<sup><xref ref-type="bibr" rid="CR29">29</xref></sup> combines the nested and densely connection with Siamese network, based on UNet++. To be fair, we choose SNUNet-24 with the same number of parameters size as DASUNet in this article.</p><p id="Par47">USSFC-Net<sup><xref ref-type="bibr" rid="CR20">20</xref></sup> designs the multi-scale decoupled convolution and uses a non-weighted shared pseudo-Siamese structure to extract bitemporal features.</p><p id="Par48">TinyCD</p><p id="Par49"><sup><xref ref-type="bibr" rid="CR28">28</xref></sup> uses a pre-trained EfficientNet backbone to extract features, mix and attention mask block for feature information enhancement, and a pixel-by-pixel classifier to generate the final output.</p><p id="Par50">ChangeFormer<sup><xref ref-type="bibr" rid="CR22">22</xref></sup> is a Transformer-based Siamese architecture that unifies a hierarchical transformer encoder with a multi-layer-aware decoder in a Siamese architecture.</p><p id="Par51">IDET<sup><xref ref-type="bibr" rid="CR33">33</xref></sup> is an iterative differential enhancement transformer that consists of three transformers, two for extracting telematics from two images and one for enhancing feature differences. At the same time, the author uses it for change detection.</p><p id="Par52">ScratchFormer<sup><xref ref-type="bibr" rid="CR23">23</xref></sup> uses a scrambled sparse attention operation to capture the intrinsic features of the CD data, and introduces a Change Detection Feature Fusion module to fuse features from input image pairs.</p><p id="Par53">Swin-UNet-CD<sup><xref ref-type="bibr" rid="CR30">30</xref></sup> is an early fusion strategy for Swin-Unet change detection network, we only adjusted the number of input channels for Swin-Unet network.</p><p id="Par54">DASUNet-32 is based on DASUNet-64 and the number of channels is halved.</p></sec><sec id="Sec14"><title>Comparison experiments</title><p id="Par55">Table <xref rid="Tab1" ref-type="table">1</xref> show the results of the comparative experiments on the two datasets, respectively. On the CDD dataset, the F1 index of DASUNet is 0.85% higher than the current best network SNUNet-24. The F1 index of DASUNet is 0.36% higher than the current best network TinyCD on the WHU-CD dataset. It is worth mentioning that DASUNet-32 can still achieve good results on the two datasets, which is more balanced on both datasets than the USSFC-Net network.<table-wrap id="Tab1"><label>Table 1</label><caption><p>Comparison of experimental results on CDD and WHU-CD.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="2">Method type</th><th align="left" rowspan="2">Network</th><th align="left" colspan="4">CDD</th><th align="left" colspan="4">WHU-CD</th></tr><tr><th align="left">P (%)</th><th align="left">R (%)</th><th align="left">F1 (%)</th><th align="left">OA (%)</th><th align="left">P (%)</th><th align="left">R (%)</th><th align="left">F1 (%)</th><th align="left">OA (%)</th></tr></thead><tbody><tr><td align="left" rowspan="10">CNN</td><td align="left">FC-EF</td><td align="left">75.88</td><td align="left">43.73</td><td align="left">55.49</td><td align="left">91.95</td><td align="left">76.67</td><td align="left">63.93</td><td align="left">69.73</td><td align="left">95.24</td></tr><tr><td align="left">FC-Siam-conc</td><td align="left">74.94</td><td align="left">50.21</td><td align="left">60.13</td><td align="left">92.36</td><td align="left">41.16</td><td align="left">85.22</td><td align="left">55.51</td><td align="left">88.29</td></tr><tr><td align="left">FC-Siam-diff</td><td align="left">80.39</td><td align="left">56.74</td><td align="left">66.53</td><td align="left">93.45</td><td align="left">43.24</td><td align="left">88.58</td><td align="left">58.12</td><td align="left">89.06</td></tr><tr><td align="left">L-UNet</td><td align="left">91.79</td><td align="left">81.35</td><td align="left">86.25</td><td align="left">97.02</td><td align="left">64.54</td><td align="left">78.29</td><td align="left">70.76</td><td align="left">94.45</td></tr><tr><td align="left">IFNet</td><td align="left">92.35</td><td align="left">84.58</td><td align="left">88.29</td><td align="left">97.43</td><td align="left"><italic>90.39</italic></td><td align="left">87.02</td><td align="left">88.67</td><td align="left">98.09</td></tr><tr><td align="left">SNUNet-24</td><td align="left"><underline>94.80</underline></td><td align="left"><underline>92.18</underline></td><td align="left"><underline>93.47</underline></td><td align="left"><underline>98.52</underline></td><td align="left"><underline>90.15</underline></td><td align="left">89.46</td><td align="left">89.81</td><td align="left">98.25</td></tr><tr><td align="left">USSFC-Net</td><td align="left">91.29</td><td align="left">83.16</td><td align="left">87.04</td><td align="left">97.16</td><td align="left">89.49</td><td align="left">90.31</td><td align="left">89.90</td><td align="left">98.26</td></tr><tr><td align="left">TinyCD</td><td align="left">90.48</td><td align="left">82.39</td><td align="left">86.24</td><td align="left">96.98</td><td align="left">89.63</td><td align="left"><italic>90.40</italic></td><td align="left"><underline>90.01</underline></td><td align="left"><underline>98.28</underline></td></tr><tr><td align="left">DASUNet-32</td><td align="left">92.93</td><td align="left">89.38</td><td align="left">91.12</td><td align="left">98.01</td><td align="left">89.81</td><td align="left">86.24</td><td align="left">87.98</td><td align="left">97.98</td></tr><tr><td align="left">DASUNet-64</td><td align="left"><italic>94.94</italic></td><td align="left"><italic>93.7</italic></td><td align="left"><italic>94.32</italic></td><td align="left"><italic>98.71</italic></td><td align="left">90.01</td><td align="left"><underline>90.39</underline></td><td align="left"><italic>90.37</italic></td><td align="left"><italic>98.33</italic></td></tr><tr><td align="left" rowspan="4">Transformer</td><td align="left">ChangeFormer</td><td align="left">90.11</td><td align="left">77.39</td><td align="left">83.36</td><td align="left">96.43</td><td align="left">88.04</td><td align="left">80.66</td><td align="left">84.19</td><td align="left">97.40</td></tr><tr><td align="left">IDET</td><td align="left">85.01</td><td align="left">62.90</td><td align="left">72.30</td><td align="left">94.47</td><td align="left">72.94</td><td align="left">81.06</td><td align="left">76.78</td><td align="left">95.80</td></tr><tr><td align="left">ScratchFormer</td><td align="left">91.17</td><td align="left">80.67</td><td align="left">85.60</td><td align="left">96.88</td><td align="left">88.77</td><td align="left">83.45</td><td align="left">86.03</td><td align="left">97.68</td></tr><tr><td align="left">Swin-UNet-CD</td><td align="left">81.96</td><td align="left">59.48</td><td align="left">68.93</td><td align="left">93.85</td><td align="left">87.39</td><td align="left">73.08</td><td align="left">79.59</td><td align="left">96.79</td></tr></tbody></table><table-wrap-foot><p>The best two results are in italics and underlined, respectively.</p></table-wrap-foot></table-wrap></p><p id="Par56">Figure&#x000a0;<xref rid="Fig5" ref-type="fig">5</xref> shows the visual comparison results on the CDD dataset. In the first row of building detection, there are obvious false detections in the upper left corner of FC-EF, FC-Siam-Conc, and DASUNet and FC-Siam-Diff achieves good results in detecting complete large areas. In the second line of road detection, there are obvious missed detections of FC-Siam-Conc, IFNet and FC-Siam-Diff, and the change area predicted by DASUNet is relatively complete. In the detection of vehicles in line three, FC-Siam-Diff, ChangeFormer and IFNet have obvious regional connections, and the network in this article can clearly see the boundaries of each vehicle. In the detection of both large area and small target in the fourth row, the other networks did not detect the small vehicle targets, and there are serious false detections. But the proposed network in this article achieves the synchronous detection of large areas and small targets. In the 5th line of vehicle and road detection, due to the influence of the season, the leaves are obviously occluded, and the other networks do not detect continuous road information, and the network in this article has clear road boundary information.<fig id="Fig5"><label>Figure 5</label><caption><p>Visual comparison results on the CDD<sup><xref ref-type="bibr" rid="CR38">38</xref></sup>; (<bold>a</bold>) Image at time1; (<bold>b</bold>) Image at time2; (<bold>c</bold>) Ground truth; (<bold>d</bold>) FC-EF; (<bold>e</bold>) FC-Siam-Conc; (<bold>f</bold>) FC-Siam-Diff; (<bold>g</bold>) IFNet; (<bold>h</bold>) SNUNet-24; (<bold>i</bold>) USSFC-Net; (<bold>j</bold>) ChangeFormer; and (<bold>k</bold>) DASUNet. The black area is the non-variation category, and the white area is the variation class.</p></caption><graphic xlink:href="41598_2024_63257_Fig5_HTML" id="MO5"/></fig></p><p id="Par57">Figure&#x000a0;<xref rid="Fig6" ref-type="fig">6</xref> shows the visual comparison results on the WHU-CD dataset. In the detection of the first line of buildings, although the boundary information of the building is obvious, there are serious boundary misdetections in FC-Siam-Conc and FC-Siam-Diff, while there are obvious missed detections in IFNet, ChangeFormer and FC-EF. In the second line of building disappearance detection, the boundary of SNUNet-24 is blurred due to the occlusion of leaves, and the complete boundary information is detected by IFNet, USSFC-Net and the proposed network. In the third row, compared with IFNet, SNUNet-24 and USSFC-Net, DASUNet detect more complete building boundary information. At last, in the fourth row of building cluster detection, IFNet, SNUNet-24, ChangeFormer and USSFC-Net all have obvious boundary connections, and the boundaries of each building can be detected by DASUNet.<fig id="Fig6"><label>Figure 6</label><caption><p>Visual comparison results on the WHU-CD<sup><xref ref-type="bibr" rid="CR38">38</xref></sup>; (<bold>a</bold>) Image at time1; (<bold>b</bold>) Image at time2; (<bold>c</bold>) Ground truth; (<bold>d</bold>) FC-EF; (<bold>e</bold>) FC-Siam-Conc; (<bold>f</bold>) FC-Siam-Diff; (<bold>g</bold>) IFNet; (<bold>h</bold>) SNUNet-24; (<bold>i</bold>) USSFC-Net; (<bold>j</bold>) ChangeFormer; and (<bold>k</bold>) DASUNet. The black area is the non-variation category, and the white area is the variation class.</p></caption><graphic xlink:href="41598_2024_63257_Fig6_HTML" id="MO6"/></fig></p></sec><sec id="Sec15"><title>Ablation experiments</title><p id="Par58">In this section, ablation experiments were performed between the ASPP module and the DS module to evaluate the performance of each module. As can be seen from Table <xref rid="Tab2" ref-type="table">2</xref>,<table-wrap id="Tab2"><label>Table 2</label><caption><p>Ablation experimental results on CDD and WHU-CD.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="2">Network</th><th align="left" colspan="4">CDD</th><th align="left" colspan="4">WHU-CD</th></tr><tr><th align="left">P (%)</th><th align="left">R (%)</th><th align="left">F1 (%)</th><th align="left">OA (%)</th><th align="left">P (%)</th><th align="left">R (%)</th><th align="left">F1 (%)</th><th align="left">OA (%)</th></tr></thead><tbody><tr><td align="left">Base</td><td align="left">94.30</td><td align="left">91.58</td><td align="left">92.92</td><td align="left">98.40</td><td align="left">88.55</td><td align="left">86.72</td><td align="left">87.63</td><td align="left">97.90</td></tr><tr><td align="left">Base&#x02009;+&#x02009;ASPP</td><td align="left">94.83</td><td align="left"><italic>93.43</italic></td><td align="left"><italic>94.12</italic></td><td align="left"><italic>98.66</italic></td><td align="left"><italic>90.60</italic></td><td align="left">88.01</td><td align="left">89.28</td><td align="left">98.19</td></tr><tr><td align="left">Base&#x02009;+&#x02009;DS</td><td align="left"><bold>95.04</bold></td><td align="left">92.86</td><td align="left">93.93</td><td align="left">98.62</td><td align="left"><bold>90.80</bold></td><td align="left"><italic>88.91</italic></td><td align="left"><italic>89.85</italic></td><td align="left"><italic>98.28</italic></td></tr><tr><td align="left">Base&#x02009;+&#x02009;ASPP&#x02009;+&#x02009;DS</td><td align="left"><italic>94.94</italic></td><td align="left"><bold>93.70</bold></td><td align="left"><bold>94.32</bold></td><td align="left"><bold>98.71</bold></td><td align="left">90.01</td><td align="left"><bold>90.39</bold></td><td align="left"><bold>90.37</bold></td><td align="left"><bold>98.33</bold></td></tr></tbody></table><table-wrap-foot><p>The best two results are in bold and italics, respectively.</p></table-wrap-foot></table-wrap></p><p id="Par59">F1 increases by 1.2% and 1.56% respectively after adding the ASPP module, indicating that the model extracts richer multi-scale features after adding the ASPP module, and F1 increases by 1.01% and 2.22% respectively after adding the deep supervision module, indicating that the added side auxiliary branches play a better role in the final prediction of semantic information at all levels. At the same time, in F1, the complete model with two modules is increased respectively by 1.4% and 2.74%, achieving a good module integration effect. It is worth noting that the indices of the complete model are more balanced, while the single-module model tends to focus on accuracy without fully considering the false positive and false negative rates, resulting in the F1 index being inferior to the complete model. This also reflects the better real performance of the complete model.</p><p id="Par60">Figure&#x000a0;<xref rid="Fig7" ref-type="fig">7</xref> show the training curves of F1 for each module in the ablation experiment, and the curve performance of each module is basically consistent with the data in Table <xref rid="Tab2" ref-type="table">2</xref> when the learning rate decay is consistent.<fig id="Fig7"><label>Figure 7</label><caption><p>F<sub>1</sub>scores training curve (<bold>a</bold>) on CDD (<bold>b</bold>) and on WHU-CD.</p></caption><graphic xlink:href="41598_2024_63257_Fig7_HTML" id="MO7"/></fig></p></sec></sec><sec id="Sec16"><title>Discussion</title><p id="Par61">We verify the effectiveness of the proposed network on the CDD dataset and the WHU-CD dataset, respectively. Compared with other SOTA networks, such as TinyCD, which performs well on the building dataset but does not perform well on the seasonal change dataset CDD, DASUNet shows good performance in both the boundary information prediction of large targets and the shape information prediction of small targets. The key reason for the better performance of this network in CD is the introduction of ASPP blocks and deep supervision modules. From the analysis, it can be seen that ordinary convolutional blocks can usually only extract single-scale image features, so we use ASPP to replace the underlying convolutional blocks, so that the receptive field is expanded and multi-scale fusion features are obtained, which contains richer feature information and is more robust to seasonal changes and objects of different scales. In addition, the general training model lacks the supervision of the middle layer and does not pay enough attention to the effective layer information, so we use the DS module to supervise the hidden layer and fully explore the value of semantic graphs at different scales.</p><p id="Par62">As can be seen from Table <xref rid="Tab3" ref-type="table">3</xref>, the proposed model still leaves something to be desired. The network in this paper does not have an advantage in terms of the number of parameters and the amount of computation. It is worth noting that the CNNs-based CD model is better than the Transformer-based method in terms of the number of parameters and the amount of computation, but the results are opposite in terms of training and testing time, and both have their own advantages. Therefore, in the future, this paper plans to combine Transformer with CNN, and at the same time choose a more novel and sophisticated feature processing method to achieve better performance and control the difficulty of training and transplantation.<table-wrap id="Tab3"><label>Table 3</label><caption><p>Computational and parametric quantities comparison of experiment results on CDD and WHU-CD.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="2">Method type</th><th align="left" rowspan="2">Network</th><th align="left" rowspan="2">Params(M)</th><th align="left" rowspan="2">Gflops(G)</th><th align="left" colspan="2">CDD</th><th align="left" colspan="2">WHU-CD</th></tr><tr><th align="left">Train_Epoch(S)</th><th align="left">Test_Epoch(S)</th><th align="left">Train_Epoch(S)</th><th align="left">Test_Epoch(S)</th></tr></thead><tbody><tr><td align="left" rowspan="10">CNN</td><td align="left">FC-EF</td><td align="left"><italic>1.35</italic></td><td align="left"><italic>3.58</italic></td><td align="left"><italic>578.95</italic></td><td align="left"><bold>61.62</bold></td><td align="left">354.50</td><td align="left">18.25</td></tr><tr><td align="left">FC-Siam-conc</td><td align="left">1.55</td><td align="left">5.33</td><td align="left">676.39</td><td align="left"><italic>65.77</italic></td><td align="left"><italic>351.09</italic></td><td align="left"><italic>17.52</italic></td></tr><tr><td align="left">FC-Siam-diff</td><td align="left"><italic>1.35</italic></td><td align="left">4.73</td><td align="left">685.02</td><td align="left">70.45</td><td align="left">356.32</td><td align="left"><bold>17.25</bold></td></tr><tr><td align="left">L-UNet</td><td align="left">8.45</td><td align="left">17.33</td><td align="left">870.42</td><td align="left">98.04</td><td align="left">443.59</td><td align="left">23.78</td></tr><tr><td align="left">IFNet</td><td align="left">35.99</td><td align="left">82.27</td><td align="left">893.78</td><td align="left">115.45</td><td align="left">450.91</td><td align="left">28.96</td></tr><tr><td align="left">SNUNet-24</td><td align="left">6.77</td><td align="left">30.90</td><td align="left">873.24</td><td align="left">103.89</td><td align="left">470.66</td><td align="left">26.31</td></tr><tr><td align="left">USSFC-Net</td><td align="left">1.52</td><td align="left">4.86</td><td align="left">830.91</td><td align="left">90.23</td><td align="left">430.95</td><td align="left">23.44</td></tr><tr><td align="left">TinyCD</td><td align="left"><bold>0.29</bold></td><td align="left"><bold>1.54</bold></td><td align="left">704.65</td><td align="left">80.57</td><td align="left">329.30</td><td align="left">20.92</td></tr><tr><td align="left">DASUNet-32</td><td align="left">2.27</td><td align="left">25.61</td><td align="left">1022.98</td><td align="left">135.88</td><td align="left">515.98</td><td align="left">34.12</td></tr><tr><td align="left">DASUNet-64</td><td align="left">9.07</td><td align="left">100.93</td><td align="left">1731.19</td><td align="left">226.62</td><td align="left">874.34</td><td align="left">56.94</td></tr><tr><td align="left" rowspan="4">Transformer</td><td align="left">ChangeFormer</td><td align="left">29.84</td><td align="left">11.65</td><td align="left">688.44</td><td align="left">77.17</td><td align="left">347.97</td><td align="left">17.81</td></tr><tr><td align="left">IDET</td><td align="left">45.09</td><td align="left">124.19</td><td align="left">1232.82</td><td align="left">126.14</td><td align="left">669.25</td><td align="left">32.28</td></tr><tr><td align="left">ScratchFormer</td><td align="left">36.92</td><td align="left">196.59</td><td align="left">2690.59</td><td align="left">291.41</td><td align="left">1387.35</td><td align="left">71.66</td></tr><tr><td align="left">Swin-UNet-CD</td><td align="left">27.15</td><td align="left">7.75</td><td align="left"><bold>534.23</bold></td><td align="left">70.77</td><td align="left"><bold>316.49</bold></td><td align="left">18.63</td></tr></tbody></table><table-wrap-foot><p>The best two results are in bold and italics, respectively.</p><p>Train_Epoch indicates the training time per epoch, and Test_Epoch indicates the time for each epoch of testing.</p></table-wrap-foot></table-wrap></p></sec><sec id="Sec17"><title>Conclusions</title><p id="Par63">In the article, we propose a CD network for high-resolution remote sensing images, which adopts an end-to-end approach and directly learns the features of the dataset without the help of transfer learning. The network adopts a Siamese architecture, which integrates the global feature information through full-scale skip connection structure, and realizes end-to-end training. At the same time, the network uses ASPP module in the coding stage and the deep supervision mechanism in the decoding stage, which integrates the change characteristics of multiple scales and makes use of the role of feature information of each scale in the final prediction. Through experimental comparison and visualization results, the proposed network has achieved competitive performance on the public dataset CDD and WHU-CD. In F1, it increased by 0.85% and 0.36%, respectively.</p><p id="Par64">There are still many shortcomings in the network of this article. In the future research, we will explore the method of using transformer to process multi-scale features to further improve the fineness of boundary detection. At the same time, through the adjustment of the model, we plan to apply the proposed method to more remote sensing image change detection scenarios such as multi-category extraction and road detection.</p></sec></body><back><fn-group><fn><p><bold>Publisher's note</bold></p><p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></fn></fn-group><notes notes-type="author-contribution"><title>Author contributions</title><p>Ru Miao, Geng Meng, Ke Zhou wrote the main manuscript text. Yi Li, Ranran Chang, Guangyu Zhang performed the data Curation and prepared all figures.</p></notes><notes notes-type="funding-information"><title>Funding</title><p>The research was funded by Major Special Project-The China High-Resolution Earth Observation System (80-Y50G19-9001-22/23) and Science and Technology Project of Henan Province (222102210061).</p></notes><notes notes-type="data-availability"><title>Data availability</title><p>The datasets in this article are public. The CDD dataset can be downloaded from the <ext-link ext-link-type="uri" xlink:href="https://drive.google.com/file/d/1GX656JqqOyBi_Ef0w65kDGVto-nHrNs9">https://drive.google.com/file/d/1GX656JqqOyBi_Ef0w65kDGVto-nHrNs9</ext-link>, and the WHU-CD dataset can be downloaded from the <ext-link ext-link-type="uri" xlink:href="http://gpcv.whu.edu.cn/data/building_dataset.html">http://gpcv.whu.edu.cn/data/building_dataset.html</ext-link>.</p></notes><notes id="FPar1" notes-type="COI-statement"><title>Competing interests</title><p id="Par65">The authors declare no competing interests.</p></notes><ref-list id="Bib1"><title>References</title><ref id="CR1"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Singh</surname><given-names>A</given-names></name></person-group><article-title>Change detection in the tropical forest environment of northeastern India using Landsat</article-title><source>Remote Sensing Trop. Land Manag.</source><year>1986</year><volume>44</volume><fpage>273</fpage><lpage>254</lpage></element-citation></ref><ref id="CR2"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jackson</surname><given-names>RD</given-names></name></person-group><article-title>Spectral indices in n-space</article-title><source>Remote Sens. Environ.</source><year>1983</year><volume>13</volume><issue>5</issue><fpage>409</fpage><lpage>421</lpage><pub-id pub-id-type="doi">10.1016/0034-4257(83)90010-x</pub-id></element-citation></ref><ref id="CR3"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Todd</surname><given-names>WJ</given-names></name></person-group><article-title>Urban and regional land use change detected by using Landsat data</article-title><source>J. Res. US Geol. Surv.</source><year>1977</year><volume>5</volume><issue>5</issue><fpage>529</fpage><lpage>534</lpage></element-citation></ref><ref id="CR4"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hussain</surname><given-names>M</given-names></name><name><surname>Chen</surname><given-names>D</given-names></name><name><surname>Cheng</surname><given-names>A</given-names></name><etal/></person-group><article-title>Change detection from remotely sensed images: From pixel-based to object-based approaches</article-title><source>ISPRS J. Photogram. Remote Sensing</source><year>2013</year><volume>80</volume><fpage>91</fpage><lpage>106</lpage><pub-id pub-id-type="doi">10.1016/j.isprsjprs.2013.03.006</pub-id></element-citation></ref><ref id="CR5"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bruzzone</surname><given-names>L</given-names></name><name><surname>Prieto</surname><given-names>DF</given-names></name></person-group><article-title>Automatic analysis of the difference image for unsupervised change detection</article-title><source>IEEE Trans. Geosci. Remote Sensing</source><year>2000</year><volume>38</volume><issue>3</issue><fpage>1171</fpage><lpage>1182</lpage><pub-id pub-id-type="doi">10.1109/36.843009</pub-id></element-citation></ref><ref id="CR6"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zerrouki</surname><given-names>N</given-names></name><name><surname>Harrou</surname><given-names>F</given-names></name><name><surname>Sun</surname><given-names>Y</given-names></name></person-group><article-title>Statistical monitoring of changes to land cover</article-title><source>IEEE Geosci. Remote Sensing Lett.</source><year>2018</year><volume>15</volume><issue>6</issue><fpage>927</fpage><lpage>931</lpage><pub-id pub-id-type="doi">10.1109/lgrs.2018.2817522</pub-id></element-citation></ref><ref id="CR7"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nielsen</surname><given-names>AA</given-names></name><name><surname>Conradsen</surname><given-names>K</given-names></name><name><surname>Simpson</surname><given-names>JJ</given-names></name></person-group><article-title>Multivariate alteration detection (MAD) and MAF postprocessing in multispectral, bitemporal image data: New approaches to change detection studies</article-title><source>Remote Sensing Environ.</source><year>1998</year><volume>64</volume><issue>1</issue><fpage>1</fpage><lpage>19</lpage><pub-id pub-id-type="doi">10.1016/s0034-4257(97)00162-4</pub-id></element-citation></ref><ref id="CR8"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Celik</surname><given-names>T</given-names></name></person-group><article-title>Unsupervised change detection in satellite images using principal component analysis and k-means clustering</article-title><source>IEEE Geosci. Remote Sensing Lett.</source><year>2009</year><volume>6</volume><issue>4</issue><fpage>772</fpage><lpage>776</lpage><pub-id pub-id-type="doi">10.1109/lgrs.2009.2025059</pub-id></element-citation></ref><ref id="CR9"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>G</given-names></name><name><surname>Hay</surname><given-names>GJ</given-names></name><name><surname>Carvalho</surname><given-names>LMT</given-names></name><etal/></person-group><article-title>Object-based change detection</article-title><source>Int. J. Remote Sensing</source><year>2012</year><volume>33</volume><issue>14</issue><fpage>4434</fpage><lpage>4457</lpage><pub-id pub-id-type="doi">10.1080/01431161.2011.648285</pub-id></element-citation></ref><ref id="CR10"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>Y</given-names></name><name><surname>Peng</surname><given-names>D</given-names></name><name><surname>Huang</surname><given-names>X</given-names></name></person-group><article-title>Object-based change detection for VHR images based on multiscale uncertainty analysis</article-title><source>IEEE Geosci. Remote Sensing Lett.</source><year>2017</year><volume>15</volume><issue>1</issue><fpage>13</fpage><lpage>17</lpage><pub-id pub-id-type="doi">10.1109/lgrs.2017.2763182</pub-id></element-citation></ref><ref id="CR11"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wu</surname><given-names>C</given-names></name><name><surname>Du</surname><given-names>B</given-names></name><name><surname>Cui</surname><given-names>X</given-names></name><etal/></person-group><article-title>A post-classification change detection method based on iterative slow feature analysis and Bayesian soft fusion</article-title><source>Remote Sensing Environ.</source><year>2017</year><volume>199</volume><fpage>241</fpage><lpage>255</lpage><pub-id pub-id-type="doi">10.1016/j.rse.2017.07.009</pub-id></element-citation></ref><ref id="CR12"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>C</given-names></name><name><surname>Yue</surname><given-names>P</given-names></name><name><surname>Tapete</surname><given-names>D</given-names></name><etal/></person-group><article-title>A deeply supervised image fusion network for change detection in high resolution bi-temporal remote sensing images</article-title><source>ISPRS J. Photogram. Remote Sensing</source><year>2020</year><volume>166</volume><fpage>183</fpage><lpage>200</lpage><pub-id pub-id-type="doi">10.1016/j.isprsjprs.2020.06.003</pub-id></element-citation></ref><ref id="CR13"><label>13.</label><mixed-citation publication-type="other">Daudt, R.C., Le Saux, B., Boulch, A. Fully convolutional siamese networks for change detection[C]. in <italic>2018 25th IEEE International Conference on Image Processing (ICIP). IEEE</italic>, <bold>2018</bold>, 4063&#x02013;4067. 10.1109/icip.2018.8451652.</mixed-citation></ref><ref id="CR14"><label>14.</label><mixed-citation publication-type="other">Long, J., Shelhamer, E., Darrell, T. Fully convolutional networks for semantic segmentation[C]. in <italic>Proceedings of the IEEE conference on computer vision and pattern recognition</italic>. <bold>2015</bold>, 3431&#x02013;3440. 10.1109/cvpr.2015.7298965</mixed-citation></ref><ref id="CR15"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alcantarilla</surname><given-names>PF</given-names></name><name><surname>Stent</surname><given-names>S</given-names></name><name><surname>Ros</surname><given-names>G</given-names></name><etal/></person-group><article-title>Street-view change detection with deconvolutional networks</article-title><source>Autonom. Robots.</source><year>2018</year><volume>42</volume><fpage>1301</fpage><lpage>1322</lpage><pub-id pub-id-type="doi">10.15607/rss.2016.xii.044</pub-id></element-citation></ref><ref id="CR16"><label>16.</label><mixed-citation publication-type="other">Papadomanolaki, M., Verma, S., Vakalopoulou, M., <italic>et al</italic>. Detecting urban changes with recurrent neural networks from multitemporal Sentinel-2 data[C]. in <italic>IGARSS 2019&#x02013;2019 IEEE international geoscience and remote sensing symposium. IEEE</italic>, <bold>2019</bold>, 214&#x02013;217. 10.1109/igarss.2019.8900330.</mixed-citation></ref><ref id="CR17"><label>17.</label><mixed-citation publication-type="other">Daudt, R.C., Le Saux, B., Boulch, A., <italic>et al</italic>. Urban change detection for multispectral earth observation using convolutional neural networks[C]. in <italic>IGARSS 2018&#x02013;2018 IEEE International Geoscience and Remote Sensing Symposium. IEEE</italic>, <bold>2018</bold>, 2115&#x02013;2118. 10.1109/igarss.2018.8518015.</mixed-citation></ref><ref id="CR18"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peng</surname><given-names>D</given-names></name><name><surname>Zhang</surname><given-names>Y</given-names></name><name><surname>Guan</surname><given-names>H</given-names></name></person-group><article-title>End-to-end change detection for high resolution satellite images using improved UNet++[J]</article-title><source>Remote Sensing</source><year>2019</year><volume>11</volume><issue>11</issue><fpage>1382</fpage><pub-id pub-id-type="doi">10.3390/rs11111382</pub-id></element-citation></ref><ref id="CR19"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>H</given-names></name><name><surname>Shi</surname><given-names>Z</given-names></name></person-group><article-title>A spatial-temporal attention-based method and a new dataset for remote sensing image change detection[J]</article-title><source>Remote Sensing</source><year>2020</year><volume>12</volume><issue>10</issue><fpage>1662</fpage><pub-id pub-id-type="doi">10.3390/rs12101662</pub-id></element-citation></ref><ref id="CR20"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lei</surname><given-names>T</given-names></name><name><surname>Geng</surname><given-names>X</given-names></name><name><surname>Ning</surname><given-names>H</given-names></name><etal/></person-group><article-title>Ultralightweight spatial-spectral feature cooperation network for change detection in remote sensing images[J]</article-title><source>IEEE Trans. Geosci. Remote Sensing</source><year>2023</year><volume>61</volume><fpage>1</fpage><lpage>14</lpage><pub-id pub-id-type="doi">10.1109/TGRS.2023.3261273</pub-id></element-citation></ref><ref id="CR21"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shi</surname><given-names>Q</given-names></name><name><surname>Liu</surname><given-names>M</given-names></name><name><surname>Li</surname><given-names>S</given-names></name><etal/></person-group><article-title>A deeply supervised attention metric-based network and an open aerial image dataset for remote sensing change detection[J]</article-title><source>IEEE Trans. Geosci. Remote Sensing</source><year>2021</year><volume>60</volume><fpage>1</fpage><lpage>16</lpage><pub-id pub-id-type="doi">10.1109/tgrs.2021.3085870</pub-id></element-citation></ref><ref id="CR22"><label>22.</label><mixed-citation publication-type="other">Bandara, W.G.C., Patel, V.M. A transformer-based siamese network for change detection[C]//IGARSS 2022&#x02013;2022 IEEE International Geoscience and Remote Sensing Symposium. IEEE, 2022, 207&#x02013;210. 10.48550/arXiv.2201.01293.</mixed-citation></ref><ref id="CR23"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>H</given-names></name><name><surname>Qi</surname><given-names>Z</given-names></name><name><surname>Shi</surname><given-names>Z</given-names></name></person-group><article-title>Remote sensing image change detection with transformers trained from scratch[J]</article-title><source>IEEE Trans. Geosci. Remote Sensing</source><year>2021</year><volume>60</volume><fpage>1</fpage><lpage>14</lpage><pub-id pub-id-type="doi">10.1109/TGRS.2021.3095166</pub-id></element-citation></ref><ref id="CR24"><label>24.</label><mixed-citation publication-type="other">Ronneberger, O., Fischer, P., Brox, T. U-net: Convolutional networks for biomedical image segmentation[C]. in <italic>Medical Image Computing and Computer-Assisted Intervention&#x02013;MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18. Springer International Publishing</italic>, <bold>2015</bold>, 234&#x02013;241. 10.1007/978-3-319-24574-4_28</mixed-citation></ref><ref id="CR25"><label>25.</label><mixed-citation publication-type="other">Chen, L.C., Zhu, Y., Papandreou, G., <italic>et al</italic>. Encoder-decoder with atrous separable convolution for semantic image segmentation[C]. in <italic>Proceedings of the European conference on computer vision (ECCV)</italic>. 2018, 801&#x02013;818. 10.1007/978-3-030-01234-2_49.</mixed-citation></ref><ref id="CR26"><label>26.</label><mixed-citation publication-type="other">Zhou, Z., Rahman Siddiquee, M.M., Tajbakhsh, N., <italic>et al</italic>. Unet++: A nested u-net architecture for medical image segmentation[C]. in <italic>Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support: 4th International Workshop, DLMIA 2018, and 8th International Workshop, ML-CDS 2018, Held in Conjunction with MICCAI 2018, Granada, Spain, September 20, 2018, Proceedings 4. Springer International Publishing</italic>, 2018, 3&#x02013;11. 10.1007/978-3-030-00889-5_1.</mixed-citation></ref><ref id="CR27"><label>27.</label><mixed-citation publication-type="other">Lin, L., Tong, R., <italic>et al</italic>. Unet 3+: A full-scale connected unet for medical image segmentation[C]. in <italic>ICASSP 2020&#x02013;2020 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE</italic>, <bold>2020</bold>: 1055&#x02013;1059. 10.1109/icassp40776.2020.9053405.</mixed-citation></ref><ref id="CR28"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Codegoni</surname><given-names>A</given-names></name><name><surname>Lombardi</surname><given-names>G</given-names></name><name><surname>Ferrari</surname><given-names>A</given-names></name></person-group><article-title>TINYCD: A (not so) deep learning model for change detection[J]</article-title><source>Neural Comput. Appl.</source><year>2023</year><volume>35</volume><issue>11</issue><fpage>8471</fpage><lpage>8486</lpage><pub-id pub-id-type="doi">10.1007/s00521-022-08122-3</pub-id></element-citation></ref><ref id="CR29"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fang</surname><given-names>S</given-names></name><name><surname>Li</surname><given-names>K</given-names></name><name><surname>Shao</surname><given-names>J</given-names></name><etal/></person-group><article-title>SNUNet-CD: A densely connected Siamese network for change detection of VHR images[J]</article-title><source>IEEE Geosci. Remote Sensing Lett.</source><year>2021</year><volume>19</volume><fpage>1</fpage><lpage>5</lpage><pub-id pub-id-type="doi">10.1109/lgrs.2021.3056416</pub-id></element-citation></ref><ref id="CR30"><label>30.</label><mixed-citation publication-type="other">Cao, H., Wang, Y., Chen, J., <italic>et al</italic>. Swin-unet: Unet-like pure transformer for medical image segmentation[C]//European conference on computer vision. (Springer Nature Switzerland, 2022), 205&#x02013;218. 10.48550/arXiv.2105.05537.</mixed-citation></ref><ref id="CR31"><label>31.</label><mixed-citation publication-type="other">Chen, Y., Zou, B., Guo, Z., <italic>et al</italic>. Scunet++: Swin-unet and cnn bottleneck hybrid architecture with multi-fusion dense skip connection for pulmonary embolism ct image segmentation[C]//Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. 2024: 7759&#x02013;7767. 10.1109/WACV57701.2024.00758.</mixed-citation></ref><ref id="CR32"><label>32.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tang</surname><given-names>Y</given-names></name><name><surname>Cao</surname><given-names>Z</given-names></name><name><surname>Guo</surname><given-names>N</given-names></name><etal/></person-group><article-title>A Siamese Swin-Unet for image change detection[J]</article-title><source>Sci. Rep.</source><year>2024</year><volume>14</volume><issue>1</issue><fpage>4577</fpage><pub-id pub-id-type="doi">10.1038/s41598-024-54096-8</pub-id><?supplied-pmid 38403711?><pub-id pub-id-type="pmid">38403711</pub-id>
</element-citation></ref><ref id="CR33"><label>33.</label><mixed-citation publication-type="other">Guo, Q., Wang, R., Huang, R., <italic>et al</italic>. IDET: Iterative difference-enhanced transformers for high-quality change detection[J]. 2022. 10.48550/arXiv.2207.09240.</mixed-citation></ref><ref id="CR34"><label>34.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vaswani</surname><given-names>A</given-names></name><name><surname>Shazeer</surname><given-names>N</given-names></name><name><surname>Parmar</surname><given-names>N</given-names></name><etal/></person-group><article-title>Attention is all you need</article-title><source>Adv. Neural Inform. Process. Syst.</source><year>2017</year><pub-id pub-id-type="doi">10.48550/arXiv.1706.03762</pub-id></element-citation></ref><ref id="CR35"><label>35.</label><mixed-citation publication-type="other">Parmar, N., Vaswani, A., Uszkoreit, J., <italic>et al</italic>. Image transformer[C]//International conference on machine learning. PMLR, 2018: 4055&#x02013;4064. 10.48550/arXiv.1802.05751.</mixed-citation></ref><ref id="CR36"><label>36.</label><mixed-citation publication-type="other">Dosovitskiy, A., Beyer, L., Kolesnikov, A., <italic>et al</italic>. An image is worth 16x16 words: Transformers for image recognition at scale. 2020. 10.48550/arXiv.2010.11929.</mixed-citation></ref><ref id="CR37"><label>37.</label><mixed-citation publication-type="other">Florian, L.C., Adam, S.H. Rethinking atrous convolution for semantic image segmentation[C]. <italic>Conference on computer vision and pattern recognition (CVPR). IEEE/CVF</italic>. <bold>2017</bold>, 6. 10.48550/arXiv.1706.05587.</mixed-citation></ref><ref id="CR38"><label>38.</label><mixed-citation publication-type="other">Microsoft Visio. (2019). Microsoft Visio [Software]. Redmond, WA: Microsoft Corporation. <ext-link ext-link-type="uri" xlink:href="https://www.microsoft.com/en-us/microsoft-365/visio/flowchart-software">https://www.microsoft.com/en-us/microsoft-365/visio/flowchart-software</ext-link>.</mixed-citation></ref><ref id="CR39"><label>39.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lebedev</surname><given-names>MA</given-names></name><name><surname>Vizilter</surname><given-names>YV</given-names></name><name><surname>Vygolov</surname><given-names>OV</given-names></name><etal/></person-group><article-title>Change detection in remote sensing images using conditional adversarial networks[J]</article-title><source>Int. Arch. Photogram. Remote Sensing Spatial Inform. Sci.</source><year>2018</year><volume>42</volume><fpage>565</fpage><lpage>571</lpage><pub-id pub-id-type="doi">10.5194/isprs-archives-xlii-2-565-2018</pub-id></element-citation></ref><ref id="CR40"><label>40.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ji</surname><given-names>S</given-names></name><name><surname>Wei</surname><given-names>S</given-names></name><name><surname>Lu</surname><given-names>M</given-names></name></person-group><article-title>Fully convolutional networks for multisource building extraction from an open aerial and satellite imagery data set[J]</article-title><source>IEEE Trans. Geosci. Remote Sensing</source><year>2018</year><volume>57</volume><issue>1</issue><fpage>574</fpage><lpage>586</lpage><pub-id pub-id-type="doi">10.1109/TGRS.2018.2858817</pub-id></element-citation></ref><ref id="CR41"><label>41.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Papadomanolaki</surname><given-names>M</given-names></name><name><surname>Vakalopoulou</surname><given-names>M</given-names></name><name><surname>Karantzalos</surname><given-names>K</given-names></name></person-group><article-title>A deep multitask learning framework coupling semantic segmentation and fully convolutional LSTM networks for urban change detection[J]</article-title><source>IEEE Trans. Geosci. Remote Sensing</source><year>2021</year><volume>59</volume><issue>9</issue><fpage>7651</fpage><lpage>7668</lpage><pub-id pub-id-type="doi">10.1109/tgrs.2021.3055584</pub-id></element-citation></ref></ref-list></back></article>