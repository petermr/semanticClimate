<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd"> 
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><?properties open_access?><?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?><?DTDIdentifier.IdentifierType public?><?SourceDTD.DTDName A++V2.4.dtd?><?SourceDTD.Version 2.4?><?ConverterInfo.XSLTName springer2nlmx2.xsl?><?ConverterInfo.Version 1?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Int J Comput Assist Radiol Surg</journal-id><journal-id journal-id-type="iso-abbrev">Int J Comput Assist Radiol Surg</journal-id><journal-title-group><journal-title>International Journal of Computer Assisted Radiology and Surgery</journal-title></journal-title-group><issn pub-type="ppub">1861-6410</issn><issn pub-type="epub">1861-6429</issn><publisher><publisher-name>Springer International Publishing</publisher-name><publisher-loc>Cham</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmcid">11178556</article-id><article-id pub-id-type="pmid">38570373</article-id>
<article-id pub-id-type="publisher-id">3091</article-id><article-id pub-id-type="doi">10.1007/s11548-024-03091-5</article-id><article-categories><subj-group subj-group-type="heading"><subject>Original Article</subject></subj-group></article-categories><title-group><article-title>EndoViT: pretraining vision transformers on a large collection of endoscopic images</article-title></title-group><contrib-group><contrib contrib-type="author" equal-contrib="yes"><name><surname>Bati&#x00107;</surname><given-names>Dominik</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author" corresp="yes" equal-contrib="yes"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-8493-9941</contrib-id><name><surname>Holm</surname><given-names>Felix</given-names></name><address><email>felix.holm@tum.de</email></address><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author" equal-contrib="yes"><name><surname>&#x000d6;zsoy</surname><given-names>Ege</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author"><name><surname>Czempiel</surname><given-names>Tobias</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author"><name><surname>Navab</surname><given-names>Nassir</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/02kkvpp62</institution-id><institution-id institution-id-type="GRID">grid.6936.a</institution-id><institution-id institution-id-type="ISNI">0000 0001 2322 2966</institution-id><institution>Chair for Computer Aided Medical Procedures, </institution><institution>Technical University Munich, </institution></institution-wrap>Munich, Germany </aff><aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="GRID">grid.424549.a</institution-id><institution-id institution-id-type="ISNI">0000 0004 0379 7801</institution-id><institution>Carl Zeiss AG, </institution></institution-wrap>Munich, Germany </aff></contrib-group><pub-date pub-type="epub"><day>3</day><month>4</month><year>2024</year></pub-date><pub-date pub-type="pmc-release"><day>3</day><month>4</month><year>2024</year></pub-date><pub-date pub-type="ppub"><year>2024</year></pub-date><volume>19</volume><issue>6</issue><fpage>1085</fpage><lpage>1091</lpage><history><date date-type="received"><day>15</day><month>1</month><year>2024</year></date><date date-type="accepted"><day>28</day><month>2</month><year>2024</year></date></history><permissions><copyright-statement>&#x000a9; The Author(s) 2024</copyright-statement><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article&#x02019;s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article&#x02019;s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p></license></permissions><abstract id="Abs1"><sec><title>Purpose</title><p id="Par1">Automated endoscopy video analysis is essential for assisting surgeons during medical procedures, but it faces challenges due to complex surgical scenes and limited annotated data. Large-scale pretraining has shown great success in natural language processing and computer vision communities in recent years. These approaches reduce the need for annotated data, which is of great interest in the medical domain. In this work, we investigate endoscopy domain-specific self-supervised pretraining on large collections of data.</p></sec><sec><title>Methods</title><p id="Par2">To this end, we first collect Endo700k, the largest publicly available corpus of endoscopic images, extracted from nine public Minimally Invasive Surgery (MIS) datasets. Endo700k comprises more than 700,000 images. Next, we introduce EndoViT, an endoscopy-pretrained Vision Transformer (ViT), and evaluate it on a diverse set of surgical downstream tasks.</p></sec><sec><title>Results</title><p id="Par3">Our findings indicate that domain-specific pretraining with EndoViT yields notable advantages in complex downstream tasks. In the case of action triplet recognition, our approach outperforms ImageNet pretraining. In semantic segmentation, we surpass the state-of-the-art (SOTA) performance. These results demonstrate the effectiveness of our domain-specific pretraining approach in addressing the challenges of automated endoscopy video analysis.</p></sec><sec><title>Conclusion</title><p id="Par4">Our study contributes to the field of medical computer vision by showcasing the benefits of domain-specific large-scale self-supervised pretraining for vision transformers. We release both our code and pretrained models to facilitate further research in this direction: <ext-link ext-link-type="uri" xlink:href="https://github.com/DominikBatic/EndoViT">https://github.com/DominikBatic/EndoViT</ext-link>.
</p></sec></abstract><kwd-group xml:lang="en"><title>Keywords</title><kwd>Endoscopy video analysis</kwd><kwd>Vision transformer</kwd><kwd>Pretraining</kwd></kwd-group><funding-group><award-group><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100008894</institution-id><institution>Stryker</institution></institution-wrap></funding-source></award-group></funding-group><funding-group><award-group><funding-source><institution>Carl Zeiss AG</institution></funding-source></award-group></funding-group><custom-meta-group><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#x000a9; CARS 2024</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1"><title>Introduction</title><p id="Par5">Minimally Invasive Surgery (MIS) is quickly becoming one of the most common styles of surgical procedures in the world&#x000a0;[<xref ref-type="bibr" rid="CR21">21</xref>]. In contrast to open surgery, MIS lowers the chance of infection and speeds up the recovery rate. As MIS procedures use endoscopic cameras, it has become possible to analyze large amounts of video data, leading to the development of surgical assistance systems. These systems can detect errors and provide decision support to improve patient outcomes&#x000a0;[<xref ref-type="bibr" rid="CR17">17</xref>]. Additionally, cataloging recorded surgical procedures provides valuable insights to surgeons, enabling them to learn and improve their techniques&#x000a0;[<xref ref-type="bibr" rid="CR23">23</xref>]. To achieve these goals, the community has thoroughly investigated the task of Surgical Phase Recognition [<xref ref-type="bibr" rid="CR6">6</xref>, <xref ref-type="bibr" rid="CR26">26</xref>] and successfully managed to detect and localize surgical instruments [<xref ref-type="bibr" rid="CR13">13</xref>]. Today, more challenging tasks are being explored, such as the newly introduced action triplet recognition [<xref ref-type="bibr" rid="CR21">21</xref>]. It requires not only detecting surgical instruments, actions, and anatomies but also determining the relationship between them. Other works focus on the segmentation of tools and tissues[<xref ref-type="bibr" rid="CR5">5</xref>], as well as multi-level learning, combining several tasks at once[<xref ref-type="bibr" rid="CR27">27</xref>].</p><p id="Par6">In general deep learning, the transformer&#x000a0;[<xref ref-type="bibr" rid="CR28">28</xref>] architecture has had a tremendous impact in recent years. Its success can be attributed to the introduction of self-supervised pretraining methods, such as Masked Language Modeling. The idea is straightforward: A percentage of input words are randomly masked out, and the model is tasked with predicting the missing input. Despite its simplicity, it presents a challenging self-supervised task. This approach has led to a paradigm shift in which a transformer network is first pretrained on large amounts of unlabeled data in order to create a model with a general understanding of the underlying domain. Later on, this model can be finetuned for a specific downstream task using significantly fewer annotations. With the advent of Vision Transformers (ViT) [<xref ref-type="bibr" rid="CR8">8</xref>], similar strategies such as Masked Image Modeling have been developed for computer vision&#x000a0;[<xref ref-type="bibr" rid="CR2">2</xref>, <xref ref-type="bibr" rid="CR9">9</xref>, <xref ref-type="bibr" rid="CR29">29</xref>], showing equally high benefit in complex computer vision tasks.</p><p id="Par7">Despite the advancements in computer vision and natural language processing, the progress of artificial intelligence methods in the medical field has been slower due to the insufficient amount of annotated data for developing data-driven approaches&#x000a0;[<xref ref-type="bibr" rid="CR27">27</xref>]. While the largest endoscopic dataset, Cholec80&#x000a0;[<xref ref-type="bibr" rid="CR26">26</xref>], only contains 200k images, computer vision datasets can reach hundreds of millions of images&#x000a0;[<xref ref-type="bibr" rid="CR25">25</xref>]. Additionally, downstream medical tasks requiring complex annotations, such as pixel-wise segmentations, often have less than 10k images&#x000a0;[<xref ref-type="bibr" rid="CR11">11</xref>]. Pretraining models on larger datasets could be used to overcome this challenge. However, so far, only natural image datasets are generally available at the required size, which leaves a significant domain gap to endoscopic videos.</p><p id="Par8">In this study, we use endoscopy domain-specific large-scale pretraining to bring advances from the computer vision community to the medical domain. Toward this objective, our contributions are threefold: <list list-type="order"><list-item><p id="Par9">We compile the largest publicly available collection of unlabeled endoscopic data, Endo700k, consisting of more than 700,000 images.</p></list-item><list-item><p id="Par10">We introduce the first publicly available endoscopy-pretrained vision transformer, EndoViT.</p></list-item><list-item><p id="Par11">We analyze, through extensive experiments and ablation studies, the effect of endoscopy pretraining on the downstream tasks of surgical phase recognition, surgical action triplet recognition, and semantic segmentation.</p></list-item></list></p></sec><sec id="Sec2"><title>Methodology</title><sec id="Sec3"><title>Dataset preparation</title><p id="Par12">To enable effective endoscopy-specific pretraining, we have created the largest publicly available collection of raw endoscopic data, Endo700k. Endo700k is formed by combining nine publicly available MIS datasets comprising more than 700,000 images. An overview of the individual datasets is provided in Table <xref rid="Tab1" ref-type="table">1</xref>. Endo700k contains a diverse set of endoscopic procedures, both manual and robot-assisted, with several surgery types such as prostatectomy, cholecystectomy, gastrectomy, proctocolectomy, rectal resection, and sigmoid resection. Furthermore, multiple different surgical actions, anatomies, and many surgical instruments, which are present in different shapes and sizes, are included. The downstream evaluation experiments are conducted on the Cholec80 dataset&#x000a0;[<xref ref-type="bibr" rid="CR26">26</xref>] and its subvariants CholecT45&#x000a0;[<xref ref-type="bibr" rid="CR21">21</xref>] and CholecSeg8k&#x000a0;[<xref ref-type="bibr" rid="CR11">11</xref>]. To eliminate any potential data leakage, we exclude any images that appear in their validation or test sets from the pretraining dataset. Furthermore, all synthetic images are excluded. Outside the previously mentioned exceptions, we use all of the images from the nine datasets. For consistency, we always downsample to 1 FPS.
<table-wrap id="Tab1"><label>Table 1</label><caption><p>An overview of the individual datasets that form the Endo700k dataset</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Dataset</th><th align="left">Surgery type</th><th align="left"># Surg.</th><th align="left"># Unique images</th></tr></thead><tbody><tr><td align="left">ESAD&#x000a0;[<xref ref-type="bibr" rid="CR3">3</xref>]</td><td align="left">Robot-assisted radical prostatectomy</td><td align="left">4</td><td align="left">49,544</td></tr><tr><td align="left">LapGyn4 (v1.2)&#x000a0;[<xref ref-type="bibr" rid="CR15">15</xref>]</td><td align="left">Gynecologic laparoscopy</td><td align="left">&#x0003e;500</td><td align="left">38,192</td></tr><tr><td align="left">Surgical Actions160&#x000a0;[<xref ref-type="bibr" rid="CR23">23</xref>]</td><td align="left">Gynecologic laparoscopy</td><td align="left">59</td><td align="left">761</td></tr><tr><td align="left">GLENDA (v1.0)&#x000a0;[<xref ref-type="bibr" rid="CR14">14</xref>]</td><td align="left">Gynecologic laparoscopy</td><td align="left">&#x0003e;400</td><td align="left">1,083</td></tr><tr><td align="left" rowspan="2">hSDB instrument&#x000a0;[<xref ref-type="bibr" rid="CR30">30</xref>]</td><td align="left">Laparoscopic cholecystectomy</td><td align="left">24</td><td align="left" rowspan="2">35,576</td></tr><tr><td align="left">Robotic gastrectomy</td><td align="left">24</td></tr><tr><td align="left" rowspan="3">HeiCo&#x000a0;[<xref ref-type="bibr" rid="CR18">18</xref>]</td><td align="left">Laparoscopic proctocolectomy</td><td align="left">10</td><td align="left" rowspan="3">347,257</td></tr><tr><td align="left">Laparoscopic rectal resection</td><td align="left">10</td></tr><tr><td align="left">Laparoscopic sigmoid resection</td><td align="left">10</td></tr><tr><td align="left">PSI-AVA&#x000a0;[<xref ref-type="bibr" rid="CR27">27</xref>]</td><td align="left">Robot-assisted radical prostatectomy</td><td align="left">8</td><td align="left">73,618</td></tr><tr><td align="left">DSAD&#x000a0;[<xref ref-type="bibr" rid="CR4">4</xref>]</td><td align="left">Robot-assisted rectal resection</td><td align="left">32</td><td align="left">13,195</td></tr><tr><td align="left">Cholec80&#x000a0;[<xref ref-type="bibr" rid="CR26">26</xref>]</td><td align="left">Laparoscopic cholecystectomy</td><td align="left">80</td><td align="left">184,498</td></tr><tr><td align="left">CholecT45&#x000a0;[<xref ref-type="bibr" rid="CR21">21</xref>]</td><td align="left">Laparoscopic cholecystectomy</td><td align="left">45</td><td align="left">0</td></tr><tr><td align="left">CholecSeg8k&#x000a0;[<xref ref-type="bibr" rid="CR11">11</xref>]</td><td align="left">Laparoscopic cholecystectomy</td><td align="left">17</td><td align="left">0</td></tr></tbody></table><table-wrap-foot><p>The first nine datasets (ESAD&#x02014;Cholec80) represent a unique collection of roughly 744k raw endoscopic images. Cholec80 and its subvariants CholecT45 and CholecSeg8k are additionally used for downstream tasks of surgical phase recognition, action triplet recognition, and semantic segmentation</p></table-wrap-foot></table-wrap></p><p id="Par13">
<fig id="Fig1"><label>Fig. 1</label><caption><p>EndoViT is first pretrained using the Masked Image Modeling strategy (<bold>a</bold>). An input image is split into non-overlapping patches, and a large proportion of them is masked out. The network is trained to reconstruct the missing patches using a per-patch MSE loss, gaining a general visual understanding. Later, the EndoViT encoder can be finetuned and used as a powerful feature extraction backbone on downstream tasks (<bold>b</bold>). No Masking is applied during use as a feature extractor</p></caption><graphic xlink:href="11548_2024_3091_Fig1_HTML" id="MO1"/></fig>
</p></sec><sec id="Sec4"><title>Model pretraining</title><p id="Par14">Most existing works &#x000a0;[<xref ref-type="bibr" rid="CR5">5</xref>, <xref ref-type="bibr" rid="CR6">6</xref>, <xref ref-type="bibr" rid="CR13">13</xref>, <xref ref-type="bibr" rid="CR21">21</xref>, <xref ref-type="bibr" rid="CR26">26</xref>, <xref ref-type="bibr" rid="CR27">27</xref>] use ImageNet-pretrained CNN models as image feature extraction backbones. However, ImageNet&#x000a0;[<xref ref-type="bibr" rid="CR7">7</xref>] contains natural images that differ significantly from endoscopic images. Therefore, in this work, we use Endo700k to pretrain a large-scale vision transformer-based [<xref ref-type="bibr" rid="CR8">8</xref>] feature extractor on the endoscopy domain. The goal of the pretraining is to give a general understanding of the domain of endoscopic procedures to benefit a wide range of downstream tasks. For pretraining, we closely follow the approach of MAE [<xref ref-type="bibr" rid="CR9">9</xref>] and employ the Masked Image Modeling strategy. The input image is first split into non-overlapping patches. Afterward, a large proportion of them is masked out. The network is trained to reconstruct the missing parts of the input. The encoder of the pretrained model can then be used as a feature extraction backbone in the downstream tasks. An overview of the pretraining procedure can be seen in Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>. We tailor the MAE approach for the endoscopic setting with three modifications:</p><p id="Par15"><bold>Layerwise learning rate decay</bold> We scale down the learning rate of each layer of the MAE encoder and decoder such that the layers closer to the latent space have larger learning rates, while those closer to the ends of the model have lower learning rates.</p><p id="Par16"><bold>Stochastic weight averaging (SWA)&#x000a0;</bold>[<xref ref-type="bibr" rid="CR12">12</xref>]: During the last 5 pretraining epochs, we average the models&#x02019; weights at each validation step.</p><p id="Par17"><bold>Frequent evaluation</bold> The evaluation is performed 6 times per epoch, and the best SWA model is saved.</p><sec id="Sec5"><title>Implementation details</title><p id="Par18">We follow most of the practices and hyperparameter choices of [<xref ref-type="bibr" rid="CR1">1</xref>, <xref ref-type="bibr" rid="CR9">9</xref>]. During pretraining only simple image augmentations are applied, including random resized crops and random horizontal flips. We use AdamW optimizer&#x000a0;[<xref ref-type="bibr" rid="CR16">16</xref>] with a learning rate of 1.5e<inline-formula id="IEq1"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$-$$\end{document}</tex-math><mml:math id="M2"><mml:mo>-</mml:mo></mml:math><inline-graphic xlink:href="11548_2024_3091_Article_IEq1.gif"/></alternatives></inline-formula>3 and batch size of 256. We pretrain for a total of 15 epochs. The training starts with 3 linear warmup epochs, continues according to the cosine scheduler until epoch 10, and ends with a constant learning rate applied during SWA. We use layer-wise learning rate decay of 0.65. Mean-squared error (MSE) is used as the reconstruction loss. We pretrain three different models, one for each of the downstream tasks. All are pretrained on Endo700k; however, the pretraining datasets are slightly different, obtained by removing validation and test datasets of CholecT45, Cholec80, and CholecSeg8k, respectively. All models have been implemented in PyTorch 1.13.0 and trained on 1 Nvidia a40 GPU.<table-wrap id="Tab2"><label>Table 2</label><caption><p>Semantic segmentation results, few shot, and full dataset (mean IoU)</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"/><th align="left">Train Set</th><th align="left">ViT w/o Pretr</th><th align="left">ViT ImageNet</th><th align="left">EndoViT</th></tr></thead><tbody><tr><td align="left" rowspan="4">Low Res</td><td align="left">1 Video</td><td align="left">29.11 &#x000b1; 2.94</td><td align="left">38.35 &#x000b1; 8.27</td><td align="left"><bold>40.95 &#x000b1; 10.32</bold></td></tr><tr><td align="left">2 Videos</td><td align="left">36.28 &#x000b1; 5.06</td><td align="left">50.36 &#x000b1; 2.71</td><td align="left"><bold>54.02 &#x000b1; 4.18</bold></td></tr><tr><td align="left">4 Videos</td><td align="left">43.29 &#x000b1; 0.96</td><td align="left">54.17 &#x000b1; 2.35</td><td align="left"><bold>57.87 &#x000b1; 2.70</bold></td></tr><tr><td align="left">Full</td><td align="left">51.70 &#x000b1; 0.54</td><td align="left">62.45 &#x000b1; 0.90</td><td align="left"><bold>65.05 &#x000b1; 0.67</bold></td></tr><tr><td align="left" rowspan="4">High Res</td><td align="left">1 Video</td><td align="left">26.66 &#x000b1; 6.64</td><td align="left">39.06 &#x000b1; 5.17</td><td align="left"><bold>41.16 &#x000b1; 10.75</bold></td></tr><tr><td align="left">2 Videos</td><td align="left">35.69 &#x000b1; 4.45</td><td align="left">50.14 &#x000b1; 4.48</td><td align="left"><bold>56.05 &#x000b1; 5.73</bold></td></tr><tr><td align="left">4 Videos</td><td align="left">44.16 &#x000b1; 0.75</td><td align="left">56.22 &#x000b1; 1.52</td><td align="left"><bold>59.81 &#x000b1; 3.27</bold></td></tr><tr><td align="left">Full</td><td align="left">53.18 &#x000b1; 1.20</td><td align="left">63.40 &#x000b1; 0.81</td><td align="left"><bold>65.32 &#x000b1; 0.56</bold></td></tr></tbody></table><table-wrap-foot><p>Bold values represent the best result</p></table-wrap-foot></table-wrap></p></sec></sec><sec id="Sec6"><title>Downstream tasks</title><p id="Par19">After pretraining our feature extractor backbones, we evaluate their performance on three downstream tasks, namely semantic segmentation, action triplet recognition, and surgical phase recognition.</p><p id="Par20"><bold>Semantic segmentation</bold> We choose the Dense Prediction Transformer (DPT)&#x000a0;[<xref ref-type="bibr" rid="CR22">22</xref>] architecture to leverage our vision transformer backbone for the semantic segmentation task. DPT assembles tokens from various stages of the ViT into image-like representations at various resolutions and progressively combines them&#x000a0;[<xref ref-type="bibr" rid="CR22">22</xref>]. Since the ViT backbone processes the input at high resolution and has a global receptive field, DPT allows for fine-grained and more globally consistent predictions compared to previous CNN approaches, especially when a larger amount of data can be provided&#x000a0;[<xref ref-type="bibr" rid="CR22">22</xref>]. We replace the encoder of DPT with our ViT encoder but otherwise keep the training setup the same. The DPT decoder is randomly initialized. We replicate the evaluation setup of&#x000a0;[<xref ref-type="bibr" rid="CR24">24</xref>].<fig id="Fig2"><label>Fig. 2</label><caption><p>Qualitative segmentation comparison. EndoViT has more globally consistent outputs (highlighted in black) and is significantly better at reconstructing instrument tips (highlighted in red)</p></caption><graphic xlink:href="11548_2024_3091_Fig2_HTML" id="MO2"/></fig></p><p id="Par21"><bold>Action triplet recognition</bold> We build a straightforward model consisting of a feature extraction backbone and a linear head to detect the <inline-formula id="IEq2"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$&#x0003c;instrument, verb, target&#x0003e;$$\end{document}</tex-math><mml:math id="M4"><mml:mrow><mml:mo>&#x0003c;</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>b</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi><mml:mo>&#x0003e;</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="11548_2024_3091_Article_IEq2.gif"/></alternatives></inline-formula> triplets. To avoid data leakage into the pretraining set, we evaluate only on fold 5 as defined by&#x000a0;[<xref ref-type="bibr" rid="CR20">20</xref>]. We chose fold 5 specifically, as it yields the most balanced distribution of classes across train, val, and test splits, in the otherwise highly imbalanced CholecT45 dataset. While most works such as [<xref ref-type="bibr" rid="CR19">19</xref>, <xref ref-type="bibr" rid="CR21">21</xref>] utilize Binary Cross-Entropy loss, we empirically find that Focal Loss brings significant improvement and therefore use it in all our experiments.</p><p id="Par22"><bold>Surgical phase recognition</bold> In the task of Surgical Phase Recognition, the objective is to detect different phases of a surgical procedure based on the surgical video stream. For this task, we choose TeCNO&#x000a0;[<xref ref-type="bibr" rid="CR6">6</xref>], a well-known benchmark model with publicly available code. TeCNO is a two-step surgical phase recognition method. In the first step, a single-frame ResNet50 model is trained to predict surgical phases. In the second step, a Multi-Stage Temporal Convolutional Network (MS-TCN) refines the extracted features using temporal context. This two-stage approach allows the MS-TCN to improve the predictions of any feature extractor regardless of the chosen architecture. In our experiments, we replace the Resnet50 backbone with a ViT model and otherwise stick to the training and evaluation setup of TeCNO.</p></sec></sec><sec id="Sec7"><title>Experiments</title><p id="Par23">We compare our EndoViT (endoscopy pretrained) with its ImageNet pretrained ViT counterpart and commonly used CNN architectures (ResNet50/ResNet18&#x000a0;[<xref ref-type="bibr" rid="CR10">10</xref>]). We evaluate the performance of the models on the full downstream dataset and also evaluate the few-shot learning performance using a reduced amount of training videos while keeping the validation and test sets the same. We report the mean and standard deviation of the corresponding metrics across 3 runs for each network in each setting.</p><p id="Par24"><bold>Semantic segmentation</bold> We report the semantic segmentation results in Table&#x000a0;<xref rid="Tab2" ref-type="table">2</xref>. The reported metric is mean Intersection over Union (IoU). The results are reported both for EndoViT&#x02019;s pretraining resolution of 224<italic>x</italic>224 (Low Res) and the resolution used in&#x000a0;[<xref ref-type="bibr" rid="CR24">24</xref>] of 256<italic>x</italic>448 (High Res). EndoViT outperforms the ImageNet pretrained ViT in all scenarios, including few shot, with a margin of 2&#x02013;6%. We report a comparison to other methods from&#x000a0;[<xref ref-type="bibr" rid="CR24">24</xref>] in Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref> and Table&#x000a0;<xref rid="Tab3" ref-type="table">3</xref>. EndoViT outperforms other Transformers (UNETR) as well as various CNN architectures, including U-Net++, the previous SOTA to our knowledge, by a significant margin of 10%. EndoViT shows more globally consistent predictions and is better at reconstructing the crucial instrument tips.</p><p id="Par25"><bold>Action triplet recognition</bold> We report the results on the full dataset in Table <xref rid="Tab4" ref-type="table">4</xref>. The reported metric is mean Average Precision (mAP) proposed by the authors of the CholecT45 dataset&#x000a0;[<xref ref-type="bibr" rid="CR21">21</xref>]. The results show that EndoViT outperforms both CNN architectures and its ImageNet pretrained counterpart by 8% and 2%, respectively, empirically showcasing the value of using endoscopy-based models. Furthermore, from the performance of the randomly initialized ViT model, it can be seen that pretraining is essential for vision transformers. In Table <xref rid="Tab5" ref-type="table">5</xref>, we report few-shot learning experiment results by training only on 2, 4, or 8 videos. We observe the same trends in our few-shot learning experiments. EndoViT outperforms ResNet50 by 5&#x02013;6.5% and the ImageNet model by 2&#x02013;5%.<table-wrap id="Tab3"><label>Table 3</label><caption><p>Semantic segmentation comparison to previous methods (mean IoU)</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">U-Net++</th><th align="left">DynUNet</th><th align="left">UNETR</th><th align="left">DeepLab V3+</th><th align="left">EndoViT</th></tr></thead><tbody><tr><td align="left">55</td><td align="left">52</td><td align="left">49</td><td align="left">50</td><td align="left"><bold>65.32 &#x000b1; 0.56</bold></td></tr></tbody></table><table-wrap-foot><p>Bold values represent the best result</p><p>Results for methods other than ours from [<xref ref-type="bibr" rid="CR24">24</xref>]</p></table-wrap-foot></table-wrap><table-wrap id="Tab4"><label>Table 4</label><caption><p>Action triplet recognition full dataset results (mAP)</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">ResNet18</th><th align="left">ResNet50</th><th align="left">ViT w/o Pretr</th><th align="left">ViT ImageNet</th><th align="left">EndoViT</th></tr></thead><tbody><tr><td align="left">21.72 &#x000b1; 1.17</td><td align="left">22.13 &#x000b1; 1.37</td><td align="left">13.93 &#x000b1; 0.43</td><td align="left">27.84 &#x000b1; 0.39</td><td align="left"><bold>30.17 &#x000b1; 0.01</bold></td></tr></tbody></table><table-wrap-foot><p>Bold values represent the best result</p></table-wrap-foot></table-wrap></p><p id="Par26"><bold>Surgical phase recognition</bold> We report the results on the full dataset in Table&#x000a0;<xref rid="Tab6" ref-type="table">6</xref>. The reported metric is Accuracy averaged over all testing videos. We report the performance of all models after both stages of TeCNO training. For reference purposes, we note that the reported performance of ResNet50 backbone in TeCNO [<xref ref-type="bibr" rid="CR6">6</xref>] is 88.56% &#x000b1; 0.27%. Once again, the CNN architecture is outperformed by the pretrained vision transformers. However, the difference between EndoViT and ImageNet pretrained backbone is negligible in both stages. We believe there are two causes. One, the semantic understanding induced by reconstructing image patches is not capable of capturing the long-term relationships required to discriminate between different surgical phases accurately. And two, the training set used in Cholec80 is relatively large (approx. 90k images), making it easier to overcome the pretraining differences. In Table <xref rid="Tab7" ref-type="table">7</xref>, we report few-shot learning experiment results by training only on 2, 4, or 8 videos. ResNet50 showcases a significant decrease in performance. When training on 2 videos only, EndoViT outperforms its ImageNet counterpart in both stages. For 4 and 8 videos, we observe comparable performance.<table-wrap id="Tab5"><label>Table 5</label><caption><p>Action triplet recognition few-shot results (mAP)</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"/><th align="left">ResNet50</th><th align="left">ViT ImageNet</th><th align="left">EndoViT</th></tr></thead><tbody><tr><td align="left">2 Videos</td><td align="left">10.88 &#x000b1; 0.50</td><td align="left">12.22 &#x000b1; 1.78</td><td align="left"><bold>17.59 &#x000b1; 2.94</bold></td></tr><tr><td align="left">4 Videos</td><td align="left">12.37 &#x000b1; 1.78</td><td align="left">14.27 &#x000b1; 1.73</td><td align="left"><bold>18.52 &#x000b1; 2.28</bold></td></tr><tr><td align="left">8 Videos</td><td align="left">17.01 &#x000b1; 1.75</td><td align="left">19.71 &#x000b1; 0.61</td><td align="left"><bold>21.91 &#x000b1; 0.12</bold></td></tr></tbody></table><table-wrap-foot><p>Bold values represent the best result</p></table-wrap-foot></table-wrap><table-wrap id="Tab6"><label>Table 6</label><caption><p>Surgical phase recognition full dataset results (mean Accuracy)</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"/><th align="left">ResNet50</th><th align="left">ViT w/o Pretr</th><th align="left">ViT ImageNet</th><th align="left">EndoViT</th></tr></thead><tbody><tr><td align="left">Stage 1</td><td align="left">79.84 &#x000b1; 0.30</td><td align="left">59.21 &#x000b1; 0.36</td><td align="left"><bold>82.94 &#x000b1; 0.69</bold></td><td align="left">82.60 &#x000b1; 1.26</td></tr><tr><td align="left">Stage 2</td><td align="left">87.84 &#x000b1; 0.58</td><td align="left">73.42 &#x000b1; 0.70</td><td align="left"><bold>89.56 &#x000b1; 0.65</bold></td><td align="left">89.37 &#x000b1; 0.95</td></tr></tbody></table><table-wrap-foot><p>Bold values represent the best result</p></table-wrap-foot></table-wrap><table-wrap id="Tab7"><label>Table 7</label><caption><p>Surgical phase recognition few-shot results (mean accuracy)</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"/><th align="left">Train Set</th><th align="left">ResNet50</th><th align="left">ViT ImageNet</th><th align="left">EndoViT</th></tr></thead><tbody><tr><td align="left" rowspan="3">Stage 1</td><td align="left">2 Videos</td><td align="left">47.51 &#x000b1; 1.33</td><td align="left">63.59 &#x000b1; 1.07</td><td align="left"><bold>67.04 &#x000b1; 2.92</bold></td></tr><tr><td align="left">4 Videos</td><td align="left">57.80 &#x000b1; 2.67</td><td align="left">67.72 &#x000b1; 0.90</td><td align="left"><bold>71.80 &#x000b1; 0.49</bold></td></tr><tr><td align="left">8 Videos</td><td align="left">63.71 &#x000b1; 1.48</td><td align="left"><bold>75.50 &#x000b1; 0.32</bold></td><td align="left">75.30 &#x000b1; 1.83</td></tr><tr><td align="left" rowspan="3">Stage 2</td><td align="left">2 Videos</td><td align="left">68.23 &#x000b1; 1.10</td><td align="left">77.05 &#x000b1; 1.71</td><td align="left"><bold>78.89 &#x000b1; 1.26</bold></td></tr><tr><td align="left">4 Videos</td><td align="left">74.50 &#x000b1; 1.76</td><td align="left">80.00 &#x000b1; 0.62</td><td align="left"><bold>80.28 &#x000b1; 0.71</bold></td></tr><tr><td align="left">8 Videos</td><td align="left">77.43 &#x000b1; 1.68</td><td align="left">84.10 &#x000b1; 0.38</td><td align="left"><bold>84.68 &#x000b1; 1.25</bold></td></tr></tbody></table></table-wrap></p></sec><sec id="Sec8"><title>Conclusion</title><p id="Par27">EndoViT performs equally or better than the same ViT model pretrained on ImageNet in all of our experiments. We observe that the improvements from our endoscopy-pretraining were more pronounced in the more complex downstream tasks. In action triplet recognition, endoscopy-specific pretraining significantly outperforms ImageNet pretraining. For the simpler task of surgical phase recognition, our pretraining was less impactful, although never hurting performance. In the most complex task of semantic segmentation, our EndoViT model outperforms the previous SOTA. Moreover, our method performing well on all of the diverse downstream tasks shows that our pretraining implementation, which reconstructs image patches in pixel space, captures general information about the objects and scenes it has seen. We therefore conclude that EndoViT would be an excellent upgrade to the ImageNet pretrained feature extraction backbones that many surgical video understanding methods rely on. We hope that the release of our dataset collection Endo700k, pretraining implementation, and pretrained EndoViT model will help the community to solve more challenging tasks with the small amount of data available.</p></sec></body><back><fn-group><fn><p><bold>Publisher's Note</bold></p><p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></fn><fn><p>Dominik Bati&#x00107;, Felix Holm, and Ege &#x000d6;zsoy have equally contributed to this work.</p></fn></fn-group><notes notes-type="funding-information"><title>Funding</title><p>Open Access funding enabled and organized by Projekt DEAL.</p></notes><notes><title>Declarations</title><notes id="FPar1" notes-type="COI-statement"><title>Conflict of interest</title><p id="Par30">Holm is supported by Carl Zeiss AG. The other authors declare that they have no conflict of interest.</p></notes><notes id="FPar2"><title>Ethical approval</title><p id="Par31">This article does not contain any studies with human participants or animals performed by any of the authors.</p></notes></notes><ref-list id="Bib1"><title>References</title><ref id="CR1"><label>1.</label><mixed-citation publication-type="other">Assran M, Caron M, Misra I, Bojanowski P, Bordes F, Vincent P, Joulin A, Rabbat M, Ballas N (2022) Masked siamese networks for label-efficient learning. In: Computer Vision&#x02013;ECCV 2022: 17th European conference, Tel Aviv, Israel, October 23&#x02013;27, 2022, Proceedings, Part XXXI, pp. 456&#x02013;473. Springer</mixed-citation></ref><ref id="CR2"><label>2.</label><mixed-citation publication-type="other">Bao H, Dong L, Piao S, Wei F (2022) BEiT: BERT pre-training of image transformers. In: International conference on learning representations</mixed-citation></ref><ref id="CR3"><label>3.</label><mixed-citation publication-type="other">Bawa VS, Singh G, Kaping AF, Skarga-Bandurova I, Oleari E, Leporini A, Landolfo C, Zhao P, Xiang X, Luo G et&#x000a0;al (2021) The saras endoscopic surgeon action detection (esad) dataset: challenges and methods. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2104.03178">arXiv:2104.03178</ext-link></mixed-citation></ref><ref id="CR4"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carstens</surname><given-names>M</given-names></name><name><surname>Rinner</surname><given-names>FM</given-names></name><name><surname>Bodenstedt</surname><given-names>S</given-names></name><name><surname>Jenke</surname><given-names>AC</given-names></name><name><surname>Weitz</surname><given-names>J</given-names></name><name><surname>Distler</surname><given-names>M</given-names></name><name><surname>Speidel</surname><given-names>S</given-names></name><name><surname>Kolbinger</surname><given-names>FR</given-names></name></person-group><article-title>The dresden surgical anatomy dataset for abdominal organ segmentation in surgical data science</article-title><source>Sci Data</source><year>2023</year><volume>10</volume><issue>1</issue><fpage>1</fpage><lpage>8</lpage><pub-id pub-id-type="doi">10.1038/s41597-022-01719-2</pub-id><pub-id pub-id-type="pmid">36596836</pub-id>
</element-citation></ref><ref id="CR5"><label>5.</label><mixed-citation publication-type="other">Chen J, Lu Y, Yu Q, Luo X, Adeli E, Wang Y, Lu L, Yuille A, Zhou Y (2021) Transunet: transformers make strong encoders for medical image segmentation. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2102.04306">arXiv:2102.04306</ext-link></mixed-citation></ref><ref id="CR6"><label>6.</label><mixed-citation publication-type="other">Czempiel T, Paschali M, Keicher M, Simson W, Feussner H, Kim ST, Navab N (2020) Tecno: surgical phase recognition with multi-stage temporal convolutional networks. In: MICCAI 2020, pp. 343&#x02013;352. Springer</mixed-citation></ref><ref id="CR7"><label>7.</label><mixed-citation publication-type="other">Deng J, Dong W, Socher R, Li LJ, Li K, Fei-Fei L (2009) Imagenet: a large-scale hierarchical image database. In: 2009 IEEE conference on computer vision and pattern recognition, pp. 248&#x02013;255</mixed-citation></ref><ref id="CR8"><label>8.</label><mixed-citation publication-type="other">Dosovitskiy A, Beyer L, Kolesnikov A, Weissenborn D, Zhai X, Unterthiner T, Dehghani M, Minderer M, Heigold G, Gelly S, Uszkoreit J, Houlsby N (2021) An image is worth 16x16 words: transformers for image recognition at scale. In: International conference on learning representations</mixed-citation></ref><ref id="CR9"><label>9.</label><mixed-citation publication-type="other">He K, Chen X, Xie S, Li Y, Doll&#x000e1;r P, Girshick R (2022) Masked autoencoders are scalable vision learners. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 16000&#x02013;16009</mixed-citation></ref><ref id="CR10"><label>10.</label><mixed-citation publication-type="other">He K, Zhang X, Ren S, Sun J (2016) Deep residual learning for image recognition. In: 2016 IEEE conference on computer vision and pattern recognition (CVPR), pp. 770&#x02013;778</mixed-citation></ref><ref id="CR11"><label>11.</label><mixed-citation publication-type="other">Hong WY, Kao CL, Kuo YH, Wang JR, Chang WL, Shih CS (2020) Cholecseg8k: a semantic segmentation dataset for laparoscopic cholecystectomy based on cholec80. <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2012.12453">arXiv:2012.12453</ext-link> [cs.CV]</mixed-citation></ref><ref id="CR12"><label>12.</label><mixed-citation publication-type="other">Izmailov P, Wilson A, Podoprikhin D, Vetrov D, Garipov T (2018) Averaging weights leads to wider optima and better generalization. In: 34th conference on uncertainty in artificial intelligence 2018, UAI 2018, pp. 876&#x02013;885</mixed-citation></ref><ref id="CR13"><label>13.</label><mixed-citation publication-type="other">Jha D, Ali S, Emanuelsen K, Hicks SA, Thambawita V, Garcia-Ceja E, Riegler MA, de&#x000a0;Lange T, Schmidt PT, Johansen HD et&#x000a0;al (2021) Kvasir-instrument: diagnostic and therapeutic tool segmentation dataset in gastrointestinal endoscopy. In: MMM 2021, pp. 218&#x02013;229. Springer</mixed-citation></ref><ref id="CR14"><label>14.</label><mixed-citation publication-type="other">Leibetseder A, Kletz S, Schoeffmann K, Keckstein S, Keckstein J (2020) Glenda: gynecologic laparoscopy endometriosis dataset. In: MultiMedia Modeling: 26th International Conference, MMM 2020, Daejeon, South Korea, January 5&#x02013;8, 2020, Proceedings, Part II 26, pp. 439&#x02013;450. Springer</mixed-citation></ref><ref id="CR15"><label>15.</label><mixed-citation publication-type="other">Leibetseder A, Petscharnig S, Primus MJ, Kletz S, M&#x000fc;nzer B, Schoeffmann K, Keckstein J (2018) Lapgyn4: a dataset for 4 automatic content analysis problems in the domain of laparoscopic gynecology. In: Proceedings of the 9th ACM multimedia systems conference, pp. 357&#x02013;362</mixed-citation></ref><ref id="CR16"><label>16.</label><mixed-citation publication-type="other">Loshchilov I, Hutter F (2019) Decoupled weight decay regularization. In: International conference on learning representations</mixed-citation></ref><ref id="CR17"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maier-Hein</surname><given-names>L</given-names></name><name><surname>Vedula</surname><given-names>SS</given-names></name><name><surname>Speidel</surname><given-names>S</given-names></name><name><surname>Navab</surname><given-names>N</given-names></name><name><surname>Kikinis</surname><given-names>R</given-names></name><name><surname>Park</surname><given-names>A</given-names></name><name><surname>Eisenmann</surname><given-names>M</given-names></name><name><surname>Feussner</surname><given-names>H</given-names></name><name><surname>Forestier</surname><given-names>G</given-names></name><name><surname>Giannarou</surname><given-names>S</given-names></name><etal/></person-group><article-title>Surgical data science for next-generation interventions</article-title><source>Nat Biomed Eng</source><year>2017</year><volume>1</volume><issue>9</issue><fpage>691</fpage><lpage>696</lpage><pub-id pub-id-type="doi">10.1038/s41551-017-0132-7</pub-id><?supplied-pmid 31015666?><pub-id pub-id-type="pmid">31015666</pub-id>
</element-citation></ref><ref id="CR18"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maier-Hein</surname><given-names>L</given-names></name><name><surname>Wagner</surname><given-names>M</given-names></name><name><surname>Ross</surname><given-names>T</given-names></name><name><surname>Reinke</surname><given-names>A</given-names></name><name><surname>Bodenstedt</surname><given-names>S</given-names></name><name><surname>Full</surname><given-names>PM</given-names></name><name><surname>Hempe</surname><given-names>H</given-names></name><name><surname>Mindroc-Filimon</surname><given-names>D</given-names></name><name><surname>Scholz</surname><given-names>P</given-names></name><name><surname>Tran</surname><given-names>TN</given-names></name><etal/></person-group><article-title>Heidelberg colorectal data set for surgical data science in the sensor operating room</article-title><source>Sci Data</source><year>2021</year><volume>8</volume><issue>1</issue><fpage>101</fpage><pub-id pub-id-type="doi">10.1038/s41597-021-00882-2</pub-id><?supplied-pmid 33846356?><pub-id pub-id-type="pmid">33846356</pub-id>
</element-citation></ref><ref id="CR19"><label>19.</label><mixed-citation publication-type="other">Nwoye CI, Gonzalez C, Yu T, Mascagni P, Mutter D, Marescaux J, Padoy N (2020) Recognition of instrument-tissue interactions in endoscopic videos via action triplets. In: Medical image computing and computer assisted intervention &#x02013; MICCAI 2020, 364&#x02013;374. Springer International Publishing</mixed-citation></ref><ref id="CR20"><label>20.</label><mixed-citation publication-type="other">Nwoye CI, Padoy N (2023) Data splits and metrics for method benchmarking on surgical action triplet datasets. <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2204.05235">arXiv:2204.05235</ext-link> [cs.CV]</mixed-citation></ref><ref id="CR21"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nwoye</surname><given-names>CI</given-names></name><name><surname>Yu</surname><given-names>T</given-names></name><name><surname>Gonzalez</surname><given-names>C</given-names></name><name><surname>Seeliger</surname><given-names>B</given-names></name><name><surname>Mascagni</surname><given-names>P</given-names></name><name><surname>Mutter</surname><given-names>D</given-names></name><name><surname>Marescaux</surname><given-names>J</given-names></name><name><surname>Padoy</surname><given-names>N</given-names></name></person-group><article-title>Rendezvous: attention mechanisms for the recognition of surgical action triplets in endoscopic videos</article-title><source>Med Image Anal</source><year>2022</year><volume>78</volume><fpage>102433</fpage><pub-id pub-id-type="doi">10.1016/j.media.2022.102433</pub-id><?supplied-pmid 35398658?><pub-id pub-id-type="pmid">35398658</pub-id>
</element-citation></ref><ref id="CR22"><label>22.</label><mixed-citation publication-type="other">Ranftl R, Bochkovskiy A, Koltun V (2021) Vision transformers for dense prediction. <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2103.13413">arXiv:2103.13413</ext-link> [cs.CV]</mixed-citation></ref><ref id="CR23"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schoeffmann</surname><given-names>K</given-names></name><name><surname>Husslein</surname><given-names>H</given-names></name><name><surname>Kletz</surname><given-names>S</given-names></name><name><surname>Petscharnig</surname><given-names>S</given-names></name><name><surname>Muenzer</surname><given-names>B</given-names></name><name><surname>Beecks</surname><given-names>C</given-names></name></person-group><article-title>Video retrieval in laparoscopic video recordings with dynamic content descriptors</article-title><source>Multimed Tools Appl</source><year>2018</year><volume>77</volume><fpage>16813</fpage><lpage>16832</lpage><pub-id pub-id-type="doi">10.1007/s11042-017-5252-2</pub-id></element-citation></ref><ref id="CR24"><label>24.</label><mixed-citation publication-type="other">Silva B, Oliveira B, Morais P, Buschle L, Correia-Pinto J, Lima E, Vila&#x000e7;a JL (2022) Analysis of current deep learning networks for semantic segmentation of anatomical structures in laparoscopic surgery. EMBC 2022:3502&#x02013;3505</mixed-citation></ref><ref id="CR25"><label>25.</label><mixed-citation publication-type="other">Sun C, Shrivastava A, Singh S, Gupta A (2017) Revisiting unreasonable effectiveness of data in deep learning era. In: Proceedings of the IEEE international conference on computer vision, pp. 843&#x02013;852</mixed-citation></ref><ref id="CR26"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Twinanda</surname><given-names>AP</given-names></name><name><surname>Shehata</surname><given-names>S</given-names></name><name><surname>Mutter</surname><given-names>D</given-names></name><name><surname>Marescaux</surname><given-names>J</given-names></name><name><surname>De Mathelin</surname><given-names>M</given-names></name><name><surname>Padoy</surname><given-names>N</given-names></name></person-group><article-title>Endonet: a deep architecture for recognition tasks on laparoscopic videos</article-title><source>IEEE Trans Med Imaging</source><year>2016</year><volume>36</volume><issue>1</issue><fpage>86</fpage><lpage>97</lpage><pub-id pub-id-type="doi">10.1109/TMI.2016.2593957</pub-id><?supplied-pmid 27455522?><pub-id pub-id-type="pmid">27455522</pub-id>
</element-citation></ref><ref id="CR27"><label>27.</label><mixed-citation publication-type="other">Valderrama N, Ruiz Puentes P, Hern&#x000e1;ndez I, Ayobi N, Verlyck M, Santander J, Caicedo J, Fern&#x000e1;ndez N, Arbel&#x000e1;ez P (2022) Towards holistic surgical scene understanding. In: MICCAI 2022, pp. 442&#x02013;452. Springer</mixed-citation></ref><ref id="CR28"><label>28.</label><mixed-citation publication-type="other">Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, Kaiser &#x00141;, Polosukhin I (2017) Attention is all you need. Adv Neural Inf Process Syst. Vol.&#x000a0;30</mixed-citation></ref><ref id="CR29"><label>29.</label><mixed-citation publication-type="other">Xie Z, Zhang Z, Cao Y, Lin Y, Bao J, Yao Z, Dai Q, Hu H (2022) Simmim: a simple framework for masked image modeling. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 9653&#x02013;9663</mixed-citation></ref><ref id="CR30"><label>30.</label><mixed-citation publication-type="other">Yoon J, Lee J, Heo S, Yu H, Lim J, Song CH, Hong S, Hong S, Park B, Park S et&#x000a0;al (2021) hsdb-instrument: instrument localization database for laparoscopic and robotic surgeries. In: MICCAI 2021, pp. 393&#x02013;402. Springer</mixed-citation></ref></ref-list></back></article>