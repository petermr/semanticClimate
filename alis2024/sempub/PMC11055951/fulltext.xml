<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd"> 
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><?properties open_access?><?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?><?DTDIdentifier.IdentifierType public?><?SourceDTD.DTDName A++V2.4.dtd?><?SourceDTD.Version 2.4?><?ConverterInfo.XSLTName springer2nlmx2.xsl?><?ConverterInfo.Version 1?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sci Rep</journal-id><journal-id journal-id-type="iso-abbrev">Sci Rep</journal-id><journal-title-group><journal-title>Scientific Reports</journal-title></journal-title-group><issn pub-type="epub">2045-2322</issn><publisher><publisher-name>Nature Publishing Group UK</publisher-name><publisher-loc>London</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmcid">11055951</article-id><article-id pub-id-type="publisher-id">60375</article-id><article-id pub-id-type="doi">10.1038/s41598-024-60375-1</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>An improved semantic segmentation algorithm for high-resolution remote sensing images based on DeepLabv3+</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Wang</surname><given-names>Yan</given-names></name><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Yang</surname><given-names>Ling</given-names></name><address><email>yangling2009@henu.edu.cn</email></address><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author"><name><surname>Liu</surname><given-names>Xinzhan</given-names></name><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author"><name><surname>Yan</surname><given-names>Pengfei</given-names></name><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff2">2</xref></contrib><aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/003xyzq10</institution-id><institution-id institution-id-type="GRID">grid.256922.8</institution-id><institution-id institution-id-type="ISNI">0000 0000 9139 560X</institution-id><institution>College of Geography and Environmental Science, </institution><institution>Henan University, </institution></institution-wrap>Kaifeng, China </aff><aff id="Aff2"><label>2</label>Key Laboratory of Geospatial Technology for Middle and Lower Yellow River Regions, Ministry of Education, Kaifeng, China </aff></contrib-group><pub-date pub-type="epub"><day>27</day><month>4</month><year>2024</year></pub-date><pub-date pub-type="pmc-release"><day>27</day><month>4</month><year>2024</year></pub-date><pub-date pub-type="collection"><year>2024</year></pub-date><volume>14</volume><elocation-id>9716</elocation-id><history><date date-type="received"><day>20</day><month>6</month><year>2023</year></date><date date-type="accepted"><day>22</day><month>4</month><year>2024</year></date></history><permissions><copyright-statement>&#x000a9; The Author(s) 2024</copyright-statement><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p></license></permissions><abstract id="Abs1"><p id="Par1">High-precision and high-efficiency Semantic segmentation of high-resolution remote sensing images is a challenge. Existing models typically require a significant amount of training data to achieve good classification results and have numerous training parameters. A novel model called MST-DeepLabv3+&#x02009;was suggested in this paper for remote sensing image classification. It&#x02019;s based on the DeepLabv3+&#x02009;and can produce better results with fewer train parameters. MST-DeepLabv3+&#x02009;made three improvements: (1) Reducing the number of model parameters by substituting MobileNetV2 for the Xception in the DeepLabv3+&#x02019;s backbone network. (2) Adding the attention mechanism module SENet to increase the precision of semantic segmentation. (3) Increasing Transfer Learning to enhance the model's capacity to recognize features, and raise the segmentation accuracy. MST-DeepLabv3+&#x02009;was tested on international society for photogrammetry and remote sensing (ISPRS) dataset, Gaofen image dataset (GID), and practically applied to the Taikang cultivated land dataset. On the ISPRS dataset, the mean intersection over union (MIoU), overall accuracy (OA), Precision, Recall, and F1-score are 82.47%, 92.13%, 90.34%, 90.12%, and 90.23%, respectively. On the GID dataset, these values are 73.44%, 85.58%, 84.10%, 84.86%, and 84.48%, respectively. The results were as high as 90.77%, 95.47%, 95.28%, 95.02%, and 95.15% on the Taikang cultivated land dataset. The experimental results indicate that MST-DeepLabv3+&#x02009;effectively improves the accuracy of semantic segmentation of remote sensing images, recognizes the edge information with more completeness, and significantly reduces the parameter size.</p></abstract><kwd-group kwd-group-type="npg-subject"><title>Subject terms</title><kwd>Information technology</kwd><kwd>Environmental sciences</kwd></kwd-group><funding-group><award-group><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100017700</institution-id><institution>Henan Provincial Science and Technology Research Project</institution></institution-wrap></funding-source><award-id>232102110288</award-id></award-group></funding-group><funding-group><award-group><funding-source><institution>the National Major Project of High-Resolution Earth Ob-servation System</institution></funding-source><award-id>80-Y50G19-9001-22/23</award-id></award-group></funding-group><funding-group><award-group><funding-source><institution>the National Science and Technology Platform Construction Project</institution></funding-source><award-id>2005DKA32300</award-id></award-group></funding-group><custom-meta-group><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#x000a9; Springer Nature Limited 2024</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1"><title>Introduction</title><p id="Par2">Remote sensing technology has gradually replaced conventional manual regional survey methods due to its wide monitoring range, quick data acquisition, and a large amount of obtained information. It is extensively used in soil research<sup><xref ref-type="bibr" rid="CR1">1</xref></sup>, geological engineering<sup><xref ref-type="bibr" rid="CR2">2</xref></sup>, land resources<sup><xref ref-type="bibr" rid="CR3">3</xref></sup>, and other fields. The quality of remote sensing images has increased along with the rapid development of the technology. Remote sensing images can provide a wealth of information about ground objects, such as ground vegetation cover, ground temperature, and land use. Semantic segmentation of remote sensing images is the fundamental and critical component of understanding and analyzing remote sensing images, which converts complex remote sensing images into feature classification information that can be understood and processed to support practical applications. As a result, the semantic segmentation technique for remote sensing images has significant research implications.</p><p id="Par3">The development of machine learning algorithms brings significant changes to remote sensing image classification. Traditional machine learning approaches include Decision Tree<sup><xref ref-type="bibr" rid="CR4">4</xref></sup>, Support Vector Machine (SVM)<sup><xref ref-type="bibr" rid="CR5">5</xref></sup>, Random Forest (RF)<sup><xref ref-type="bibr" rid="CR6">6</xref></sup>, Conditional Random Field (CRF)<sup><xref ref-type="bibr" rid="CR7">7</xref></sup>, and others. Li et al.<sup><xref ref-type="bibr" rid="CR8">8</xref></sup> combined color features with a support vector machine classifier to detect multiple classes of features in remote sensing images. Volpi et al.<sup><xref ref-type="bibr" rid="CR9">9</xref></sup> used a structured support vector machine to classify urban scenes. Sun et al.<sup><xref ref-type="bibr" rid="CR10">10</xref></sup> employed random forest integrated learning techniques to categorize the pixels of remote sensing images, then enhanced the classification findings with an improved conditional random field. Most traditional machine learning-based remote sensing image interpretation algorithms adopt feature extraction and feature analysis, and the interpretation effect is good for specific scenes and datasets<sup><xref ref-type="bibr" rid="CR11">11</xref></sup>. However, classic machine learning algorithms have restricted feature extraction and cannot accurately capture the nuances of the input<sup><xref ref-type="bibr" rid="CR12">12</xref></sup>. When the background level of the remote sensing image to be processed is complicated and the target scale has large fluctuations, the model accuracy suffers and under-fitting or over-fitting occurs.</p><p id="Par4">High-resolution remote sensing images can give rich feature information and finely present the spatial structure and textural features due to their complex and diversified information, rich features, and vast size<sup><xref ref-type="bibr" rid="CR13">13</xref></sup>. However, while high-resolution remote sensing images provide more data and information, they also pose significant challenges to remote sensing image interpretation, such as high interpretation costs, time-consuming, which makes it difficult to meet the urgent demand for rapid extraction and updating of resource information at present. Because of the rapid advancement of artificial intelligence technology, semantic segmentation algorithms are widely used in natural image processing<sup><xref ref-type="bibr" rid="CR14">14</xref>&#x02013;<xref ref-type="bibr" rid="CR16">16</xref></sup>. The method based on the convolutional neural network (CNN) was gradually adopted in remote sensing image interpretation<sup><xref ref-type="bibr" rid="CR17">17</xref></sup>. Zhu et al.<sup><xref ref-type="bibr" rid="CR18">18</xref></sup> compared the GoogLeNet model to the SVM method for the extraction of urban construction land in Landsat8 remote sensing images to demonstrate the advantages of deep learning for construction land. Jadhav et al.<sup><xref ref-type="bibr" rid="CR19">19</xref></sup> used a ResNet101 network for automatic semantic segmentation of high-resolution remote sensing images for land cover and crop type to achieve classification accuracy of major crops. Kussul et al.<sup><xref ref-type="bibr" rid="CR20">20</xref></sup> demonstrated that the design with an ensemble of convolutional neural networks (CNNs) performs better than the one with multilayer perceptrons(MLPs) in distinguishing crop types in remote sensing images.</p><p id="Par5">In comparison to conventional machine learning techniques, CNNs have significantly increased the segmentation accuracy of remote sensing images, but the classical CNN model has redundant computations during the batch operation, which will result in higher memory consumption and lower segmentation efficiency<sup><xref ref-type="bibr" rid="CR21">21</xref></sup>. Therefore, researchers have created a variety of improvements based on convolutional neural networks. Fully Convolutional Networks (FCN) were suggested by Long<sup><xref ref-type="bibr" rid="CR22">22</xref></sup> and replaced CNN's fully connected layers with convolutional layers to produce images with contextual spatial features. Fu et al.<sup><xref ref-type="bibr" rid="CR23">23</xref></sup> optimized the FCN model by using the atrous convolution and used the conditional random field to post-process the segmented data, which greatly improved the segmentation accuracy. To increase algorithm accuracy and reduce the impact of noise, Chen et al.<sup><xref ref-type="bibr" rid="CR24">24</xref></sup> used the method of overlapping the SNFCN and SDFCN semantic segmentation frameworks based on the shortcut-block structure, which significantly increased remote sensing accuracy in urban areas.</p><p id="Par6">However, FCN does not consider the relationship between pixels while upsampling, which could result in information loss, and the segmentation results are still rough. To improve segmentation accuracy, numerous improved models based on the FCN were invented one after the other. For example, the UNet network with a U-shaped structure proposed by Ronneberger<sup><xref ref-type="bibr" rid="CR25">25</xref></sup> uses an encoder to generate deep semantic information, a decoder to recover image spatial resolution, and a jump connection to splice and fuse deep abstract features with shallow detailed features in each level to integrate more feature information than the FCN, resulting in more accurate pixel boundary localization and significantly improved segmentation accuracy. The SegNet network proposed by Badrinarayanan<sup><xref ref-type="bibr" rid="CR26">26</xref></sup> is also an encoder-decoder structure. Unlike the FCN network, which directly copies the feature maps, the decoder upsamples the low-resolution feature maps by pooling indexes with fewer training parameters, which has great advantages in storage and computational efficiency. Weng et al.<sup><xref ref-type="bibr" rid="CR27">27</xref></sup> applied the separable residual module to SegNet for water body segmentation, and the accuracy was greatly increased compared with FCN. Zhao et al.<sup><xref ref-type="bibr" rid="CR28">28</xref></sup> suggested the PSPNet network with a pyramidal pooling structure, which separates the feature map into multiple levels and sub-regions, combines the context data from various regions, completes multi-level semantic feature fusion, and mines global data completely.</p><p id="Par7">DeepLab networks<sup><xref ref-type="bibr" rid="CR29">29</xref>&#x02013;<xref ref-type="bibr" rid="CR32">32</xref></sup> are deep learning networks open-sourced by the Google research team, which has introduced Atrous Convolution<sup><xref ref-type="bibr" rid="CR33">33</xref></sup>, Conditional Random Field (CRF)<sup><xref ref-type="bibr" rid="CR7">7</xref></sup>, and Atrous Spatial Pyramid Pooling(ASPP)<sup><xref ref-type="bibr" rid="CR34">34</xref></sup> modules in succession. These modules fully utilize the feature graph's multi-scale information, enhancing the model's ability to capture fine details and raising the performance of&#x000a0;the deep learning semantic segmentation network to a new level. DeepLabv3+&#x02009;adds a simple but effective decoder module based on DeepLabv3, which improves the model&#x02019;s effect in dealing with image boundaries and better preserves the target's edge details. The resolution of coding features can be output using the proposed encoder-decoder structure by controlling the atrous convolution, and the accuracy and running time can be balanced.</p><p id="Par8">DeepLabv3+&#x02009;is one of the best general segmentation networks available today, with a smooth segmentation edge and segmentation accuracy that leads in a number of public datasets<sup><xref ref-type="bibr" rid="CR31">31</xref></sup>. However, there are still some problems. First, the DeepLabv3+&#x02009;encoder&#x02019;s model Xception<sup><xref ref-type="bibr" rid="CR35">35</xref></sup> has a complex structure, requiring a large amount of parameter calculation and memory, resulting in slow fitting speed and low segmentation efficiency; second, it is hard to precisely capture the contour of ground objects in semantic segmentation of high-resolution remote sensing images, small targets are missed, and similar objects are easily misjudged, resulting in low segmentation accuracy.</p><p id="Par9">In light of the aforementioned issues, the three improvements made to the DeepLabv3+&#x02009;network in this paper are as follows:<list list-type="order"><list-item><p id="Par10">At the coding layer, the DeepLabv3+&#x02009;model&#x02019;s feature extraction module Xception network is replaced with a lightweight network MobileNetV2<sup><xref ref-type="bibr" rid="CR36">36</xref></sup> to reduce the number of parameters in the semantic segmentation model and improve model training efficiency.</p></list-item><list-item><p id="Par11">The SENet<sup><xref ref-type="bibr" rid="CR37">37</xref></sup> is added to distribute channel weight and improve the problem of missed segmentation and target misjudgment, thereby increasing segmentation accuracy.</p></list-item><list-item><p id="Par12">Transfer learning<sup><xref ref-type="bibr" rid="CR38">38</xref></sup> is added to the original model, and the model obtained from the ImageNet<sup><xref ref-type="bibr" rid="CR39">39</xref></sup> dataset is used as the pre-training model to improve the model's capacity to collect features and promote network segmentation accuracy.</p></list-item></list></p><p id="Par13">The remainder of this paper is structured as follows: the datasets and pre-processing methods are described in section &#x0201c;<xref rid="Sec2" ref-type="sec">Data</xref>&#x0201d;. The DeepLabv3+&#x02009;model, the MST-DeepLabv3+&#x02009;model, and the semantic segmentation evaluation metrics are covered in section &#x0201c;<xref rid="Sec6" ref-type="sec">Methods</xref>&#x0201d;. In section &#x0201c;<xref rid="Sec13" ref-type="sec">Experimental results and analysis</xref>&#x0201d;, the experimental configuration is then briefly introduced, and the results are thoroughly analyzed. The findings of the experiment are discussed in section &#x0201c;<xref rid="Sec18" ref-type="sec">Discussion</xref>&#x0201d;. This paper is concluded in section &#x0201c;<xref rid="Sec19" ref-type="sec">Conclusions</xref>&#x0201d;, which also outlines some potential research topics.</p></sec><sec id="Sec2"><title>Data</title><sec id="Sec3"><title>ISPRS dataset</title><p id="Par14">The ISPRS dataset<sup><xref ref-type="bibr" rid="CR40">40</xref></sup> contains two sub-datasets, Vaihingen and Postdam, both of which cover the majority of the urban scenes. The Vaihingen dataset includes 33 different sizes of remote sensing images extracted from a larger top-level orthoimage. The top-level image and DSM (Digital Surface Model) have a spatial resolution of 9&#x000a0;cm. The remote sensing images consist of three bands: near-infrared, red, and green. The Postdam dataset has 38 UAV (Unmanned Aerial Vehicle) images with a resolution of 5&#x000a0;cm, which are 6000 pixels&#x02009;&#x000d7;&#x02009;6000 pixels. Both datasets were manually classified into the six land cover types: background, impervious surface, tree, building, car, and low vegetation.</p><p id="Par15">This paper makes use of the entire Vaihingen dataset. Since the dataset is small, this paper expands it by flipping, cropping, and rotating, finally obtains 3720 images, randomly selects 2976 images of which are used as the training set and 744 images are used as the test set. All of the images are 512 pixels&#x02009;&#x000d7;&#x02009;512 pixels in size.</p></sec><sec id="Sec4"><title>GID dataset</title><p id="Par16">The GID dataset<sup><xref ref-type="bibr" rid="CR41">41</xref></sup> is a large-scale high-resolution remote sensing image land cover data set based on data collected by the Chinese Gaofen-2 satellite. The GID dataset includes&#x000a0;two parts: the large-scale classification dataset and the land-cover dataset, both of which have a large number of samples from the same region, different seasons, and different light conditions, and are very close to the true distribution characteristics of ground features. 150 images from over 60 different Chinese cities are included in the large-scale classification dataset, which spans an area of over 50,000 square kilometers. The size of each image is 6800 pixels&#x02009;&#x000d7;&#x02009;7200 pixels, with a spatial resolution of 1&#x000a0;m. The land cover categories in the large-scale classification dataset are water, built-up, farmland, meadow and forest. The land-cover dataset includes 15 categories. There are 30,000 image blocks in total.</p><p id="Par17">The large-scale classification dataset of the GID is used for the experiments in the paper. 150 images are cropped to 512 pixels&#x02009;&#x000d7;&#x02009;512 pixels without overlap, and 27,300 images are obtained, 80% of which are randomly used as the training set and 20% as the test set.</p></sec><sec id="Sec5"><title>Taikang cultivated land dataset</title><p id="Par18">To verify the feasibility of the MST-DeepLabv3+&#x02009;model in practice, we selected high-resolution images from the Gaofen-1 remote sensing satellite in Taikang County, Zhoukou City, Henan Province, China, and created a dataset named Taikang cultivated land dataset for land use classification. The Gaofen-1 remote sensing satellite images of the study area are from the Gaofen Hubei Center<sup><xref ref-type="bibr" rid="CR42">42</xref></sup>.</p><p id="Par19">The Gaofen-1 satellite is an Earth observation remote sensing satellite independently developed by China. It has an average orbital altitude of 644.5&#x000a0;km and a lifespan of 5&#x02013;8&#x000a0;years. It is equipped with two 2&#x000a0;m resolution panchromatic and 8&#x000a0;m resolution multispectral PMS cameras and four 16m resolution WFV camera, coverage period is 41&#x000a0;days and 4&#x000a0;days respectively. The ground width of the PMS camera is greater than 60&#x000a0;km, and it has five bands, namely panchromatic (wavelength 0.45&#x02013;0.90&#x000a0;&#x003bc;m), blue (Band1, 0.45&#x02013;0.52&#x000a0;&#x003bc;m), green (Band2, 0.52&#x02013;0.59&#x000a0;&#x003bc;m), and red (Band3, 0.63&#x02013;0.69&#x000a0;&#x003bc;m) and near-infrared (Band4, 0.77&#x02013;0.89&#x000a0;&#x003bc;m). The WFV camera has a ground width greater than 800&#x000a0;km and has four bands, namely near-infrared, red, green and blue band<sup><xref ref-type="bibr" rid="CR43">43</xref></sup>.</p><p id="Par20">We selected PMS images with higher resolution for experiments, and filtered the images according to the criteria of clearly visible farmland texture and less cloud coverage. Based on the growth and maturity cycles of farmland crops in the study area, two images in February and May 2017 were finally selected as source data, with scene IDs of 3350241 and 3661472 respectively. The sensors for panchromatic and multispectral images are PAN1 and MSS1 respectively, and the unified projection coordinate system is WGS_1984_UTM_Zone_50N. The dataset creation process is shown in Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>, using ENVI, ArcGIS, and Python tools in turn for preprocessing, drawing labels and cropping. After image processing, 6084 images of 512 pixels&#x000a0;&#x000d7;&#x000a0;512 pixels are finally obtained and randomly divided into 5475 training images and 609 prediction images.<fig id="Fig1"><label>Figure 1</label><caption><p>Taikang cultivated land dataset production flow chart.</p></caption><graphic xlink:href="41598_2024_60375_Fig1_HTML" id="MO1"/></fig></p></sec></sec><sec id="Sec6"><title>Methods</title><sec id="Sec7"><title>DeepLabv3+</title><p id="Par21">DeepLabv3+&#x02009;model<sup><xref ref-type="bibr" rid="CR31">31</xref></sup> uses an encoder-decoder structure, with DeepLabv3 serving as the network's encoder, optimizing the extraction effect of the target's edge information, and then using the decoder to recover the feature information and output the predicted results, which improves the segmentation effect and retains the target's edge details. DeepLabv3+&#x02009;takes the Xception model as the backbone network and applies the deep separable convolution to the ASPP module and the decoder module to create an encoder-decoder network with better segmentation effects. The DeepLabv3+&#x02009;model&#x02019;s structure is depicted in Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref>.<fig id="Fig2"><label>Figure 2</label><caption><p>DeepLabv3+&#x02009;model structure.</p></caption><graphic xlink:href="41598_2024_60375_Fig2_HTML" id="MO2"/></fig><list list-type="order"><list-item><p id="Par22">Encoder: Serial atrous convolution is used in the backbone DCNN. After the image passes through the backbone network, the results are provided to the Decoder and ASPP modules for feature extraction, respectively. Serial atrous convolution is used in the backbone DCNN. After the image passes through the backbone network, the results are provided to the Decoder and ASPP modules for feature extraction, respectively. The deep features are extracted by the ASPP module and then merged. It enters the decoder after 1&#x02009;&#x000d7;&#x02009;1 convolution is used to change the number of channels;</p></list-item><list-item><p id="Par23">Decoder: After four-fold upsampling of the output of the deep features from the Encoder part, the features are fused with the shallow features that are downsampled using 1&#x02009;&#x000d7;&#x02009;1 convolution. And then, the features are further fused using 3&#x02009;&#x000d7;&#x02009;3 convolution. Finally, four-fold upsampling is performed using a bilinear interpolation method to get results of the same size as the original image.</p></list-item></list></p></sec><sec id="Sec8"><title>MST-DeepLabv3+</title><p id="Par24">This paper suggests a model called MST-DeepLabv3+&#x02009;that is based on DeepLabv3+. We use the MobileNetV2 network instead of Xception as the backbone network; employ the transfer learning method to reduce the model complexity while improving the segmentation performance; and fuse the attention mechanism at appropriate locations in the network to improve the weight of the feature channel with good network performance, so as to improve the efficiency of remote sensing image semantic segmentation. MST is obtained as an acronym combination of MobileNetV2, SENet and Transfer learning. Figure&#x000a0;<xref rid="Fig3" ref-type="fig">3</xref> shows the structure of the MST-DeepLabv3+&#x02009;model.<fig id="Fig3"><label>Figure 3</label><caption><p>Structure of the MST-DeepLabv3+&#x02009;model.</p></caption><graphic xlink:href="41598_2024_60375_Fig3_HTML" id="MO3"/></fig></p><sec id="Sec9"><title>MobileNetV2</title><p id="Par25">MobileNetV2 network<sup><xref ref-type="bibr" rid="CR36">36</xref></sup> uses expansion coefficients to help control the network size. The network structure is deep but less computationally intensive, which can save training resources and has great advantages for target extraction in remote sensing images<sup><xref ref-type="bibr" rid="CR44">44</xref></sup>. MobileNetV2 introduces the structure of inverted residual, as seen in Fig.&#x000a0;<xref rid="Fig4" ref-type="fig">4</xref>, which increases the dimensionality of the convolution, enhances the model feature extraction ability, and lowers the number of model parameters. Additionally, MobileNetV2 uses the linear bottle-necks structure to prevent information extraction loss due to the destruction of target features by ReLU after dimensionality reduction<sup><xref ref-type="bibr" rid="CR36">36</xref></sup>.<fig id="Fig4"><label>Figure 4</label><caption><p>The structure of Inverted residual block.</p></caption><graphic xlink:href="41598_2024_60375_Fig4_HTML" id="MO4"/></fig></p><p id="Par26">Instead of Xception, we employ the lightweight MobileNetV2 network, which is capable of faster semantic segmentation of remotely sensed images. And the MobileNetV2 network is optimized by deleting the standard convolution operation and the global average pooling layer set by the last three layers to achieve classification, making it more compatible with the DeepLabv3+&#x02009;model for semantic segmentation operation. And the step size s of the seventh layer is changed from 2 to 1, and only four downsampling operations are performed to ensure the image resolution and segmentation effect. The information of MobileNetV2 network structure used in this paper is shown in Table <xref rid="Tab1" ref-type="table">1</xref>.<table-wrap id="Tab1"><label>Table 1</label><caption><p>MobileNetV2 network structure.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Input</th><th align="left">Operator</th><th align="left">t</th><th align="left">c</th><th align="left">n</th><th align="left">s</th></tr></thead><tbody><tr><td align="left">5122&#x02009;&#x000d7;&#x02009;3</td><td align="left">conv2d</td><td align="left">&#x02013;</td><td align="left">32</td><td align="left">1</td><td align="left">2</td></tr><tr><td align="left">2562&#x02009;&#x000d7;&#x02009;32</td><td align="left">Bottleneck</td><td align="left">1</td><td align="left">16</td><td align="left">1</td><td align="left">1</td></tr><tr><td align="left">2562&#x02009;&#x000d7;&#x02009;16</td><td align="left">Bottleneck</td><td align="left">6</td><td align="left">24</td><td align="left">2</td><td align="left">2</td></tr><tr><td align="left">1282&#x02009;&#x000d7;&#x02009;24</td><td align="left">Bottleneck</td><td align="left">6</td><td align="left">32</td><td align="left">3</td><td align="left">2</td></tr><tr><td align="left">642&#x02009;&#x000d7;&#x02009;32</td><td align="left">Bottleneck</td><td align="left">6</td><td align="left">64</td><td align="left">4</td><td align="left">2</td></tr><tr><td align="left">322&#x02009;&#x000d7;&#x02009;64</td><td align="left">Bottleneck</td><td align="left">6</td><td align="left">96</td><td align="left">3</td><td align="left">1</td></tr><tr><td align="left">322&#x02009;&#x000d7;&#x02009;96</td><td align="left">Bottleneck</td><td align="left">6</td><td align="left">160</td><td align="left">3</td><td align="left">1</td></tr><tr><td align="left">322&#x02009;&#x000d7;&#x02009;160</td><td align="left">Bottleneck</td><td align="left">6</td><td align="left">320</td><td align="left">1</td><td align="left">1</td></tr></tbody></table><table-wrap-foot><p>The t stands for the channels&#x02019; expansion multiple, the c stands for how many output channels there are, The n indicates how many times the current operator will be repeated, The s is the stride<sup><xref ref-type="bibr" rid="CR45">45</xref></sup>.</p></table-wrap-foot></table-wrap></p></sec><sec id="Sec10"><title>SENet</title><p id="Par27">SENet (Squeeze-and-Excitation Networks)<sup><xref ref-type="bibr" rid="CR37">37</xref></sup> enables the network to obtain the importance of different feature channels in the feature map and assign weight values to the feature channels according to their importance, so as to focus on certain feature channels. SENet begins with global information to accomplish the goals of emphasizing key traits while suppressing others, as well as to realize the automatic selection and weight distribution of attention regions. In this paper, we add SENet before 1&#x02009;&#x000d7;&#x02009;1 convolution in DeepLabv3+&#x02019;s encoder to reduce the influence of irrelevant features after stitching on recognition accuracy. Different weights are applied to the outputs within the coding region to achieve optimization of the feature map, which brings significant performance improvement to the existing segmentation model with a small additional computational cost. Figure&#x000a0;<xref rid="Fig5" ref-type="fig">5</xref> depicts the structure of the SENet.<fig id="Fig5"><label>Figure 5</label><caption><p>The structure of the SENet.</p></caption><graphic xlink:href="41598_2024_60375_Fig5_HTML" id="MO5"/></fig></p><p id="Par28">Squeeze and Excitation are the two operations that makeup SENet. The Squeeze operation is responsible for the global pooling of spatial dimensions, while the Excitation operation learns the pooled channel dependencies and assigns channel weights. The final output of the SENet module is produced by multiplying the output of the Excitation operation by the original input features.</p><p id="Par29">The equation for Squeeze is:<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ z = F_{sq} \left( f \right) = \frac{1}{H \times W}\sum\nolimits_{i = 1}^{H} {\sum\nolimits_{j = 1}^{W} {f\left( {i,j} \right)} } , $$\end{document}</tex-math><mml:math id="M2" display="block"><mml:mrow><mml:mi>z</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi mathvariant="italic">sq</mml:mi></mml:mrow></mml:msub><mml:mfenced close=")" open="("><mml:mi>f</mml:mi></mml:mfenced><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>H</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:mfrac><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>H</mml:mi></mml:msubsup><mml:mrow><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>W</mml:mi></mml:msubsup><mml:mrow><mml:mi>f</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math><graphic xlink:href="41598_2024_60375_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula></p><p id="Par30">In the above equation,<inline-formula id="IEq1"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$f \in R^{H \times W}$$\end{document}</tex-math><mml:math id="M4"><mml:mrow><mml:mi>f</mml:mi><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_60375_Article_IEq1.gif"/></alternatives></inline-formula> is a two-dimensional feature map set and, <italic>f(i,j)</italic> is one of the elements,<italic> H</italic> and <italic>W</italic> denote the height and width of the feature map spatial information, respectively; z is the Squeeze operation output.</p><p id="Par31">The equation for Excitation is:<disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ s = F_{ex} \left( {z,w} \right) = \sigma \left[ {W_{2} \delta \left( {W_{1} z} \right)} \right], $$\end{document}</tex-math><mml:math id="M6" display="block"><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi mathvariant="italic">ex</mml:mi></mml:mrow></mml:msub><mml:mfenced close=")" open="("><mml:mrow><mml:mi>z</mml:mi><mml:mo>,</mml:mo><mml:mi>w</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>&#x003c3;</mml:mi><mml:mfenced close="]" open="["><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mi>&#x003b4;</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mi>z</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:math><graphic xlink:href="41598_2024_60375_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula></p><p id="Par32">In the above equation, <italic>&#x003c3;</italic> and <italic>&#x003b4;</italic> denote the Sigmoid and ReLU activation functions, respectively; <inline-formula id="IEq2"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$W_{1} \in R^{{\frac{c}{r} \times C}}$$\end{document}</tex-math><mml:math id="M8"><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mfrac><mml:mi>c</mml:mi><mml:mi>r</mml:mi></mml:mfrac><mml:mo>&#x000d7;</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_60375_Article_IEq2.gif"/></alternatives></inline-formula>, <inline-formula id="IEq3"><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$W_{2} \in R^{{C \times \frac{c}{r}}}$$\end{document}</tex-math><mml:math id="M10"><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mfrac><mml:mi>c</mml:mi><mml:mi>r</mml:mi></mml:mfrac></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_60375_Article_IEq3.gif"/></alternatives></inline-formula>, <inline-formula id="IEq4"><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$W_{1}$$\end{document}</tex-math><mml:math id="M12"><mml:msub><mml:mi>W</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_60375_Article_IEq4.gif"/></alternatives></inline-formula> and <inline-formula id="IEq5"><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$W_{2}$$\end{document}</tex-math><mml:math id="M14"><mml:msub><mml:mi>W</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_60375_Article_IEq5.gif"/></alternatives></inline-formula> are some elements of them, respectively, and <italic>r</italic> is the imensionality reduction coefficient;<italic> s</italic> is the output of Excitation operation.</p><p id="Par33">After the Excitation operation, the resulting output weights are multiplied by the original input features:<disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ x = F_{scale} \left( {f,s} \right) = s \cdot f\left( {i,j} \right), $$\end{document}</tex-math><mml:math id="M16" display="block"><mml:mrow><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi mathvariant="italic">scale</mml:mi></mml:mrow></mml:msub><mml:mfenced close=")" open="("><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>s</mml:mi><mml:mo>&#x000b7;</mml:mo><mml:mi>f</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:math><graphic xlink:href="41598_2024_60375_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula></p><p id="Par34">In the equation, <italic>x</italic> is a value in the final output X of the SENet. X&#x02009;=&#x02009;[x<sub>1</sub>, x<sub>2</sub>,&#x02026;, x<sub>c</sub>].</p></sec><sec id="Sec11"><title>Transfer learning</title><p id="Par35">Transfer learning<sup><xref ref-type="bibr" rid="CR38">38</xref></sup> is a method in deep learning that starts model training on a new dataset with a model that has already been trained on an existing dataset. Usually, when we conduct deep learning experiments, the model&#x02019;s parameters, such as its weights and biases, are generated by the system&#x02019;s initialization at the beginning. Training the model on a new dataset from scratch in this way often takes a long time to make the function converge. By using transfer learning techniques, the model can perform better under the same conditions, and reducing the cost of resource consumption<sup><xref ref-type="bibr" rid="CR45">45</xref></sup>.</p><p id="Par36">In this paper, the feature extraction network trained on the ImageNet dataset is transferred to the MST-DeepLabv3+&#x02009;model using transfer learning, which can enhance the model's ability to obtain features and effectively improve the model segmentation accuracy.</p></sec></sec><sec id="Sec12"><title>Accuracy evaluation</title><p id="Par37">In this paper, we use the visual comparison of segmentation results and common evaluation metrics to comprehensively evaluate the model segmentation performance. The evaluation metrics used are MIoU, OA, Precision, Recall, and F1-Score.</p><p id="Par38">MIoU is the most commonly used metric in semantic segmentation experiments. Its value is calculated by first calculating the ratio between the intersection and the concatenation of the two sets of true and predicted values on each category, and then finding the average of all categories. As shown in Eq.&#x000a0;(<xref rid="Equ4" ref-type="disp-formula">4</xref>).<disp-formula id="Equ4"><label>4</label><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ MIoU = \frac{1}{{{\text{k}} + 1}}\sum\nolimits_{i = 0}^{k} {\frac{TP}{{FN + FP + TP}}} , $$\end{document}</tex-math><mml:math id="M18" display="block"><mml:mrow><mml:mi>M</mml:mi><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:mi>U</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mtext>k</mml:mtext><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mi>k</mml:mi></mml:msubsup><mml:mfrac><mml:mrow><mml:mi mathvariant="italic">TP</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:math><graphic xlink:href="41598_2024_60375_Article_Equ4.gif" position="anchor"/></alternatives></disp-formula></p><p id="Par39">OA is the proportion of properly identified pixels to all pixels, which can represent the overall accuracy of the model. As shown in Eq.&#x000a0;(<xref rid="Equ5" ref-type="disp-formula">5</xref>).<disp-formula id="Equ5"><label>5</label><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ OA = \frac{TP + TN}{{TP + TN + FP + FN}}, $$\end{document}</tex-math><mml:math id="M20" display="block"><mml:mrow><mml:mi>O</mml:mi><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:math><graphic xlink:href="41598_2024_60375_Article_Equ5.gif" position="anchor"/></alternatives></disp-formula></p><p id="Par40">Precision indicates the number of true positive pixels in the pixels that are predicted to be positive. As shown in Eq.&#x000a0;(<xref rid="Equ6" ref-type="disp-formula">6</xref>).<disp-formula id="Equ6"><label>6</label><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ P{\text{recision}} = \frac{TP}{{TP + FP}}, $$\end{document}</tex-math><mml:math id="M22" display="block"><mml:mrow><mml:mi>P</mml:mi><mml:mtext>recision</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="italic">TP</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:math><graphic xlink:href="41598_2024_60375_Article_Equ6.gif" position="anchor"/></alternatives></disp-formula></p><p id="Par41">Recall is the ratio of the model&#x02019;s correctly predicted positive pixels to the total positive pixels. As shown in Eq.&#x000a0;(<xref rid="Equ7" ref-type="disp-formula">7</xref>).<disp-formula id="Equ7"><label>7</label><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ {\text{Re}} call = \frac{TP}{{TP + FN}}, $$\end{document}</tex-math><mml:math id="M24" display="block"><mml:mrow><mml:mtext>Re</mml:mtext><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="italic">TP</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:math><graphic xlink:href="41598_2024_60375_Article_Equ7.gif" position="anchor"/></alternatives></disp-formula></p><p id="Par42">F1-Score is the harmonic mean of Precision and Recall, which is a comprehensive evaluation metric. It can solve the problem that when the number of pixels in each category deviates greatly, the OA index cannot accurately evaluate the specific classification results<bold>.</bold> Its equation is as follows:<disp-formula id="Equ8"><label>8</label><alternatives><tex-math id="M25">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ F = \frac{{2{\text{Precison}} \times {\text{Re}} call}}{{\Pr ecison + {\text{Re}} call{\text{l}}}} = \frac{2TP}{{2TP + FN + FP}}, $$\end{document}</tex-math><mml:math id="M26" display="block"><mml:mrow><mml:mi>F</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mtext>Precison</mml:mtext><mml:mo>&#x000d7;</mml:mo><mml:mtext>Re</mml:mtext><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mo>Pr</mml:mo><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mtext>Re</mml:mtext><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mtext>l</mml:mtext></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:math><graphic xlink:href="41598_2024_60375_Article_Equ8.gif" position="anchor"/></alternatives></disp-formula></p><p id="Par43">In the above equations, <italic>k</italic>&#x02009;+&#x02009;<italic>1</italic> represents the number of data categories, including the background categories. <italic>TP</italic> is True Positive (The model predicts a positive case, and the actual case is positive), <italic>FP</italic> is False Positive (The model predicts a positive case, but the actual case is negative), <italic>FN</italic> is False Negative (The model predicts a negative case, but the actual case is positive), <italic>TN</italic> is True Negative (The model predicts a negative case, and the actual case is negative).</p></sec></sec><sec id="Sec13"><title>Experimental results and analysis</title><p id="Par44">The operating system is CentOS7.9, the CPU is AMD EPYC 7402 48@ 2.8GHz, the GPU is 8*NVDIA&#x000ae;GeForce&#x000ae;RTX 3090, and the video memory is 8*24GB. The deep learning framework used is pytorch3.6. The batch_size is set to 8 and the number of iterations is set to 100. Experiments have proven that when the number of iterations reaches the maximum, the loss function has converged and the accuracy is no longer significantly improved. The basic learning rate is set to 0.0005, and the Adam optimizer is used to dynamically adjust the learning rate to make the learning rate closer to the parameter update state, thereby allowing the model to converge better.</p><p id="Par45">To validate MST-DeepLabv3+&#x02019;s effectiveness, it was compared to DeepLabv3+, PSPNet, and UNet, in terms of accuracy and segmentation details. UNet can achieve higher segmentation accuracy while using less data<sup><xref ref-type="bibr" rid="CR25">25</xref></sup>. The pyramid pooling module, used by PSPNet, may aggregate contextual information from different regions, making it easier to gather global information<sup><xref ref-type="bibr" rid="CR28">28</xref></sup>.</p><sec id="Sec14"><title>Experimental results of ISPRS dataset</title><p id="Par46">Table <xref rid="Tab2" ref-type="table">2</xref> statistically compares the evaluation results of MST-DeepLabv3+&#x02009;on the ISPRS dataset to those of the other three models. In the comparison of the results of MIoU, OA, Precision, Recall, and F1-score, MST-DeepLabv3+&#x02009;obtained the highest values. In the MIoU comparison, MST-DeepLabv3+&#x02009;has a MIoU of 82.47%, which is 14.13%, 10.48%, and 13.85% higher than PSPNet, UNet, and DeepLabv3+, respectively. In the OA comparison, MST-DeepLabv3+&#x02009;has an OA value of 92.13%, which is 5.38%, 4.98%, and 6.02% higher than other three models, respectively.<table-wrap id="Tab2"><label>Table 2</label><caption><p>Segmentation results on the ISPRS dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Method</th><th align="left">MIoU (%)</th><th align="left">OA (%)</th><th align="left">Precision (%)</th><th align="left">Recall (%)</th><th align="left">F1-score(%)</th></tr></thead><tbody><tr><td align="left">PSPNet</td><td char="." align="char">68.34</td><td char="." align="char">86.75</td><td char="." align="char">80.67</td><td char="." align="char">80.98</td><td char="." align="char">80.82</td></tr><tr><td align="left">UNet</td><td char="." align="char">71.99</td><td char="." align="char">87.15</td><td char="." align="char">84.53</td><td char="." align="char">81.91</td><td char="." align="char">83.20</td></tr><tr><td align="left">DeepLabv3+</td><td char="." align="char">68.62</td><td char="." align="char">86.11</td><td char="." align="char">81.80</td><td char="." align="char">80.13</td><td char="." align="char">80.96</td></tr><tr><td align="left">MST-DeepLabv3+</td><td char="." align="char">82.47</td><td char="." align="char">92.13</td><td char="." align="char">90.34</td><td char="." align="char">90.12</td><td char="." align="char">90.23</td></tr></tbody></table></table-wrap></p><p id="Par47">Precision and Recall measure the correctness and completeness of segmentation, respectively, and the ideal segmentation situation is one in which both Precision and Recall are high. The Precision value of MST-DeepLabv3+&#x02009;was 90.34%, the recall rate was 90.12%, and the F1-score reached the highest value of 90.23%.</p><p id="Par48">Table <xref rid="Tab3" ref-type="table">3</xref> shows the specific classification results of the ISPRS dataset to further demonstrate the effectiveness of MST-DeepLabv3+. In the MIoU comparison, MST-DeepLabv3+&#x02009;has the highest MIoU of all types. For the background, car, and low vegetation categories, the MIoU values of the PSPNet, UNet, and DeepLabv3+&#x02009;are relatively low, MST-DeepLabv3+&#x02019;s MIoU values for these three classes are 84.04%, 70.02%, and 77.51%, which are 26.14%, 24.82%, and 12.19% higher than PSPNet, 11.47%, 19.14%, and 12.84% higher than UNet, 27.89%, 17.07%, and 14.01% higher than DeepLabv3+. MST-DeepLabv3+&#x02009;also has the highest MIoU values for the impervious surface, tree, and building of all types of methods.<table-wrap id="Tab3"><label>Table 3</label><caption><p>Comparison of IoU(%) for the ISPRS dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Method</th><th align="left">Background</th><th align="left">Impervious surface</th><th align="left">Tree</th><th align="left">Building</th><th align="left">Car</th><th align="left">Low vegetation</th></tr></thead><tbody><tr><td align="left">PSPNet</td><td char="." align="char">57.90</td><td char="." align="char">79.54</td><td char="." align="char">74.98</td><td char="." align="char">87.21</td><td char="." align="char">45.20</td><td char="." align="char">65.32</td></tr><tr><td align="left">UNet</td><td char="." align="char">72.57</td><td char="." align="char">81.02</td><td char="." align="char">74.89</td><td char="." align="char">87.91</td><td char="." align="char">50.88</td><td char="." align="char">64.67</td></tr><tr><td align="left">DeepLabv3+</td><td char="." align="char">56.15</td><td char="." align="char">78.82</td><td char="." align="char">73.91</td><td char="." align="char">86.37</td><td char="." align="char">52.95</td><td char="." align="char">63.5</td></tr><tr><td align="left">MST-DeepLabv3+</td><td char="." align="char">84.04</td><td char="." align="char">87.80</td><td char="." align="char">82.17</td><td char="." align="char">93.28</td><td char="." align="char">70.02</td><td char="." align="char">77.51</td></tr></tbody></table></table-wrap></p><p id="Par49">Three cropped images were analyzed to further compare the classification results of different models. As shown in Fig.&#x000a0;<xref rid="Fig6" ref-type="fig">6</xref>.<fig id="Fig6"><label>Figure 6</label><caption><p>Example of classification result visualization of ISPRS dataset.</p></caption><graphic xlink:href="41598_2024_60375_Fig6_HTML" id="MO6"/></fig></p><p id="Par50">In group I, The models PSPNet, UNet, and DeepLabv3+&#x02009;all missed the small car patches in the black box, only the MST-DeepLabv3+&#x02009;can correctly identify the overall contour and location. For the tree shown in the yellow box, compared with the classical model segmentation result of scattered and no general outline, MST-DeepLabv3+&#x02009;model can accurately identify the category regions with complete boundaries. The building segmentation results in the red box show that the classical model is not accurate for contour identification, especially the DeepLabv3+&#x02009;model segmentation results, which are fragmented and the boundary is obviously incorrect. However, MST-DeepLabv3+, on the other hand, accurately identifies the boundary between buildings and low vegetation.</p><p id="Par51">In group II, the building, impervious surface and background areas are regularly arranged, and the boundary contrast is more obvious. PSPNet and DeepLabv3+&#x02009;cannot identify the impervious surface and the background boundary, and the impervious surface and the building boundary better. PSPNet is worse and identifies most of the impervious surface areas as background and building. The segmentation boundaries of UNet and DeepLabv3+&#x02009;are rough. MST-DeepLabv3+&#x02009;not only has the best segmentation effect but also has smoother edges.</p><p id="Par52">In group III, the region shown in the red box is the segmentation result comparison of the low vegetation, PSPNet does not identify the low vegetation at all, the segmentation results of UNet and DeepLabv3+&#x02009;are fragmented, and the low vegetation were misclassified into the tree. In addition, the results of the DeepLabv3+&#x02009;also have a large area of impervious surface misclassified into background. MST-DeepLabv3+&#x02009;can completely identify the overall region of the category, and the segmentation effect is the best.</p><p id="Par53">Overall, in terms of classification effect, MST-DeepLabv3+&#x02009;outperforms PSPNet, UNet, and DeepLabv3+.</p></sec><sec id="Sec15"><title>Experimental results of GID dataset</title><p id="Par54">Table <xref rid="Tab4" ref-type="table">4</xref> compares the classification accuracy evaluation results of the four models on the GID dataset. In the comparison results of MIoU, Recall, OA, and F1-score, MST-DeepLabv3+&#x02009;all obtained the highest values. In the MIoU comparison, the MIoU of MST-DeepLabv3+&#x02009;is 73.44%, which is 1.56%, 2.2%, and 7.11% higher than PSPNet, UNet, and DeepLabv3+, respectively. In the OA comparison, MST-DeepLabv3+&#x02009;has the highest OA value of all models at 85.58%.<table-wrap id="Tab4"><label>Table 4</label><caption><p>Segmentation results on the GID dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Method</th><th align="left">MIoU (%)</th><th align="left">OA (%)</th><th align="left">Precision (%)</th><th align="left">Recall (%)</th><th align="left">F1-score (%)</th></tr></thead><tbody><tr><td align="left">PSPNet</td><td char="." align="char">71.88</td><td char="." align="char">84.91</td><td char="." align="char">85.39</td><td char="." align="char">81.66</td><td char="." align="char">83.48</td></tr><tr><td align="left">UNet</td><td char="." align="char">71.24</td><td char="." align="char">83.50</td><td char="." align="char">83.92</td><td char="." align="char">81.99</td><td char="." align="char">82.94</td></tr><tr><td align="left">DeepLabv3+</td><td char="." align="char">66.33</td><td char="." align="char">80.51</td><td char="." align="char">78.81</td><td char="." align="char">80.11</td><td char="." align="char">79.45</td></tr><tr><td align="left">MST-DeepLabv3+</td><td char="." align="char">73.44</td><td char="." align="char">85.58</td><td char="." align="char">84.10</td><td char="." align="char">84.86</td><td char="." align="char">84.48</td></tr></tbody></table></table-wrap></p><p id="Par55">MST-DeepLabv3+&#x02009;has a Precision value of 84.10%, which is higher than UNet and DeepLabv3+&#x02009;but slightly lower than PSPNet. MST-DeepLabv3+&#x02009;has the highest Recall of 84.86%, with an F1-score of 84.48%. The F1-score is 1%, 1.54%, and 5.03% higher than the PSPNet, UNet, and DeepLabv3+&#x02009;models, respectively.</p><p id="Par56">Table <xref rid="Tab5" ref-type="table">5</xref> displays the specific classification results of the GID dataset. In the MIoU comparison, MST-DeepLabv3+&#x02009;has the highest MIoU in five types: background, water, farmland, build-up, and forest. And DeepLabv3+&#x02009;has the lowest IoU of the five types. UNet has the highest accuracy in the meadow category, which is 1.75% higher than MST-DeepLabv3+.<table-wrap id="Tab5"><label>Table 5</label><caption><p>Comparison of IoU(%) for the GID dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Method</th><th align="left">Background</th><th align="left">Water</th><th align="left">Farmland</th><th align="left">Built-up</th><th align="left">Meadow</th><th align="left">Forest</th></tr></thead><tbody><tr><td align="left">PSPNet</td><td char="." align="char">71.33</td><td char="." align="char">88.44</td><td char="." align="char">76.05</td><td char="." align="char">69.62</td><td char="." align="char">63.85</td><td char="." align="char">61.97</td></tr><tr><td align="left">UNet</td><td char="." align="char">69.03</td><td char="." align="char">88.73</td><td char="." align="char">72.99</td><td char="." align="char">67.05</td><td char="." align="char">67.98</td><td char="." align="char">61.64</td></tr><tr><td align="left">DeepLabv3+</td><td char="." align="char">63.76</td><td char="." align="char">84.71</td><td char="." align="char">70.20</td><td char="." align="char">64.59</td><td char="." align="char">62.11</td><td char="." align="char">52.6</td></tr><tr><td align="left">MST-DeepLabv3+</td><td char="." align="char">71.91</td><td char="." align="char">88.86</td><td char="." align="char">77.32</td><td char="." align="char">70.09</td><td char="." align="char">66.23</td><td char="." align="char">66.24</td></tr></tbody></table></table-wrap></p><p id="Par57">Four cropped images were selected to show the visualization of semantic segmentation results of different models on the GID dataset. As shown in Fig.&#x000a0;<xref rid="Fig7" ref-type="fig">7</xref>.<fig id="Fig7"><label>Figure 7</label><caption><p>Example of classification result visualization of GID dataset.</p></caption><graphic xlink:href="41598_2024_60375_Fig7_HTML" id="MO7"/></fig></p><p id="Par58">In group I,it is mainly the boundary division of the farmland that occupies most of the area from the water and the background. As shown in the yellow box, PSPNet, UNet, and DeepLabv3+&#x02009;models have poor recognition of farmland and background, in which DeepLabv3+&#x02009;is especially obvious and the overall region segmentation is more fragmented. MST-DeepLabv3+&#x02009;is relatively accurate for the boundary recognition of farmland and background. The region shown in the red box is the water category segmentation result, which clearly shows that PSPNet and UNet incorrectly identify water as background, while DeepLabv3+&#x02009;recognizes part of the water region, but the boundary is incomplete, MST-DeepLabv3+&#x02009;identifies the boundary between water and background more accurately, as well as the boundary between water and farmland.</p><p id="Par59">In group II, PSPNet, UNet and DeepLabv3+&#x02009;can hardly recognize the small patch area of built-up, but MST-DeepLabv3+&#x02009;can identify it accurately and optimizes the category segmentation range.</p><p id="Par60">In group III, PSPNet recognizes forest as background, UNet and DeepLabv3+&#x02009;can recognize a portion of the forest's outline, but the edges are rough. MST-DeepLabv3+&#x02009;identifies the entire forest region more effectively, and the edges are smoother and more continuous.</p><p id="Par61">In group IV, PSPNet completely fails to recognize the meadow, UNet incorrectly recognizes meadow as forest, and DeepLabv3+, which is oversegmented. In comparison, MST-DeepLabv3+&#x02009;extracts more information on the meadow.</p><p id="Par62">In general, MST-DeepLabv3+&#x02009;outperforms other models in terms of classification effect, improves the phenomenon of incomplete classification, unclear boundary, misclassification, omission, and over-segmentation, and significantly improves recognition accuracy.</p></sec><sec id="Sec16"><title>Experimental results of Taikang cultivated land dataset</title><p id="Par63">Table <xref rid="Tab6" ref-type="table">6</xref> shows the segmentation results of each model on the Taikang cultivated land dataset. Among the classic models, UNet has the highest values of all evaluation metrics. The MIoU of MST-Deeplapv3+&#x02009;reaches 90.77%, which is 3.71% higher than UNet. The OA, Precision, Recall and F1-score of MST-Deeplapv3+&#x02009;reach 95.47%, 95.28%, 95.02%, and 95.15%, which are 1.94%, 2.06%, 2.21%, and 2.09% higher than UNet, respectively. PSPNet has the lowest values for all metrics, with MIoU lower than MST-DeepLabv3+&#x02009;by 5.94%, and OA, Precision, Recall and F1-score lower than MST-Deeplapv3+&#x02009;by 3.17%, 3.44%, 3.36%, and 3.40%, respectively. Compared to DeepLabv3+, the specific improvement of each evaluation value of MST-DeepLabv3+&#x02009;is 5.37% for MIoU, 2.83% for OA, 2.91% for Precision, 3.19% for Recall, and 3.05% for F1-score.<table-wrap id="Tab6"><label>Table 6</label><caption><p>Segmentation results on the Taikang cultivated land dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Method</th><th align="left">MIoU (%)</th><th align="left">OA (%)</th><th align="left">Precision (%)</th><th align="left">Recall (%)</th><th align="left">F1-score (%)</th></tr></thead><tbody><tr><td align="left">PSPNet</td><td char="." align="char">84.83</td><td char="." align="char">92.30</td><td char="." align="char">91.84</td><td char="." align="char">91.66</td><td char="." align="char">91.75</td></tr><tr><td align="left">UNet</td><td char="." align="char">87.06</td><td char="." align="char">93.53</td><td char="." align="char">93.22</td><td char="." align="char">92.9</td><td char="." align="char">93.06</td></tr><tr><td align="left">DeepLabv3+</td><td char="." align="char">85.40</td><td char="." align="char">92.64</td><td char="." align="char">92.37</td><td char="." align="char">91.83</td><td char="." align="char">92.10</td></tr><tr><td align="left">MST-DeepLabv3+</td><td char="." align="char">90.77</td><td char="." align="char">95.47</td><td char="." align="char">95.28</td><td char="." align="char">95.02</td><td char="." align="char">95.15</td></tr></tbody></table></table-wrap></p><p id="Par64">The comparison of segmentation results of Taikang cultivated land dataset is shown in Table <xref rid="Tab7" ref-type="table">7</xref>. The comparison results show that MST-DeepLabv3+&#x02009;has the best segmentation effect, and the IoU of cultivated land reaches 93.06%, which is an increase of 4.59%, 2.83% and 4.05% compared to PSPNet, UNet and DeepLabv3+&#x02009;models, respectively. The IoU for background categories reached 88.48%, which is an increase of 7.28%, 4.59% and 6.68% compared to PSPNet, UNet and DeepLabv3+, respectively.<table-wrap id="Tab7"><label>Table 7</label><caption><p>Comparison of IoU(%) for the Taikang cultivated land dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Method</th><th align="left">Background</th><th align="left">Cultivated land</th></tr></thead><tbody><tr><td align="left">PSPNet</td><td char="." align="char">81.20</td><td char="." align="char">88.47</td></tr><tr><td align="left">UNet</td><td char="." align="char">83.89</td><td char="." align="char">90.23</td></tr><tr><td align="left">DeepLabv3+</td><td char="." align="char">81.80</td><td char="." align="char">89.01</td></tr><tr><td align="left">MST-DeepLabv3+</td><td char="." align="char">88.48</td><td char="." align="char">93.06</td></tr></tbody></table></table-wrap></p><p id="Par65">Three groups of images of cultivated land segmentation results are selected for comparison and analyzed with the specific information of background categories in the images. As shown in Fig.&#x000a0;<xref rid="Fig8" ref-type="fig">8</xref>.<fig id="Fig8"><label>Figure 8</label><caption><p>Example of classification result visualization of Taikang cultivated land dataset.</p></caption><graphic xlink:href="41598_2024_60375_Fig8_HTML" id="MO8"/></fig></p><p id="Par66">In group I, the red box shows the segmentation of the residential area bordering with the cultivated land. PSPNet can hardly recognize the small patch residential area.UNet and DeepLabv3+&#x02009;can roughly recognize this area, but there is a rough and unsmooth border segmentation of the cultivated land.MST-DeepLabv3+&#x02009;recognizes the cultivated land and the background in this area more clearly. The yellow box, is the distinction between the boundary of the cultivated land and the road. PSPNet and DeepLabv3+&#x02009;can&#x02019;t recognize the boundary. The segmentation result of UNet is not continuous, and the middle of the road produces a discontinuity.MST-DeepLabv3+&#x02009;can recognize it accurately.</p><p id="Par67">In group II, the background category is unused land in non-residential areas. PSPNet, UNet and DeepLabv3+&#x02009;all have misclassification phenomena at the intersection between the background and cultivated land, and the contours are inaccurate. The segmentation result of MST-DeepLabv3+&#x02009;is closer to the label.</p><p id="Par68">In group III, for the segmentation between the cultivated land and the road on both sides of the river, PSPNet, UNet and DeepLabv3+&#x02009;all showed the phenomenon of adhesion. MST-DeepLabv3+&#x02009;can effectively improve this phenomenon with clear boundaries.</p><p id="Par69">In conclusion, MST-DeepLabv3+&#x02009;is able to effectively optimize the phenomena of rough boundaries, inaccurate contour prediction, and adhesion between categories that occur in other model segmentations.</p></sec><sec id="Sec17"><title>Ablation experiment and model parameter comparison</title><p id="Par70">The ablation experiment can demonstrate the changes of the model itself and the segmentation effect during each step of the improvement process. In this paper, the most commonly used MIoU metrics and model parameter size metrics are selected to illustrate the model improvement process. Since the training time is closely related to the parameter size, it is also affected by subjective factors such as the experimental platform and training environment. Therefore, this article only describes the size of the model parameters, not the training time. The results of ablation experiments based on the ISPRS dataset are shown in Table <xref rid="Tab8" ref-type="table">8</xref>. The parameter size of DeepLabv3+&#x02009;model with Xception as the backbone network is 208.7 MB. After replacing the Xception network with MobileNetV2 network, the model parameters are reduced to 22.19 MB, and the MIoU value is also reduced by 4.36% due to the impact of the lightweight network. After adding the attention mechanism SENet, the model parameters increased slightly by 0.77 MB, but the MIoU increased by 5.35%, which nicely fills the accuracy loss in the previous step. The addition of transfer learning does not change the size of the model parameters, which again significantly improves the segmentation accuracy of the model.<table-wrap id="Tab8"><label>Table 8</label><caption><p>Ablation experiment.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="2">DeepLabv3+</th><th align="left" colspan="2">Backbone</th><th align="left" rowspan="2">SENet</th><th align="left" rowspan="2">Transfer learning</th><th align="left" rowspan="2">MIoU (%)</th><th align="left" rowspan="2">Parameter amount (MB)</th></tr><tr><th align="left">Xception</th><th align="left">MobileNetV2</th></tr></thead><tbody><tr><td align="left">&#x0221a;</td><td align="left">&#x0221a;</td><td align="left"/><td align="left"/><td align="left"/><td char="." align="char">68.62</td><td char="." align="char">208.7</td></tr><tr><td align="left">&#x0221a;</td><td align="left"/><td align="left">&#x0221a;</td><td align="left"/><td align="left"/><td char="." align="char">64.26</td><td char="." align="char">22.19</td></tr><tr><td align="left">&#x0221a;</td><td align="left"/><td align="left">&#x0221a;</td><td align="left">&#x0221a;</td><td align="left"/><td char="." align="char">69.61</td><td char="." align="char">22.96</td></tr><tr><td align="left">&#x0221a;</td><td align="left"/><td align="left">&#x0221a;</td><td align="left">&#x0221a;</td><td align="left">&#x0221a;</td><td char="." align="char">82.47</td><td char="." align="char">22.96</td></tr></tbody></table></table-wrap></p><p id="Par71">The size of model parameters is the main factor of image training efficiency. The smaller the number of model parameters, the shorter the training time, which can effectively improve the model training speed. Table <xref rid="Tab8" ref-type="table">8</xref> compares the parameter size changes during model improvement through ablation experiments, and Table <xref rid="Tab9" ref-type="table">9</xref> shows the parameter comparison of the MST-DeepLabv3+&#x02009;model with other models. The parameter size of the MST-DeepLabv3+&#x02009;model is 22.96 MB, which is about 91% reduction compared to the PSPNet model and about 76% reduction compared to the UNet model. The parameters of MST-DeepLabv3+&#x02009;model are much lower than the PSPNet, UNet and DeepLabv3+&#x02009;models.<table-wrap id="Tab9"><label>Table 9</label><caption><p>Comparison of parameter sizes for different models.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Method</th><th align="left">Parameter amount (MB)</th></tr></thead><tbody><tr><td align="left">PSPNet</td><td char="." align="char">259.64</td></tr><tr><td align="left">UNet</td><td char="." align="char">94.95</td></tr><tr><td align="left">DeepLabv3+&#x02009;</td><td char="." align="char">208.70</td></tr><tr><td align="left">MST-DeepLabv3+&#x02009;</td><td char="." align="char">22.96</td></tr></tbody></table></table-wrap></p></sec></sec><sec id="Sec18"><title>Discussion</title><p id="Par72">Due to the vast scene, complicated details, and effects of illumination and imaging angle in high-resolution remote sensing images, classic semantic segmentation models frequently have issues, such as low training efficiency, inaccurate target recognition, and low accuracy. We propose the MST-DeepLabv3+&#x02009;semantic segmentation model to solve these problems. This model fully integrates the advantages of lightweight network, attention mechanism, and transfer learning to provide the best performance in processing remote sensing images.</p><p id="Par73">Firstly, the lightweight network is applied in the model to reduce the number of model parameters. Assuncao et al.<sup><xref ref-type="bibr" rid="CR46">46</xref></sup> used MobileNet as the DeepLabv3 model's backbone network for semantic segmentation of crops and weeds, which effectively increased the speed of model execution segmentation. Huang et al.<sup><xref ref-type="bibr" rid="CR47">47</xref></sup> used MobileNetV1 and MobileNetV2 instead of various models' backbone networks to reduce network training time. When the input dimension is low, the ReLU activation function used by MobileNetV1 loses more information<sup><xref ref-type="bibr" rid="CR36">36</xref></sup>, whereas MobileNetV2 uses Linear bottleneck and Inverted residuals to maximize information retention. MST-DeepLabv3+&#x02009;uses the lightweight network MobileNetV2 to replace the Xception network used for feature extraction, which greatly reduces model parameter and memory consumption, and improves model training speed. That is supported by the experimental results. When using MobileNetV2 as the backbone network, the model&#x02019;s parameter size is only 22.19&#x000a0;MB, about one-tenth the size of DeepLabv3+, effectively reducing training consumption.</p><p id="Par74">Secondly, SENet is introduced to the encoding part to distribute channel weight, so that the network starts from global information and makes up for the accuracy loss caused by the lightweight feature extraction network. At present, there are other types of attention mechanisms applied to remote sensing image segmentation. Liu et al.<sup><xref ref-type="bibr" rid="CR48">48</xref></sup> embedded DAMM (Dual Attention Mechanism Module) into the model to improve urban building detection in remote sensing images. Wang et al.<sup><xref ref-type="bibr" rid="CR49">49</xref></sup> introduced CBAM (Convolutional Block Attention Module) into the model to improve road detection performance in high-resolution remote sensing images. The DAMM contains a position attention module that mainly considers the global information of fusion features, which is similar to the function of DeepLabv3+&#x02019;s ASPP module. Although CBAM has both spatial and channel attention modules, it cannot make reasonable use of spatial information at different scales. We added the SENet module to make the model portable and effective, and the model parameter size increased from 22.19 to 22.96&#x000a0;MB. When classifying the ISPRS dataset, MIoU is increased from 64.26% to 69.61%, the accuracy is significantly improved with a small increase in computational effort.</p><p id="Par75">Finally, transfer learning is introduced into the model feature extraction network, and the pre-training model parameters are used as the initial weight parameters of the network, which can make the model segmentation effect better.</p><p id="Par76">Combined with the preceding three points, MST-DeepLabv3+&#x02009;achieves MIoU of 82.47%, 73.44%, and 90.77% on the ISPRS dataset with aspatial resolution of 9&#x000a0;cm, the GID dataset with a aspatial resolution of 1m, and the Taikang cultivated land dataset with a aspatial resolution of 2&#x000a0;m, respectively. The segmentation accuracy is improved, the whole and detailed information of the high-resolution remote sensing image is better identified, and the final model parameter size is 22.96&#x000a0;MB, significantly improving training efficiency.</p></sec><sec id="Sec19"><title>Conclusions</title><p id="Par77">This paper proposes a remote sensing image classification algorithm to address the problems of low precision and low model training efficiency in remote sensing image semantic segmentation. Replace the DeepLabv3+&#x02009;model&#x02019;s backbone network with MobileNetV2 to decrease the number of model parameters and memory occupation to speed up training; add an attention mechanism to make up for the accuracy loss brought on by the lightweight feature extraction network and improve the model's deficiency in capturing ground information; introduce the transfer learning method and use the pre-training model parameters as the network's initial weight parameters to improve the model segmentation effect. The classification results of the ISPRS dataset, GID dataset, and Taikang cultivated land dataset show that MST-DeepLabv3+&#x02009;can effectively improve segmentation accuracy and training efficiency, and its overall performance is the best among the compared models.</p><p id="Par78">Aiming at the problem of insufficient boundary information extraction that still exists in the experiment, the next work can combine the edge extraction model with the semantic segmentation model to optimize the segmentation boundary. Simultaneously, the generalization and learning migration capability of the model needs to be improved for remote sensing image segmentation with different terrains. In addition, MST-DeepLabv3+&#x02009;does not consider multispectral information, and adding spectral information may improve segmentation precision.</p></sec></body><back><fn-group><fn><p><bold>Publisher's note</bold></p><p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></fn></fn-group><notes notes-type="author-contribution"><title>Author contributions</title><p>Conceptualization, Y.W. and L.Y.; 
methodology, Y.W. and L.Y.; 
software, Y.W.; 
validation, X.L. and P.Y.; 
formal analysis, Y.W.; 
investigation, Y.W. and L.Y.; 
data curation, Y.W.; 
writing&#x02014;original draft preparation, Y.W.; 
writing&#x02014;review and editing, L.Y.; visualization, Y.W.; 
supervision, X.L. and P.Y.</p></notes><notes notes-type="funding-information"><title>Funding</title><p>This research was funded by Henan Province Science and Technology Research Project(NO: 232102110288), the National Major Project of High-Resolution Earth Observation System (NO: 80-Y50G19-9001-22/23), and the National Science and Technology Platform Construction Project (NO: 2005DKA32300).</p></notes><notes notes-type="data-availability"><title>Data availability</title><p>The processed ISPRS dataset in the current study can be downloaded from the following link: <ext-link ext-link-type="uri" xlink:href="https://www.scidb.cn/en/s/ERFnAb">https://www.scidb.cn/en/s/ERFnAb</ext-link>. The processed GID dataset in the current study can be downloaded from the following link: <ext-link ext-link-type="uri" xlink:href="https://www.scidb.cn/en/s/eaiY7f">https://www.scidb.cn/en/s/eaiY7f</ext-link>. The Taikang cultivated land dataset used in the current study is not publicly available due to its current confidential status, but is available from the corresponding author on reasonable request.</p></notes><notes id="FPar1" notes-type="COI-statement"><title>Competing interests</title><p id="Par79">The authors declare no competing interests.</p></notes><ref-list id="Bib1"><title>References</title><ref id="CR1"><label>1.</label><mixed-citation publication-type="other">Yang, Y. &#x00026; Tan, S. Application of remote sensing in the research of soil erosion. In <italic>Proceedings of the International Conference on Advances in Energy and Environmental Science (ICAEES)</italic> 807&#x02013;809. 10.4028/www.scientific.net/AMR.807-809.1658 (2013).</mixed-citation></ref><ref id="CR2"><label>2.</label><mixed-citation publication-type="other">Dong, X., Yan, B., Gan, F. &#x00026; Li, N. Progress and prospectives on engineering application of hyperspectral remote sensing for geology and mineral resources. In <italic>Proceedings of the 5th Symposium on Novel Optoelectronic Detection Technology and Application</italic> 11023. 10.1117/12.2521828 (2018).</mixed-citation></ref><ref id="CR3"><label>3.</label><mixed-citation publication-type="other">Gan, F., Mu, X. &#x00026; Xiao, C. The operational application of Chinese high-resolution satellite in the investigation of land and resources. In <italic>Proceedings of the 36th IEEE International Geoscience and Remote Sensing Symposium (IGARSS)</italic> 3754&#x02013;3757. 10.1109/igarss.2016.7729973 (2016).</mixed-citation></ref><ref id="CR4"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friedl</surname><given-names>MA</given-names></name><name><surname>Brodley</surname><given-names>CE</given-names></name></person-group><article-title>Decision tree classification of land cover from remotely sensed data</article-title><source>Remote Sens. Environ.</source><year>1997</year><volume>61</volume><fpage>399</fpage><lpage>409</lpage><pub-id pub-id-type="doi">10.1016/S0034-4257(97)00049-7</pub-id></element-citation></ref><ref id="CR5"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vapnik</surname><given-names>VN</given-names></name><name><surname>Chervonenkis</surname><given-names>A</given-names></name></person-group><article-title>A note on one class of perceptrons</article-title><source>Autom. Remote Control</source><year>1964</year><volume>25</volume><fpage>145</fpage></element-citation></ref><ref id="CR6"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gislason</surname><given-names>PO</given-names></name><name><surname>Benediktsson</surname><given-names>JA</given-names></name><name><surname>Sveinsson</surname><given-names>JR</given-names></name></person-group><article-title>Random forests for land cover classification</article-title><source>Pattern Recogn. Lett.</source><year>2006</year><volume>27</volume><fpage>294</fpage><lpage>300</lpage><pub-id pub-id-type="doi">10.1016/j.patrec.2005.08.011</pub-id></element-citation></ref><ref id="CR7"><label>7.</label><mixed-citation publication-type="other">Lafferty, J., McCallum, A. &#x00026; Pereira, F. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In <italic>Proc ICML</italic>. 10.1109/ICIP.2012.6466940 (2002).</mixed-citation></ref><ref id="CR8"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>H</given-names></name><name><surname>Gu</surname><given-names>H</given-names></name><name><surname>Han</surname><given-names>Y</given-names></name><name><surname>Yang</surname><given-names>J</given-names></name></person-group><article-title>Object-oriented classification of high-resolution remote sensing imagery based on an improved colour structure code and a support vector machine</article-title><source>Int. J. Remote Sens.</source><year>2010</year><volume>31</volume><fpage>1453</fpage><lpage>1470</lpage><pub-id pub-id-type="doi">10.1080/01431160903475266</pub-id></element-citation></ref><ref id="CR9"><label>9.</label><mixed-citation publication-type="other">Volpi, M. &#x00026; Ferrari, V. Semantic segmentation of urban scenes by learning local class interactions. In <italic>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</italic>. 10.1109/cvprw.2015.7301377 (2015).</mixed-citation></ref><ref id="CR10"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sun</surname><given-names>X</given-names></name><name><surname>Lin</surname><given-names>X</given-names></name><name><surname>Shen</surname><given-names>S</given-names></name><name><surname>Hu</surname><given-names>Z</given-names></name></person-group><article-title>High-resolution remote sensing data classification over urban areas using random forest ensemble and fully connected conditional random field</article-title><source>Isprs Int. J. Geo-Inf.</source><year>2017</year><volume>6</volume><fpage>245</fpage><pub-id pub-id-type="doi">10.3390/ijgi6080245</pub-id></element-citation></ref><ref id="CR11"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Y</given-names></name><name><surname>Shi</surname><given-names>H</given-names></name><name><surname>Zhuang</surname><given-names>Y</given-names></name><name><surname>Sang</surname><given-names>Q</given-names></name><name><surname>Chen</surname><given-names>L</given-names></name></person-group><article-title>Bidirectional grid fusion network for accurate land cover classification of high-resolution remote sensing images</article-title><source>IEEE J. Sel. Top. Appl. Earth Observ. Remote Sens.</source><year>2020</year><volume>13</volume><fpage>5508</fpage><lpage>5517</lpage><pub-id pub-id-type="doi">10.1109/jstars.2020.3023645</pub-id></element-citation></ref><ref id="CR12"><label>12.</label><mixed-citation publication-type="other">Hall, M. A. <italic>Correlation-Based Feature Selection for Machine Learning</italic> (Morgan Kaufmann Publishers Inc., 2000). <ext-link ext-link-type="uri" xlink:href="https://hdl.handle.net/10289/1024">https://hdl.handle.net/10289/1024</ext-link>.</mixed-citation></ref><ref id="CR13"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Erus</surname><given-names>G</given-names></name><name><surname>Lomenie</surname><given-names>N</given-names></name></person-group><article-title>How to involve structural modeling for cartographic object recognition tasks in high-resolution satellite images?</article-title><source>Pattern Recogn. Lett.</source><year>2010</year><volume>31</volume><fpage>1109</fpage><lpage>1119</lpage><pub-id pub-id-type="doi">10.1016/j.patrec.2010.01.013</pub-id></element-citation></ref><ref id="CR14"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Konstantinidis</surname><given-names>D</given-names></name><name><surname>Argyriou</surname><given-names>V</given-names></name><name><surname>Stathaki</surname><given-names>T</given-names></name><name><surname>Grammalidis</surname><given-names>N</given-names></name></person-group><article-title>A modular CNN-based building detector for remote sensing images</article-title><source>Comput. Netw.</source><year>2020</year><volume>168</volume><fpage>145</fpage><pub-id pub-id-type="doi">10.1016/j.comnet.2019.107034</pub-id></element-citation></ref><ref id="CR15"><label>15.</label><mixed-citation publication-type="other">Quan, J., Wu, C., Wang, H. &#x00026; Wang, Z. Scene classification of optical remote sensing images based on CNN automatic transfer. In <italic>Proceedings of the IEEE International Conference on Automation, Electronics and Electrical Engineering (AUTEEE)</italic> 110&#x02013;114. 10.1109/AUTEEE.2018.8720785 (2018).</mixed-citation></ref><ref id="CR16"><label>16.</label><mixed-citation publication-type="other">Tun, N. L., Gavrilov, A., Tun, N. M., Trieu, D. M. &#x00026; Aung, H. Remote sensing data classification using a hybrid pre-trained VGG16 CNN-SVM classifier. In <italic>Proceedings of the IEEE Conference of Russian Young Researchers in Electrical and Electronic Engineering (ElConRus)</italic>, <italic>Saint Petersburg Electrotechn Univ</italic> 2171&#x02013;2175. 10.1109/ElConRus51938.2021.9396706 (2021).</mixed-citation></ref><ref id="CR17"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lecun</surname><given-names>Y</given-names></name><name><surname>Bottou</surname><given-names>L</given-names></name></person-group><article-title>Gradient-based learning applied to document recognition</article-title><source>Proc. IEEE</source><year>1998</year><volume>86</volume><fpage>2278</fpage><lpage>2324</lpage><pub-id pub-id-type="doi">10.1109/5.726791</pub-id></element-citation></ref><ref id="CR18"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhu</surname><given-names>Q</given-names></name><name><surname>Zhao</surname><given-names>J</given-names></name><name><surname>Chen</surname><given-names>L</given-names></name><name><surname>Li</surname><given-names>Y</given-names></name></person-group><article-title>Urban contryction land extraction of the remote sensing image based on depth learning</article-title><source>Softw. Guide</source><year>2018</year><volume>17</volume><fpage>18</fpage><lpage>21</lpage></element-citation></ref><ref id="CR19"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jadhav</surname><given-names>J</given-names></name><name><surname>Singh</surname><given-names>R</given-names></name></person-group><article-title>Automatic semantic segmentation and classification of remote sensing data for agriculture</article-title><source>Math. Models Eng.</source><year>2018</year><volume>4</volume><fpage>112</fpage><lpage>137</lpage><pub-id pub-id-type="doi">10.21595/mme.2018.19840</pub-id></element-citation></ref><ref id="CR20"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kussul</surname><given-names>N</given-names></name><name><surname>Lavreniuk</surname><given-names>M</given-names></name><name><surname>Skakun</surname><given-names>S</given-names></name><name><surname>Shelestov</surname><given-names>A</given-names></name></person-group><article-title>Deep learning classification of land cover and crop types using remote sensing data</article-title><source>IEEE Geosci. Remote Sens. Lett.</source><year>2017</year><volume>14</volume><fpage>778</fpage><lpage>782</lpage><pub-id pub-id-type="doi">10.1109/lgrs.2017.2681128</pub-id></element-citation></ref><ref id="CR21"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nakandala</surname><given-names>S</given-names></name><name><surname>Nagrecha</surname><given-names>K</given-names></name><name><surname>Kumar</surname><given-names>A</given-names></name><name><surname>Papakonstantinou</surname><given-names>Y</given-names></name></person-group><article-title>Incremental and approximate computations for accelerating deep CNN inference</article-title><source>Acm Trans. Database Syst.</source><year>2020</year><volume>45</volume><fpage>42</fpage><pub-id pub-id-type="doi">10.1145/3397461</pub-id></element-citation></ref><ref id="CR22"><label>22.</label><mixed-citation publication-type="other">Long, J., Shelhamer, E. &#x00026; Darrell, T. Fully convolutional networks for semantic segmentation. In <italic>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</italic> 3431&#x02013;3440. 10.1109/cvpr.2015.7298965 (2015).</mixed-citation></ref><ref id="CR23"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fu</surname><given-names>G</given-names></name><name><surname>Liu</surname><given-names>C</given-names></name><name><surname>Zhou</surname><given-names>R</given-names></name><name><surname>Sun</surname><given-names>T</given-names></name><name><surname>Zhang</surname><given-names>Q</given-names></name></person-group><article-title>Classification for high resolution remote sensing imagery using a fully convolutional network</article-title><source>Remote Sens.</source><year>2017</year><volume>9</volume><fpage>498</fpage><pub-id pub-id-type="doi">10.3390/rs9050498</pub-id></element-citation></ref><ref id="CR24"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>G</given-names></name><etal/></person-group><article-title>Symmetrical dense-shortcut deep fully convolutional networks for semantic segmentation of very-high-resolution remote sensing images</article-title><source>IEEE J. Sel. Top. Appl Earth Observ. Remote Sens.</source><year>2018</year><volume>11</volume><fpage>1633</fpage><lpage>1644</lpage><pub-id pub-id-type="doi">10.1109/jstars.2018.2810320</pub-id></element-citation></ref><ref id="CR25"><label>25.</label><mixed-citation publication-type="other">Ronneberger, O., Fischer, P. &#x00026; Brox, T. U-Net: Convolutional networks for biomedical image segmentation. In <italic>Proceedings of the 18th International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI), vol. 9351</italic> 234&#x02013;241 10.1007/978-3-319-24574-4_28 (2015).</mixed-citation></ref><ref id="CR26"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Badrinarayanan</surname><given-names>V</given-names></name><name><surname>Kendall</surname><given-names>A</given-names></name><name><surname>Cipolla</surname><given-names>R</given-names></name></person-group><article-title>SegNet: A deep convolutional encoder-decoder architecture for image segmentation</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2015</year><pub-id pub-id-type="doi">10.1109/TPAMI.2016.2644615</pub-id></element-citation></ref><ref id="CR27"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weng</surname><given-names>L</given-names></name><etal/></person-group><article-title>Water areas segmentation from remote sensing images using a separable residual segnet network</article-title><source>Isprs Int. J. Geo-Inf.</source><year>2020</year><volume>9</volume><fpage>256</fpage><pub-id pub-id-type="doi">10.3390/ijgi9040256</pub-id></element-citation></ref><ref id="CR28"><label>28.</label><mixed-citation publication-type="other">Zhao, H., Shi, J., Qi, X., Wang, X. &#x00026; Jia, J. Pyramid scene parsing network. In <italic>Proceedings of the</italic><italic>30th IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</italic> 6230&#x02013;6239. 10.1109/cvpr.2017.660 (2017).</mixed-citation></ref><ref id="CR29"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>L-C</given-names></name><name><surname>Papandreou</surname><given-names>G</given-names></name><name><surname>Kokkinos</surname><given-names>I</given-names></name><name><surname>Murphy</surname><given-names>K</given-names></name><name><surname>Yuille</surname><given-names>A</given-names></name></person-group><article-title>Semantic image segmentation with deep convolutional nets and fully connected CRFs</article-title><source>CoRR</source><year>2014</year><pub-id pub-id-type="doi">10.48550/arXiv.1412.7062</pub-id></element-citation></ref><ref id="CR30"><label>30.</label><mixed-citation publication-type="other">Chen, L.-C., Papandreou, G., Schroff, F. &#x00026; Adam, H. Rethinking Atrous convolution for semantic image segmentation (2017).</mixed-citation></ref><ref id="CR31"><label>31.</label><mixed-citation publication-type="other">Chen, L.-C., Zhu, Y., Papandreou, G., Schroff, F. &#x00026;Adam, H. Encoder-decoder with atrous separable convolution for semantic image segmentation. In <italic>Proceedings of the 15th European Conference on Computer Vision (ECCV)</italic>, <italic>vol. 11211</italic> 833&#x02013;851. 10.1007/978-3-030-01234-2_49 (2018).</mixed-citation></ref><ref id="CR32"><label>32.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>LC</given-names></name><name><surname>Papandreou</surname><given-names>G</given-names></name><name><surname>Kokkinos</surname><given-names>I</given-names></name><name><surname>Murphy</surname><given-names>K</given-names></name><name><surname>Yuille</surname><given-names>AL</given-names></name></person-group><article-title>DeepLab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2018</year><volume>40</volume><fpage>834</fpage><lpage>848</lpage><pub-id pub-id-type="doi">10.1109/tpami.2017.2699184</pub-id><?supplied-pmid 28463186?><pub-id pub-id-type="pmid">28463186</pub-id>
</element-citation></ref><ref id="CR33"><label>33.</label><mixed-citation publication-type="other">Yu, F., Koltun, V. &#x00026; Funkhouser, T. Dilated residual networks. In <italic>Proceedings of the 30th IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</italic> 636&#x02013;644. 10.1109/cvpr.2017.75 (2017).</mixed-citation></ref><ref id="CR34"><label>34.</label><mixed-citation publication-type="other">He, K., Zhang, X., Ren, S. &#x00026; Sun, J. Spatial pyramid pooling in deep convolutional networks for visual recognition. In <italic>Proceedings of the 13th European Conference on Computer Vision (ECCV), vol. 8691</italic> 346&#x02013;361. 10.1007/978-3-319-10578-9_23 (2014).</mixed-citation></ref><ref id="CR35"><label>35.</label><mixed-citation publication-type="other">Chollet, F. Xception: Deep learning with depthwise separable convolutions. In <italic>Proceedings of the 30th IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</italic> 1800&#x02013;1807. 10.1109/cvpr.2017.195 (2017).</mixed-citation></ref><ref id="CR36"><label>36.</label><mixed-citation publication-type="other">Sandler, M., Howard, A., Zhu, M., Zhmoginov, A. &#x00026; Chen, L.-C. MobileNetV2: Inverted residuals and linear bottlenecks. In <italic>Proceedings of the 31st IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</italic> 4510&#x02013;4520. doi:10.1109/cvpr.2018.00474 (2018).</mixed-citation></ref><ref id="CR37"><label>37.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hu</surname><given-names>J</given-names></name><name><surname>Shen</surname><given-names>L</given-names></name><name><surname>Albanie</surname><given-names>S</given-names></name><name><surname>Sun</surname><given-names>G</given-names></name><name><surname>Wu</surname><given-names>E</given-names></name></person-group><article-title>Squeeze-and-excitation networks</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2020</year><volume>42</volume><fpage>2011</fpage><lpage>2023</lpage><pub-id pub-id-type="doi">10.1109/tpami.2019.2913372</pub-id><?supplied-pmid 31034408?><pub-id pub-id-type="pmid">31034408</pub-id>
</element-citation></ref><ref id="CR38"><label>38.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pan</surname><given-names>SJ</given-names></name><name><surname>Yang</surname><given-names>Q</given-names></name></person-group><article-title>A survey on transfer learning</article-title><source>IEEE Trans. Knowl. Data Eng.</source><year>2010</year><volume>22</volume><fpage>1345</fpage><lpage>1359</lpage><pub-id pub-id-type="doi">10.1109/tkde.2009.191</pub-id></element-citation></ref><ref id="CR39"><label>39.</label><mixed-citation publication-type="other">Deng, J. <italic>et al</italic>. ImageNet: A large-scale hierarchical image database. In <italic>Proceedings of the IEEE-Computer-Society Conference on Computer Vision and Pattern Recognition Workshops</italic> 248&#x02013;255. 10.1109/cvpr.2009.5206848 (2009).</mixed-citation></ref><ref id="CR40"><label>40.</label><mixed-citation publication-type="other">ISPRS. International society for photogrammetry and remote sensing. 2D Semantic Labeling Contest (2022, accessed 13 Aug 2022). <ext-link ext-link-type="uri" xlink:href="https://www.isprs.org/education/benchmarks/UrbanSemLab/semantic-labeling.aspx">https://www.isprs.org/education/benchmarks/UrbanSemLab/semantic-labeling.aspx</ext-link>.</mixed-citation></ref><ref id="CR41"><label>41.</label><mixed-citation publication-type="other">GID. Land-cover classification with high-resolution remote sensing images using transferable deep models (2022, accessed 25 Sep 2022). <ext-link ext-link-type="uri" xlink:href="https://x-ytong.github.io/project/GID.html">https://x-ytong.github.io/project/GID.html</ext-link>.</mixed-citation></ref><ref id="CR42"><label>42.</label><mixed-citation publication-type="other">Gaofen Hubei Center. (2022, accessed 20 Oct 2022). <ext-link ext-link-type="uri" xlink:href="http://datasearch.hbeos.org.cn:3000/">http://datasearch.hbeos.org.cn:3000/</ext-link>#.</mixed-citation></ref><ref id="CR43"><label>43.</label><mixed-citation publication-type="other">Bai, Z. Technical characteristics of Gaofen-1 satellite. China Aerospace, 5-9, CNKI:SUN:ZGHT.0.2013-08-002 (2022).</mixed-citation></ref><ref id="CR44"><label>44.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>H</given-names></name><name><surname>Ge</surname><given-names>X</given-names></name></person-group><article-title>Lightweight DeepLabv3 plus building extraction method from remote sensing image</article-title><source>Remote Sens. Nat. Resourc.</source><year>2022</year><volume>34</volume><fpage>128</fpage><lpage>135</lpage></element-citation></ref><ref id="CR45"><label>45.</label><mixed-citation publication-type="other">Li, T. W. &#x00026; Lee, G. C. Performance analysis of fine-tune transferred deep learning. In <italic>Proceedings of the 2021 IEEE 3rd Eurasia Conference on IOT, Communication and Engineering (ECICE)</italic> 315&#x02013;319. 10.1109/ECICE52819.2021.9645649 (2021).</mixed-citation></ref><ref id="CR46"><label>46.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Assuncao</surname><given-names>E</given-names></name><etal/></person-group><article-title>Real-time weed control application using a jetson nano edge device and a spray mechanism</article-title><source>Remote Sens.</source><year>2022</year><volume>14</volume><fpage>17</fpage><pub-id pub-id-type="doi">10.3390/rs14174217</pub-id></element-citation></ref><ref id="CR47"><label>47.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huang</surname><given-names>L</given-names></name><name><surname>Wu</surname><given-names>X</given-names></name><name><surname>Peng</surname><given-names>Q</given-names></name><name><surname>Yu</surname><given-names>X</given-names></name></person-group><article-title>Depth semantic segmentation of tobacco planting areas from unmanned aerial vehicle remote sensing images in plateau mountains</article-title><source>J. Spectrosc.</source><year>2021</year><volume>1&#x02013;14</volume><fpage>2021</fpage><pub-id pub-id-type="doi">10.1155/2021/6687799</pub-id></element-citation></ref><ref id="CR48"><label>48.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>W</given-names></name><name><surname>Shu</surname><given-names>Y</given-names></name><name><surname>Tang</surname><given-names>X</given-names></name><name><surname>Liu</surname><given-names>J</given-names></name></person-group><article-title>Remote sensing image segmentation using dual attention mechanism Deeplabv3+ algorithm</article-title><source>Trop. Geogr.</source><year>2020</year><volume>40</volume><fpage>303</fpage><lpage>313</lpage><pub-id pub-id-type="doi">10.13284/j.cnki.rddl.003229</pub-id></element-citation></ref><ref id="CR49"><label>49.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Y</given-names></name><etal/></person-group><article-title>DDU-Net: Dual-Decoder-U-Net for road extraction using high-resolution remote sensing images</article-title><source>IEEE Trans. Geosci. Remote Sens.</source><year>2022</year><volume>60</volume><fpage>12</fpage><pub-id pub-id-type="doi">10.1109/tgrs.2022.3197546</pub-id></element-citation></ref></ref-list></back></article>