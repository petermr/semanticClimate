<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd"> 
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><?properties open_access?><?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?><?DTDIdentifier.IdentifierType public?><?SourceDTD.DTDName A++V2.4.dtd?><?SourceDTD.Version 2.4?><?ConverterInfo.XSLTName springer2nlmx2.xsl?><?ConverterInfo.Version 1?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sci Rep</journal-id><journal-id journal-id-type="iso-abbrev">Sci Rep</journal-id><journal-title-group><journal-title>Scientific Reports</journal-title></journal-title-group><issn pub-type="epub">2045-2322</issn><publisher><publisher-name>Nature Publishing Group UK</publisher-name><publisher-loc>London</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmcid">10894874</article-id><article-id pub-id-type="pmid">38403711</article-id>
<article-id pub-id-type="publisher-id">54096</article-id><article-id pub-id-type="doi">10.1038/s41598-024-54096-8</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>A Siamese Swin-Unet for image change detection</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Tang</surname><given-names>Yizhuo</given-names></name><xref ref-type="aff" rid="Aff1"/></contrib><contrib contrib-type="author"><name><surname>Cao</surname><given-names>Zhengtao</given-names></name><xref ref-type="aff" rid="Aff1"/></contrib><contrib contrib-type="author"><name><surname>Guo</surname><given-names>Ningbo</given-names></name><xref ref-type="aff" rid="Aff1"/></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Jiang</surname><given-names>Mingyong</given-names></name><address><email>jiangmingyong2010@163.com</email></address><xref ref-type="aff" rid="Aff1"/></contrib><aff id="Aff1"><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/04rj1td02</institution-id><institution-id institution-id-type="GRID">grid.510280.e</institution-id><institution>Space Engineering University, </institution></institution-wrap>Beijing, China </aff></contrib-group><pub-date pub-type="epub"><day>25</day><month>2</month><year>2024</year></pub-date><pub-date pub-type="pmc-release"><day>25</day><month>2</month><year>2024</year></pub-date><pub-date pub-type="collection"><year>2024</year></pub-date><volume>14</volume><elocation-id>4577</elocation-id><history><date date-type="received"><day>5</day><month>8</month><year>2023</year></date><date date-type="accepted"><day>8</day><month>2</month><year>2024</year></date></history><permissions><copyright-statement>&#x000a9; The Author(s) 2024</copyright-statement><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p></license></permissions><abstract id="Abs1"><p id="Par1">The problem of change detection in remote sensing image processing is both difficult and important. It is extensively used in a variety of sectors, including land resource planning, monitoring and forecasting of agricultural plant health, and monitoring and assessment of natural disasters. Remote sensing images provide a large amount of long-term and fully covered data for earth environmental monitoring. A lot of progress has been made thanks to deep learning's quick development. But the majority of deep learning-based change detection techniques currently in use rely on the well-known Convolutional neural network (CNN). However, considering the locality of convolutional operation, CNN unable to master the interplay between global and distant semantic information. Some researches has employ Vision Transformer as a backbone in remote sensing field. Inspired by these researches, in this paper, we propose a network named Siam-Swin-Unet, which is a Siamesed pure Transformer with U-shape construction for remote sensing image change detection. Swin Transformer is a hierarchical vision transformer with shifted windows that can extract global feature. To learn local and global semantic feature information, the dual-time image are fed into Siam-Swin-Unet which is composed of Swin Transformer, Unet Siamesenet and two feature fusion module. Considered the Unet and Siamesenet are effective for change detection, We applied it to the model. The feature fusion module is designed for fusion of dual-time image features, and is efficient and low-compute confirmed by our experiments. Our network achieved 94.67 F1 on the CDD dataset (season varying).</p></abstract><kwd-group xml:lang="en"><title>Keywords</title><kwd>Change detection</kwd><kwd>Remote sensing</kwd><kwd>Swin transformer</kwd><kwd>Swin-Unet</kwd><kwd>Siamesenet</kwd></kwd-group><kwd-group kwd-group-type="npg-subject"><title>Subject terms</title><kwd>Electrical and electronic engineering</kwd><kwd>Computer science</kwd><kwd>Computational science</kwd><kwd>Environmental impact</kwd></kwd-group><custom-meta-group><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#x000a9; Springer Nature Limited 2024</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1"><title>Introduction</title><p id="Par2">Change detection (CD) aims to detect change area between dual-time remote sensing (RS) images. The change defined in RS image CD generally refers to the change of land cover or land use status, and detection is to achieve the purpose of identifying changes in specific areas through visual interpretation or related algorithms. CD plays a very important role in various practical applications, for instance, disaster evaluation, ecological environment detection, urban development planning and civil map revision.</p><p id="Par3">The traditional RS image CD has the problems of complex procedures, low accuracy. Besides, the traditional methods require high-quality dual-time images. In this paper, we use ai algorithms to solve this problem. Because CD can be viewed as a unique task for semantic segmentation, we use the idea of semantic segmentation to do CD. The early semantic segmentation model was implemented by removing the full connection layer from the full convolutional neural network<sup><xref ref-type="bibr" rid="CR1">1</xref></sup> and adding deconvolution to restore the original resolution, but a lot of semantic information would be lost in this process. In deep learning, the deep convolution neural network based on U-Net<sup><xref ref-type="bibr" rid="CR2">2</xref></sup> is the most classic semantic segmentation network structure. U-Net<sup><xref ref-type="bibr" rid="CR2">2</xref></sup> is composed of skip connection structure and symmetric encoder and decoder. Through a succession of convolutions and down-samples operations, the encoder extract the features of the input image. The decoder recovers the resolution of the image through up-sampling and convolution, and the skip connection structure integrates the features of each layer in the process of down-sampling which alleviates the loss of spatial information. This technical path led several algorithms, including 3D U-Net<sup><xref ref-type="bibr" rid="CR3">3</xref></sup>,Res-UNet<sup><xref ref-type="bibr" rid="CR4">4</xref></sup>, U-Net&#x02009;+&#x02009;&#x02009;+&#x02009;<sup><xref ref-type="bibr" rid="CR5">5</xref></sup> and UNET3&#x02009;+&#x02009;<sup><xref ref-type="bibr" rid="CR6">6</xref></sup>.Those algorithms developed for various semantic segmentation and CD tasks, and they are effective. So we apply U-net<sup><xref ref-type="bibr" rid="CR2">2</xref></sup> as part of our module.</p><p id="Par4">The locality of convolution operations makes it challenging for models based on convolution operations to learn global semantic information, even though CNN-based models have produced good results, which makes such methods unable to completely&#x000a0;meet the accuracy requirements of semantic segmentation and CD. Swin Transformer<sup><xref ref-type="bibr" rid="CR7">7</xref></sup> applies the Transformer<sup><xref ref-type="bibr" rid="CR8">8</xref></sup> structure that performs well in natural language processing(NLP) to the field of computer vision. Swin-Unet<sup><xref ref-type="bibr" rid="CR9">9</xref></sup> based on Swin Transformer block is a network look like U-shaped based solely on Swin Transformer<sup><xref ref-type="bibr" rid="CR7">7</xref></sup>, with encoder, decoder, bottleneck, and skip connection. Like U-Net<sup><xref ref-type="bibr" rid="CR2">2</xref></sup> The Swin Unet structure is an ideal backbone for segmentation of RS images with only a few spatial information, and the self-attention structure's global feature extraction capabilities can also extract significant features from RS images.</p><p id="Par5">Siamesenet<sup><xref ref-type="bibr" rid="CR10">10</xref></sup> is designed to measure how similar two inputs are.To conduct end-to-end detection, researchers develop a variety of fully convolutional networks in<sup><xref ref-type="bibr" rid="CR11">11</xref>&#x02013;<xref ref-type="bibr" rid="CR13">13</xref></sup> Recurrent neural networks and CNN are used in<sup><xref ref-type="bibr" rid="CR14">14</xref>,<xref ref-type="bibr" rid="CR15">15</xref></sup> to extract characteristics from multi-temporal pictures. For CD with multisource VHR pictures, convolutional multiple-layer recurrent neural networks are also suggested<sup><xref ref-type="bibr" rid="CR16">16</xref></sup>. These networks either use a two-stream structure to learn picture characteristics or combine two images into a single multi-channel input to do so. But in our model, the two pictures are passed through the network in turn with the same weights in the lowest layers, using a siamese architecture. The technique of learning common characteristics through shared and wholly same weights is fair because the two photographs were captured at separate times while still being in the same location. For CD, a siamese convolutional network is suggested in<sup><xref ref-type="bibr" rid="CR17">17</xref></sup>. The model in<sup><xref ref-type="bibr" rid="CR17">17</xref></sup> utilizes a straightforward Euclidean distance-based thresholding segmentation independent from the model while combining the information collected by the siamese CNN. For better information fusion in our approach, deeper modules are designed.</p><p id="Par6">Combining Swin Transformer<sup><xref ref-type="bibr" rid="CR7">7</xref></sup>,U-Net<sup><xref ref-type="bibr" rid="CR2">2</xref></sup> and Siamesnet<sup><xref ref-type="bibr" rid="CR10">10</xref></sup>, we designed a special network for RS image CD: Siam-Swin-Unet. Siam-Swin-Unet is made up of Siamesenet<sup><xref ref-type="bibr" rid="CR10">10</xref></sup>, encoder, decoder, skip connection structure and feature fusion module. The encoder and decoder are both built based on the Swin Transformer block. The dual-time RS images are respectively processed by the Swin Transformer encoder with the same weights to extract features of dual-time images. The two feature maps are fused through the feature fusion module. The fused features are up-sampled by the Swin Transformer decoder with shared weights, and multi-scale features from the encoder are fused through the skip connection structure to perform CD task. Finally, the RS dual-time image features with resolution restored by the siamesenet are multiplied to ensure that the network uses the information extracted by the two siamesenet equally. Specifically, We can sum up our contributions as follows:<list list-type="bullet"><list-item><p id="Par7">Combining Siamesenet<sup><xref ref-type="bibr" rid="CR10">10</xref></sup> with the improved Swin-Unet, and applying it to RS image CD.</p></list-item><list-item><p id="Par8">The fusion features are obtained by adding the dual-time image features instead of using convolution, which reduce the model parameter.</p></list-item><list-item><p id="Par9">Dual-time image feature are up-sampled separately after feature fusion module1 with the two siamesenet to protect feature fusion.</p></list-item><list-item><p id="Par10">The impact of different Swin Transformer window sizes on CDD<sup><xref ref-type="bibr" rid="CR18">18</xref></sup> dataset CD performance is studied.</p></list-item></list></p></sec><sec id="Sec2"><title>Methods</title><p id="Par11">In this section, we will introduce the Siam-Swin-Unet model, which is integrated by Siamesenet<sup><xref ref-type="bibr" rid="CR10">10</xref></sup>, Swin Transformer<sup><xref ref-type="bibr" rid="CR7">7</xref></sup>, U-Net<sup><xref ref-type="bibr" rid="CR2">2</xref></sup> and feature fusion modules. First, we'll describe the overall design of Siam-Swin-Unet, and then we will introduce Swin Transformer block, Siamesenet<sup><xref ref-type="bibr" rid="CR10">10</xref></sup>, Patch Merging, Patch Expanding, feature fusion modules, and skip connection.</p><sec id="Sec3"><title>Overall</title><p id="Par12">An overview of the model of Siame-Swin-Unet is displayed in Fig. <xref rid="Fig1" ref-type="fig">1</xref>. Siame-Swin-Unet is made up of siamesenet<sup><xref ref-type="bibr" rid="CR10">10</xref></sup>, encoder, decoder, skip connection, feature fusion module 1 and feature fusion module 2. Among them, the basic building blocks of both encoder and decoder are Swin Transformer blocks, and the weight values are shared between the siamesenet. Dual-time images (W&#x02009;&#x000d7;&#x02009;H&#x02009;&#x000d7;&#x02009;3) are inputted in the two siamese nets. The patch partition segments the input image into patches without overlapping to a size of 4&#x02009;&#x000d7;&#x02009;4. The number of channels through which the dual-time RS images were operated became 4&#x02009;&#x000d7;&#x02009;4&#x02009;&#x000d7;&#x02009;3&#x02009;=&#x02009;48. Moreover, the linear embedding layer maps the image features to a fixed number of channels (C). The purpose of Swin Transformer block is to learn the con-textual semantic information of input image and improve the model's global modeling ability for the image. The size does not change when the image goes through a Swin Transformer block. Patch merging down-samples images while increasing the quantity of feature channels of the model. Feature Fusion Module 1 uses summation to fuse dual-time image features. The encoder and decoder are symmetrical to each other, except that Patch Expanding, as opposed to Patch Merging, up-samples the feature to improve the resolution of the image. The skip connection is designed to utilize more spatial and details information from down-sampling. Furthermore, The skip connection operation can solve the problem of vanishing gradient. The last linear projection corresponds to the patch partition, which restores the image size to (W&#x02009;&#x000d7;&#x02009;H) by sampling the feature map up four times. Finally, the number of channels of the output image can be changed by a convolution operation so that the output of the model is (W&#x02009;&#x000d7;&#x02009;H&#x02009;&#x000d7;&#x02009;N). Thus, the two- or multi-classification tasks for RS image CD can be implemented. Finally, feature fusion module 2 uses multiplication to fuse features from two Siamesenets, which improves the model's effectiveness. Multiplication operations can combine multiple features or factors, allowing the model to make better use of multidimensional data. In many cases, there are certain associations between multiple features or factors in the data, and these associations can be taken into account through multiplication operations, so as to improve the prediction accuracy of the model.<fig id="Fig1"><label>Figure 1</label><caption><p>The architecture of the proposed Siam-Swin-Unet.</p></caption><graphic xlink:href="41598_2024_54096_Fig1_HTML" id="MO1"/></fig></p><p id="Par13">In Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>, T1 is the input image of the first moment, T2 is the input image of the second moment, the change map represents the area that changes or does not change between the T1 and T2 images. The white represents the changing area, and the black represents the unchanged area. Encoder is used to extract the features of the image, and the decoder is used to restore the resolution of the image. Patch Partion and Embedding are used to divide an image into small pieces and map them to a fixed dimension, which is equivalent to a convolution operation. Siamesenet, Swin Transformer block, Patch Merging, skip connection,conv2D and patch expanding will be introduced later.</p></sec><sec id="Sec4"><title>Siamesenet</title><p id="Par14">Siamesenet<sup><xref ref-type="bibr" rid="CR10">10</xref></sup> has a siamesed feature extraction network with shared and wholly same weights, which can extract the features of dual-time images separately and help the neural network generate Change Map. Currently, siamesenet<sup><xref ref-type="bibr" rid="CR10">10</xref></sup> has been used for CD of dual-time RS images. In this model, weights are shared among Siamesenets, which can extract features from dual-time RS images. In Siamesenet<sup><xref ref-type="bibr" rid="CR10">10</xref></sup> we conduct down-sample, up-sample, and finally the size of the feature map was restored to the same size of input. The idea of Siamesenet<sup><xref ref-type="bibr" rid="CR10">10</xref></sup> is illustrated in Fig. <xref rid="Fig2" ref-type="fig">2</xref>.<fig id="Fig2"><label>Figure 2</label><caption><p>The idea of Siamesenet.</p></caption><graphic xlink:href="41598_2024_54096_Fig2_HTML" id="MO2"/></fig></p></sec><sec id="Sec5"><title>Swin transformer block</title><p id="Par15">The swin transformer block is consisted of several cells in Fig. <xref rid="Fig3" ref-type="fig">3</xref>.LN is a layer normalization operation. MLP is a Multi layer Perceptron. W-MSA and SW-MSA stand for window multi-head attention and shift window multi-head attention, respectively. To learn about context and semantics, The Swin transformer block compute the attention score in W-MSA and SW-MSA. To compute attention score in a small window can reduce computational complexity compared to the computing in the whole picture. And then, the shift window operation helps the network learn the attention information between the adjacent windows. The construction of the swin transformer block is displayed in Fig. <xref rid="Fig3" ref-type="fig">3</xref>.The calculation of attention can be explained in the following formula<fig id="Fig3"><label>Figure 3</label><caption><p>Swin transformer block.</p></caption><graphic xlink:href="41598_2024_54096_Fig3_HTML" id="MO3"/></fig><disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\text{Attention}}({\text{Q,K,V}})={\text{SoftMax}}(\frac{{\text{Q}}\times {{\text{K}}}^{{\mathrm{T}}}}{\sqrt{{\text{d}}}}+{\mathrm{B}})\times {V}^{T}$$\end{document}</tex-math><mml:math id="M2" display="block"><mml:mrow><mml:mtext>Attention</mml:mtext><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mtext>Q,K,V</mml:mtext><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mtext>SoftMax</mml:mtext><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mfrac><mml:mrow><mml:mtext>Q</mml:mtext><mml:mo>&#x000d7;</mml:mo><mml:msup><mml:mrow><mml:mtext>K</mml:mtext></mml:mrow><mml:mi mathvariant="normal">T</mml:mi></mml:msup></mml:mrow><mml:msqrt><mml:mtext>d</mml:mtext></mml:msqrt></mml:mfrac><mml:mo>+</mml:mo><mml:mi mathvariant="normal">B</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>&#x000d7;</mml:mo><mml:msup><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:math><graphic xlink:href="41598_2024_54096_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula></p><p id="Par16">The Q, K, V Respectively are query, key and value array. B are Relative position offset. The mechanism of the shift window is illustrate in Fig. <xref rid="Fig4" ref-type="fig">4</xref> (Shift window is just a thought, it can be used as a square window or a free form rectangle. But our experiments were based on rectangular windows.).<fig id="Fig4"><label>Figure 4</label><caption><p>Mechanism of shift window.</p></caption><graphic xlink:href="41598_2024_54096_Fig4_HTML" id="MO4"/></fig></p></sec><sec id="Sec6"><title>Patch merging</title><p id="Par17">This module separates the input feature into 4 pieces, which are then concatenated. Such processing will cause a 2&#x02009;&#x000d7;&#x02009;downsampling of the feature resolution. Additionally, as the concatenation procedure causes the feature dimension to increase to 4&#x02009;&#x000d7;&#x02009;as much as the original, a linear layer is applied to the concatenated features in order to bring the feature dimension back to 2&#x02009;&#x000d7;&#x02009;as much as the original.The idea of the patch merging is displayed in Fig.&#x000a0;<xref rid="Fig5" ref-type="fig">5</xref><fig id="Fig5"><label>Figure 5</label><caption><p>Mechanism of patch merging.</p></caption><graphic xlink:href="41598_2024_54096_Fig5_HTML" id="MO5"/></fig></p></sec><sec id="Sec7"><title>Skip connection</title><p id="Par18">The multiresolution features come from the encoder are combined with the up-sampled data by the skip-connections, much like the U-Net<sup><xref ref-type="bibr" rid="CR2">2</xref></sup>. To lessen the loss of spatial information due to downsampling, we combine the superficial features with the profound features. The dimension of the features after concatenation operation is kept constant with dimension of up-sampled features after a linear layer.</p></sec><sec id="Sec8"><title>Patch expanding</title><p id="Par19">As an illustration, consider the first patch expanding layer. Prior to upsampling, The input features (W/16&#x02009;&#x000d7;&#x02009;H/16&#x02009;&#x000d7;&#x02009;C4) are given a linear layer to apply, which doubles the original feature dimension to (W/16&#x02009;&#x000d7;&#x02009;H/16&#x02009;&#x000d7;&#x02009;C8). Then, we apply the rearrange operation to lower the feature dimension to one-fourth of the input feature and extend the resolution of the input features to two times, which means from [(W/16&#x02009;&#x000d7;&#x02009;H/16&#x02009;&#x000d7;&#x02009;C8) to (W/8&#x02009;&#x000d7;&#x02009;H/8&#x02009;&#x000d7;&#x02009;C2)].</p></sec><sec id="Sec9"><title>Conv2D</title><p id="Par20">The purpose of this module is to study a general network capable of performing tasks of two- and multi-classification change monitoring framework. Conv2D therefore converts the number of the channel of feature map to N. N represents the number of classifications.</p></sec><sec id="Sec10"><title>Ferture fusion module 1</title><p id="Par21">The function of feature fusion module 1 is to fuse the feature maps of two dual-time RS images after down-sampling, and learn the semantic information that has changed in the two features. Conv3D can be used to fuse the two features, but it will make training more difficult. Consider that dual-time RS images are equally important. Therefore, this paper tries to fuse the dual-time RS image features with addition operation, which is equivalent to Conv3D with the same weight. It greatly reduces the computational complexity and training difficulty.</p></sec><sec id="Sec11"><title>Ferture fusion module 2</title><p id="Par22">Feature Fusion Module 2 fuses two up-sampled feature maps, making full use of each layer of semantic information of the dual-time image, and further improve the network's ability to express changing features. Considering the need to improve the network's ability, we use multiplication for feature fusion. Multiplication can enhance the nonlinear representation of the model. Nonlinearity is an important feature in many datasets, and multiplication operations can introduce nonlinear factors to enable models to better handle nonlinear problems.</p></sec></sec><sec id="Sec12"><title>Dataset</title><p id="Par23">A dataset known as CDD<sup><xref ref-type="bibr" rid="CR18">18</xref></sup>, one of the most used datasets in RS CD, was used to test this network. The CDD<sup><xref ref-type="bibr" rid="CR18">18</xref></sup> dataset comes from Google Earth images with a resolution of 3&#x02013;100&#x000a0;cm and a size of 256&#x02009;&#x000d7;&#x02009;256 pixels. The dataset has 10,000 groups of training sets, 3000 groups of test sets and 3,000 groups of verification sets. There are various types of surface changes, including buildings, cars, land, roads and warehouses. The dataset has a long time span and contains data with large or small changes. The seasonal difference and illumination difference of RS images are considered, which can effectively detect the performance of Siam-Swin-Unet in detecting changes in RS images. The example of this dataset is illustrate in Fig.&#x000a0;<xref rid="Fig6" ref-type="fig">6</xref>.The A is time1 image and B is time2 image, C is the label of change area. It is dichotomous class dataset, so the label is a binary image. The label help the network learn the weight.<fig id="Fig6"><label>Figure 6</label><caption><p>The example of the CDD dataset.</p></caption><graphic xlink:href="41598_2024_54096_Fig6_HTML" id="MO6"/></fig></p></sec><sec id="Sec13"><title>Experimental details</title><p id="Par24">Siam-Swin-Unet is implemented based on python and pytorch. The GPU used in the test is NVIDIA TITAN X, and the memory is 24G. The super parameters of the training of the network are as follows: the number of model training epoch is 200, and the global optimal model is selected. The optimizer is Adam, and the initial learning rate is 1&#x02009;&#x000d7;&#x02009;10^(&#x02212;4). Referring to the existing research, the loss function is a mixture of Jaccard Loss, Focal Loss,Dice Loss and Edge Loss. Among them, because the sample of CD task is extremely uneven, Diceloss can mitigate the adverse impact on backpropagation caused by the imbalance of CD sample, making the training unstable. Focal Loss can increase the weight of difficult to classify samples, making the model training more effective. Jaccard Loss is a kind of cross merger ratio loss, which can improve the IOU index of the model. Edge Loss can improve the edge details and further improve the accuracy of CD.<disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$FocalLoss(p)=-a(1-p{)}^{\lambda }\mathit{log}(p)$$\end{document}</tex-math><mml:math id="M4" display="block"><mml:mrow><mml:mi>F</mml:mi><mml:mi>o</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>L</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>p</mml:mi><mml:msup><mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>&#x003bb;</mml:mi></mml:msup><mml:mi mathvariant="italic">log</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math><graphic xlink:href="41598_2024_54096_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\text{DiceLoss}}=\left.1-\frac{2|{\text{X}}\cap {\text{Y}}|}{|{\text{X}}|+|{\text{Y}}|}\right|$$\end{document}</tex-math><mml:math id="M6" display="block"><mml:mrow><mml:mtext>DiceLoss</mml:mtext><mml:mo>=</mml:mo><mml:mfenced close="|"><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo stretchy="false">|</mml:mo><mml:mtext>X</mml:mtext><mml:mo>&#x02229;</mml:mo><mml:mtext>Y</mml:mtext><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mtext>X</mml:mtext><mml:mo stretchy="false">|</mml:mo><mml:mo>+</mml:mo><mml:mo stretchy="false">|</mml:mo><mml:mtext>Y</mml:mtext><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mfrac></mml:mfenced></mml:mrow></mml:math><graphic xlink:href="41598_2024_54096_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ4"><label>4</label><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathrm{Jaccard Loss}=1-\frac{|{\text{X}}\cap {\text{Y}}|}{|{\text{X}}|+|{\text{Y}}|-|{\text{X}}\cap {\text{Y}}|}$$\end{document}</tex-math><mml:math id="M8" display="block"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Jaccard</mml:mi><mml:mi mathvariant="normal">Loss</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mfrac><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mtext>X</mml:mtext><mml:mo>&#x02229;</mml:mo><mml:mtext>Y</mml:mtext><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mtext>X</mml:mtext><mml:mo stretchy="false">|</mml:mo><mml:mo>+</mml:mo><mml:mo stretchy="false">|</mml:mo><mml:mtext>Y</mml:mtext><mml:mo stretchy="false">|</mml:mo><mml:mo>-</mml:mo><mml:mo stretchy="false">|</mml:mo><mml:mtext>X</mml:mtext><mml:mo>&#x02229;</mml:mo><mml:mtext>Y</mml:mtext><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:math><graphic xlink:href="41598_2024_54096_Article_Equ4.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ5"><label>5</label><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\text{Edge Loss}=\frac{{\sum }_{{\text{x}}=1}^{{\text{W}}}\sum_{{\text{y}}=1}^{{\text{H}}}{{\text{E}}}_{{\text{i}},{\text{j}}}.(|{{\text{Y}}}_{{\text{i}},{\text{j}}}|-|{{\text{X}}}_{{\text{i}},{\text{j}}}|)}{{\text{W}}\times {\text{H}}}$$\end{document}</tex-math><mml:math id="M10" display="block"><mml:mrow><mml:mtext>Edge Loss</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mtext>x</mml:mtext><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mtext>W</mml:mtext></mml:msubsup><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mtext>y</mml:mtext><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mtext>H</mml:mtext></mml:msubsup><mml:msub><mml:mtext>E</mml:mtext><mml:mrow><mml:mtext>i</mml:mtext><mml:mo>,</mml:mo><mml:mtext>j</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>.</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mtext>Y</mml:mtext><mml:mrow><mml:mtext>i</mml:mtext><mml:mo>,</mml:mo><mml:mtext>j</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mo>-</mml:mo><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mtext>X</mml:mtext><mml:mrow><mml:mtext>i</mml:mtext><mml:mo>,</mml:mo><mml:mtext>j</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mtext>W</mml:mtext><mml:mo>&#x000d7;</mml:mo><mml:mtext>H</mml:mtext></mml:mrow></mml:mfrac></mml:mrow></mml:math><graphic xlink:href="41598_2024_54096_Article_Equ5.gif" position="anchor"/></alternatives></disp-formula></p><p id="Par25">In order to test the performance of the model, this paper uses confusion matrix as the evaluation method of the model performance. The evaluation index is F1. The higher the F1 value, the better the model performance. Formula of F1 and confusion matrix is as follows:<disp-formula id="Equ6"><label>6</label><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{ccc}confusion \,\, matreix&#x00026; predict1&#x00026; predict0\\ lable=1&#x00026; TP&#x00026; FN\\ lable=0&#x00026; FP&#x00026; TP\end{array}$$\end{document}</tex-math><mml:math id="M12" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>f</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mspace width="0.166667em"/><mml:mspace width="0.166667em"/><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mn>0</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mrow/><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>b</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mi mathvariant="italic">TP</mml:mi></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mi mathvariant="italic">FN</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mrow/><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>b</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mi mathvariant="italic">FP</mml:mi></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mi mathvariant="italic">TP</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_54096_Article_Equ6.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ7"><label>7</label><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$F1=\frac{2TP}{2TP+FP+FN}$$\end{document}</tex-math><mml:math id="M14" display="block"><mml:mrow><mml:mi>F</mml:mi><mml:mn>1</mml:mn><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math><graphic xlink:href="41598_2024_54096_Article_Equ7.gif" position="anchor"/></alternatives></disp-formula></p><p id="Par26">TP is True Positive, FP is False Positive and FN represent False Negative.</p></sec><sec id="Sec14"><title>Experiment results on CDD</title><p id="Par27">A comparison of the performance of some models is shown in Table <xref rid="Tab1" ref-type="table">1</xref>.The experimental results show that, in the CDD<sup><xref ref-type="bibr" rid="CR18">18</xref></sup> dataset, The F1 index of the model proposed in this paper is 94.67, which is superior to each comparison model. This proves the effectiveness of this model for CD. Through the Fig.&#x000a0;<xref rid="Fig7" ref-type="fig">7</xref>, we can observed the output of Siam-Swin-Unet is closer to the label. To be more specific, The Siam-Swin-Unet is better at edge and detail detection and have Stronger capabilities to detect the change target.<table-wrap id="Tab1"><label>Table 1</label><caption><p>The F1 score of some networks.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Model</th><th align="left">F1</th></tr></thead><tbody><tr><td align="left">CNnet</td><td char="." align="char">0.822</td></tr><tr><td align="left">FC-EF</td><td char="." align="char">0.596</td></tr><tr><td align="left">FC-Siam-Diff</td><td char="." align="char">0.691</td></tr><tr><td align="left">FC-Siam-conc</td><td char="." align="char">0.687</td></tr><tr><td align="left">BiDateNet</td><td char="." align="char">0.898</td></tr><tr><td align="left">DSANet(VGG16)</td><td char="." align="char">0.919</td></tr><tr><td align="left">DSANet(ResNet50)</td><td char="." align="char">0.927</td></tr><tr><td align="left">Siame-Swin-Unet</td><td char="." align="char">0.9467</td></tr></tbody></table></table-wrap><fig id="Fig7"><label>Figure 7</label><caption><p>Results of several CD techniques on the CDD\* MERGEFORMAT<sup><xref ref-type="bibr" rid="CR18">18</xref></sup> dataset are visually compared: (<bold>a</bold>) The time1 image, (<bold>b</bold>) The time 2 image, (<bold>c</bold>) The label of the change area, (<bold>d</bold>) CDnet, (<bold>e</bold>) FC-EF,(<bold>f</bold>) FC-Siam-Diff, (<bold>g</bold>) FC-Siam- Con, (<bold>h</bold>) BiDateNet, (<bold>i</bold>) DASNet(VGG16), (<bold>j</bold>) DASN,(<bold>k</bold>) Siam-Swin-Unet.</p></caption><graphic xlink:href="41598_2024_54096_Fig7_HTML" id="MO7"/></fig></p></sec><sec id="Sec15"><title>Ablation study</title><p id="Par28">We carried out ablation study to investigate the impact of various parameters on the model performance. Specifically, the impact of Window Size, Feature Fusion Module 1 on model performance will be disscussed below.</p><sec id="Sec16"><title>Effect of window size</title><p id="Par29">Swin Transformer blocks with different window sizes are applied to this model. We compare the effects of Window Size 4 and Window Size 8 on the model. In the Table <xref rid="Tab2" ref-type="table">2</xref>, we can find that when the window is small, the CD performance is better, which is related to the size of the image in the dataset.<table-wrap id="Tab2"><label>Table 2</label><caption><p>The effect&#x000a0;of windowsize.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Window size</th><th align="left">Layers</th><th align="left">Method</th><th align="left">F1</th></tr></thead><tbody><tr><td align="left">4</td><td align="left">3</td><td align="left">Add</td><td char="." align="char">0.9467</td></tr><tr><td align="left">8</td><td align="left">3</td><td align="left">Add</td><td char="." align="char">0.9281</td></tr><tr><td align="left">4</td><td align="left">3</td><td align="left">Conv3D</td><td char="." align="char">0.9455</td></tr><tr><td align="left">8</td><td align="left">3</td><td align="left">Conv3D</td><td char="." align="char">0.9258</td></tr></tbody></table></table-wrap></p></sec><sec id="Sec17"><title>Effect of feature fusion module</title><p id="Par30">Considering that the information contribution of dual-time RS images in CD should be the same, and the use of convolution layers will increase the training difficulty and computational complexity, for feature fusion module 1, we respectively use addition and Conv3D to fuse the features of dual-time RS images for comparison. In the Table <xref rid="Tab3" ref-type="table">3</xref>, we can find that the simplest addition operation is directly used, and the effect of the model is better than that of a convolution operation.<table-wrap id="Tab3"><label>Table 3</label><caption><p>The effect of feature fusion module.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Method</th><th align="left">Layers</th><th align="left">Window size</th><th align="left">F1</th></tr></thead><tbody><tr><td align="left">Conv3D</td><td align="left">3</td><td align="left">4</td><td char="." align="char">0.9467</td></tr><tr><td align="left">Add</td><td align="left">3</td><td align="left">4</td><td char="." align="char">0.9281</td></tr><tr><td align="left">Conv3D</td><td align="left">3</td><td align="left">8</td><td char="." align="char">0.9455</td></tr><tr><td align="left">Add</td><td align="left">3</td><td align="left">8</td><td char="." align="char">0.9258</td></tr></tbody></table></table-wrap></p></sec></sec><sec id="Sec18"><title>Conclusion</title><p id="Par31">In this article, we propose Siam-Swin-Unet that can perform dual-time RS image classification and multi classification CD tasks. The experimental results show that the use of additive operation to fuse the features of dual-time RS images instead of traditional convolution operations can retain the global semantic information of the features, reduce the model parameters without affecting the effect of the model. In addition, Though Siame-Swin-Unet does not perform any image enhancement operations on the dataset, the performance is very good, we get 94.67 points measured by F1 metrics. Next, we plan to use a feature fusion module that is more learnable and does not lose global semantic information for feature fusion. And try to add appropriate image enhancement module to further improve the prediction accuracy of the model.</p></sec></body><back><fn-group><fn><p><bold>Publisher's note</bold></p><p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></fn></fn-group><notes notes-type="author-contribution"><title>Author contributions</title><p>Conceptualization, Y.T and M.J.; methodology, Y.T and Z.C.; software, Y.T and Z.C; validation, Y.T., Z.C. and N.G.; formal analysis, Y.Y.; investigation, Y.T.; resources, M.J.; data curation, M.J; writing&#x02014;original draft preparation, Y.T.; writing&#x02014;review and editing, Y.T.; supervision, M.J.; project administration, M.J.; All authors have read and agreed to the published version of the manuscript.</p></notes><notes notes-type="data-availability"><title>Data availability</title><p>The datasets analysed during the current study are available in <ext-link ext-link-type="uri" xlink:href="https://gitlab.citius.usc.es/hiperespectral/ChangeDetectionDataset">https://gitlab.citius.usc.es/hiperespectral/ChangeDetectionDataset</ext-link>.</p></notes><notes id="FPar1" notes-type="COI-statement"><title>Competing interests</title><p id="Par32">The authors declare no competing interests.</p></notes><ref-list id="Bib1"><title>References</title><ref id="CR1"><label>1.</label><mixed-citation publication-type="other">Bai, F., Marques, M., Gibson, S. <italic>Cystoid macular edema segmentation of optical coherence tomography images using fullyconvolutional neural networks and fully connected crfs</italic>. arXiv 2017, <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1709.05324">arXiv:1709.05324</ext-link>.</mixed-citation></ref><ref id="CR2"><label>2.</label><mixed-citation publication-type="other">Ronneberger, O., Fischer, P., Brox, T. U-net: Convolutional networks forbiomedical image segmentation. In <italic>Medical Image Computing and Computer-Assisted Intervention (MICCAI)</italic>, ser. LNCS, Vol. 9351. 234&#x02013;241. (Springer, 2015).</mixed-citation></ref><ref id="CR3"><label>3.</label><mixed-citation publication-type="other">C&#x000b8;i&#x000b8;cek, O., Abdulkadir, A., Lienkamp, S., Brox, T., Ronneberger, O. 3d u-net:Learning dense volumetric segmentation from sparse annotation. In <italic>Medical ImageComputing and Computer-Assisted Intervention</italic> (MICCAI), ser. LNCS, Vol. 9901, 424&#x02013;432 (Springer, Oct 2016).</mixed-citation></ref><ref id="CR4"><label>4.</label><mixed-citation publication-type="other">Xiao, X., Lian, S., Luo, Z., Li, S. Weighted res-unet for high-quality retina vesselsegmentation. In <italic>2018 9th International Conference on Information Technology in Medicine and Education</italic> (ITME), 327&#x02013;331, (2018).</mixed-citation></ref><ref id="CR5"><label>5.</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Zhou</surname><given-names>Z</given-names></name><name><surname>Rahman Siddiquee</surname><given-names>M</given-names></name><name><surname>Tajbakhsh</surname><given-names>N</given-names></name><name><surname>Liang</surname><given-names>J</given-names></name></person-group><source>Unet++: A nestedu-net architecture for medical image segmentation</source><year>2018</year><publisher-name>Springer Verlag</publisher-name><fpage>3</fpage><lpage>11</lpage></element-citation></ref><ref id="CR6"><label>6.</label><mixed-citation publication-type="other">Huang, H., Lin, L., Tong, R., Hu, H., Zhang, Q., Iwamoto, Y., Han, X., Chen, Y.-W., Wu, J. <italic>Unet 3+: A full-scale connected unet for medical image segmentation</italic> (2020).</mixed-citation></ref><ref id="CR7"><label>7.</label><mixed-citation publication-type="other">Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B. <italic>Swintransformer: Hierarchical vision transformer using shifted windows</italic>, CoRR, vol.abs/2103.14030, (2021).</mixed-citation></ref><ref id="CR8"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vaswani</surname><given-names>A</given-names></name><name><surname>Shazeer</surname><given-names>N</given-names></name><name><surname>Parmar</surname><given-names>N</given-names></name><name><surname>Uszko-reit</surname><given-names>J</given-names></name><name><surname>Jones</surname><given-names>L</given-names></name><name><surname>Gomez</surname><given-names>AN</given-names></name><name><surname>Kaiser</surname><given-names>L</given-names></name><name><surname>Polosukhin</surname><given-names>I</given-names></name></person-group><article-title>Attention is all you need</article-title><source>Adv NeuralInf Process Syst</source><year>2017</year><volume>4</volume><fpage>5998</fpage><lpage>6008</lpage></element-citation></ref><ref id="CR9"><label>9.</label><mixed-citation publication-type="other">Cao, H. et al. <italic>Swin-unet: Unet-like pure transformer for medical imagesegmentation</italic> (2021). <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2105.05537">arXiv:2105.05537</ext-link>.</mixed-citation></ref><ref id="CR10"><label>10.</label><mixed-citation publication-type="other">Daudt, R. C., Le Saux, B., Boulch, A. <italic>Fully convolutional siamese networks for change detection (</italic>2018).</mixed-citation></ref><ref id="CR11"><label>11.</label><mixed-citation publication-type="other">Lei, T., Zhang, Q., Xue, D., Chen, T., Meng, H., Nandi, A.K. End-to-end change detection using a symmetric fully convolutional network for landslide mapping. In <italic>Proceedings of the ICASSP 2019&#x02013;2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Brighton, UK</italic>, 3027&#x02013;3031 (2019).</mixed-citation></ref><ref id="CR12"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>X</given-names></name><name><surname>Yuan</surname><given-names>Z</given-names></name><name><surname>Wang</surname><given-names>Q</given-names></name></person-group><article-title>Unsupervised deep noise modeling for hyperspectral image change detection</article-title><source>Remote Sens.</source><year>2019</year><volume>11</volume><fpage>258</fpage><pub-id pub-id-type="doi">10.3390/rs11030258</pub-id></element-citation></ref><ref id="CR13"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peng</surname><given-names>D</given-names></name><name><surname>Zhang</surname><given-names>Y</given-names></name><name><surname>Guan</surname><given-names>H</given-names></name></person-group><article-title>End-to-end change detection for high resolution satellite images using improved UNet++</article-title><source>Remote Sens.</source><year>2019</year><volume>11</volume><fpage>1382</fpage><pub-id pub-id-type="doi">10.3390/rs11111382</pub-id></element-citation></ref><ref id="CR14"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mou</surname><given-names>L</given-names></name><name><surname>Bruzzone</surname><given-names>L</given-names></name><name><surname>Zhu</surname><given-names>XX</given-names></name></person-group><article-title>Learning spectral-spatial-temporal features via a recurrent convolutional neural network for change detection in multispectral imagery</article-title><source>IEEE Trans. Geosci. Remote Sens.</source><year>2018</year><volume>57</volume><fpage>924</fpage><lpage>935</lpage><pub-id pub-id-type="doi">10.1109/TGRS.2018.2863224</pub-id></element-citation></ref><ref id="CR15"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Song</surname><given-names>A</given-names></name><name><surname>Choi</surname><given-names>J</given-names></name><name><surname>Han</surname><given-names>Y</given-names></name><name><surname>Kim</surname><given-names>Y</given-names></name></person-group><article-title>Change detection in hyperspectral images using recurrent 3D fully convolutional networks</article-title><source>Remote Sens.</source><year>1827</year><volume>2018</volume><fpage>10</fpage></element-citation></ref><ref id="CR16"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>H</given-names></name><name><surname>Wu</surname><given-names>C</given-names></name><name><surname>Du</surname><given-names>B</given-names></name><name><surname>Zhang</surname><given-names>L</given-names></name><name><surname>Wang</surname><given-names>L</given-names></name></person-group><article-title>Change detection in multisource VHR images via deep siamese convolutional multiple-layers recurrent neural network</article-title><source>IEEE Trans. Geosci. Remote Sens.</source><year>2019</year><volume>58</volume><fpage>2848</fpage><lpage>2864</lpage><pub-id pub-id-type="doi">10.1109/TGRS.2019.2956756</pub-id></element-citation></ref><ref id="CR17"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhan</surname><given-names>Y</given-names></name><name><surname>Fu</surname><given-names>K</given-names></name><name><surname>Yan</surname><given-names>M</given-names></name><name><surname>Sun</surname><given-names>X</given-names></name><name><surname>Wang</surname><given-names>H</given-names></name><name><surname>Qiu</surname><given-names>X</given-names></name></person-group><article-title>Change detection based on deep siamese convolutional network for optical aerial images</article-title><source>IEEE Geosci. Remote Sens. Lett.</source><year>2017</year><volume>14</volume><fpage>1845</fpage><lpage>1849</lpage><pub-id pub-id-type="doi">10.1109/LGRS.2017.2738149</pub-id></element-citation></ref><ref id="CR18"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lebedev</surname><given-names>MA</given-names></name><name><surname>Vizilter</surname><given-names>YV</given-names></name><name><surname>Vygolov</surname><given-names>OV</given-names></name><name><surname>Knyaz</surname><given-names>VA</given-names></name><name><surname>Rubis</surname><given-names>AY</given-names></name></person-group><article-title>Change detection in remote sensing images using conditional adversarial networks. Int. Arch. Photogrammetry</article-title><source>Int. Arch. Photogr. Remote Sens. Spatial Inf. Sci.</source><year>2018</year><volume>422</volume><fpage>565</fpage><lpage>571</lpage><pub-id pub-id-type="doi">10.5194/isprs-archives-XLII-2-565-2018</pub-id></element-citation></ref><ref id="CR19"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>H</given-names></name><name><surname>Shi</surname><given-names>Z</given-names></name></person-group><article-title>A spatial-temporal attention-based method and a new dataset for remote sensing image change detection</article-title><source>Remote Sens.</source><year>2020</year><volume>12</volume><fpage>10</fpage></element-citation></ref></ref-list></back></article>