#  Scholarly publishing and the future of the Library

Why do we still have scholarly publishing? It is not technically necessary to have commercial publishers. Take the examples of theses or research grant proposals, which undergo intensive peer-review and where the University/Funder is fully capable of reviewing and publishing. There are many price-free "publishers" where neither the author or reader pays (opaquely labelled "diamond"), and where the material is available to all for "ever". We [AG and PMR] have published with J.O.S.S which has intensive peer review and also (unlike almost all other publishers) validates the material. One invitee [ABG] runs a world-class "diamond" publisher for the Global South and another [BB] has shown that the actual costs of publication are affordable (<= 500 USD) within the current scholarly/grant system. 

Yet the world of scholpub is increasingly moving towards a model dominated by megacapitalist surveillance publishers and no funders or institutions knows how to change this. The sole purpose of this is now to reward CEOs and shareholders with massive profit margins. The scale of this is reviewed by Buranyi in the (UK) Guardian newspaper [].
He interviewed extensively and showed that the driving force for authors was career advancement and ranking for universities. The values are broken. Even society publishers are dragged into this mess.

This is not primarily about cost; the world can and should sponsor global publishing for everyone. It's about control and values, specifically both neocolonialist Global North and megacapitalism combined. This is coupled with an effective oligarchy with no price control and no subtitutability (non-fungible). The major researchers (universities) have ceded power to the publishing industry in return for "glory". The only 
value that most publishers add is their badge, which allows algorithms to decide the computable worth of people and their instutions. Read Buranyi again.

I have called this the "Publisher-Academic Complex" in similarity to the "Military-Industrial-Aacademic Complex" identified by Eisenhower[]. Value- and resource-flows are shown in this diagram []

<img src="pmr_alis/pac1.png" alt="Publisher-academic-complex CC0" width="200"/>
<!--![publisher-academic complex](pmr_alis/pac1.png)-->

Our world needs open, peer-reviewed, trustable scholarship for everything and especially for urgent global matters such as climate

## societies, megapublishers and the idol of cash

Maxwell's colonialisation of the publishing community led to an ever-increasing centralisation, run by industrial managers often from widely different backgrounds. The system became driven by what would generate income rather than what was needed either by the authors or the world. We now see editors fired becuase they didn't dramatically increase the numer of articles and hence, in an author-pays model, income. Nobel prizewinners Sanger, Crick and Watson, Higgs would probably have been kicked out for "not publishing enough". In protest editorial boards regular resign.

Many learned societies have generated large incomes through scholarly publishing and for some this is their major source of revenue. This distorts the role of publishing (we have reviewed this[]) and they are unlikely to lead the calls for democratisation. They effectively become neocolonialists.b


## scholarly content, versioning, and the value of aggregation 
The traditional, and still prevalent, model of scholarly publications is that each is a distinct jewel of knowledge, perfected by its authors and polsihed by the reviewling process and the journal editors. It's compounded by the outdated concept of "authors' final manuscript" and "version of record (VoR)". This was perhaps useful when the only version of the article was in the printed journal but it has no place in today's dynamic knowledge. 

Although there are isolated cases where this applies, the reality in most disciplines is that there is a river of content, being re-fed every hour from many sources. When one drop is finished, another must be immediately added to keep the grants flowing. Many/Most papers are never read and of little value except to the authors' careers. 

Even when published, most of the value in the decribed work is omitted. The C20th limitation of "10 pages of double-column PDF with references in Harvard style" should have died with electronic publishing. Why is it still maintained? Because there is no market value in innovation - why change something that has almost zero cost to the publisher? We and others have tried several times to innovate, but the publishers, not the academic editors, controls the system as we have seen that as journals get aggregtaed into megapublishers, upwards innovation and control is impossible, maerked 

### contrast publications with software

In software development there are arbitrarily many versions, and each has a unique unchanging identifier. We may create and save a new version every day or even more frequently. we also allow for different versions from different authors ("branches" - or larger "forks"). All these are carefully maintained by automated software - who added what , when, and for what purpose. Different versions can be merged, often automatically. `git` and other systems maintain this  and can retrieve any version. Wikimedia has adopted this philosophy and its creations have a complete history, and can be reverted if required. An additional benefit is that automatic tools ("bots") can maintain consistent styles, and create indexes.

There is no technical reason why publications should 

## surveillance publishing and AI

The "success" of "AI" is largely based on the huge amount of (mainly public) data that has been scraped by the megacorporations. It's so effective that comanies can use generativeAI to mimic the output of talented humans. This is not intelligence, it's the combination of huge chunks of dat, remixed from the scrapings. 

However it's highly valuable to the scrapers who now have a vastly larger knowledge store than single humans or groups of them. They "know" all Wikimedia content, and probably much of the scientific literature , both Open and Closed. They also know our transactions within their systems such as who is working on what, in what community and for what purpose. This is highly monetizable. The hundred million scholarly publications contain a wealth of unrealised value. Up till now this has been undiscoverable, unsemantic (PDFs and GIFs), unlabelled (no schemas), anglophone, unvalidated and messy in many other ways. But megapublishers now know its value and are starting to scrape it (text, images, and specific domains) into Large Language Models (LLMs).

To fuel the LLMs, companies monitor not only the traffic but the humans in the system (https://en.wikipedia.org/wiki/Surveillance_capitalism). This data is hightly valuable for personalising advertisinng and controlling the what individuals and see and do. We know of an individual whose every retrieved scholpub has been logged by a megapublisher and assume this is universal.  



## the end of the Librarian...?

With the rise of LLMs, Librarians become redundant. Machines have aggregated far more factual knowledge than any human, or groups of humans, can hold. Tools such as Google Chrome can locate paragraphs on any subject and offer iterative browsing within seconds. Google, not the library, saves the time of the reader. And can do it in any human language and for any level of reader (school, university, specialist, general citizen).

Do librarians help to create knowledge? An increasing number of information products are created by megapublishers, and librarians act as their publicly subsidised sales and support team. With increasing automation and control publishers will interact directly with readers - students and researchers. Megapublishers will lead this as they do megadeals with universities and provide tuition - and research- customisation packages. Customise the environment to save the time of reader by packaging commoditisable and bespoke knowedge services.

How can we resist this? Traditional political discourse on Open Access has failed - the publishers have been gifted a seat at the table ever since the UK Finch Report. Europe (EC) has challenged the monopoly of content providers abut lobbying is huge and the universal use of copyright distorts the free flow and creation of knowledge.

In this article we show that, given political will, we can build a better system than the publishers. Better in that the readers, not the publishers control the discourse and the tools. Do we have the will to do it? Or is it easier to accept what megapublishers create?

## ...or?

There's now a clear clash of values and practice which is essentially a fight for the soul of human knowledge. We see this in the efforts over 30 years to create new public collections of electronic knowledge. These include LibGen [] (inspired by the Greek Library of Alexandria), and Scihub. These take electronic scientific articles including those behind paywalls and make them available to the world without charge or barrier. They are authored by scholars who transfer the copyright to megapublishers in return for glory. The publishers control the reuse of these articles and can generate huge licence fees through CCC/Rightslink [] (sometimes over 1 million USD). The publishers have no moral rights and have added minimal technical value, but they can legally claim copyright in most jurisdictions. There is an ongoing courtcase in India to determine the legality thare []. 

Is the future of the C21st Library to be a force for liberation? 

Your choice.
